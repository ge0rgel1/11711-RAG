Alexander Hauptmann
Paper count: 543
- Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann. 2023. Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data. Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.
- Haoyang Wen, Zhenxin Xiao, E. Hovy, Alexander Hauptmann. 2023. Towards Open-Domain Twitter User Profile Inference. Abstract: ,
- Yijun Qian, Guoliang Kang, Lijun Yu, Wenhe Liu, Alexander Hauptmann. 2022. TRM:Temporal Relocation Module for Video Recognition. Abstract: One of the key differences between video and image understanding lies in how to model the temporal information. Due to the limit of convolution kernel size, most previous methods try to model long-term temporal information via sequentially stacked convolution layers. Such conventional manner doesn’t explicitly differentiate regions/pixels with various temporal receptive requirements and may suffer from temporal information distortion. In this paper, we propose a novel Temporal Relocation Module (TRM), which can capture the long-term temporal dependence in a spatial-aware manner adaptively. Specifically, it relocates the spatial features along the temporal dimension, through which an adaptive temporal receptive field is aligned to within the global temporal interval of input video, TRM can potentially model the long-term temporal information with an equivalent receptive field of the entire video. Experiment results on three representative video recognition benchmarks demonstrate TRM outperforms previous state-of-the-arts noticeably and verifies the effectiveness of our method.
- Liangke Gui, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, Jianfeng Gao. 2022. Training Vision-Language Transformers from Captions Alone. Abstract: Vision-Language Transformers can be learned without low-level human labels (e.g. class labels, bounding boxes, etc). Existing work, whether explicitly utilizing bounding boxes or patches, assumes that the visual backbone must first be trained on ImageNet class prediction before being integrated into a multimodal linguistic pipeline. We show that this is not necessary and introduce a new model Vision-Language from Captions (VLC) built on top of Masked Auto-Encoders that does not require this supervision. In fact, in a head-to-head comparison between ViLT, the current state-of-the-art patch-based vision-language transformer which is pretrained with supervised object classification, and our model, VLC, we find that our approach 1. outperforms ViLT on standard benchmarks, 2. provides more interpretable and intuitive patch visualizations, and 3. is competitive with many larger models that utilize ROIs trained on annotated bounding-boxes.
- João Magalhães, Alexander Hauptmann, R. Sousa, Carlos Santiago. 2021. MuCAI'21: 2nd ACM Multimedia Workshop on Multimodal Conversational AI. Abstract: The second edition of the International Workshop on Multimodal Conversational AI puts forward a diverse set of contributions that aim to brainstorm this new field. Conversational agents are now becoming a commodity as this technology is being applied to a wide range of domains. Healthcare, assisting technologies, e-commerce, information seeking, are some of the domains where multimodal conversational AI is being explored. The wide use of multimodal conversational agents exposes the many challenges in achieving more natural, human-like, and engaging conversational agents. The research contributions of the Workshop actively address several of relevant challenges: How to include assistive-technologies in dialog systems? How can agents engage in negotiation in dialogs? How to handle the embodiment of conversational agents? Keynote speakers, both with real-world experience in conversational AI, will share their most recent and exciting work. The panel will address technological, ethical, legal and social aspects of conversational search. Finally, invited contributions from research projects will showcase how the different domains can benefit from conversational technology.
- Salvador Medina Maza, Sarah L. Taylor, M. Tiede, Alexander Hauptmann, Iain A. Matthews. 2021. Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis. Abstract: Our study examines the information obtained by adding two parasagittal sensors to the standard midsagittal configuration of an Electromagnetic Articulography (EMA) observation of lingual articulation. In this work, we present a large and phonetically balanced corpus obtained from an EMA recording session of a single English native speaker reading 1899 sentences from the Harvard and TIMIT corpora. According to a statistical analysis of the diphones produced during the recording session, the motion captured by the parasagittal sensors has a low correlation to the midsagittal sensors in the mediolateral direction. We perform a geometric analysis of the lateral tongue by the measure of its width and using a proxy of the tongue’s curvature that is computed using the Menger curvature. To provide a better understanding of the tongue sensor motion we present dynamic visualizations of all diphones. Finally, we present a summary of the velocity information computed from the tongue sensor information.
- Liangke Gui, Adrien Bardes, R. Salakhutdinov, Alexander Hauptmann, M. Hebert, Yu-Xiong Wang. 2021. Learning to Hallucinate Examples from Extrinsic and Intrinsic Supervision. Abstract: Learning to hallucinate additional examples has recently been shown as a promising direction to address few-shot learning tasks. This work investigates two important yet overlooked natural supervision signals for guiding the hallucination process – (i) extrinsic: classifiers trained on hallucinated examples should be close to strong classifiers that would be learned from a large amount of real examples; and (ii) intrinsic: clusters of hallucinated and real examples belonging to the same class should be pulled together, while simultaneously pushing apart clusters of hallucinated and real examples from different classes. We achieve (i) by introducing an additional mentor model on data-abundant base classes for directing the hallucinator, and achieve (ii) by performing contrastive learning between hallucinated and real examples. As a general, model-agnostic framework, our dual mentor-and self-directed (DMAS) hallucinator significantly improves few-shot learning performance on widely-used benchmarks in various scenarios.
- Xiaojun Chang, Pengzhen Ren, Pengfei Xu, Zhihui Li, Xiaojiang Chen, Alexander Hauptmann. 2021. Scene Graphs: A Review of Generations and Applications. Abstract: —The scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisﬁed with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also know the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and ﬁnd similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More speciﬁcally, we ﬁrst summarized the general deﬁnition of the scene graph, then conducted a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigated the main applications of scene graphs and summarized the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs. We believe this will be a very helpful foundation for future research on scene graphs.
- Ting-yao Hu, Alexander Hauptmann. 2021. Statistical Distance Metric Learning for Image Set Retrieval. Abstract: Measuring similarity between two image sets is instrumental in many computer vision tasks, such as video face recognition, multi-shot person re-identification and gait recognition. In most of the recent works, it is done by aggregating the embedding features of images as a fixed size vector, and calculating a metric in vector space (i.e. Euclidean distance). The embedding feature function can be learned by deep metric learning (DML) technique. However, methods relying on feature aggregation fail to capture the diversity and uncertainty within image sets. In this paper, we obviate the need of feature aggregation and propose a novel Statistical Distance Metric Learning (SDML) framework, which represents each image set as a probability distribution in embedding feature space and compares two image sets by statistical distance between their distributions. Among all types of statistical distance, we choose Jeffrey’s divergence (JD), which can be obtained from two embedding feature sets by kNN based density estimator. We also design a statistical centroid loss function to enhance the discriminative power of training process. Our SDML framework naturally preserves the diversity within an image set, and the relation between two sets. We evaluate our proposed approach on gait recognition and multi-shot person re-id. The experiment results show that SDML outperforms conventional DML, and also receives competitive/superior performance comparing to the previous state-of-the-arts on the aforementioned tasks.
- Bei Liu, Jianlong Fu, Shizhe Chen, Qin Jin, Alexander Hauptmann, Yong Rui. 2021. MMPT'21: International Joint Workshop on Multi-Modal Pre-Training for Multimedia Understanding. Abstract: Pre-training has been an emerging topic that provides a way to learn strong representation in many fields (e.g., natural language processing, computing vision). In the last few years, we have witnessed many research works on multi-modal pre-training which have achieved state-of-the-art performances on many multimedia tasks (e.g., image-text retrieval, video localization, speech recognition). In this workshop, we aim to gather peer researchers on related topics for more insightful discussion. We also intend to attract more researchers to explore and investigate more opportunities of designing and using innovative pre-training models for multimedia tasks.
- R. Sousa, Pedro Ferreira, Pedro Costa, Pedro Azevedo, J. Costeira, Carlos Santiago, João Magalhães, David Semedo, Rafael Ferreira, Alexander I. Rudnicky, Alexander Hauptmann. 2021. iFetch: Multimodal Conversational Agents for the Online Fashion Marketplace. Abstract: Most of the interaction between large organizations and their users will be mediated by AI agents in the near future. This perception is becoming undisputed as online shopping dominates entire market segments, and the new "digitally-native" generations become consumers. iFetch is a new generation of task-oriented conversational agents that interact with users seamlessly using verbal and visual information. Through the conversation, iFetch provides targeted advice and a "physical store-like" experience while maintaining user engagement. This context entails the following vital components: 1) highly complex memory models that keep track of the conversation, 2) extraction of key semantic features from language and images that reveal user intent, 3) generation of multimodal responses that will keep users engaged in the conversation and 4) an interrelated knowledge base of products from which to extract relevant product lists.
- Zhong Zhou, I. C. Etinger, Florian Metze, Alexander Hauptmann, A. Waibel. 2020. Gun Source and Muzzle Head Detection. Abstract: 
 There is a surging need across the world for protection against gun violence. There are three main areas that we have identified as challenging in research that tries to curb gun violence: temporal location of gunshots, gun type prediction and gun source (shooter) detection. Our
 task is gun source detection and muzzle head detection, where the muzzle head is the round opening of the firing end of the gun. We would like to locate the muzzle head of the gun in the video visually, and identify who has fired the shot. In our formulation, we turn the problem of muzzle
 head detection into two sub-problems of human object detection and gun smoke detection. Our assumption is that the muzzle head typically lies between the gun smoke caused by the shot and the shooter. We have interesting results both in bounding the shooter as well as detecting the gun smoke.
 In our experiments, we are successful in detecting the muzzle head by detecting the gun smoke and the shooter.

- Lijun Yu, Yijun Qian, Wenhe Liu, Alexander Hauptmann. 2020. CMU Informedia at TRECVID 2020: Activity Detection with Dense Spatio-temporal Proposals. Abstract: We propose an action recognition system for surveillance scenarios, which wins TRECVID 2020 [1] Activities in Extended Video (ActEV 2 ) Challenge with a large advantage of 23.8% ahead the runner up system. Our system develops a dense spatial-temporal proposal generation model which collaborates with the state-of-the-art action classiﬁers. The proposed system utilizes multiple state-of-the-art modules and is trained on VIRAT Dataset with only released annotations. In this paper, we demonstrate the architecture and algorithms with technique details of the winner system.
- Junwei Liang, Lu Jiang, Alexander Hauptmann. 2020. SimAug: Learning Robust Representations from 3D Simulation for Pedestrian Trajectory Prediction in Unseen Cameras. Abstract: This paper focuses on the problem of predicting future trajectories of people in unseen scenarios and camera views. We propose a method to efficiently utilize multi-view 3D simulation data for training. Our approach finds the hardest camera view to mix up with adversarial data from the original camera view in training, thus enabling the model to learn robust representations that can generalize to unseen camera views. We refer to our method as SimAug. We show that SimAug achieves best results on three out-of-domain real-world benchmarks, as well as getting state-of-the-art in the Stanford Drone and the VIRAT/ActEV dataset with in-domain training data. We will release our models and code.
- Po-Yao (Bernie) Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann. 2020. Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting. Abstract: Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.
- Xinru Yang, Haozhi Qi, Mingyang Li, Alexander Hauptmann. 2020. From A Glance to "Gotcha": Interactive Facial Image Retrieval with Progressive Relevance Feedback. Abstract: Facial image retrieval plays a significant role in forensic investigations where an untrained witness tries to identify a suspect from a massive pool of images. However, due to the difficulties in describing human facial appearances verbally and directly, people naturally tend to depict by referring to well-known existing images and comparing specific areas of faces with them and it is also challenging to provide complete comparison at each time. Therefore, we propose an end-to-end framework to retrieve facial images with relevance feedback progressively provided by the witness, enabling an exploitation of history information during multiple rounds and an interactive and iterative approach to retrieving the mental image. With no need of any extra annotations, our model can be applied at the cost of a little response effort. We experiment on \texttt{CelebA} and evaluate the performance by ranking percentile and achieve 99\% under the best setting. Since this topic remains little explored to the best of our knowledge, we hope our work can serve as a stepping stone for further research.
- Evangelia Spiliopoulou, Salvador Medina Maza, E. Hovy, Alexander Hauptmann. 2020. Event-Related Bias Removal for Real-time Disaster Events. Abstract: Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. This poses a complex problem due to the large amount of posts that do not contain any actionable information. Furthermore, the classification of information in real-time systems requires training on out-of-domain data, as we do not have any data from a new emerging crisis. Prior work focuses on models pre-trained on similar event types. However, those models capture unnecessary event-specific biases, like the location of the event, which affect the generalizability and performance of the classifiers on new unseen data from an emerging new event. In our work, we train an adversarial neural model to remove latent event-specific biases and improve the performance on tweet importance classification.
- Po-Yao (Bernie) Huang, Xiaojun Chang, Alexander Hauptmann, E. Hovy. 2020. Forward and Backward Multimodal NMT for Improved Monolingual and Multilingual Cross-Modal Retrieval. Abstract: We explore methods to enrich the diversity of captions associated with pictures for learning improved visual-semantic embeddings (VSE) in cross-modal retrieval. In the spirit of "A picture is worth a thousand words", it would take dozens of sentences to parallel each picture's content adequately. But in fact, real-world multimodal datasets tend to provide only a few (typically, five) descriptions per image. For cross-modal retrieval, the resulting lack of diversity and coverage prevents systems from capturing the fine-grained inter-modal dependencies and intra-modal diversities in the shared VSE space. Using the fact that the encoder-decoder architectures in neural machine translation (NMT) have the capacity to enrich both monolingual and multilingual textual diversity, we propose a novel framework leveraging multimodal neural machine translation (MMT) to perform forward and backward translations based on salient visual objects to generate additional text-image pairs which enables training improved monolingual cross-modal retrieval (English-Image) and multilingual cross-modal retrieval (English-Image and German-Image) models. Experimental results show that the proposed framework can substantially and consistently improve the performance of state-of-the-art models on multiple datasets. The results also suggest that the models with multilingual VSE outperform the models with monolingual VSE.
- Xiaoyu Zhu, Junwei Liang, Alexander Hauptmann. 2020. MSNet: A Multilevel Instance Segmentation Network for Natural Disaster Damage Assessment in Aerial Videos. Abstract: In this paper, we study the problem of efficiently assessing building damage after natural disasters like hurricanes, floods or fires, through aerial video analysis. We make two main contributions. The first contribution is a new dataset, consisting of user-generated aerial videos from social media with annotations of instance-level building damage masks. This provides the first benchmark for quantitative evaluation of models to assess building damage using aerial videos. The second contribution is a new model, namely MSNet, which contains novel region proposal network designs and an unsupervised score refinement network for confidence score calibration in both bounding box and mask branches. We show that our model achieves state-of-the-art results compared to previous methods in our dataset.1
- Mandela Patrick, Po-Yao (Bernie) Huang, Yuki M. Asano, Florian Metze, Alexander Hauptmann, João F. Henriques, A. Vedaldi. 2020. Support-set bottlenecks for video-text representation learning. Abstract: The dominant paradigm for learning video-text representations -- noise contrastive learning -- increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related -- for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample's caption must be reconstructed as a weighted combination of other support samples' visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX and ActivityNet, for video-to-text and text-to-video retrieval.
- Lingling Zhang, Xiaojun Chang, Jun Liu, Minnan Luo, Sen Wang, ZongYuan Ge, Alexander Hauptmann. 2020. ZSTAD: Zero-Shot Temporal Activity Detection. Abstract: An integral part of video analysis and surveillance is temporal activity detection, which means to simultaneously recognize and localize activities in long untrimmed videos. Currently, the most effective methods of temporal activity detection are based on deep learning, and they typically perform very well with large scale annotated videos for training. However, these methods are limited in real applications due to the unavailable videos about certain activity classes and the time-consuming data annotation. To solve this challenging problem, we propose a novel task setting called zero-shot temporal activity detection (ZSTAD), where activities that have never been seen in training can still be detected. We design an end-to-end deep network based on R-C3D as the architecture for this solution. The proposed network is optimized with an innovative loss function that considers the embeddings of activity labels and their super-classes while learning the common semantics of seen and unseen activities. Experiments on both the THUMOS’14 and the Charades datasets show promising performance in terms of detecting unseen activities.
- Ruonan Liu, Boyuan Yang, Alexander Hauptmann. 2020. Simultaneous Bearing Fault Recognition and Remaining Useful Life Prediction Using Joint-Loss Convolutional Neural Network. Abstract: Fault diagnosis and remaining useful life (RUL) prediction are always two major issues in modern industrial systems, which are usually regarded as two separated tasks to make the problem easier but ignore the fact that there are certain information of these two tasks that can be shared to improve the performance. Therefore, to capture common features between different relative problems, a joint-loss convolutional neural network (JL-CNN) architecture is proposed in this paper, which can implement bearing fault recognition and RUL prediction in parallel by sharing the parameters and partial networks, meanwhile keeping the output layers of different tasks. The JL-CNN is constructed based on a CNN, which is a widely used deep learning method because of its powerful feature extraction ability. During optimization phase, a JL function is designed to enable the proposed approach to learn the diagnosis–prognosis features and improve generalization while reducing the overfitting risk and computation cost. Moreover, because the information behind the signals of different problems has been shared and exploited deeper, the generalization and the accuracy of results can also be improved. Finally, the effectiveness of the JL-CNN method is validated by run-to-failure dataset. Compared with support vector regression and traditional CNN, the mean-square-error of the proposed method decreases 82.7$\%$ and 24.9$\%$, respectively. Therefore, results and comparisons show that the proposed method can be applied for the intercrossed applications between fault diagnosis and RUL prediction.
- Wenhe Liu, Dong Gong, Mingkui Tan, Javen Qinfeng Shi, Yi Yang, Alexander Hauptmann. 2020. Learning Distilled Graph for Large-Scale Social Network Data Clustering. Abstract: Spectral analysis is critical in social network analysis. As a vital step of the spectral analysis, the graph construction in many existing works utilizes content data only. Unfortunately, the content data often consists of noisy, sparse, and redundant features, which makes the resulting graph unstable and unreliable. In practice, besides the content data, social network data also contain link information, which provides additional information for graph construction. Some of previous works utilize the link data. However, the link data is often incomplete, which makes the resulting graph incomplete. To address these issues, we propose a novel Distilled Graph Clustering (DGC) method. It pursuits a distilled graph based on both the content data and the link data. The proposed algorithm alternates between two steps: in the feature selection step, it finds the most representative feature subset w.r.t. an intermediate graph initialized with link data; in graph distillation step, the proposed method updates and refines the graph based on only the selected features. The final resulting graph, which is referred to as the distilled graph, is then utilized for spectral clustering on the large-scale social network data. Extensive experiments demonstrate the superiority of the proposed method.
- Junwei Liang, Liangliang Cao, Xuehan Xiong, Ting Yu, Alexander Hauptmann. 2020. Spatial-Temporal Alignment Network for Action Recognition and Detection. Abstract: This paper studies how to introduce viewpoint-invariant feature representations that can help action recognition and detection. Although we have witnessed great progress of action recognition in the past decade, it remains challenging yet interesting how to efficiently model the geometric variations in large scale datasets. This paper proposes a novel Spatial-Temporal Alignment Network (STAN) that aims to learn geometric invariant representations for action recognition and action detection. The STAN model is very light-weighted and generic, which could be plugged into existing action recognition models like ResNet3D and the SlowFast with a very low extra computational cost. We test our STAN model extensively on AVA, Kinetics-400, AVA-Kinetics, Charades, and Charades-Ego datasets. The experimental results show that the STAN model can consistently improve the state of the arts in both action detection and action recognition tasks. We will release our data, models and code.
- Junpei Zhou, Xinyu Wang, Po-yao Huang, Alexander Hauptmann. 2019. CMU-Informedia at TREC 2019 Incident Streams Track. Abstract: We describe CMU-Informedia’s models for the TREC 2019 Incident Streams track. The goal of this track is classifying event/incident related tweets by High-level Information Types such as ‘SearchAndRescue’, ‘InformationWanted’ and so on. Each tweet should be assigned as many categories as are appropriate. What’s more, this track requires predicting the Importance Scores, which is converted from the Importance Labels including ‘Critical’, ‘High’, ‘Medium’, ‘Low’ and ‘Irrelevant’. For predicting the information types, we use feature extractors to extract features including meta-information, user entity, and textual embeddings, and then we build an information type predictor on those features. For predicting the importance scores, we build an importance score predictor which combines the scores derived from the predicted information types and the scores produced by a regression model. Evaluation results show that our models perform well on all metrics, and different models perform particularly well on different aspects.
- E. Hovy, J. Carbonell, Hans Chalupsky, A. Gershman, Alexander Hauptmann, Florian Metze, T. Mitamura, Zaid A. W. Sheikh, Ankit Dangi, Aditi Chaudhary, Xianyang Chen, Xiang Kong, Bernie Huang, Salvador Medina, H. Liu, Xuezhe Ma, Maria Ryskina, Ramon Sanabria, Varun Gangal. 2019. OPERA: Operations-oriented Probabilistic Extraction, Reasoning, and Analysis. Abstract: The OPERA system of CMU and USC/ISI performs end-to-end information extraction from multiple media and languages (English, Russian, Ukrainian), integrates the results, builds Knowledge Bases about the domain, and does hypothesis creation and reasoning to answer questions. 
- Jia Chen, Jiang Liu, Junwei Liang, Ting-yao Hu, Wei Ke, Wayner Barrios, Dong Huang, Alexander Hauptmann. 2019. Minding the Gaps in a Video Action Analysis Pipeline. Abstract: We present an event detection system, which shares many similarities with standard object detection pipelines. It is composed of four modules: feature extraction, event proposal generation, event classification and event localization. We developed and assessed each module separately by evaluating several candidate options given oracle input using intermediate evaluation metric. This particular process results in a mismatch gap between training and testing when we integrate the module into the complete system pipeline. This results from the fact that each module is trained on clean oracle input, but during testing the module can only receive system generated input, which can be significantly different from the oracle data. Furthermore, we discovered that all the gaps between the different modules can contribute to a decrease in accuracy and they represent the major bottleneck for a system developed in this way. Fortunately, we were able to develop a set of relatively simple fixes in our final system to address and mitigate some of the gaps.
- Shih-Fu Chang, Alexander Hauptmann, Louis-Philippe Morency, S. Oviatt, H. Sundaram. 2019. Key Challenges for Multimedia Research in the Next Ten Years. Abstract: This short paper is based on summary of the full report [1] of the NSF Workshop on Multimedia Challenges, Opportunities and Research Roadmaps held on March 30-31, 2017 in Washington DC. This material is based upon work supported by the National Science Foundation, award IIS-1735591. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
- Xiaojun Chang, Wenhe Liu, Po-Yao (Bernie) Huang, Changlin Li, Fengda Zhu, Mingfei Han, Mingjie Li, Mengyuan Ma, Siyi Hu, Guoliang Kang, Junwei Liang, Liangke Gui, Lijun Yu, Yijun Qian, Jing Wen, Alexander Hauptmann. 2019. MMVG-INF-Etrol@TRECVID 2019: Activities in Extended Video. Abstract: We propose a video analysis system detecting activities in surveillance scenarios which wins Trecvid Activities in Extended Video (ActEV 1 ) challenge 2019. For detecting and localizing surveillance events in videos, Argus employs a spatial-temporal activity proposal generation module facilitating object detection and tracking, followed by a sequential classiﬁcation module to spatially and temporally localize persons and objects involved in the activity. We detail the design challenges and provide our insights and solutions in developing the state-of-the-art surveillance video analysis system
- Shih-Fu Chang, Louis-Philippe Morency, Alexander Hauptmann, A. Bimbo, C. Gurrin, H. Hung, Heng Ji, A. Smeaton. 2019. PANEL: Challenges for Multimedia/Multimodal Research in the Next Decade. Abstract: The multimedia and multi-modal community is witnessing an explosive transformation in the recent years with major societal impact. With the unprecedented deployment of multimedia devices and systems, multimedia research is critical to our abilities and prospects in advancing state-of-the-art technologies and solving real-world challenges facing the society and the nation. To respond to these challenges and further advance the frontiers of the field of multimedia, this panel will discuss the challenges and visions that may guide future research in the next ten years.
- Shih-Fu Chang, Alexander Hauptmann, Louis-Philippe Morency, Sameer Kiran Antani, D. Bulterman, C. Busso, J. Chai, Julia Hirschberg, R. Jain, Ketan Mayer-Patel, R. Meth, R. Mooney, K. Nahrstedt, Shrikanth S. Narayanan, P. Natarajan, S. Oviatt, B. Prabhakaran, A. Smeulders, H. Sundaram, Zhengyou Zhang, Michelle X. Zhou. 2019. Report of 2017 NSF Workshop on Multimedia Challenges, Opportunities and Research Roadmaps. Abstract: With the transformative technologies and the rapidly changing global R&D landscape, the multimedia and multimodal community is now faced with many new opportunities and uncertainties. With the open source dissemination platform and pervasive computing resources, new research results are being discovered at an unprecedented pace. In addition, the rapid exchange and influence of ideas across traditional discipline boundaries have made the emphasis on multimedia multimodal research even more important than before. To seize these opportunities and respond to the challenges, we have organized a workshop to specifically address and brainstorm the challenges, opportunities, and research roadmaps for MM research. The two-day workshop, held on March 30 and 31, 2017 in Washington DC, was sponsored by the Information and Intelligent Systems Division of the National Science Foundation of the United States. Twenty-three (23) invited participants were asked to review and identify research areas in the MM field that are most important over the next 10-15 year timeframe. Important topics were selected through discussion and consensus, and then discussed in depth in breakout groups. Breakout groups reported initial discussion results to the whole group, who continued with further extensive deliberation. For each identified topic, a summary was produced after the workshop to describe the main findings, including the state of the art, challenges, and research roadmaps planned for the next 5, 10, and 15 years in the identified area.
- Junwei Liang, Lu Jiang, K. Murphy, Ting Yu, Alexander Hauptmann. 2019. The Garden of Forking Paths: Towards Multi-Future Trajectory Prediction. Abstract: This paper studies the problem of predicting the distribution over multiple possible future paths of people as they move through various visual scenes. We make two main contributions. The first contribution is a new dataset, created in a realistic 3D simulator, which is based on real world trajectory data, and then extrapolated by human annotators to achieve different latent goals. This provides the first benchmark for quantitative evaluation of the models to predict multi-future trajectories. The second contribution is a new model to generate multiple plausible future trajectories, which contains novel designs of using multi-scale location encodings and convolutional RNNs over graphs. We refer to our model as Multiverse. We show that our model achieves the best results on our dataset, as well as on the real-world VIRAT/ActEV dataset (which just contains one possible future).
- Guoliang Kang, Lu Jiang, Yi Yang, Alexander Hauptmann. 2019. Contrastive Adaptation Network for Unsupervised Domain Adaptation. Abstract: Unsupervised Domain Adaptation (UDA) makes predictions for the target domain data while manual annotations are only available in the source domain. Previous methods minimize the domain discrepancy neglecting the class information, which may lead to misalignment and poor generalization performance. To address this issue, this paper proposes Contrastive Adaptation Network (CAN) optimizing a new metric which explicitly models the intra-class domain discrepancy and the inter-class domain discrepancy. We design an alternating update strategy for training CAN in an end-to-end manner. Experiments on two real-world benchmarks Office-31 and VisDA-2017 demonstrate that CAN performs favorably against the state-of-the-art methods and produces more discriminative features.
- Junwei Liang, J. D. Aronson, Alexander Hauptmann. 2019. Shooter Localization Using Social Media Videos. Abstract: Nowadays a huge number of user-generated videos are uploaded to social media every second, capturing glimpses of events all over the world. These videos provide important and useful information for reconstructing events like the Las Vegas Shooting in 2017. In this paper, we describe a system that can localize the shooter location only based on a couple of user-generated videos that capture the gunshot sound. Our system first utilizes established video analysis techniques like video synchronization and gunshot temporal localization to organize the unstructured social media videos for users to understand the event effectively. By combining multimodal information from visual, audio and geo-locations, our system can then visualize all possible locations of the shooter in the map. Our system provides a web interface for human-in-the-loop verification to ensure accurate estimations. We present the results of estimating the shooter's location of the Las Vegas Shooting in 2017 and show that our system is able to get accurate location using only the first few gunshots. The full technical report, all relevant source code including the web interface and machine learning models are available.
- Zhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, Jun-Yan He, Alexander Hauptmann. 2019. Improving the Learning of Multi-column Convolutional Neural Network for Crowd Counting. Abstract: Tremendous variation in the scale of people/head size is a critical problem for crowd counting. To improve the scale invariance of feature representation, recent works extensively employ Convolutional Neural Networks with multi-column structures to handle different scales and resolutions. However, due to the substantial redundant parameters in columns, existing multi-column networks invariably exhibit almost the same scale features in different columns, which severely affects counting accuracy and leads to overfitting. In this paper, we attack this problem by proposing a novel Multicolumn Mutual Learning (McML) strategy. It has two main innovations: 1) A statistical network is incorporated into the multi-column framework to estimate the mutual information between columns, which can approximately indicate the scale correlation between features from different columns. By minimizing the mutual information, each column is guided to learn features with different image scales. 2) We devise a mutual learning scheme that can alternately optimize each column while keeping the other columns fixed on each mini-batch training data. With such asynchronous parameter update process, each column is inclined to learn different feature representation from others, which can efficiently reduce the parameter redundancy and improve generalization ability. More remarkably, McML can be applied to all existing multi-column networks and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that McML can significantly improve the original multi-column networks and outperform the other state-of-the-art approaches.
- Shizhe Chen, Yuqing Song, Yida Zhao, Qin Jin, Zhaoyang Zeng, Bei Liu, Jianlong Fu, Alexander Hauptmann. 2019. Activitynet 2019 Task 3: Exploring Contexts for Dense Captioning Events in Videos. Abstract: Contextual reasoning is essential to understand events in long untrimmed videos. In this work, we systematically explore different captioning models with various contexts for the dense-captioning events in video task, which aims to generate captions for different events in the untrimmed video. We propose five types of contexts as well as two categories of event captioning models, and evaluate their contributions for event captioning from both accuracy and diversity aspects. The proposed captioning models are plugged into our pipeline system for the dense video captioning challenge. The overall system achieves the state-of-the-art performance on the dense-captioning events in video task with 9.91 METEOR score on the challenge testing set.
- Po-Yao (Bernie) Huang, Xiaojun Chang, Alexander Hauptmann. 2019. Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations. Abstract: With the aim of promoting and understanding the multilingual version of image search, we leverage visual object detection and propose a model with diverse multi-head attention to learn grounded multilingual multimodal representations. Specifically, our model attends to different types of textual semantics in two languages and visual objects for fine-grained alignments between sentences and images. We introduce a new objective function which explicitly encourages attention diversity to learn an improved visual-semantic embedding space. We evaluate our model in the German-Image and English-Image matching tasks on the Multi30K dataset, and in the Semantic Textual Similarity task with the English descriptions of visual content. Results show that our model yields a significant performance gain over other methods in all of the three tasks.
- En Yu, Wenhe Liu, Guoliang Kang, Xiaojun Chang, Jiande Sun, Alexander Hauptmann. 2019. Inf@TRECVID 2019: Instance Search Task. Abstract: We participated in one of the two types of Instance Search task in TRECVID 2019: Fully Automatic Search, without any human intervention. Firstly, the specific person and action are searched separately, and then we re-rank the two sorts of search results by ranking the one type scores according to the other type, as well as the score fusion. And thus, three kinds of final instance search results are submitted. Specifically, for the person search, our baseline consists of face detection, alignment and face feature selection. And for the action search, we integrate person detection, person tracking and feature selection into a framework to get the final 3D features for all tracklets in video shots. The official evaluations showed that our best search result gets the 4th place in the Automatic search.
- Junwei Liang, J. D. Aronson, Alexander Hauptmann. 2019. Technical Report of the Video Event Reconstruction and Analysis (VERA) System -- Shooter Localization, Models, Interface, and Beyond. Abstract: Every minute, hundreds of hours of video are uploaded to social media sites and the Internet from around the world. This material creates a visual record of the experiences of a significant percentage of humanity and can help illuminate how we live in the present moment. When properly analyzed, this video can also help analysts to reconstruct events of interest, including war crimes, human rights violations, and terrorist acts. Machine learning and computer vision can play a crucial role in this process. In this technical report, we describe the Video Event Reconstruction and Analysis (VERA) system. This new tool brings together a variety of capabilities we have developed over the past few years (including video synchronization and geolocation to order unstructured videos lacking metadata over time and space, and sound recognition algorithms) to enable the reconstruction and analysis of events captured on video. Among other uses, VERA enables the localization of a shooter from just a few videos that include the sound of gunshots. To demonstrate the efficacy of this suite of tools, we present the results of estimating the shooter's location of the Las Vegas Shooting in 2017 and show that VERA accurately predicts the shooter's location using only the first few gunshots. We then point out future directions that can help improve the system and further reduce unnecessary human labor in the process. All of the components of VERA run through a web interface that enables human-in-the-loop verification to ensure accurate estimations. All relevant source code, including the web interface and machine learning models, is freely available on Github. We hope that researchers and software developers will be inspired to improve and expand this system moving forward to better meet the needs of human rights and public safety.
- Junwei Liang, J. D. Aronson, Alexander Hauptmann. 2019. Shooter Localization Using Videos in the Wild. Abstract: Nowadays a huge number of user-generated videos are uploaded to social media every second, capturing glimpses of events all over the world. These videos in the wild provide important and useful information for reconstructing events like the Las Vegas Shooting in 2017. In this paper, we describe a system that can localize the shooter location only based on a couple of user-generated videos that capture the gunshot sound. Our system first utilizes established video analysis techniques like video synchronization and automatic gunshot processing to organize the unstructured videos in the wild for users to understand the event effectively. By combining multimodal information from visual, audio and geo-locations, our system can then visualize all possible locations of the shooter in the map. Our system provides a web interface for human-in-the-loop verification to ensure accurate estimations. We present the results of estimating the shooter's location of the Las Vegas Shooting in 2017 and show that our system is able to get accurate location using only the first few gunshots. All relevant source code including the web interface and machine learning models are available11https://vera.cs.cmu.edu.
- Po-Yao (Bernie) Huang, Guoliang Kang, Wenhe Liu, Xiaojun Chang, Alexander Hauptmann. 2019. Annotation Efficient Cross-Modal Retrieval with Adversarial Attentive Alignment. Abstract: Visual-semantic embeddings are central to many multimedia applications such as cross-modal retrieval between visual data and natural language descriptions. Conventionally, learning a joint embedding space relies on large parallel multimodal corpora. Since massive human annotation is expensive to obtain, there is a strong motivation in developing versatile algorithms to learn from large corpora with fewer annotations. In this paper, we propose a novel framework to leverage automatically extracted regional semantics from un-annotated images as additional weak supervision to learn visual-semantic embeddings. The proposed model employs adversarial attentive alignments to close the inherent heterogeneous gaps between annotated and un-annotated portions of visual and textual domains. To demonstrate its superiority, we conduct extensive experiments on sparsely annotated multimodal corpora. The experimental results show that the proposed model outperforms state-of-the-art visual-semantic embedding models by a significant margin for cross-modal retrieval tasks on the sparse Flickr30k and MS-COCO datasets. It is also worth noting that, despite using only 20% of the annotations, the proposed model can achieve competitive performance (Recall at 10 > 80.0% for 1K and > 70.0% for 5K text-to-image retrieval) compared to the benchmarks trained with the complete annotations.
- Shizhe Chen, Qin Jin, Alexander Hauptmann. 2019. Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data. Abstract: Bilingual lexicon induction, translating words from the source language to the target language, is a long-standing natural language processing task. Recent endeavors prove that it is promising to employ images as pivot to learn the lexicon induction without reliance on parallel corpora. However, these vision-based approaches simply associate words with entire images, which are constrained to translate concrete words and require object-centered images. We humans can understand words better when they are within a sentence with context. Therefore, in this paper, we propose to utilize images and their associated captions to address the limitations of previous approaches. We propose a multi-lingual caption model trained with different mono-lingual multimodal data to map words in different languages into joint spaces. Two types of word representation are induced from the multi-lingual caption model: linguistic features and localized visual features. The linguistic feature is learned from the sentence contexts with visual semantic constraints, which is beneficial to learn translation for words that are less visual-relevant. The localized visual feature is attended to the region in the image that correlates to the word, so that it alleviates the image restriction for salient visual representation. The two types of features are complementary for word translation. Experimental results on multiple language pairs demonstrate the effectiveness of our proposed method, which substantially outperforms previous vision-based approaches without using any parallel sentences or supervision of seed word pairs.
- Po-Yao (Bernie) Huang, Vaibhav, Xiaojun Chang, Alexander Hauptmann. 2019. Improving What Cross-Modal Retrieval Models Learn through Object-Oriented Inter- and Intra-Modal Attention Networks. Abstract: Although significant progress has been made for cross-modal retrieval models in recent years, few have explored what those models truly learn and what makes one model superior to another. Start by training two state-of-the-art text-to-image retrieval models with adversarial text inputs, we investigate and quantify the importance of syntactic structure and lexical information in learning the joint visual-semantic embedding space for cross-modal retrieval. The results show that the retrieval power mainly comes from localizing and connecting the visual objects and their cross-modal counter-parts, the textual phrases. Inspired by this observation, we propose a novel model which employs object-oriented encoders along with inter- and intra-modal attention networks to improve inter-modal dependencies for cross-modal retrieval. In addition, we develop a new multimodal structure-preserving objective which additionally emphasizes intra-modal hard negative examples to promote intra-modal discrepancies. Extensive experiments show that the proposed approach outperforms the existing best method by a large margin (16.4% and 6.7% relatively with Recall@1 in the text-to-image retrieval task on the Flickr30K dataset and the MS-COCO dataset respectively).
- Rafael Martín Nieto, Álvaro García-Martín, Alexander Hauptmann, José M. Martínez. 2019. Automatic Vacant Parking Places Management System Using Multicamera Vehicle Detection. Abstract: This paper presents a multicamera system for vehicles detection and their corresponding mapping into the parking spots of a parking lot. Approaches from the state-of-the-art system, which work properly in controlled scenarios, have been validated using small amount of sequences and without more challenging realistic conditions (illumination changes and different weather conditions). On the other hand, most of them are not complete systems, but provide only parts of them, usually detectors. The proposed system has been designed for realistic scenarios considering different cases of occlusion, illumination changes, and different climatic conditions; a real scenario (the International Pittsburgh Airport parking lot) has been targeted with the condition that existing parking security cameras can be used, avoiding the deployment of new cameras or other sensors infrastructures. For design and validation, a new multicamera data set has been recorded. The system is based on existing object detectors (the results of two of them are shown) and different proposed postprocessing stages. The results clearly show that the proposed system works correctly in challenging scenarios including almost total occlusions, illumination changes, and different weather conditions.
- Soham Ghosh, Anuva Agarwal, Zarana Parekh, Alexander Hauptmann. 2019. ExCL: Extractive Clip Localization Using Natural Language Descriptions. Abstract: The task of retrieving clips within videos based on a given natural language query requires cross-modal reasoning over multiple frames. Prior approaches such as sliding window classifiers are inefficient, while text-clip similarity driven ranking-based approaches such as segment proposal networks are far more complicated. In order to select the most relevant video clip corresponding to the given text description, we propose a novel extractive approach that predicts the start and end frames by leveraging cross-modal interactions between the text and video - this removes the need to retrieve and re-rank multiple proposal segments. Using recurrent networks we encode the two modalities into a joint representation which is then used in different variants of start-end frame predictor networks. Through extensive experimentation and ablative analysis, we demonstrate that our simple and elegant approach significantly outperforms state of the art on two datasets and has comparable performance on a third.
- Shizhe Chen, Qin Jin, Jia Chen, Alexander Hauptmann. 2019. Generating Video Descriptions With Latent Topic Guidance. Abstract: Automatic video description generation (a.k.a video captioning) is one of the ultimate goals for video understanding. Despite the wide range of applications such as video indexing and retrieval etc., the video captioning task remains quite challenging due to the complexity and diversity of video content. First, open-domain videos cover a broad range of topics, which results in highly variable vocabularies and expression styles to describe the video contents. Second, videos naturally contain multiple modalities including image, motion, and acoustic media. The information provided by different modalities differs in different conditions. In this paper, we propose a novel topic-guided video captioning model to address the above-mentioned challenges in video captioning. Our model consists of two joint tasks, namely, latent topic generation and topic-guided caption generation. The topic generation task aims to automatically predict the latent topic of the video. Since there is no groundtruth topic information, we mine multimodal topics in an unsupervised fashion based on video contents and annotated captions, and then distill the topic distribution to a topic prediction model. In the topic-guided generation task, we employ the topic guidance for two purposes. The first is to narrow down the language complexity across topics, where we propose the topic-aware decoder to leverage the latent topics to induce topic-related language models. The decoder is also generic and can be integrated with a temporal attention mechanism. The second is to dynamically attend to important modalities by topics, where we propose a flexible topic-guided multimodal ensemble framework and use the topic gating network to determine the attention weights. The two tasks are correlated with each other, and they collaborate to generate more detailed and accurate video captions. Our extensive experiments on two public benchmark datasets MSR-VTT and Youtube2Text demonstrate the effectiveness of the proposed topic-guided video captioning system, which achieves state-of-the-art performance on both datasets.
- Junwei Liang, J. D. Aronson, Alexander Hauptmann. 2019. Technical Report of the DAISY System - Shooter Localization, Models, Interface, and Beyond. Abstract: Nowadays a huge number of user-generated videos are uploaded to social media every second, capturing glimpses of events all over the world. These videos provide important and useful information for reconstructing the events. In this paper, we describe the DAISY system, enabled by established machine learning techniques and physics models, that can localize the shooter location only based on a couple of user-generated videos that capture the gun shot sound. The DAISY system utilizes machine learning techniques like video synchronization and gunshot temporal localization to organize the unstructured social media videos and quickly localize gunshot in the videos. It provides a web interface for human-in-the-loop verification to ensure accurate estimations. We present the results of estimating the shooter's location of the Las Vegas Shooting in 2017 and show that DAISY is able to get accurate location using only the first few shots. We then point out future directions that can help improve the system and further reduces human labor in the process. We publish all relevant source code including the web interface and machine learning models in the hope that such tool can be of use to help preserve life and get contributions from the research and software engineering community to make the tool better.
- En Yu, Jiande Sun, Jing Li, Xiaojun Chang, Xianhua Han, Alexander Hauptmann. 2019. Adaptive Semi-Supervised Feature Selection for Cross-Modal Retrieval. Abstract: In order to exploit the abundant potential information of the unlabeled data and contribute to analyzing the correlation among heterogeneous data, we propose the semi-supervised model named adaptive semi-supervised feature selection for cross-modal retrieval. First, we utilize the semantic regression to strengthen the neighboring relationship between the data with the same semantic. And the correlation between heterogeneous data can be optimized via keeping the pairwise closeness when learning the common latent space. Second, we adopt the graph-based constraint to predict accurate labels for unlabeled data, and it can also keep the geometric structure consistency between the label space and the feature space of heterogeneous data in the common latent space. Finally, an efficient joint optimization algorithm is proposed to update the mapping matrices and the label matrix for unlabeled data simultaneously and iteratively. It makes samples from different classes to be far apart, while the samples from same class lie as close as possible. Meanwhile, the ${l_{2,1}}$-norm constraint is used for feature selection and outlier reduction when the mapping matrices are learned. In addition, we propose learning different mapping matrices corresponding to different sub-tasks to emphasize the semantic and structural information of query data. Experiment results on three datasets demonstrate that our method performs better than the state-of-the-art methods.
- Zhi-Qi Cheng, Jun-Xiu Li, Qi Dai, Xiao Wu, Alexander Hauptmann. 2019. Learning Spatial Awareness to Improve Crowd Counting. Abstract: The aim of crowd counting is to estimate the number of people in images by leveraging the annotation of center positions for pedestrians' heads. Promising progresses have been made with the prevalence of deep Convolutional Neural Networks. Existing methods widely employ the Euclidean distance (i.e., $L_2$ loss) to optimize the model, which, however, has two main drawbacks: (1) the loss has difficulty in learning the spatial awareness (i.e., the position of head) since it struggles to retain the high-frequency variation in the density map, and (2) the loss is highly sensitive to various noises in crowd counting, such as the zero-mean noise, head size changes, and occlusions. Although the Maximum Excess over SubArrays (MESA) loss has been previously proposed by~\cite{nips-10} to address the above issues by finding the rectangular subregion whose predicted density map has the maximum difference from the ground truth, it cannot be solved by gradient descent, thus can hardly be integrated into the deep learning framework. In this paper, we present a novel architecture called SPatial Awareness Network (SPANet) to incorporate spatial context for crowd counting. The Maximum Excess over Pixels (MEP) loss is proposed to achieve this by finding the pixel-level subregion with high discrepancy to the ground truth. To this end, we devise a weakly supervised learning scheme to generate such region with a multi-branch architecture. The proposed framework can be integrated into existing deep crowd counting methods and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that our method can significantly improve the performance of baselines. More remarkably, our approach outperforms the state-of-the-art methods on all benchmark datasets.
- Siyu Huang, Zhi-Qi Cheng, Xi Li, Xiao Wu, Zhongfei Zhang, Alexander Hauptmann. 2018. Perceiving Physical Equation by Observing Visual Scenarios. Abstract: Inferring universal laws of the environment is an important ability of human intelligence as well as a symbol of general AI. In this paper, we take a step toward this goal such that we introduce a new challenging problem of inferring invariant physical equation from visual scenarios. For instance, teaching a machine to automatically derive the gravitational acceleration formula by watching a free-falling object. To tackle this challenge, we present a novel pipeline comprised of an Observer Engine and a Physicist Engine by respectively imitating the actions of an observer and a physicist in the real world. Generally, the Observer Engine watches the visual scenarios and then extracting the physical properties of objects. The Physicist Engine analyses these data and then summarizing the inherent laws of object dynamics. Specifically, the learned laws are expressed by mathematical equations such that they are more interpretable than the results given by common probabilistic models. Experiments on synthetic videos have shown that our pipeline is able to discover physical equations on various physical worlds with different visual appearances.
- Ankit Shah, Anurag Kumar, Alexander Hauptmann, B. Raj. 2018. A Closer Look at Weak Label Learning for Audio Events. Abstract: Audio content analysis in terms of sound events is an important research problem for a variety of applications. Recently, the development of weak labeling approaches for audio or sound event detection (AED) and availability of large scale weakly labeled dataset have finally opened up the possibility of large scale AED. However, a deeper understanding of how weak labels affect the learning for sound events is still missing from literature. In this work, we first describe a CNN based approach for weakly supervised training of audio events. The approach follows some basic design principle desirable in a learning method relying on weakly labeled audio. We then describe important characteristics, which naturally arise in weakly supervised learning of sound events. We show how these aspects of weak labels affect the generalization of models. More specifically, we study how characteristics such as label density and corruption of labels affects weakly supervised training for audio events. We also study the feasibility of directly obtaining weak labeled data from the web without any manual label and compare it with a dataset which has been manually labeled. The analysis and understanding of these factors should be taken into picture in the development of future weak label learning methods. Audioset, a large scale weakly labeled dataset for sound events is used in our experiments.
- Lijun Yu, Dawei Zhang, Xiangqun Chen, Alexander Hauptmann. 2018. Traffic Danger Recognition With Surveillance Cameras Without Training Data. Abstract: We propose a traffic danger recognition model that works with arbitrary traffic surveillance cameras to identify and predict car crashes. There are too many cameras to monitor manually. Therefore, we developed a model to predict and identify car crashes from surveillance cameras based on a 3D reconstruction of the road plane and prediction of trajectories. For normal traffic, it supports real-time proactive safety checks of speeds and distances between vehicles to provide insights about possible high-risk areas. We achieve good prediction and recognition of car crashes without using any labeled training data of crashes. Experiments on the BrnoCompSpeed dataset show that our model can accurately monitor the road, with mean errors of 1.80% for distance measurement, 2.77 km/h for speed measurement, 0.24 m for car position prediction, and 2.53 km/h for speed prediction.
- Shizhe Chen, Jia Chen, Qin Jin, Alexander Hauptmann. 2018. Class-aware Self-Attention for Audio Event Recognition. Abstract: Audio event recognition (AER) has been an important research problem with a wide range of applications. However, it is very challenging to develop large scale audio event recognition models. On the one hand, usually there are only "weak" labeled audio training data available, which only contains labels of audio events without temporal boundaries. On the other hand, the distribution of audio events is generally long-tailed, with only a few positive samples for large amounts of audio events. These two issues make it hard to learn discriminative acoustic features to recognize audio events especially for long-tailed events. In this paper, we propose a novel class-aware self-attention mechanism with attention factor sharing to generate discriminative clip-level features for audio event recognition. Since a target audio event only occurs in part of an entire audio clip and its corresponding temporal interval varies, the proposed class-aware self-attention approach learns to highlight relevant temporal intervals and to suppress irrelevant noises at the same time. In order to learn attention patterns effectively for those long-tailed events, we combine both the domain knowledge and data driven strategies to share attention factors in the proposed attention mechanism, which transfers the common knowledge learned from other similar events to the rare events. The proposed attention mechanism is a pluggable component and can be trained end-to-end in the overall AER model. We evaluate our model on a large-scale audio event corpus "Audio Set" with both short-term and long-term acoustic features. The experimental results demonstrate the effectiveness of our model, which improves the overall audio event recognition performance with different acoustic features especially for events with low resources. Moreover, the experiments also show that our proposed model is able to learn new audio events with a few training examples effectively and efficiently without disturbing the previously learned audio events.
- Liangke Gui, Xiaodan Liang, Xiaojun Chang, Alexander Hauptmann. 2018. Adaptive Context-aware Reinforced Agent for Handwritten Text Recognition. Abstract: Handwritten text recognition has been a ubiquitous research problem in the ﬁeld of computer vision. Most existing approaches focus on the recognition of handwritten words without considering the cursive nature and signiﬁcant differences in the writing of individuals. In this paper, we address these problems by leveraging an adaptive context-aware reinforced agent which learns the actions to determine the scales of context regions during inference. We formulate our approach in a reinforcement learning framework. Speciﬁcally, we construct the action set with a number of context lengths. Given an image feature sequence, our model is trained to adaptively choose the optimal context length according to the observed state. An attention mechanism is then used to selectively attend the context region. Our model can generalize well from recognizing isolated words to recognizing individual lines of text while remain low computation overheads. We conduct extensive experiments on three large-scale handwritten text recognition datasets. The experimental results show that our proposed model is superior to the state-of-the-art alternatives.
- Wenhe Liu, Xiaojun Chang, Yan Yan, Yi Yang, Alexander Hauptmann. 2018. Few-Shot Text and Image Classification via Analogical Transfer Learning. Abstract: Learning from very few samples is a challenge for machine learning tasks, such as text and image classification. Performance of such task can be enhanced via transfer of helpful knowledge from related domains, which is referred to as transfer learning. In previous transfer learning works, instance transfer learning algorithms mostly focus on selecting the source domain instances similar to the target domain instances for transfer. However, the selected instances usually do not directly contribute to the learning performance in the target domain. Hypothesis transfer learning algorithms focus on the model/parameter level transfer. They treat the source hypotheses as well-trained and transfer their knowledge in terms of parameters to learn the target hypothesis. Such algorithms directly optimize the target hypothesis by the observable performance improvements. However, they fail to consider the problem that instances that contribute to the source hypotheses may be harmful for the target hypothesis, as instance transfer learning analyzed. To relieve the aforementioned problems, we propose a novel transfer learning algorithm, which follows an analogical strategy. Particularly, the proposed algorithm first learns a revised source hypothesis with only instances contributing to the target hypothesis. Then, the proposed algorithm transfers both the revised source hypothesis and the target hypothesis (only trained with a few samples) to learn an analogical hypothesis. We denote our algorithm as Analogical Transfer Learning. Extensive experiments on one synthetic dataset and three real-world benchmark datasets demonstrate the superior performance of the proposed algorithm.
- Carla Viegas, S. Lau, R. Maxion, Alexander Hauptmann. 2018. Towards Independent Stress Detection: A Dependent Model Using Facial Action Units. Abstract: Our society is increasingly more susceptible to chronic stress. Reasons are daily worries, workload, and the wish to fulfil a myriad of expectations. Unfortunately, long-exposure to stress leads to physical and mental health problems. To avoid the described consequences, mobile applications have been studied to track stress in combination with wearables. However, wearables need to be worn all day long and can be costly. Given that most laptops have inbuilt cameras, using video data for personal tracking of stress levels could be a more affordable alternative. In previous work, videos have been used to detect cognitive stress during driving by measuring the presence of anger or fear through a limited number of facial expressions. In contrast, we propose the use of 17 facial action units (AUs) not solely restricted to those emotions. We used five one-hour long videos from the dataset collected by Lau [1]. The videos show subjects while typing, resting, and exposed to a stressor, being a multitasking exercise combined with social evaluation. We performed binary classification using several simple classifiers on AUs extracted in each video frame and were able to achieve an accuracy of up to 74% in subject independent classification and 91% in subject dependent classification. These preliminary results indicate that the AUs most relevant for stress detection are not consistently the same for all 5 subjects. Also in previous work, using facial cues, a strong person-specific component was found during classification.
- Ankit Shah, Harini Kesavamoorthy, Poorva Rane, Pramati Kalwad, Alexander Hauptmann, Florian Metze. 2018. Activity Recognition on a Large Scale in Short Videos - Moments in Time Dataset. Abstract: Moments capture a huge part of our lives. Accurate recognition of these moments is challenging due to the diverse and complex interpretation of the moments. Action recognition refers to the act of classifying the desired action/activity present in a given video. In this work, we perform experiments on Moments in Time dataset to recognize accurately activities occurring in 3 second clips. We use state of the art techniques for visual, auditory and spatio temporal localization and develop method to accurately classify the activity in the Moments in Time dataset. Our novel approach of using Visual Based Textual features and fusion techniques performs well providing an overall 89.23 % Top - 5 accuracy on the 20 classes - a significant improvement over the Baseline TRN model.
- Minnan Luo, F. Nie, Xiaojun Chang, Yi Yang, Alexander Hauptmann, Q. Zheng. 2018. Adaptive Unsupervised Feature Selection With Structure Regularization. Abstract: Feature selection is one of the most important dimension reduction techniques for its efficiency and interpretation. Since practical data in large scale are usually collected without labels, and labeling these data are dramatically expensive and time-consuming, unsupervised feature selection has become a ubiquitous and challenging problem. Without label information, the fundamental problem of unsupervised feature selection lies in how to characterize the geometry structure of original feature space and produce a faithful feature subset, which preserves the intrinsic structure accurately. In this paper, we characterize the intrinsic local structure by an adaptive reconstruction graph and simultaneously consider its multiconnected-components (multicluster) structure by imposing a rank constraint on the corresponding Laplacian matrix. To achieve a desirable feature subset, we learn the optimal reconstruction graph and selective matrix simultaneously, instead of using a predetermined graph. We exploit an efficient alternative optimization algorithm to solve the proposed challenging problem, together with the theoretical analyses on its convergence and computational complexity. Finally, extensive experiments on clustering task are conducted over several benchmark data sets to verify the effectiveness and superiority of the proposed unsupervised feature selection algorithm.
- Ryota Hinami, Junwei Liang, S. Satoh, Alexander Hauptmann. 2018. Multimodal Co-Training for Selecting Good Examples from Webly Labeled Video. Abstract: We tackle the problem of learning concept classifiers from videos on the web without using manually labeled data. Although metadata attached to videos (e.g., video titles, descriptions) can be of help collecting training data for the target concept, the collected data is often very noisy. The main challenge is therefore how to select good examples from noisy training data. Previous approaches firstly learn easy examples that are unlikely to be noise and then gradually learn more complex examples. However, hard examples that are much different from easy ones are never learned. In this paper, we propose an approach called multimodal co-training (MMCo) for selecting good examples from noisy training data. MMCo jointly learns classifiers for multiple modalities that complement each other to select good examples. Since MMCo selects examples by consensus of multimodal classifiers, a hard example for one modality can still be used as a training example by exploiting the power of the other modalities. The algorithm is very simple and easily implemented but yields consistent and significant boosts in example selection and classification performance on the FCVID and YouTube8M benchmarks.
- Zhi-Qi Cheng, Xiao Wu, Siyu Huang, Jun-Xiu Li, Alexander Hauptmann, Qiang Peng. 2018. Learning to Transfer: Generalizable Attribute Learning with Multitask Neural Model Search. Abstract: As attribute leaning brings mid-level semantic properties for objects, it can benefit many traditional learning problems in multimedia and computer vision communities. When facing the huge number of attributes, it is extremely challenging to automatically design a generalizable neural network for other attribute learning tasks. Even for a specific attribute domain, the exploration of the neural network architecture is always optimized by a combination of heuristics and grid search, from which there is a large space of possible choices to be searched. In this paper, Generalizable Attribute Learning Model (GALM) is proposed to automatically design the neural networks for generalizable attribute learning. The main novelty of GALM is that it fully exploits the Multi-Task Learning and Reinforcement Learning to speed up the search procedure. With the help of parameter sharing, GALM is able to transfer the pre-searched architecture to different attribute domains. In experiments, we comprehensively evaluate GALM on 251 attributes from three domains: animals, objects, and scenes. Extensive experimental results demonstrate that GALM significantly outperforms the state-of-the-art attribute learning approaches and previous neural architecture search methods on two generalizable attribute learning scenarios.
- J. D. Aronson, McKenna Cole, Alexander Hauptmann, Dan J. Miller, Bradley Samuels. 2018. Reconstructing Human Rights Violations Using Large Eyewitness Video Collections: The Case of Euromaidan Protester Deaths. Abstract: The widespread availability of mobile phones with high quality cameras means that events around the world can be live streamed or captured on video and rapidly shared via social media. Because this video is multi-perspectival, it can tell the story of an event from many different vantage points, providing a synthetic and composite form of documentation that has the potential to enrich our understanding of events of interest. While video has the potential to provide valuable information, variability in recording platforms and metadata can make a large video archive complex and very difficult to analyse. This paper describes a platform developed by a multidisciplinary team to organize and analyse a large collection of event-based video. It also explains how the system is being deployed to aid in the investigation of allegations of abuses by security forces during the 2013–2014 Euromaidan Protests in Kiev, Ukraine. This platform includes a video archiving system, semiautomated tools for video synchronization and geolocation, and visual interfaces for exploring video data. This system will be useful for the investigation and analysis of protests and demonstrations, mass government repression, police brutality, conflict events, and disasters. The paper concludes by noting that video—even in high volume—does not tell the entire story of an event. As with all other forms of evidence, it must be combined with other available data and relevant knowledge in order to provide a nuanced understanding of what has taken place. * Jay D. Aronson (aronson@andrew.cmu.edu) is the founder and director of the Center for Human Rights Science, and associate professor of science, technology, and society, at Carnegie Mellon University. Alex Hauptmann is a principal systems scientist in Carnegie Mellon University’s Language Technologies Institute. Bradley Samuels is a founding partner at SITU Research, an interdisciplinary practice working in design, visualization, and spatial analysis in the service of human rights reporting and fact finding, where McKenna Cole and Dan Miller carried out research, design, and software development on this project. VC The Author(s) 2018. Published by Oxford University Press. All rights reserved. For permissions, please email: journals.permissions@oup.com Journal of Human Rights Practice, 2018, 1–20 doi: 10.1093/jhuman/huy005 Policy and Practice Note Downloaded from https://academic.oup.com/jhrp/advance-article-abstract/doi/10.1093/jhuman/huy005/4976458 by guest on 19 April 2018
- Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, Alexander Hauptmann. 2018. GNAS: A Greedy Neural Architecture Search Method for Multi-Attribute Learning. Abstract: A key problem in deep multi-attribute learning is to effectively discover the inter-attribute correlation structures. Typically, the conventional deep multi-attribute learning approaches follow the pipeline of manually designing the network architectures based on task-specific expertise prior knowledge and careful network tunings, leading to the inflexibility for various complicated scenarios in practice. Motivated by addressing this problem, we propose an efficient greedy neural architecture search approach (GNAS) to automatically discover the optimal tree-like deep architecture for multi-attribute learning. In a greedy manner, GNAS divides the optimization of global architecture into the optimizations of individual connections step by step. By iteratively updating the local architectures, the global tree-like architecture gets converged where the bottom layers are shared across relevant attributes and the branches in top layers more encode attribute-specific features. Experiments on three benchmark multi-attribute datasets show the effectiveness and compactness of neural architectures derived by GNAS, and also demonstrate the efficiency of GNAS in searching neural architectures.
- Ting-yao Hu, Xiaojun Chang, Alexander Hauptmann. 2018. Learning Distributional Representation and Set Distance for Multi-shot Person Re-identification. Abstract: Person re-identification aims to identify a specific person at distinct time and locations. It is challenging because of occlusion, illumination, and viewpoint change in camera views. Recently, multi-shot person re-id task receives more attention because it is closer to real world application. A key point of a good algorithm for multi-shot person re-id is how to aggregate appearance features of all images temporally. Most of the current approaches apply pooling strategies and obtain a fixed size representation. We argue that representing a set of images as a feature vector may lose the matching evidences between examples. %A few very recent methods handle this issue by introducing multi-stage attention mechanism. However, In this work, we propose the idea of distributional representation, which interprets a image set as samples generated from a distribution in appearance feature space, and learn a distributional set distance function to compare two image sets. Specifically, we choose Wasserstein distance in this study. In this way, the proper alignment between two image sets can be discovered naturally in an non-parametric manner. Furthermore, the distance between distributions can serve as a supervision signal to finetune the appearance feature extractor in our model. Experiment results show that our proposed method achieve state-of-the-art performance on MARS dataset.
- Shizhe Chen, Yuqing Song, Yida Zhao, Jiarong Qiu, Qin Jin, Alexander Hauptmann. 2018. RUC+CMU: System Report for Dense Captioning Events in Videos. Abstract: This notebook paper presents our system in the ActivityNet Dense Captioning in Video task (task 3). Temporal proposal generation and caption generation are both important to the dense captioning task. Therefore, we propose a proposal ranking model to employ a set of effective feature representations for proposal generation, and ensemble a series of caption models enhanced with context information to generate captions robustly on predicted proposals. Our approach achieves the state-of-the-art performance on the dense video captioning task with 8.529 METEOR score on the challenge testing set.
- Jianan Yao, Alexander Hauptmann. 2018. News Recommendation and Filter Bubble. Abstract: Recently many literatures have studied the problem of rumor detection on social media and proposed various automatic detection algorithms. In this ongoing work report we exploit the power of the crowd and formulate the reviewer selection problem, which aim to find reliable reviews for a possible rumor. Our reviewer selection scheme can be considered complementary to existing methods. We give theoretical analysis and provide a greedy algorithm with approximation guarantee. We conduct experiments on a Twitter dataset about rumors, which validates the effectiveness and efficiency of our algorithm.
- Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, Fei Wu, Alexander Hauptmann. 2018. Crowd Counting with Stacked Pooling for Boosting Scale Invariance. Abstract: —In this work, we take insight into the crowd counting problem by exploring the phenomenon of cross-scale visual similarity caused by perspective distortions. It is a quite common phenomenon in crowd scenarios, suggesting the crowd counting model to enable a good performance of scale invariance. Existing deep crowd counting approaches mainly focus on the multi-scale techniques over convolutional layers to capture scale-adaptive features, resulting in high computing costs. In this paper, we propose simple but effective pooling variants, i.e., multi-kernel pooling and stacked pooling, to take place of the vanilla pooling layers in convolutional neural networks (CNNs) for boosting the scale invariance. Speciﬁcally, the multi-kernel pooling comprises of pooling kernels with multiple receptive ﬁelds to capture the responses at multi-scale local ranges. The stacked pooling is an equivalent form of multi-kernel pooling, while it reduces considerable computing cost. Our proposed pooling modules do not introduce extra parameters and can be easily implemented in practice. Empirical studies on two benchmark crowd counting datasets show that the proposed pooling modules beat the vanilla pooling layer in most experimental cases.
- Junwei Liang, Lu Jiang, Liangliang Cao, Li-Jia Li, Alexander Hauptmann. 2018. Focal Visual-Text Attention for Visual Question Answering. Abstract: Recent insights on language and vision with neural networks have been successfully applied to simple single-image visual question answering. However, to tackle real-life question answering problems on multimedia collections such as personal photos, we have to look at whole collections with sequences of photos or videos. When answering questions from a large collection, a natural problem is to identify snippets to support the answer. In this paper, we describe a novel neural network called Focal Visual-Text Attention network (FVTA) for collective reasoning in visual question answering, where both visual and text sequence information such as images and text metadata are presented. FVTA introduces an end-to-end approach that makes use of a hierarchical process to dynamically determine what media and what time to focus on in the sequential data to answer the question. FVTA can not only answer the questions well but also provides the justifications which the system results are based upon to get the answers. FVTA achieves state-of-the-art performance on the MemexQA dataset and competitive results on the MovieQA dataset.
- Anurag Kumar, Ankit Shah, Alexander Hauptmann, B. Raj. 2018. Learning Sound Events From Webly Labeled Data. Abstract: In the last couple of years, weakly labeled learning has turned out to be an exciting approach for audio event detection. In this work, we introduce webly labeled learning for sound events which aims to remove human supervision altogether from the learning process. We first develop a method of obtaining labeled audio data from the web (albeit noisy), in which no manual labeling is involved. We then describe methods to efficiently learn from these webly labeled audio recordings. In our proposed system, WeblyNet, two deep neural networks co-teach each other to robustly learn from webly labeled data, leading to around 17% relative improvement over the baseline method. The method also involves transfer learning to obtain efficient representations.
- Ting-yao Hu, Alexander Hauptmann. 2018. Multi-shot Person Re-identification through Set Distance with Visual Distributional Representation. Abstract: Person re-identification aims to identify a specific person at distinct times and locations. It is challenging because of occlusion, illumination, and viewpoint change in camera views. Recently, multi-shot person re-id task receives more attention since it is closer to real-world application. A key point of a good algorithm for multi-shot person re-id is the temporal aggregation of the person appearance features. While most of the current approaches apply pooling strategies and obtain a fixed-size vector representation, these may lose the matching evidence between examples. In this work, we propose the idea of visual distributional representation, which interprets an image set as samples drawn from an unknown distribution in appearance feature space. Based on the supervision signals from a downstream task of interest, the method reshapes the appearance feature space and further learns the unknown distribution of each image set. In the context of multi-shot person re-id, we apply this novel concept along with Wasserstein distance and jointly learn a distributional set distance function between two image sets. In this way, the proper alignment between two image sets can be discovered naturally in a non-parametric manner. Our experiment results on three public datasets show the advantages of our proposed method compared to other state-of-the-art approaches.
- Ankit Shah, Jean-Baptiste Lamare, T. Anh, Alexander Hauptmann. 2018. Accident Forecasting in CCTV Traffic Camera Videos. Abstract: This paper presents a novel dataset for traffic accidents analysis.Our goal is to resolve the lack of public data for research about automatic spatio-temporal annotations for traffic safety in the roads. Our Car Accident Detection and Prediction(CADP) dataset consists of 1,416 video segments collected from YouTube, with 205 video segments having full spatio-temporal annotations. To the best of our knowledge, our dataset is largest in terms of number of traffic accidents, compared to related datasets. Through the analysis of the proposed dataset, we observed a significant degradation of object detection in pedestrian category in our dataset, due to the object sizes and complexity of the scenes. To this end, we propose to integrate contextual information into conventional Faster R-CNN using Context Mining(CM) and Augmented Context Mining(ACM) to complement the accuracy for small pedestrian detection. Our experiments indicate a considerable improvement in object detection accuracy: +8.51 % for CM and +6.20 % for ACM. For person(pedestrian) category, we observed significant improvements:+46.45 % for CM and 45.22 % for ACM, compared to Faster R-CNN. Finally, we demonstrate the performance of accident forecasting in our dataset using Faster R-CNN and an Accident LSTM architecture. We achieved an average of 1.359 seconds in terms of Time-To-Accident measure with an Average Precision of 47.36 %. Our Webpage for the paper is this https URL
- Zhigang Ma, Xiaojun Chang, Zhongwen Xu, N. Sebe, Alexander Hauptmann. 2018. Joint Attributes and Event Analysis for Multimedia Event Detection. Abstract: Semantic attributes have been increasingly used the past few years for multimedia event detection (MED) with promising results. The motivation is that multimedia events generally consist of lower level components such as objects, scenes, and actions. By characterizing multimedia event videos with semantic attributes, one could exploit more informative cues for improved detection results. Much existing work obtains semantic attributes from images, which may be suboptimal for video analysis since these image-inferred attributes do not carry dynamic information that is essential for videos. To address this issue, we propose to learn semantic attributes from external videos using their semantic labels. We name them video attributes in this paper. In contrast with multimedia event videos, these external videos depict lower level contents such as objects, scenes, and actions. To harness video attributes, we propose an algorithm established on a correlation vector that correlates them to a target event. Consequently, we could incorporate video attributes latently as extra information into the event detector learnt from multimedia event videos in a joint framework. To validate our method, we perform experiments on the real-world large-scale TRECVID MED 2013 and 2014 data sets and compare our method with several state-of-the-art algorithms. The experiments show that our method is advantageous for MED.
- Jia Chen, Shizhe Chen, Qin Jin, Alexander Hauptmann, Po-Yao (Bernie) Huang, Junwei Liang, Vaibhav, Xiaojun Chang, Jiang Liu, Ting-yao Hu, Wenhe Liu, Wei Ke, Wayner Barrios, Haroon Idrees, Donghyun Yoo, Yaser Sheikh, R. Salakhutdinov, Kris Kitani, Dong Huang. 2018. Informedia @ TRECVID 2018: Ad-hoc Video Search, Video to Text Description, Activities in Extended video. Abstract: In this section of the notebook, we present our system in the TRECVID Video to Text description generation task. The optimization target is critical to train the encoder-decoder based video captioning models. However, there are two main limitations of the most widely used cross-entropy (CE) function as the training target, namely exposure bias and mismatched targets in training and testing. Therefore, we propose to utilize the reinforcement-learning algorithm with different rewards to improve the video captioning performance, which has achieved substantial gains over the CE-trained baselines.
- Po-Yao (Bernie) Huang, Junwei Liang, Jean-Baptiste Lamare, Alexander Hauptmann. 2018. Multimodal Filtering of Social Media for Temporal Monitoring and Event Analysis. Abstract: Developing an efficient and effective social media monitoring system has become one of the important steps towards improved public safety. With the explosive availability of user-generated content documenting most conflicts and human rights abuses around the world, analysts and first-responders increasingly find themselves overwhelmed with massive amounts of noisy data from social media. In this paper, we construct a large-scale public safety event dataset with retrospective automatic labeling for 4.2 million multimodal tweets from 7 public safety events occurred in 2013~2017. We propose a new multimodal social media filtering system composed of encoding, classification, and correlation networks to jointly learn shared and complementary visual and textual information to filter out the most relevant and useful items among the noisy social media influx. The proposed model is verified and achieves significant improvement over competitive baselines under the retrospective and real-time experimental protocols.
- Carla Viegas, S. Lau, R. Maxion, Alexander Hauptmann. 2018. Distinction of stress and non-stress tasks using facial action units. Abstract: Long-exposure to stress is known to lead to physical and mental health problems. But how can we as individuals track and monitor our stress? Wearables which measure heart variability have been studied to detect stress. Such devices, however, need to be worn all day long and can be expensive. As an alternative, we propose the use of frontal face videos to distinguish between stressful and non-stressful activities. Affordable personal tracking of stress levels could be obtained by analyzing the video stream of inbuilt cameras in laptops. In this work, we present a preliminary analysis of 114 one-hour long videos. During the video, the subjects perform a typing exercise before and after being exposed to a stressor. We performed a binary classification using Random Forest (RF) to distinguish between stressful and non-stressful activities. As features, facial action units (AUs) extracted from each video frame were used. We obtained an average accuracy of over 97% and 50% for subject dependent and subject independent classification, respectively.
- Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, Alexander Hauptmann. 2018. Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance. Abstract: In this work, we explore the cross-scale similarity in crowd counting scenario, in which the regions of different scales often exhibit high visual similarity. This feature is universal both within an image and across different images, indicating the importance of scale invariance of a crowd counting model. Motivated by this, in this paper we propose simple but effective variants of pooling module, i.e., multi-kernel pooling and stacked pooling, to boost the scale invariance of convolutional neural networks (CNNs), benefiting much the crowd density estimation and counting. Specifically, the multi-kernel pooling comprises of pooling kernels with multiple receptive fields to capture the responses at multi-scale local ranges. The stacked pooling is an equivalent form of multi-kernel pooling, while, it reduces considerable computing cost. Our proposed pooling modules do not introduce extra parameters into model and can easily take place of the vanilla pooling layer in implementation. In empirical study on two benchmark crowd counting datasets, the stacked pooling beats the vanilla pooling layer in most cases.
- E. Hovy, Taylor Berg-Kirkpatrick, J. Carbonell, Hans Chalupsky, A. Gershman, Alexander Hauptmann, Florian Metze, T. Mitamura, Aditi Chaudhary, Xianyang Chen, Bernie Huang, H. Liu, Xuezhe Ma, Shruti Palaskar, Dheeraj Rajagopal, Maria Ryskina, Ramon Sanabria. 2018. OPERA: Operations-oriented Probabilistic Extraction, Reasoning, and Analysis. Abstract: This paper describes CMU and USC/ISI’s OPERA system that performs endto-end information extraction from multiple media, integrates results across English, Russian, and Ukrainian, produces Knowledge Bases containing the extracted information, and performs hypothesis reasoning over the results.
- Alexander Hauptmann. 2017. First Critique Title : A Prototype. Abstract: Probably one of the reasons the paper was awarded the best paper award is the social implication and the practical applicability that the technology described in the paper has. It is not so often that you come across papers in computer science that have such a broad social implication. As the authors argue, even a partial solution to the problem that the paper attempts to solve would have a broad economic and social impact. As we become more and more dependent on Information Technology in the everyday life, the negative effect of illiteracy on members of the society is bound to increase, further increasing the need for solving the problem.
- Xiaojun Chang, Zhigang Ma, Ming Lin, Yi Yang, Alexander Hauptmann. 2017. Feature Interaction Augmented Sparse Learning for Fast Kinect Motion Detection. Abstract: The Kinect sensing devices have been widely used in current Human-Computer Interaction entertainment. A fundamental issue involved is to detect users’ motions accurately and quickly. In this paper, we tackle it by proposing a linear algorithm, which is augmented by feature interaction. The linear property guarantees its speed whereas feature interaction captures the higher order effect from the data to enhance its accuracy. The Schatten-p norm is leveraged to integrate the main linear effect and the higher order nonlinear effect by mining the correlation between them. The resulted classification model is a desirable combination of speed and accuracy. We propose a novel solution to solve our objective function. Experiments are performed on three public Kinect-based entertainment data sets related to fitness and gaming. The results show that our method has its advantage for motion detection in a real-time Kinect entertaining environment.
- Minnan Luo, F. Nie, Xiaojun Chang, Yi Yang, Alexander Hauptmann, Q. Zheng. 2017. Avoiding Optimal Mean ℓ2,1-Norm Maximization-Based Robust PCA for Reconstruction. Abstract: Robust principal component analysis (PCA) is one of the most important dimension-reduction techniques for handling high-dimensional data with outliers. However, most of the existing robust PCA presupposes that the mean of the data is zero and incorrectly utilizes the average of data as the optimal mean of robust PCA. In fact, this assumption holds only for the squared -norm-based traditional PCA. In this letter, we equivalently reformulate the objective of conventional PCA and learn the optimal projection directions by maximizing the sum of projected difference between each pair of instances based on -norm. The proposed method is robust to outliers and also invariant to rotation. More important, the reformulated objective not only automatically avoids the calculation of optimal mean and makes the assumption of centered data unnecessary, but also theoretically connects to the minimization of reconstruction error. To solve the proposed nonsmooth problem, we exploit an efficient optimization algorithm to soften the contributions from outliers by reweighting each data point iteratively. We theoretically analyze the convergence and computational complexity of the proposed algorithm. Extensive experimental results on several benchmark data sets illustrate the effectiveness and superiority of the proposed method.
- A. Scherp, V. Mezaris, T. Köhler, Alexander Hauptmann. 2017. MultiEdTech 2017: 1st International Workshop on Multimedia-based Educational and Knowledge Technologies for Personalized and Social Online Training. Abstract: Educational and Knowledge Technologies (EdTech), especially in connection to multimedia content and the vision of mobile and personalized learning, is a hot topic in both academia and the business start-ups ecosystem. The driver and enabler of this is on the one side the development and widespread availability of multimedia materials and MOOCs, which represent multimedia content produced specifically for supporting e-learning; and, on the other side, the ever increasing availability of all sorts on information on the Internet and in social media channels (e. g. lectures, research papers, user-generated videos, news items), which, despite not directly targeting e-learning, can prove to be valuable complements to the more targeted learning materials. Although the availability of such content is not a problem these days, finding the right content and associating different relevant pieces of multimedia so as to enable a comprehensive learning experience on a chosen subject is by no means a trivial task. This workshop provides research in areas related to multimedia-based educational and knowledge technologies and particularly on the use of multimedia search and retrieval, analysis and understanding, browsing, summarization, recommendation, and visualization technologies on multimedia content available in specialized learning platforms, the Web, mobile devices and/or social networks for supporting personalized and adaptive e-learning and training.
- Xiaojun Chang, Zhigang Ma, Yi Yang, Zhi-qiang Zeng, Alexander Hauptmann. 2017. Bi-Level Semantic Representation Analysis for Multimedia Event Detection. Abstract: Multimedia event detection has been one of the major endeavors in video event analysis. A variety of approaches have been proposed recently to tackle this problem. Among others, using semantic representation has been accredited for its promising performance and desirable ability for human-understandable reasoning. To generate semantic representation, we usually utilize several external image/video archives and apply the concept detectors trained on them to the event videos. Due to the intrinsic difference of these archives, the resulted representation is presumable to have different predicting capabilities for a certain event. Notwithstanding, not much work is available for assessing the efficacy of semantic representation from the source-level. On the other hand, it is plausible to perceive that some concepts are noisy for detecting a specific event. Motivated by these two shortcomings, we propose a bi-level semantic representation analyzing method. Regarding source-level, our method learns weights of semantic representation attained from different multimedia archives. Meanwhile, it restrains the negative influence of noisy or irrelevant concepts in the overall concept-level. In addition, we particularly focus on efficient multimedia event detection with few positive examples, which is highly appreciated in the real-world scenario. We perform extensive experiments on the challenging TRECVID MED 2013 and 2014 datasets with encouraging results that validate the efficacy of our proposed approach.
- Steven F. Roth, Alexander Hauptmann, Matthew Kane. 2017. First Critique. Abstract: Probably one of the reasons the paper was awarded the best paper award is the social implication and the practical applicability that the technology described in the paper has. It is not so often that you come across papers in computer science that have such a broad social implication. As the authors argue, even a partial solution to the problem that the paper attempts to solve would have a broad economic and social impact. As we become more and more dependent on Information Technology in the everyday life, the negative effect of illiteracy on members of the society is bound to increase, further increasing the need for solving the problem.
- Lu Jiang, Liangliang Cao, Yannis Kalantidis, S. Farfade, Alexander Hauptmann. 2017. Visual Memory QA: Your Personal Photo and Video Search Agent. Abstract: 
 
 The boom of mobile devices and cloud services has led to an explosion of personal photo and video data. However, due to the missing user-generated metadata such as titles or descriptions, it usually takes a user a lot of swipes to find some video on the cell phone. To solve the problem, we present an innovative idea called Visual Memory QA which allow a user not only to search but also to ask questions about her daily life captured in the personal videos. The proposed system automatically analyzes the content of personal videos without user-generated metadata, and offers a conversational interface to accept and answer questions. To the best of our knowledge, it is the first to answer personal questions discovered in personal photos or videos. The example questions are "what was the lat time we went hiking in the forest near San Francisco?"; "did we have pizza last week?"; "with whom did I have dinner in AAAI 2015?".
 

- Junwei Liang, Lu Jiang, Alexander Hauptmann. 2017. Temporal localization of audio events for conflict monitoring in social media. Abstract: With the explosion in the availability of user-generated videos documenting any conflicts and human rights abuses around the world, analysts and researchers increasingly find themselves overwhelmed with massive amounts of video data to acquire and analyze useful information. In this paper, we develop a temporal localization framework for intense audio events in videos which addresses the problem. The proposed method utilizes Localized Self-Paced Reranking (LSPaR) to refine the localization results. LSPaR utilizes samples from easy to noisier ones so that it can overcome the noisiness of the initial retrieval results from user-generated videos. We show our framework's efficacy on localizing intense audio event like gunshot, and further experiments also indicate that our methods can be generalized to localizing other audio events in noisy videos.
- Qin Jin, Shizhe Chen, Jia Chen, Alexander Hauptmann. 2017. Knowing Yourself: Improving Video Caption via In-depth Recap. Abstract: Generating natural language descriptions for videos (a.k.a video captioning) has attracted much research attention in recent years, and a lot of models have been proposed to improve the caption performance. However, due to the rapid progress in dataset expansion and feature representation, newly proposed caption models have been evaluated on different settings, which makes it unclear about the contributions from either features or models. Therefore, in this work we aim to gain a deep understanding about "where are we" for the current development of video captioning. First, we carry out extensive experiments to identify the contribution from different components in video captioning task and make fair comparison among several state-of-the-art video caption models. Second, we discover that these state-of-the-art models are complementary so that we could benefit from "wisdom of the crowd" through ensembling and reranking. Finally, we give a preliminary answer to the question "how far are we from the human-level performance in general'' via a series of carefully designed experiments. In summary, our caption models achieve the state-of-the-art performance on the MSR-VTT 2017 challenge, and it is comparable with the average human-level performance on current caption metrics. However, our analysis also shows that we still have a long way to go, such as further improving the generalization ability of current caption models.
- Minnan Luo, F. Nie, Xiaojun Chang, Yi Yang, Alexander Hauptmann, Q. Zheng. 2017. Probabilistic Non-Negative Matrix Factorization and Its Robust Extensions for Topic Modeling. Abstract: 
 
 Traditional topic model with maximum likelihood estimate inevitably suffers from the conditional independence of words given the document’s topic distribution. In this paper, we follow the generative procedure of topic model and learn the topic-word distribution and topics distribution via directly approximating the word-document co-occurrence matrix with matrix decomposition technique. These methods include: (1) Approximating the normalized document-word conditional distribution with the documents probability matrix and words probability matrix based on probabilistic non-negative matrix factorization (NMF); (2) Since the standard NMF is well known to be non-robust to noises and outliers, we extended the probabilistic NMF of the topic model to its robust versions using l21-norm and capped l21-norm based loss functions, respectively. The proposed framework inherits the explicit probabilistic meaning of factors in topic models and simultaneously makes the conditional independence assumption on words unnecessary. Straightforward and efficient algorithms are exploited to solve the corresponding non-smooth and non-convex problems. Experimental results over several benchmark datasets illustrate the effectiveness and superiority of the proposed methods.
 

- A. Scherp, V. Mezaris, T. Köhler, Alexander Hauptmann. 2017. Proceedings of the 2017 ACM Workshop on Multimedia-based Educational and Knowledge Technologies for Personalized and Social Online Training. Abstract: It is our great pleasure to welcome you to the 1st International Workshop on Multimedia-based Educational and Knowledge Technologies for Personalized and Social Online Training -- MultiEdTech 2017. Educational and Knowledge Technologies (EdTech), especially in connection to multimedia content and the vision of mobile and personalized learning, is a hot topic in both academia and the business start-ups ecosystem. The driver and enabler of this is on the one side the development and widespread availability of multimedia materials and MOOCs, which represent multimedia content produced specifically for supporting e-learning; and, on the other side, the ever increasing availability of all sorts on information on the Internet and in social media channels (e. g., lectures, research papers, user-generated videos, news items), which, despite not directly targeting e-learning, can prove to be valuable complements to the more targeted learning materials. Although the availability of such content is not a problem these days, finding the right content and associating different relevant pieces of multimedia so as to enable a comprehensive learning experience on a chosen subject is by no means a trivial task. 
 
The MultiEdTech 2017 workshop provides a forum for presenting research in areas related to multimedia-based educational and knowledge technologies and particularly on the use of multimedia search and retrieval, analysis and understanding, browsing, summarization, recommendation, and visualization technologies on multimedia content available in specialized learning platforms, the Web, mobile devices and/or social networks for supporting personalized and adaptive e-learning and training. 
 
The workshop will be kicked off with an exciting keynote talk: 
Dr. Pablo Cesar from CWI, Amsterdam on "Sensing Engagement: Helping Performers to Evaluate their Impact". Dr. Cesar leads the Interactive and Distributed Systems group at CWI, which focuses on facilitating and improving the way people access media and communicate with others and with the environment. The keynote will overview on gathering data and understanding the experience of people attending cultural events, public lectures, and courses by using wearable sensor technology. Through practical case studies in different areas of the creative industries and education, it will showcase results and discuss about failures. 
 
 
 
As paper presentations, we cover: 
"Train in Virtual Court: Basketball Tactic Training via Virtual Reality" by Wan-Lun Tsai, Ming-Fen Chung, Tse-Yu Pan, and Min-Chun Hu, presents a basketball tactic training system based on multimedia and virtual reality (VR) technologies. 
Sabrina Kletz, Klaus Schoeffmann, Bernd Munzer, and Manfred J. Primus present with "Surgical Action Retrieval for Assisting Video Review of Laparoscopic Skills" an information retrieval system to find surgical actions from video collections of gynecologic surgeries based on two novel content descriptors. 
Houssem Chatbri, Kevin McGuinness, Suzanne Little, Jiang Zhou, Keisuke Kameyama, Paul Kwan, and Noel E. O'Connor demonstrate with "Automatic MOOC video classification using transcript features and convolutional neural network" the use of modern deep learning techniques for topic classification of MOOC videos. 
"Chat2Doc: From Chats to How-to Instructions, FAQ, and Reports" by Britta Meixner, Matt Lee, and Scott Carter demonstrates a system that aims to collect, store, and automatically extract procedural knowledge from messaging interactions.
- Po-Yao (Bernie) Huang, Ye Yuan, Zhenzhong Lan, Lu Jiang, Alexander Hauptmann. 2017. Video Representation Learning and Latent Concept Mining for Large-scale Multi-label Video Classification. Abstract: We report on CMU Informedia Lab's system used in Google's YouTube 8 Million Video Understanding Challenge. In this multi-label video classification task, our pipeline achieved 84.675% and 84.662% GAP on our evaluation split and the official test set. We attribute the good performance to three components: 1) Refined video representation learning with residual links and hypercolumns 2) Latent concept mining which captures interactions among concepts. 3) Learning with temporal segments and weighted multi-model ensemble. We conduct experiments to validate and analyze the contribution of our models. We also share some unsuccessful trials leveraging conventional approaches such as recurrent neural networks for video representation learning for this large-scale video dataset. All the codes to reproduce our results are publicly available at this https URL.
- Yi Zhu, Zhenzhong Lan, S. Newsam, Alexander Hauptmann. 2017. Guided Optical Flow Learning. Abstract: We study the unsupervised learning of CNNs for optical flow estimation using proxy ground truth data. Supervised CNNs, due to their immense learning capacity, have shown superior performance on a range of computer vision problems including optical flow prediction. They however require the ground truth flow which is usually not accessible except on limited synthetic data. Without the guidance of ground truth optical flow, unsupervised CNNs often perform worse as they are naturally ill-conditioned. We therefore propose a novel framework in which proxy ground truth data generated from classical approaches is used to guide the CNN learning. The models are further refined in an unsupervised fashion using an image reconstruction loss. Our guided learning approach is competitive with or superior to state-of-the-art approaches on three standard benchmark datasets yet is completely unsupervised and can run in real time.
- Jia Chen, Junwei Liang, Han Lu, Shoou-I Yu, Alexander Hauptmann. 2017. Reconstruction Dataset for Synchronization and Localization. Abstract: Event reconstruction is about reconstructing the event truth from a large amount of videos which capture different moments of the same event at different positions from different perspectives. Up to now, there are no related public datasets. In this paper, we introduce the first real-world event reconstruction dataset to promote research in this field. We focus on synchronization and localization, which are the two basic and essential elements for other tasks in event reconstruction such as person tracking, scene reconstruction, and object retrieval. It covers 347 original videos and 1, 066 segmented clips of the real-world event of the explosions at the 2013 Boston Marathon finish line. We provide high precision ground truth labels for localization and two granularity ground truth labels for synchronization on 109 clips. We derive several metrics on video level and frame level to evaluate the two tasks. We also provide auxiliary data including video comments with timestamp, map meta data, environment images, and a 3d point cloud which are helpful for the synchronization and the localization tasks. Finally, we position our dataset as a real-world test dataset, without limiting the usage of extra training data. The dataset is released at http://aladdin1.inf.cs.
- Junwei Liang, Lu Jiang, Alexander Hauptmann. 2017. Webly-Supervised Learning of Multimodal Video Detectors. Abstract: 
 
 Given any complicated or specialized video content search query, e.g. ”Batkid (a kid in batman costume)” or ”destroyed buildings”, existing methods require manually labeled data to build detectors for searching. We present a demonstration of an artificial intelligence application, Webly-labeled Learning (WELL) that enables learning of ad-hoc concept detectors over unlimited Internet videos without any manual an-notations. A considerable number of videos on the web are associated with rich but noisy contextual information, such as the title, which provides a type of weak annotations or la-bels of the video content. To leverage this information, our system employs state-of-the-art webly-supervised learning(WELL) (Liang et al. ). WELL considers multi-modal information including deep learning visual, audio and speech features, to automatically learn accurate video detectors based on the user query. The learned detectors from a large number of web videos allow users to search relevant videos over their personal video archives, not requiring any textual metadata,but as convenient as searching on Youtube.
 

- Freda Shi, Jia Chen, Alexander Hauptmann. 2017. Joint Saliency Estimation and Matching using Image Regions for Geo-Localization of Online Video. Abstract: In this paper, we study automatic geo-localization of online event videos. Different from general image localization task through matching, the appearance of an environment during significant events varies greatly from its daily appearance, since there are usually crowds, decorations or even destruction when a major event happens. This introduces a major challenge: matching the event environment to the daily environment, e.g. as recorded by Google Street View. We observe that some regions in the image, as part of the environment, still preserve the daily appearance even though the whole image (environment) looks quite different. Based on this observation, we formulate the problem as joint saliency estimation and matching at the image region level, as opposed to the key point or whole-image level. As image-level labels of daily environment are easily generated with GPS information, we treat region based saliency estimation and matching as a weakly labeled learning problem over the training data. Our solution is to iteratively optimize saliency and the region-matching model. For saliency optimization, we derive a closed form solution, which has an intuitive explanation. For region matching model optimization, we use self-paced learning to learn from the pseudo labels generated by (sub-optimal) saliency values. We conduct extensive experiments on two challenging public datasets: Boston Marathon 2013 and Tokyo Time Machine. Experimental results show that our solution significantly improves over matching on whole images and the automatically learned saliency is a strong predictor of distinctive building areas.
- Jiang Liu, Jia Chen, De Cheng, Chenqiang Gao, Alexander Hauptmann. 2017. Rewind to track: Parallelized apprenticeship learning with backward tracklets. Abstract: Data association, which could be categorized into offline approaches and the online counterparts, is a crucial part of a multi-object tracker in the tracking-by-detection framework. On the one hand, classical offline data association methods exploit all the video data and have high computation cost, which makes them unscalable to long-term offline video data. On the other hand, online approaches have much lower computation cost, but they suffer from ID-switches and tracklet drifting problem when directly applied to offline data as they are only aware of “past” observations. In this paper, we propose a mixed style tracker, which is not only as efficient as the online tracker but also aware of “future” observations in offline setting. We start from a Markov Decision Process (MDP) online tracker and design a parallelized apprenticeship learning algorithm to learn both the reward function and transition policy in MDP. By proposing a rewind to track strategy to generate backward tracklets, future detections in offline data are efficiently utilized to obtain a more stable similarity measurement for association. Experiment results show that our approach achieves the state-of-the-art performance on challenging datasets.
- Junwei Liang, Lu Jiang, Deyu Meng, Alexander Hauptmann. 2017. Leveraging Multi-modal Prior Knowledge for Large-scale Concept Learning in Noisy Web Data. Abstract: Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web is associated with rich but noisy contextual information, such as the title and other multi-modal information, which provides weak annotations or labels about the video content. To tackle the problem of large-scale noisy learning, We propose a novel method called Multi-modal WEbly-Labeled Learning (WELL-MM), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL-MM introduces a novel multi-modal approach to incorporate meaningful prior knowledge called curriculum from the noisy web videos. We empirically study the curriculum constructed from the multi-modal features of the Internet videos and images. The comprehensive experimental results on FCVID and YFCC100M demonstrate that WELL-MM outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL-MM is robust to the level of noisiness in the video data. Notably, WELL-MM trained on sufficient noisy web labels is able to achieve a better accuracy to supervised learning methods trained on the clean manually labeled data.
- Zhigang Ma, Xiaojun Chang, Yi Yang, N. Sebe, Alexander Hauptmann. 2017. The Many Shades of Negativity. Abstract: Complex event detection has been progressively researched in recent years for the broad interest of video indexing and retrieval. To fulfill the purpose of event detection, one needs to train a classifier using both positive and negative examples. Current classifier training treats the negative videos as equally negative. However, we notice that many negative videos resemble the positive videos in different degrees. Intuitively, we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning. Aiming for this, we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event. Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation. The resulting fine-grained labels may be not optimal to capture the discriminative cues from the negative videos. Hence, we propose to jointly optimize the fine-grained labels with the classifier learning, which brings mutual reciprocality. Meanwhile, the labels of positive examples are supposed to remain unchanged. We thus additionally introduce a constraint for this purpose. On the other hand, the state-of-the-art deep convolutional neural network features are leveraged in our approach for event detection to further boost the performance. Extensive experiments on the challenging TRECVID MED 2014 dataset have validated the efficacy of our proposed approach.
- Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander Hauptmann. 2017. DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation. Abstract: In real-world crowd counting applications, the crowd densities vary greatly in spatial and temporal domains. A detection based counting method will estimate crowds accurately in low density scenes, while its reliability in congested areas is downgraded. A regression based approach, on the other hand, captures the general density information in crowded regions. Without knowing the location of each person, it tends to overestimate the count in low density areas. Thus, exclusively using either one of them is not sufficient to handle all kinds of scenes with varying densities. To address this issue, a novel end-to-end crowd counting framework, named DecideNet (DEteCtIon and Density Estimation Network) is proposed. It can adaptively decide the appropriate counting mode for different locations on the image based on its real density conditions. DecideNet starts with estimating the crowd density by generating detection and regression based density maps separately. To capture inevitable variation in densities, it incorporates an attention module, meant to adaptively assess the reliability of the two types of estimations. The final crowd counts are obtained with the guidance of the attention module to adopt suitable estimations from the two kinds of density maps. Experimental results show that our method achieves state-of-the-art performance on three challenging crowd counting datasets.
- De Cheng, Xiaojun Chang, Li Liu, Alexander Hauptmann, Yihong Gong, N. Zheng. 2017. Discriminative Dictionary Learning With Ranking Metric Embedded for Person Re-Identification. Abstract: The goal of person re-identification (Re-Id) is to match pedestrians captured from multiple non-overlapping cameras. In this paper, we propose a novel dictionary learning based method with ranking metric embedded, for person Re-Id. A new and essential ranking graph Laplacian term is introduced, which minimizes the intra-personal compactness and maximizes the inter-personal dispersion in the objective. Different from the traditional dictionary learning based approaches and their extensions, which just use the same or not information, our proposed method can explore the ranking relationship among the person images, which is essential for such retrieval related tasks. Simultaneously, one distance measurement matrix has been explicitly learned in the model to further improve the performance. Since we have reformulated these ranking constraints into the graph Laplacian form, the proposed method is easy-to-implement but effective. We conduct extensive experiments on three widely used person Re-Id benchmark datasets, and achieve state-of-the-art performances.
- Shizhe Chen, Jia Chen, Qin Jin, Alexander Hauptmann. 2017. Video Captioning with Guidance of Multimodal Latent Topics. Abstract: The topic diversity of open-domain videos leads to various vocabularies and linguistic expressions in describing video contents, and therefore, makes the video captioning task even more challenging. In this paper, we propose an unified caption framework, M&M TGM, which mines multimodal topics in unsupervised fashion from data and guides the caption decoder with these topics. Compared to pre-defined topics, the mined multimodal topics are more semantically and visually coherent and can reflect the topic distribution of videos better. We formulate the topic-aware caption generation as a multi-task learning problem, in which we add a parallel task, topic prediction, in addition to the caption task. For the topic prediction task, we use the mined topics as the teacher to train a student topic prediction model, which learns to predict the latent topics from multimodal contents of videos. The topic prediction provides intermediate supervision to the learning process. As for the caption task, we propose a novel topic-aware decoder to generate more accurate and detailed video descriptions with the guidance from latent topics. The entire learning procedure is end-to-end and it optimizes both tasks simultaneously. The results from extensive experiments conducted on the MSR-VTT and Youtube2Text datasets demonstrate the effectiveness of our proposed model. M&M TGM not only outperforms prior state-of-the-art methods on multiple evaluation metrics and on both benchmark datasets, but also achieves better generalization ability.
- Jia Chen, Junwei Liang, Jiang Liu, Shizhe Chen, Chenqiang Gao, Qin Jin, Alexander Hauptmann. 2017. Informedia @ TRECVID 2017. Abstract: We report on our system used in the TRECVID 2017 Multimedia Event Detection (MED) and Ad-hoc Video Search (AVS) tasks. On the MED task, the CMU team submitted runs in 010Ex settings for the Pre-specified and Ad-hoc Events. On the AVS task, the CMU team submitted runs for fully-automatic system with no annotation condition.
- Lu Jiang, Yannis Kalantidis, Liangliang Cao, S. Farfade, Jiliang Tang, Alexander Hauptmann. 2017. Delving Deep into Personal Photo and Video Search. Abstract: The ubiquity of mobile devices and cloud services has led to an unprecedented growth of online personal photo and video collections. Due to the scarcity of personal media search log data, research to date has mainly focused on searching images and videos on the web. However, in order to manage the exploding amount of personal photos and videos, we raise a fundamental question: what are the differences and similarities when users search their own photos versus the photos on the web? To the best of our knowledge, this paper is the first to study personal media search using large-scale real-world search logs. We analyze different types of search sessions mined from Flickr search logs and discover a number of interesting characteristics of personal media search in terms of information needs and click behaviors. The insightful observations will not only be instrumental in guiding future personal media search methods, but also benefit related tasks such as personal photo browsing and recommendation. Our findings suggest there is a significant gap between personal queries and automatically detected concepts, which is responsible for the low accuracy of many personal media search queries. To bridge the gap, we propose the deep query understanding model to learn a mapping from the personal queries to the concepts in the clicked photos. Experimental results verify the efficacy of the proposed method in improving personal media search, where the proposed method consistently outperforms baseline methods.
- Hehe Fan, Xiaojun Chang, De Cheng, Yi Yang, Dong Xu, Alexander Hauptmann. 2017. Complex Event Detection by Identifying Reliable Shots from Untrimmed Videos. Abstract: The goal of complex event detection is to automatically detect whether an event of interest happens in temporally untrimmed long videos which usually consist of multiple video shots. Observing some video shots in positive (resp. negative) videos are irrelevant (resp. relevant) to the given event class, we formulate this task as a multi-instance learning (MIL) problem by taking each video as a bag and the video shots in each video as instances. To this end, we propose a new MIL method, which simultaneously learns a linear SVM classifier and infers a binary indicator for each instance in order to select reliable training instances from each positive or negative bag. In our new objective function, we balance the weighted training errors and a l1-l2 mixed-norm regularization term which adaptively selects reliable shots as training instances from different videos to have them as diverse as possible. We also develop an alternating optimization approach that can efficiently solve our proposed objective function. Extensive experiments on the challenging real-world Multimedia Event Detection (MED) datasets MEDTest-14, MEDTest-13 and CCV clearly demonstrate the effectiveness of our proposed MIL approach for complex event detection.
- Lu Jiang, Junwei Liang, Liangliang Cao, Yannis Kalantidis, S. Farfade, Alexander Hauptmann. 2017. MemexQA: Visual Memex Question Answering. Abstract: This paper proposes a new task, MemexQA: given a collection of photos or videos from a user, the goal is to automatically answer questions that help users recover their memory about events captured in the collection. Towards solving the task, we 1) present the MemexQA dataset, a large, realistic multimodal dataset consisting of real personal photos and crowd-sourced questions/answers, 2) propose MemexNet, a unified, end-to-end trainable network architecture for image, text and video question answering. Experimental results on the MemexQA dataset demonstrate that MemexNet outperforms strong baselines and yields the state-of-the-art on this novel and challenging task. The promising results on TextQA and VideoQA suggest MemexNet's efficacy and scalability across various QA tasks.
- Junwei Liang, Po-Yao (Bernie) Huang, Jia Chen, Alexander Hauptmann. 2017. Synchronization for multi-perspective videos in the wild. Abstract: In the era of social media, a large number of user-generated videos are uploaded to the Internet every day, capturing events all over the world. Reconstructing the event truth based on information mined from these videos has been an emerging challenging task. Temporal alignment of videos “in the wild” which capture different moments at different positions with different perspectives is the critical step. In this paper, we propose a hierarchical approach to synchronize videos. Our system utilizes clustered audio-signatures to align video pairs. Global alignment for all videos is then achieved via forming alignable video groups with self-paced learning. Experiments on the Boston Marathon dataset show that the proposed method achieves excellent precision and robustness.
- Junwei Liang, Desai Fan, Han Lu, Po-Yao (Bernie) Huang, Jia Chen, Lu Jiang, Alexander Hauptmann. 2017. An Event Reconstruction Tool for Conflict Monitoring Using Social Media. Abstract: 
 
 What happened during the Boston Marathon in 2013? Nowadays, at any major event, lots of people take videos and share them on social media. To fully understand exactly what happened in these major events, researchers and analysts often have to examine thousands of these videos manually. To reduce this manual effort, we present an investigative system that automatically synchronizes these videos to a global timeline and localizes them on a map. In addition to alignment in time and space, our system combines various functions for analysis, including gunshot detection, crowd size estimation, 3D reconstruction and person tracking. To our best knowledge, this is the first time a unified framework has been built for comprehensive event reconstruction for social media videos.
 

- Ionut Cosmin Duta, J. Uijlings, T. Nguyen, K. Aizawa, Alexander Hauptmann, B. Ionescu, N. Sebe. 2016. Histograms of Motion Gradients for real-time video classification. Abstract: Besides appearance information, the video contains temporal evolution, which represents an important and useful source of information about its content. Many video representation approaches are based on the motion information within the video. The common approach to extract the motion information is to compute the optical flow from the vertical and the horizontal temporal evolution of two consecutive frames. However, the computation of optical flow is very demanding in terms of computational cost, in many cases being the most significant processing step within the overall pipeline of the target video analysis application. In this work we propose a very efficient approach to capture the motion information within the video. Our method is based on a simple temporal and spatial derivation, which captures the changes between two consecutive frames. The proposed descriptor, Histograms of Motion Gradients (HMG), is validated on the UCF50 human action recognition dataset. Our HMG pipeline with several additional speed-ups is able to achieve real-time video processing and outperforms several well-known descriptors including descriptors based on the costly optical flow.
- Jia Chen, Junwei Liang, Han Lu, Shoou-I Yu, Alexander Hauptmann. 2016. Videos from the 2013 Boston Marathon: An Event Reconstruction Dataset for Synchronization and Localization. Abstract: Event reconstruction is about reconstructing the event truth from a large amount of videos which capture different moments of the same event at different positions from different perspectives. Up to now, there are no related public datasets. In this paper, we introduce the first real-world event reconstruction dataset to promote research in this field. We focus on synchronization and localization, which are the two basic and essential elements for other tasks in event reconstruction such as person tracking, scene reconstruction, and object retrieval. It covers 347 original videos and 1, 066 segmented clips of the real-world event of the explosions at the 2013 Boston Marathon finish line. We provide high precision ground truth labels for localization and two granularity ground truth labels for synchronization on 109 clips. We derive several metrics on video level and frame level to evaluate the two tasks. We also provide auxiliary data including video comments with timestamp, map meta data, environment images, and a 3d point cloud which are helpful for the synchronization and the localization tasks. Finally, we position our dataset as a real-world test dataset, without limiting the usage of extra training data. The dataset is released at http://aladdin1.inf.cs.
- Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang, Alexander Hauptmann. 2016. Dynamic Concept Composition for Zero-Example Event Detection. Abstract: 
 
 In this paper, we focus on automatically detecting events in unconstrained videos without the use of any visual training exemplars. In principle, zero-shot learning makes it possible to train an event detection model based on the assumption that events (e.g. birthday party) can be described by multiple mid-level semantic concepts (e.g. ``blowing candle'', ``birthday cake''). Towards this goal, we first pre-train a bundle of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept w.r.t. the event of interest and pick up the relevant concept classifiers, which are applied on all test videos to get multiple prediction score vectors. While most existing systems combine the predictions of the concept classifiers with fixed weights, we propose to learn the optimal weights of the concept classifiers for each testing video by exploring a set of online available videos with free-form text descriptions of their content. To validate the effectiveness of the proposed approach, we have conducted extensive experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset. The experimental results confirm the superiority of the proposed approach.
 

- Junwei Liang, Lu Jiang, Deyu Meng, Alexander Hauptmann. 2016. Learning to Detect Concepts from Webly-Labeled Video Data. Abstract: Learning detectors that can recognize concepts, such as people actions, objects, etc., in video content is an interesting but challenging problem. In this paper, we study the problem of automatically learning detectors from the big video data on the web without any additional manual annotations. The contextual information available on the web provides noisy labels to the video content. To leverage the noisy web labels, we propose a novel method called WEbly-Labeled Learning (WELL). It is established on two theories called curriculum learning and self-paced learning and exhibits useful properties that can be theoretically verified. We provide compelling insights on the latent non-convex robust loss that is being minimized on the noisy data. In addition, we propose two novel techniques that not only enable WELL to be applied to big data but also lead to more accurate results. The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set. Experimental results show that WELL significantly outperforms the state-of-the-art methods. To the best of our knowledge, WELL achieves by far the best reported performance on these two webly-labeled big video datasets.
- Minnan Luo, F. Nie, Xiaojun Chang, Yi Yang, Alexander Hauptmann, Q. Zheng. 2016. Avoiding Optimal Mean Robust PCA/2DPCA with Non-greedy ℓ1-Norm Maximization. Abstract: Robust principal component analysis (PCA) is one of the most important dimension reduction techniques to handle high-dimensional data with outliers. However, the existing robust PCA presupposes that the mean of the data is zero and incorrectly utilizes the Euclidean distance based optimal mean for robust PCA with l1-norm. Some studies consider this issue and integrate the estimation of the optimal mean into the dimension reduction objective, which leads to expensive computation. In this paper, we equivalently reformulate the maximization of variances for robust PCA, such that the optimal projection directions are learned by maximizing the sum of the projected difference between each pair of instances, rather than the difference between each instance and the mean of the data. Based on this reformulation, we propose a novel robust PCA to automatically avoid the calculation of the optimal mean based on l1-norm distance. This strategy also makes the assumption of centered data unnecessary. Additionally, we intuitively extend the proposed robust PCA to its 2D version for image recognition. Efficient non-greedy algorithms are exploited to solve the proposed robust PCA and 2D robust PCA with fast convergence and low computational complexity. Some experimental results on benchmark data sets demonstrate the effectiveness and superiority of the proposed approaches on image reconstruction and recognition.
- Linchao Zhu, Xuanyi Dong, Yi Yang, Alexander Hauptmann. 2016. UTS-CMU-D2DCRC Submission at TRECVID 2016 Video Localization. Abstract: In this report, we summarize our solution to TRECVID 2016 Video Localization task. We mainly use Faster R-CNN to localize objects in the spatial domain which is combined with frame-level and shot-level detectors to localize concepts in the temporal domain. We collected images with annotated bounding box from external sources, e.g., ImageNet Detection dataset and manually annotate bounding boxes for categories without any annotations. We trained frame-level detectors using ResNet-200 features pre-trained on ImageNet and for classes of “Running”, “Sitting Down” and “Dancing”, we also use improved Dense Trajectories features. Finally, we fuse bounding box score, frame score and shot score to get the final score for each bounding box.
- Liang Zheng, Yi Yang, Alexander Hauptmann. 2016. Person Re-identification: Past, Present and Future. Abstract: Person re-identification (re-ID) has become increasingly popular in the community due to its application and research significance. It aims at spotting a person of interest in other cameras. In the early days, hand-crafted algorithms and small-scale evaluation were predominantly reported. Recent years have witnessed the emergence of large-scale datasets and deep learning systems which make use of large data volumes. Considering different tasks, we classify most current re-ID methods into two classes, i.e., image-based and video-based; in both tasks, hand-crafted and deep learning systems will be reviewed. Moreover, two new re-ID tasks which are much closer to real-world applications are described and discussed, i.e., end-to-end re-ID and fast re-ID in very large galleries. This paper: 1) introduces the history of person re-ID and its relationship with image classification and instance retrieval; 2) surveys a broad selection of the hand-crafted systems and the large-scale methods in both image- and video-based re-ID; 3) describes critical future directions in end-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs some important yet under-developed issues.
- Junwei Liang, Susanne Burger, Alexander Hauptmann, J. D. Aronson. 2016. Video Synchronization and Sound Search for Human Rights Documentation and Conflict Monitoring. Abstract: In	  this	  technical	  report,	  we	  present	  a	  powerful	  new	  machine	  learning-­‐based audio	  processing	  system	  that	  enables	  synchronization	  of	  audio-­‐rich	  video	  and	  discovery	  of specific	  sounds	  at	  the	  frame	  level	  within	  a	  video.	  This	  tool	  is	  particularly	  useful	  when analyzing	  large	  volumes	  of	  video	  obtained	  from	  social	  media	  and	  other	  open	  source Internet	  platforms	  that	  strip	  technical	  metadata	  during	  the	  uploading	  process.	  The	  tool creates	  a	  unique	  sound	  signature	  at	  the	  frame	  level	  for	  each	  video	  in	  a	  collection,	  and synchronizes	  videos	  that	  are	  recorded	  at	  the	  same	  time	  and	  location.	  The	  use	  of	  this	  tool for	  synchronization	  ultimately	  provides	  a	  multi-­‐perspectival	  view	  of	  a	  specific	  event, enabling	  efficient	  event	  reconstruction	  and	  analysis	  by	  investigators.	  The	  tool	  can	  also	  be used	  to	  search	  for	  specific	  sounds	  within	  a	  video	  collection	  (such	  as	  gunshots).	  Both	  of these	  tasks	  are	  labor	  intensive	  when	  carried	  out	  manually	  by	  human	  investigators.	  We demonstrate	  the	  utility	  of	  this	  system	  by	  analyzing	  video	  from	  Ukraine	  and	  Nigeria,	  two countries	  currently	  relevant	  to	  the	  work	  of	  Center	  for	  Human	  Rights	  Science	  collaborators. Keywords:	  Audio	  Signal	  Processing,	  Machine	  Learning,	  Multi-­‐Camera	  Video Synchronization,	  Conflict	  Monitoring,	  Human	  Rights	  Documentation Acknowledgements:	  The	  authors	  would	  like	  to	  thank	  the	  MacArthur	  Foundation,	  Oak Foundation,	  and	  Humanity	  United	  for	  their	  generous	  support	  of	  this	  project.
- Junwei Liang, Jia Chen, Po-Yao (Bernie) Huang, Xuanchong Li, Lu Jiang, Zhenzhong Lan, Pingbo Pan, Hehe Fan, Qin Jin, Jiande Sun, Yang Chen, Yi Yang, Alexander Hauptmann. 2016. Informedia @ TRECVID 2016. Abstract: We report on our system used in the TRECVID 2016 Multimedia Event Detection (MED) and Ad-hoc Video Search (AVS) tasks. On the MED task, the CMU team submitted runs in 000Ex, 010Ex and 100Ex settings for the Pre-specified Events. On the AVS task, the CMU team submitted runs for fully-automatic system with no annotation condition.
- Chuang Gan, Ming Lin, Yi Yang, Gerard de Melo, Alexander Hauptmann. 2016. Concepts Not Alone: Exploring Pairwise Relationships for Zero-Shot Video Activity Recognition. Abstract: 
 
 Vast quantities of videos are now being captured at astonishing rates, but the majority of these are not labelled. To cope with such data, we consider the task of content-based activity recognition in videos without any manually labelled examples, also known as zero-shot video recognition. To achieve this, videos are represented in terms of detected visual concepts, which are then scored as relevant or irrelevant according to their similarity with a given textual query. In this paper, we propose a more robust approach for scoring concepts in order to alleviate many of the brittleness and low precision problems of previous work. Not only do we jointly consider semantic relatedness, visual reliability, and discriminative power. To handle noise and non-linearities in the ranking scores of the selected concepts, we propose a novel pairwise order matrix approach for score aggregation. Extensive experiments on the large-scale TRECVID Multimedia Event Detection data show the superiority of our approach.
 

- Shoou-I Yu, Yi Yang, Zhongwen Xu, Shicheng Xu, Deyu Meng, Zexi Mao, Zhigang Ma, Ming Lin, Xuanchong Li, Huan Li, Zhenzhong Lan, Lu Jiang, Alexander Hauptmann, Chuang Gan, Xingzhong Du, Xiaojun Chang. 2016. Strategies for Searching Video Content with Text Queries or Video Examples. Abstract: The large number of user-generated videos uploaded on to the Internet everyday has led to many commercial video search engines, which mainly rely on text metadata for search. However, metadata is often lacking for user-generated videos, thus these videos are unsearchable by current search engines. Therefore, content-based video retrieval (CBVR) tackles this metadata-scarcity problem by directly analyzing the visual and audio streams of each video. CBVR encompasses multiple research topics, including low-level feature design, feature fusion, semantic detector training and video search/reranking. We present novel strategies in these topics to enhance CBVR in both accuracy and speed under different query inputs, including pure textual queries and query by video examples. Our proposed strategies have been incorporated into our submission for the TRECVID 2014 Multimedia Event Detection evaluation, where our system outperformed other submissions in both text queries and video example queries, thus demonstrating the effectiveness of our proposed approaches.
- Qin Jin, Jia Chen, Shizhe Chen, Yifan Xiong, Alexander Hauptmann. 2016. Describing Videos using Multi-modal Fusion. Abstract: Describing videos with natural language is one of the ultimate goals of video understanding. Video records multi-modal information including image, motion, aural, speech and so on. MSR Video to Language Challenge provides a good chance to study multi-modality fusion in caption task. In this paper, we propose the multi-modal fusion encoder and integrate it with text sequence decoder into an end-to-end video caption framework. Features from visual, aural, speech and meta modalities are fused together to represent the video contents. Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) are then used as the decoder to generate natural language sentences. Experimental results show the effectiveness of multi-modal fusion encoder trained in the end-to-end framework, which achieved top performance in both common metrics evaluation and human evaluation.
- Mengyi Liu, Lu Jiang, S. Shan, Alexander Hauptmann. 2016. Self-paced Learning for Weakly Supervised Evidence Discovery in Multimedia Event Search. Abstract: Multimedia event detection has been receiving increasing attention in recent years. Besides recognizing an event, the discovery of evidences (which is refered to as "recounting") is also crucial for user to better understand the searching result. Due to the difficulty of evidence annotation, only limited supervision of event labels are available for training a recounting model. To deal with the problem, we propose a weakly supervised evidence discovery method based on self-paced learning framework, which follows a learning process from easy "evidences" to gradually more complex ones, and simultaneously exploit more and more positive evidence samples from numerous weakly annotated video segments. Moreover, to evaluate our method quantitatively, we also propose two metrics, \textit{PctOverlap} and \textit{F1-score}, for measuring the performance of evidence localization specifically. The experiments are conducted on a subset of TRECVID MED dataset and demonstrate the promising results obtained by our method.
- Shoou-I Yu, Yi Yang, Xuanchong Li, Alexander Hauptmann. 2016. Long-Term Identity-Aware Multi-Person Tracking for Surveillance Video Summarization. Abstract: Multi-person tracking plays a critical role in the analysis of surveillance video. However, most existing work focus on shorter-term (e.g. minute-long or hour-long) video sequences. Therefore, we propose a multi-person tracking algorithm for very long-term (e.g. month-long) multi-camera surveillance scenarios. Long-term tracking is challenging because 1) the apparel/appearance of the same person will vary greatly over multiple days and 2) a person will leave and re-enter the scene numerous times. To tackle these challenges, we leverage face recognition information, which is robust to apparel change, to automatically reinitialize our tracker over multiple days of recordings. Unfortunately, recognized faces are unavailable oftentimes. Therefore, our tracker propagates identity information to frames without recognized faces by uncovering the appearance and spatial manifold formed by person detections. We tested our algorithm on a 23-day 15-camera data set (4,935 hours total), and we were able to localize a person 53.2% of the time with 69.8% precision. We further performed video summarization experiments based on our tracking output. Results on 116.25 hours of video showed that we were able to generate a reasonable visual diary (i.e. a summary of what a person did) for different people, thus potentially opening the door to automatic summarization of the vast amount of surveillance video generated every day.
- D. Ellis, Alexander Hauptmann. 2016. DRAFT On Machine Perception of Sound Ph. Abstract: One of the desiderata in machine intelligence is that computers must be able to comprehend sounds as humans do. They must know about various sounds, associate them with physical objects, entities or events, be able to recognize and categorize them, know or discover relationships between them, etc. Successful solutions to these tasks is critical to and can have immediate effect on a variety of applications including content based indexing and retrieval of multimedia data on web which has grown exponentially in past few years. Automated machine understanding of sounds in specific forms such as speech, language and music has become fairly advanced, and has successfully been deployed into systems which are now part of daily life. However, the same cannot be said about natural occurring sounds in our environment. The problem is exacerbated by the sheer vastness of number of sound types, the diversity and variability of sound types, the variations in their structure, and even their interpretation. This dissertation aims to expand the scale and scope of machine hearing capabilities by addressing challenges in recognition of sound events, cataloging large number of potential word phrases that identify sounds and using them to categorize and learn relationships for sounds and by finding ways to efficiently evaluate trained models on large scale. On the sound event recognition front we address the major hindrance of lack of labeled data by describing ways to effectively use the vast amount of data on web. We describe methods for audio event detection which uses only weak labels in the learning process, combines weakly supervised learning with fully supervised learning to leverage labeled data in both forms and finally semi-supervised learning approaches which exploits the vast amount of available unlabeled data on web. Further, we describe methods to automatically mine sound related knowledge and relationships from vast amount of information stored in textual data. The third part once again addresses labeling challenges but now during evaluation phase. Evaluation of trained models on large scale once again requires data labeling. We describe ways to precisely estimate the performance of a trained model under restricted labeling budget. In this proposal we describe the completed works in the above directions. Empirical evaluation shows the effectiveness of the proposed methods. For each component framework described, we discuss expected research directions for successful completion of this dissertation. November 20, 2016 DRAFT
- Alexander I. Rudnicky, Alexander Hauptmann. 2016. Errors , Repetition , and Contrastive Emphasis in Speech Recognition. Abstract: In repeating an utterance for the benefit of a listener, talkers can use contrastive stress to mark those parts of an utterance that were misrecognized. In this study, we found that talkers use a consistent set of markers to indicate contrastive stress: Listeners unaware of the nature of the misrecognition can readily identify a contrastively stressed word. Based on an analysis of these markers, we have implemented an automatic algorithm that identifies contrastive stress with about the same accuracy as humans, using amplitude, duration, silence, and pitch cues. This ability to detect contrastive stress can be effectively exploited, as part of error recovery strategies, by a recognition system.
- Junwei Liang, Lu Jiang, Deyu Meng, Alexander Hauptmann. 2016. Exploiting Multi-modal Curriculum in Noisy Web Data for Large-scale Concept Learning. Abstract: Learning video concept detectors automatically from the big but noisy web data with no additional manual annotations is a novel but challenging area in the multimedia and the machine learning community. A considerable amount of videos on the web are associated with rich but noisy contextual information, such as the title, which provides weak annotations or labels about the video content. To leverage the big noisy web labels, this paper proposes a novel method called WEbly-Labeled Learning (WELL), which is established on the state-of-the-art machine learning algorithm inspired by the learning process of human. WELL introduces a number of novel multi-modal approaches to incorporate meaningful prior knowledge called curriculum from the noisy web videos. To investigate this problem, we empirically study the curriculum constructed from the multi-modal features of the videos collected from YouTube and Flickr. The efficacy and the scalability of WELL have been extensively demonstrated on two public benchmarks, including the largest multimedia dataset and the largest manually-labeled video set. The comprehensive experimental results demonstrate that WELL outperforms state-of-the-art studies by a statically significant margin on learning concepts from noisy web video data. In addition, the results also verify that WELL is robust to the level of noisiness in the video data. Notably, WELL trained on sufficient noisy web labels is able to achieve a comparable accuracy to supervised learning methods trained on the clean manually-labeled data.
- Zhiyong Cheng, Xuanchong Li, Jialie Shen, Alexander Hauptmann. 2016. Which Information Sources are More Effective and Reliable in Video Search. Abstract: It is common that users are interested in finding video segments, which contain further information about the video contents in a segment of interest. To facilitate users to find and browse related video contents, video hyperlinking aims at constructing links among video segments with relevant information in a large video collection. In this study, we explore the effectiveness of various video features on the performance of video hyperlinking, including subtitle, metadata, content features (i.e., audio and visual), surrounding context, as well as the combinations of those features. Besides, we also test different search strategies over different types of queries, which are categorized according to their video contents. Comprehensive experimental studies have been conducted on the dataset of TRECVID 2015 video hyperlinking task. Results show that (1) text features play a crucial role in search performance, and the combination of audio and visual features cannot provide improvements; (2) the consideration of contexts cannot obtain better results; and (3) due to the lack of training examples, machine learning techniques cannot improve the performance.
- Shoou-I Yu, Deyu Meng, W. Zuo, Alexander Hauptmann. 2016. The Solution Path Algorithm for Identity-Aware Multi-object Tracking. Abstract: We propose an identity-aware multi-object tracker based on the solution path algorithm. Our tracker not only produces identity-coherent trajectories based on cues such as face recognition, but also has the ability to pinpoint potential tracking errors. The tracker is formulated as a quadratic optimization problem with ℓ0 norm constraints, which we propose to solve with the solution path algorithm. The algorithm successively solves the same optimization problem but under different ℓp norm constraints, where p gradually decreases from 1 to 0. Inspired by the success of the solution path algorithm in various machine learning tasks, this strategy is expected to converge to a better local minimum than directly minimizing the hardly solvable ℓ0 norm or the roughly approximated ℓ1 norm constraints. Furthermore, the acquired solution path complies with the "decision making process" of the tracker, which provides more insight to locating potential tracking errors. Experiments show that not only is our proposed tracker effective, but also the solution path enables automatic pinpointing of potential tracking failures, which can be readily utilized in an active learning framework to improve identity-aware multi-object tracking.
- Yan Yan, Yi Yang, Haoquan Shen, Deyu Meng, Gaowen Liu, Alexander Hauptmann, N. Sebe. 2015. Complex Event Detection via Event Oriented Dictionary Learning. Abstract: 
 
 Complex event detection is a retrieval task with the goal of finding videos of a particular event in a large-scale unconstrained internet video archive, given example videos and text descriptions. Nowadays, different multimodal fusion schemes of low-level and high-level features are extensively investigated and evaluated for the complex event detection task. However, how to effectively select the high-level semantic meaningful concepts from a large pool to assist complex event detection is rarely studied in the literature. In this paper, we propose two novel strategies to automatically select semantic meaningful concepts for the event detection task based on both the events-kit text descriptions and the concepts high-level feature descriptions. Moreover, we introduce a novel event oriented dictionary representation based on the selected semantic concepts. Towards this goal, we leverage training samples of selected concepts from the Semantic Indexing (SIN) dataset with a pool of 346 concepts, into a novel supervised multi-task dictionary learning framework. Extensive experimental results on TRECVID Multimedia Event Detection (MED) dataset demonstrate the efficacy of our proposed method.
 

- Lei Bao, Shoou-I Yu, Zhenzhong Lan, Arnold Overwijk, Qin Jin, B. Langner, Michael Garbus, Susanne Burger, Florian Metze, Alexander Hauptmann. 2015. Graduate University of the Chinese Academy of Sciences. Abstract: The Informedia group participated in three tasks this year, including: Multimedia Event Detection (MED), Semantic Indexing (SIN) and Surveillance Event Detection. Generally, all of these tasks consist of three main steps: extracting feature, training detector and fusing. In the feature extraction part, we extracted a lot of low-level features, high-level features and text features. Especially, we used the Spatial-Pyramid Matching technique to represent the low-level visual local features, such as SIFT and MoSIFT, which describe the location information of feature points. In the detector training part, besides the traditional SVM, we proposed a Sequential Boosting SVM classifier to deal with the large-scale unbalance classification problem. In the fusion part, to take the advantages from different features, we tried three different fusion methods: early fusion, late fusion and double fusion. Double fusion is a combination of early fusion and late fusion. The experimental results demonstrated that double fusion is consistently better, or at least comparable than early fusion and late fusion. 1 Multimedia Event Detection (MED) 1.1 Feature Extraction In order to encompass all aspects of a video, we extracted a wide variety of visual and audio features as shown in figure 1. Table 1: Features used for the MED task. Visual Features Audio Features Low-level Features • SIFT [19] • Color SIFT [19] • Transformed Color Histogram [19] • Motion SIFT [3] • STIP [9] Mel-Frequency Cepstral Coefficients High-level Features • PittPatt Face Detection [12] • Semantic Indexing Concepts [15] Acoustic Scene Analysis Text Features Optical Character Recognition Automatic Speech Recognition 1.1.1 SIFT, Color SIFT (CSIFT), Transformed Color Histogram (TCH) These three features describe the gradient and color information of a static image. We used the Harris-Laplace detector for corner detection. For more details, please see [19]. Instead of extracting features from all frames for all videos, we first run shot-break detection and only extract features from the keyframe of a corresponding shot. The shot-break detection algorithm detects large color histogram differences between adjacent frames and a shot-boundary is detected when the histogram difference is larger than a threshold. For the 16507 training videos, we extracted 572,881 keyframes. For the 32061 testing videos, we extracted 1,035,412 keyframes. Once we have the keyframes, we extract the three features by the executable provided by [19]. Given the raw feature files, a 4096 word codebook is acquired using the K-Means clustering algorithm. According to the codebook and given a region in an image, we can create a 4096 dimensional vector representing that region. Using the Spatial-Pyramid Matching [10] technique, we extract 8 regions from an keyframe image and calculate a bag-of-words vector for each region. At the end, we get a 8× 4096 = 32768 dimensional bag-of-words vector. The 8 regions are calculated as follows. • The whole image as one region. • Split the image into 4 quadrants and each quadrant is a region. • Split the image horizontally into 3 equally sized rectangles and each rectangle is a region. Since we only have feature vectors describing a keyframe, and a video is described by many keyframes, we compute a vector representing a whole video by averaging over the feature vectors from each keyframe. The features are then provided to a classifier for classification. 1.1.2 Motion SIFT (MoSIFT) Motion SIFT [3] is a motion-based feature that combines information from SIFT and optical flow. The algorithm first extract SIFT points, and for each SIFT point, it checks whether there is a large enough optical flow near the point. If the optical flow value is larger than a threshold, a 256 dimensional feature is computed for that point. The first 128 dimensions of the feature vector is the SIFT descriptor, and the latter 128 dimensions describes the optical flow near the point. We extracted Motion SIFT by calculating the optical flow between neighboring frames, but due to speed issues, we only extract Motion SIFT for the every third frame. Once we have the raw features, a 4096 dimensional codebook is computed, and using the same process as SIFT, a 32768 dimensional vector is created for classification. 1.1.3 Space-Time Interest Points (STIP) Space-Time Interest Points are computed using code from [9]. Given the raw features, a 4096 dimensional code is computed, and using the same process as SIFT, a 32768 dimensional vector is created for classification. 1.1.4 Semantic Indexing (SIN) We predicted the 346 semantic concepts from Semantic Indexing 11 onto the MED keyframes. For details on how we created the models for the 346 concepts, please refer to section 2. Once we have the prediction scores of each concept on each keyframe, we compute a 346 dimensional feature that represents a video. The value of each dimension is the mean value of the concept prediction scores on all keyframes in a given video. We tried out different kinds of score merging techniques, including mean and max, and mean had the best performance. These features are then provided to a classifier for classification.
- Shicheng Xu, Huan Li, Xiaojun Chang, Shoou-I Yu, Xingzhong Du, Xuanchong Li, Lu Jiang, Zexi Mao, Zhenzhong Lan, Susanne Burger, Alexander Hauptmann. 2015. Incremental Multimodal Query Construction for Video Search. Abstract: Recent improvements in content-based video search have led to systems with promising accuracy, thus opening up the possibility for interactive content-based video search to the general public. We present an interactive system based on a state-of-the-art content-based video search pipeline which enables users to do multimodal text-to-video and video-to-video search in large video collections, and to incrementally refine queries through relevance feedback and model visualization. Also, the comprehensive functionalities enhance a flexible formulation of multimodal queries with different characteristics. Quantitative and qualitative analysis shows that our system is capable of assisting users to incrementally build effective queries over complex event topics.
- Lu Jiang, Deyu Meng, Qian Zhao, S. Shan, Alexander Hauptmann. 2015. Self-Paced Curriculum Learning. Abstract: 
 
 Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to "instructor-student-collaborative" learning mode, as opposed to "instructor-driven" in CL or "student-driven" in SPL. Empirically, we show that the advantage of SPCL on two tasks.
 

- Zhenzhong Lan, Alexander Hauptmann. 2015. Beyond Spatial Pyramid Matching: Space-time Extended Descriptor for Action Recognition. Abstract: We address the problem of generating video features for action recognition. The spatial pyramid and its variants have been very popular feature models due to their success in balancing spatial location encoding and spatial invariance. Although it seems straightforward to extend spatial pyramid to the temporal domain (spatio-temporal pyramid), the large spatio-temporal diversity of unconstrained videos and the resulting significantly higher dimensional representations make it less appealing. This paper introduces the space-time extended descriptor, a simple but efficient alternative way to include the spatio-temporal location into the video features. Instead of only coding motion information and leaving the spatio-temporal location to be represented at the pooling stage, location information is used as part of the encoding step. This method is a much more effective and efficient location encoding method as compared to the fixed grid model because it avoids the danger of over committing to artificial boundaries and its dimension is relatively low. Experimental results on several benchmark datasets show that, despite its simplicity, this method achieves comparable or better results than spatio-temporal pyramid.
- Zhuo Chen, Lu Jiang, Wenlu Hu, Kiryong Ha, Brandon Amos, P. Pillai, Alexander Hauptmann, M. Satyanarayanan. 2015. Early Implementation Experience with Wearable Cognitive Assistance Applications. Abstract: A cognitive assistance application combines a wearable device such as Google Glass with cloudlet processing to provide step-by-step guidance on a complex task. In this paper, we focus on user assistance for narrow and well-defined tasks that require specialized knowledge and/or skills. We describe proof-of-concept implementations for four different tasks: assembling 2D Lego models, freehand sketching, playing ping-pong, and recommending context-relevant YouTube tutorials. We then reflect on the difficulties we faced in building these applications, and suggest future research that could simplify the creation of similar applications.
- Xiaojun Chang, Yaoliang Yu, Yi Yang, Alexander Hauptmann. 2015. Searching Persuasively: Joint Event Detection and Evidence Recounting with Limited Supervision. Abstract: Multimedia event detection (MED) and multimedia event recounting (MER) are fundamental tasks in managing large amounts of unconstrained web videos, and have attracted a lot of attention in recent years. Most existing systems perform MER as a post-processing step on top of the MED results. In order to leverage the mutual benefits of the two tasks, we propose a joint framework that simultaneously detects high-level events and localizes the indicative concepts of the events. Our premise is that a good recounting algorithm should not only explain the detection result, but should also be able to assist detection in the first place. Coupled in a joint optimization framework, recounting improves detection by pruning irrelevant noisy concepts while detection directs recounting to the most discriminative evidences. To better utilize the powerful and interpretable semantic video representation, we segment each video into several shots and exploit the rich temporal structures at shot level. The consequent computational challenge is carefully addressed through a significant improvement of the current ADMM algorithm, which, after eliminating all inner loops and equipping novel closed-form solutions for all intermediate steps, enables us to efficiently process extremely large video corpora. We test the proposed method on the large scale TRECVID MEDTest 2014 and MEDTest 2013 datasets, and obtain very promising results for both MED and MER.
- Chuang Gan, Naiyan Wang, Yi Yang, D. Yeung, Alexander Hauptmann. 2015. DevNet: A Deep Event Network for multimedia event detection and evidence recounting. Abstract: In this paper, we focus on complex event detection in internet videos while also providing the key evidences of the detection results. Convolutional Neural Networks (CNNs) have achieved promising performance in image classification and action recognition tasks. However, it remains an open problem how to use CNNs for video event detection and recounting, mainly due to the complexity and diversity of video events. In this work, we propose a flexible deep CNN infrastructure, namely Deep Event Network (DevNet), that simultaneously detects pre-defined events and provides key spatial-temporal evidences. Taking key frames of videos as input, we first detect the event of interest at the video level by aggregating the CNN features of the key frames. The pieces of evidences which recount the detection results, are also automatically localized, both temporally and spatially. The challenge is that we only have video level labels, while the key evidences usually take place at the frame levels. Based on the intrinsic property of CNNs, we first generate a spatial-temporal saliency map by back passing through DevNet, which then can be used to find the key frames which are most indicative to the event, as well as to localize the specific spatial position, usually an object, in the frame of the highly indicative area. Experiments on the large scale TRECVID 2014 MEDTest dataset demonstrate the promising performance of our method, both for event detection and evidence recounting.
- Zhongwen Xu, Linchao Zhu, Yi Yang, Alexander Hauptmann. 2015. UTS-CMU at THUMOS 2015. Abstract: This notebook paper describes our solution from UTSCMU team in the THUMOS 2015 action recognition challenge. Our system contains two major components, video representation generated by VLAD encoding from ConvNet features and multi-skip improved Dense Trajectories. In addition, we explore optical flow ConvNet and acoustic features such as MFCC and ASR in our system. We demonstrate that our complete system can achieve state-of-the-art performance in large-scale action recognition tasks.
- Chuang Gan, Ming Lin, Yi Yang, Yueting Zhuang, Alexander Hauptmann. 2015. Exploring Semantic Inter-Class Relationships (SIR) for Zero-Shot Action Recognition. Abstract: 
 
 Automatically recognizing a large number of action categories from videos is of significant importance for video understanding. Most existing works focused on the design of more discriminative feature representation, and have achieved promising results when the positive samples are enough. However, very limited efforts were spent on recognizing a novel action without any positive exemplars, which is often the case in the real settings due to the large amount of action classes and the users' queries dramatic variations. To address this issue, we propose to perform action recognition when no positive exemplars of that class are provided, which is often known as the zero-shot learning. Different from other zero-shot learning approaches, which exploit attributes as the intermediate layer for the knowledge transfer, our main contribution is SIR, which directly leverages the semantic inter-class relationships between the known and unknown actions followed by label transfer learning. The inter-class semantic relationships are automatically measured by continuous word vectors, which learned by the skip-gram model using the large-scale text corpus. Extensive experiments on the UCF101 dataset validate the superiority of our method over fully-supervised approaches using few positive exemplars.
 

- Shoou-I Yu, Lu Jiang, Zhongwen Xu, Yi Yang, Alexander Hauptmann. 2015. Content-Based Video Search over 1 Million Videos with 1 Core in 1 Second. Abstract: Many content-based video search (CBVS) systems have been proposed to analyze the rapidly-increasing amount of user-generated videos on the Internet. Though the accuracy of CBVS systems have drastically improved, these high accuracy systems tend to be too inefficient for interactive search. Therefore, to strive for real-time web-scale CBVS, we perform a comprehensive study on the different components in a CBVS system to understand the trade-offs between accuracy and speed of each component. Directions investigated include exploring different low-level and semantics-based features, testing different compression factors and approximations during video search, and understanding the time v.s. accuracy trade-off of reranking. Extensive experiments on data sets consisting of more than 1,000 hours of video showed that through a combination of effective features, highly compressed representations, and one iteration of reranking, our proposed system can achieve an 10,000-fold speedup while retaining 80% accuracy of a state-of-the-art CBVS system. We further performed search over 1 million videos and demonstrated that our system can complete the search in 0.975 seconds with a single core, which potentially opens the door to interactive web-scale CBVS for the general public.
- J. D. Aronson, Shicheng Xu, Alexander Hauptmann. 2015. Video Analytics for Conflict Monitoring and Human Rights Documentation. Abstract: In	 this	 technical	 report,	 we	 describe	 how	 a	 powerful	 machine	 learning	 and computer	 vision-­‐based	 video	 analysis	 system	 called	 Event	 Labeling	 through	 Analytic	 Media Processing	 (E-­‐LAMP)	 can	 be	 used	 to	 monitor	 conflicts	 and	 human	 rights	 abuse	 situations. E-­‐LAMP	 searches	 through	 large	 volumes	 of	 video	 for	 objects	 (e.g.,	 weapons,	 military vehicles,	 buildings,	 etc.),	 actions	 (e.g.,	 explosions,	 tank	 movement,	 gunfire,	 structures collapsing,	 etc.),	 written	 text	 (assuming	 it	 can	 be	 processed	 by	 optical	 character	 recognition systems),	 speech	 acts,	 and	 human	 behaviors	 (running,	 crowd	 formation,	 crying,	 screaming, etc.)	 without	 recourse	 to	 metadata.	 It	 can	 also	 identify	 particular	 classes	 of	 people	 such	 as soldiers,	 children,	 or	 corpses.	 We	 first	 describe	 the	 history	 of	 E-­‐LAMP	 and	 explain	 how	 it works.	 We	 then	 provide	 an	 introduction	 to	 building	 novel	 classifiers	 (search	 models)	 for use	 in	 conflict	 monitoring	 and	 human	 rights	 documentation.	 Finally,	 we	 offer	 preliminary accuracy	 data	 on	 four	 test	 classifiers	 we	 built	 in	 the	 context	 of	 the	 Syria	 conflict	 (helicopter, tank,	 corpse,	 and	 gunshots),	 and	 highlight	 the	 limitations	 that	 E-­‐LAMP	 currently	 possesses. Moving	 forward,	 we	 will	 be	 working	 with	 several	 conflict	 monitoring	 and	 human	 rights organizations	 to	 help	 them	 identify	 the	 benefits	 and	 challenges	 of	 implementing	 E-­‐LAMP into	 their	 workflows. Keywords:	 Computer	 Vision,	 Machine	 Learning,	 Human	 Rights,	 Multimedia	 Event Detection,	 Multimedia	 Content	 Analysis Acknowledgements:	 The	 authors	 would	 like	 to	 thank	 the	 MacArthur	 Foundation	 and Humanity	 United	 for	 their	 generous	 support	 of	 this	 research.
- Mengyi Liu, Xin Liu, Yan Li, Xilin Chen, Alexander Hauptmann, S. Shan. 2015. Exploiting Feature Hierarchies with Convolutional Neural Networks for Cultural Event Recognition. Abstract: Cultural events are kinds of typical events closely related to history and nationality, which play an important role in cultural heritage through generations. However, automatically recognizing cultural events still remains a great challenge since it depends on understanding of complex image contents such as people, objects, and scene context. Therefore, it is intuitive to associate this task with other high-level vision problems, e.g., object detection, recognition, and scene understanding. In this paper, we address this problem by combining both ideas of object / scene contents mining and strong image representation via CNN into a whole framework. Specifically, for object / scene contents mining, we employ selective search to extract a batch of bottom-up region proposals, which are served as key object / scene candidates in each event image, while for representation via CNN, we investigate two state-of-the-art deep architectures, VGGNet and GoogLeNet, and adapt them to our task by performing domain-specific (i.e., event) fine-tuning on both global image and hierarchical region proposals. These two models can complementarily exploit feature hierarchies spatially, which simultaneously capture the global context and local evidences within the image. In our final submission for ChaLearn LAP Challenge ICCV 2015, nine kinds of features extracted from five different deep models were exploited and followed with two kinds of classifiers for decision level fusion. Our method achieves the best performance of mAP=0.854 among all the participants in the track of cultural event recognition.
- Ming Lin, Zhenzhong Lan, Alexander Hauptmann. 2015. Density Corrected Sparse Recovery when R.I.P. Condition Is Broken. Abstract: The Restricted Isometric Property (R.I.P.) is a very important condition for recovering sparse vectors from high dimensional space. Traditional methods often rely on R.I.P or its relaxed variants. However, in real applications, features are often correlated to each other, which makes these assumptions too strong to be useful. In this paper, we study the sparse recovery problem in which the feature matrix is strictly non-R.I.P. We prove that when features exhibit cluster structures, which often happens in real applications, we are able to recover the sparse vector consistently. The consistency comes from our proposed density correction algorithm, which removes the variance of estimated cluster centers using cluster density. The proposed algorithm converges geometrically, achieves nearly optimal recovery bound O(s2 log(d)) where s is the sparsity and d is the nominal dimension.
- Xiaojun Chang, Yi Yang, Alexander Hauptmann, E. Xing, Yaoliang Yu. 2015. Semantic Concept Discovery for Large-Scale Zero-Shot Event Detection. Abstract: We focus on detecting complex events in unconstrained Internet videos. While most existing works rely on the abundance of labeled training data, we consider a more difficult zero-shot setting where no training data is supplied. We first pre-train a number of concept classifiers using data from other sources. Then we evaluate the semantic correlation of each concept w.r.t. the event of interest. After further refinement to take prediction inaccuracy and discriminative power into account, we apply the discovered concept classifiers on all test videos and obtain multiple score vectors. These distinct score vectors are converted into pairwise comparison matrices and the nuclear norm rank aggregation framework is adopted to seek consensus. To address the challenging optimization formulation, we propose an efficient, highly scalable algorithm that is an order of magnitude faster than existing alternatives. Experiments on recent TRECVID datasets verify the superiority of the proposed approach.
- Zhenzhong Lan, Shoou-I Yu, Ming Lin, B. Raj, Alexander Hauptmann. 2015. Handcrafted Local Features are Convolutional Neural Networks. Abstract: Image and video classification research has made great progress through the development of handcrafted local features and learning based features. These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history. However, they are typically viewed as distinct approaches. In this paper, we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness. As an example, we study the problem of designing efficient video feature learning algorithms for action recognition. 
We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks (CNNs) share the same convolution-pooling network structure. We then propose a two-stream Convolutional ISA (ConvISA) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm. Through custom designed network structures for pixels and optical flow, our method also reflects distinctive characteristics of these two data sources. 
Our experimental results on standard action recognition benchmarks show that by focusing on the structure of CNNs, rather than end-to-end training methods, we are able to design an efficient and powerful video feature learning algorithm.
- Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, T. Mitamura, Alexander Hauptmann. 2015. Fast and Accurate Content-based Semantic Search in 100 M Internet Videos. Abstract: Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. MM ’15 Brisbane, Australia Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00. and thus the solution of the original problem is
- Jia Chen, Qin Jin, Yong Yu, Alexander Hauptmann. 2015. Image Profiling for History Events on the Fly. Abstract: History event related knowledge is precious and imagery is a powerful medium that records diverse information about the event. In this paper, we propose to automatically construct an image profile given a one sentence description of the historic event which contains where, when, who and what elements. Such a simple input requirement makes our solution easy to scale up and support a wide range of culture preservation and curation related applications ranging from wikipedia enrichment to history education. However, history relevant information on the web is available as "wild and dirty" data, which is quite different from clean, manually curated and structured information sources. There are two major challenges to build our proposed image profiles: 1) unconstrained image genre diversity. We categorize images into genres of documents/maps, paintings or photos. Image genre classification involves a full-spectrum of features from low-level color to high-level semantic concepts. 2) image content diversity. It can include faces, objects and scenes. Furthermore, even within the same event, the views and subjects of images are diverse and correspond to different facets of the event. To solve this challenge, we group images at two levels of granularity: iconic image grouping and facet image grouping. These require different types of features and analysis from near exact matching to soft semantic similarity. We develop a full-range feature analysis module which is composed of several levels, each suitable for different types of image analysis tasks. The wide range of features are based on both classical hand-crafted features and different layers of a convolutional neural network. We compare and study the performance of the different levels in the full-range features and show their effectiveness on handling such a wild, unconstrained dataset.
- Zhiyong Cheng, Xuanchong Li, Jialie Shen, Alexander Hauptmann. 2015. CMU-SMU@TRECVID 2015: Video Hyperlinking. Abstract: In this report, we describe CMU-SMU’s participation in the Video Hyperlinking task of TRECVID 2015. We treat video hyperlinking as ad-hoc retrieval scenario and use a variety of retrieval methods. Our experiments mainly focus on the study of different features on the performance of video hyperlinking, including subtitle, metadata, audio and visual features, as well as the consideration of surrounding context. Different combination strategies are used to combine those features. Besides, we also attempt to categorize the queries and use different search strategies for different categories. Experiments results show that (1) the context does not generally improve results, (2) the search performance mainly rely on textual features, and the combination of audio and visual feature cannot provide improvements; (3) due to the lack of training examples, machine learning techniques cannot provide contributions.
- Lu Jiang, Shoou-I Yu, Deyu Meng, Yi Yang, T. Mitamura, Alexander Hauptmann. 2015. Fast and Accurate Content-based Semantic Search in 100M Internet Videos. Abstract: Large-scale content-based semantic search in video is an interesting and fundamental problem in multimedia analysis and retrieval. Existing methods index a video by the raw concept detection score that is dense and inconsistent, and thus cannot scale to "big data" that are readily available on the Internet. This paper proposes a scalable solution. The key is a novel step called concept adjustment that represents a video by a few salient and consistent concepts that can be efficiently indexed by the modified inverted index. The proposed adjustment model relies on a concise optimization framework with interpretations. The proposed index leverages the text-based inverted index for video retrieval. Experimental results validate the efficacy and the efficiency of the proposed method. The results show that our method can scale up the semantic search while maintaining state-of-the-art search performance. Specifically, the proposed method (with reranking) achieves the best result on the challenging TRECVID Multimedia Event Detection (MED) zero-example task. It only takes 0.2 second on a single CPU core to search a collection of 100 million Internet videos.
- Linchao Zhu, Zhongwen Xu, Yi Yang, Alexander Hauptmann. 2015. Uncovering Temporal Context for Video Question and Answering. Abstract: In this work, we introduce Video Question Answering in temporal domain to infer the past, describe the present and predict the future. We present an encoder-decoder approach using Recurrent Neural Networks to learn temporal structures of videos and introduce a dual-channel ranking loss to answer multiple-choice questions. We explore approaches for finer understanding of video content using question form of "fill-in-the-blank", and managed to collect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD, MEDTest 14 datasets, while the corresponding 390,744 questions are generated from annotations. Extensive experiments demonstrate that our approach significantly outperforms the compared baselines.
- Zhenzhong Lan, Shoou-I Yu, Alexander Hauptmann. 2015. Improving Human Activity Recognition Through Ranking and Re-ranking. Abstract: We propose two well-motivated ranking-based methods to enhance the performance of current state-of-the-art human activity recognition systems. First, as an improvement over the classic power normalization method, we propose a parameter-free ranking technique called rank normalization (RaN). RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD. Second, inspired by curriculum learning, we introduce a training-free re-ranking technique called multi-class iterative re-ranking (MIR). MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly. We demonstrate that our methods significantly improve the performance of state-of-the-art motion features on six real-world datasets.
- Shoou-I Yu, Lu Jiang, Zhongwen Xu, Zhenzhong Lan, Shicheng Xu, Xiaojun Chang, Xuanchong Li, Zexi Mao, Chuang Gan, Yajie Miao, Xingzhong Du, Yang Cai, Lara Martin, Nikolas Wolfe, Anurag Kumar, Hua Li, Ming Lin, Zhigang Ma, Yi Yang, Deyu Meng, Shiguang Shan, Pinar Duygulu Sahin, Susanne Burger, Florian Metze, Rita Singh, Bhiksha Raj, Teruko Mitamura, Richard Stern, Alexander Hauptmann. 2015. CMU Informedia@TRECVID 2015: MED/SIN/LNK/SED. Abstract: We report on our system used in the TRECVID 2015 Multimedia Event Detection (MED) task. On the MED task, the CMU team submitted runs in the Semantic Query (SQ) and 10Ex settings. The proposed system is essentially the same as our MED 2014 system. 1.1 MED System Description On the MED task, the CMU team uses the MED 2014 [1] system which has enabled the system to achieve good performance in the 000Ex and 010Ex settings. Furthermore, our system is very efficient in that it can complete Event Query Generation (EQG) in 16 minutes and Event Search (ES) over 200,000 videos in less than 5 minutes on a single workstation. Please see [1] for the detailed system for the details about our 000Ex and 010Ex runs. The CMU system utilizes multiple modalities, classifiers and fusion methods to perform Multimedia Event Detection. The multiple modalities include visual, audio and text modalities. For 10Ex, two classifiers were used: linear SVM and linear regression. The fusion method used for 010Ex is the Multistage Hybrid Late Fusion, which is a combination of many different fusion algorithms. For the 000Ex runs, we utilize concept detection results from 3000 concept detectors during the SQG and ES stage. We submitted four runs for this year's PS condition: CMU_MED15_MED15EvalFull_PS_10Ex_MED_p-baseline_1: The baseline 10Ex system similar to our 2014 system (using same set of features). CMU_MED15_MED15EvalFull_PS_0Ex_MED_p-expert_1: The 0Ex system using the manual queries selected by experts (using the same queries in our 2014 system). CMU_MED15_MED15EvalFull_PS_0Ex_MED_c-autosqg_1: The 0Ex system using the automatically generated queries. The automatic query generation process is detailed in [3]. CMU_MED15_MED15EvalFull_PS_0Ex_MED_c-autosqgvisualonly_1: The 0Ex system using the automatically generated queries in [3] with only visual features (not including ASR and OCR). 1.2 Hardware Description We utilize the following hardware for metadata generation: 1. PSC Blacklight cluster 100 nodes, each with 4 Intel(R) Xeon(R) CPU E5620 2.40 GHz CPUs (4 cores), 128 GB RAM. Lustre fistributed filesystem, where we used around 50TB. 2. Rocks cluster 20 nodes, each with 2 Intel XEON E5649 2.53 GHz CPUs (6 cores), 64 GB RAM 4 nodes, each with 4 Intel XEON E5-2660 2.20 GHz CPUs (8 cores), 128 GB RAM. 3 nodes, each with 4 NVIDIA TESLA K20 GPUs. 2 data servers, 30TB each For 10Ex event search, we use: 1 Intel(R) Xeon(R) CPU E5-2640 2.50 GHz CPU (12 cores), 128GB RAM, 4 NVIDIA TESLA K20s (2496 cores each), SSD RAID with 4TB storage. For 0Ex event search, we use: 1 Intel(R) Xeon(R) CPU E5649 @ 2.53GHz, 64GB RAM, with a 256GB non-SSD Hard Disk. 1.3 System Performance We report our performance on MED15EvalFull Pre-Specified Events. Runs (MED15EvalFull) Performance MAP% iP10 iP50 infAP200 000Ex autosqgvisualonly 6.4 0.13 0.135 0.0611 000Ex autosqg 7.8 0.2 0.188 0.1005 000Ex expert 15.1 0.38 0.307 0.2137 010Ex baseline 19.2 0.495 0.394 0.2376 Runs (MED15EvalSub) Performance MAP% iP10 iP50 infAP200 000Ex autosqgvisualonly 9.7 0.19 0.148 0.0895 000Ex autosqg 11.0 0.245 0.174 0.1223 000Ex expert 20.6 0.39 0.285 0.246 010Ex baseline 25.5 0.515 0.343 0.2882 Acknowledgments This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI-1053575. Specifically, it used the Blacklight system at the Pittsburgh Supercomputing Center (PSC). This work was supported in part by the US Department of Defense, U. S. Army Research Office (W911NF-13-1-0277) and by the National Science Foundation under Grant No. IIS-1251187. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ARO, the National Science Foundation or the U.S. Government. References [1]$ Yu,$Shoou+I.,$et$al.$"Informedia@$TRECVID$2014$MED$and$MER."$NIST$TRECVID$Video$Retrieval$ Evaluation$Workshop.$2014.$ [2]$ Paul$Over$and$George$Awad$and$Martial$Michel$and$Jonathan$Fiscus$and$Wessel$Kraaij$and$Alan$ F.$Smeaton$and$Georges$Quéenot$and$Roeland$Ordelman.$TRECVID$2015$++$An$Overview$of$the$ Goals,$Tasks,$Data,$Evaluation$Mechanisms$and$Metrics.$Proceedings$of$TRECVID$2015,$2015$ [3]$ Lu$Jiang,$Shoou+I$Yu,$Teruko$Mitamura,$Alexander$Hauptmann.$Bridging$the$Ultimate$Semantic$ Gap:$A$Semantic$Search$Engine$for$Internet$Videos.$In$ACM$International$Conference$on$ Multimedia$Retrieval$(ICMR),$2015.$ $ $ $
- Alexander Hauptmann, James Hodson, Juan-Zi Li, N. Sebe, Achim Rettinger. 2015. Cross-Lingual Cross-Media Content Linking: Annotations and Joint Representations (Dagstuhl Seminar 15201). Abstract: Dagstuhl Seminar 15201 was conducted on "Cross-Lingual Cross-Media Content Linking: Annotations and Joint Representations". Participants from around the world participated in the seminar and presented state-of-the-art and ongoing research related to the seminar topic. An executive summary of the seminar, abstracts of the talks from participants and working group 
discussions are presented in the forthcoming sections.
- Xuanchong Li, K. Chang, Yueran Yuan, Alexander Hauptmann. 2015. Massive Open Online Proctor: Protecting the Credibility of MOOCs certificates. Abstract: Massive Open Online Courses (MOOCs) enable everyone to receive high-quality education. However, current MOOC creators cannot provide an effective, economical, and scalable method to detect cheating on tests, which would be required for any certification. In this paper, we propose a Massive Open Online Proctoring (MOOP) framework, which combines both automatic and collaborative approaches to detect cheating behaviors in online tests. The MOOP framework consists of three major components: Automatic Cheating Detector (ACD), Peer Cheating Detector (PCD), and Final Review Committee (FRC). ACD uses webcam video or other sensors to monitor students and automatically flag suspected cheating behavior. Ambiguous cases are then sent to the PCD, where students peer-review flagged webcam video to confirm suspicious cheating behaviors. Finally, the list of suspicious cheating behaviors is sent to the FRC to make the final punishing decision. Our experiment show that ACD and PCD can detect usage of a cheat sheet with good accuracy and can reduce the overall human resources required to monitor MOOCs for cheating.
- Yan Yan, Yi Yang, Deyu Meng, Gaowen Liu, Wei Tong, Alexander Hauptmann, N. Sebe. 2015. Event Oriented Dictionary Learning for Complex Event Detection. Abstract: Complex event detection is a retrieval task with the goal of finding videos of a particular event in a large-scale unconstrained Internet video archive, given example videos and text descriptions. Nowadays, different multimodal fusion schemes of low-level and high-level features are extensively investigated and evaluated for the complex event detection task. However, how to effectively select the high-level semantic meaningful concepts from a large pool to assist complex event detection is rarely studied in the literature. In this paper, we propose a novel strategy to automatically select semantic meaningful concepts for the event detection task based on both the events-kit text descriptions and the concepts high-level feature descriptions. Moreover, we introduce a novel event oriented dictionary representation based on the selected semantic concepts. Toward this goal, we leverage training images (frames) of selected concepts from the semantic indexing dataset with a pool of 346 concepts, into a novel supervised multitask ℓp-norm dictionary learning framework. Extensive experimental results on TRECVID multimedia event detection dataset demonstrate the efficacy of our proposed method.
- Qian Zhao, Deyu Meng, Lu Jiang, Qi Xie, Zongben Xu, Alexander Hauptmann. 2015. Self-Paced Learning for Matrix Factorization. Abstract: 
 
 Matrix factorization (MF) has been attracting much attention due to its wide applications. However, since MF models are generally non-convex, most of the existing methods are easily stuck into bad local minima, especially in the presence of outliers and missing data. To alleviate this deficiency, in this study we present a new MF learning methodology by gradually including matrix elements into MF training from easy to complex. This corresponds to a recently proposed learning fashion called self-paced learning (SPL), which has been demonstrated to be beneficial in avoiding bad local minima. We also generalize the conventional binary (hard) weighting scheme for SPL to a more effective real-valued (soft) weighting manner. The effectiveness of the proposed self-paced MF method is substantiated by a series of experiments on synthetic, structure from motion and background subtraction data.
 

- Zhenzhong Lan, Xuanchong Li, Ming Lin, Alexander Hauptmann. 2015. Long-short Term Motion Feature for Action Classification and Retrieval. Abstract: We propose a method for representing motion information for video classification and retrieval. We improve upon local descriptor based methods that have been among the most popular and successful models for representing videos. The desired local descriptors need to satisfy two requirements: 1) to be representative, 2) to be discriminative. Therefore, they need to occur frequently enough in the videos and to be be able to tell the difference among different types of motions. To generate such local descriptors, the video blocks they are based on must contain just the right amount of motion information. However, current state-of-the-art local descriptor methods use video blocks with a single fixed size, which is insufficient for covering actions with varying speeds. In this paper, we introduce a long-short term motion feature that generates descriptors from video blocks with multiple lengths, thus covering motions with large speed variance. Experimental results show that, albeit simple, our model achieves state-of-the-arts results on several benchmark datasets.
- Zhenzhong Lan, Dezhong Yao, Ming Lin, Shoou-I Yu, Alexander Hauptmann. 2015. The Best of BothWorlds: Combining Data-Independent and Data-Driven Approaches for Action Recognition. Abstract: Motivated by the success of CNNs in object recognition on images, researchers are striving to develop CNN equivalents for learning video features. However, learning video features globally has proven to be quite a challenge due to the difficulty of getting enough labels, processing large-scale video data, and representing motion information. Therefore, we propose to leverage effective techniques from both data-driven and data-independent approaches to improve action recognition system. Our contribution is three-fold. First, we explicitly show that local handcrafted features and CNNs share the same convolution-pooling network structure. Second, we propose to use independent subspace analysis (ISA) to learn descriptors for state-of-the-art handcrafted features. Third, we enhance ISA with two new improvements, which make our learned descriptors significantly outperform the handcrafted ones. Experimental results on standard action recognition benchmarks show competitive performance.
- Xingzhong Du, Xuanchong Li, Xiaofang Zhou, Alexander Hauptmann. 2015. Ward-cmu @ Trecvid 2015. Abstract: We present a retrospective system for event detection in surveillance videos automatically, which is built on the Gatwick development data. It is an enhanced version of the retrospective system in [1]. The changes come from four aspects. First, dense trajectory [2] and improved dense trajectory [3] are used together in
- Zhongwen Xu, Duo Ding, Waito Sze, Francisco Vicente, Zhenzhong Lan, Yang Cai, Lu Jiang, Shourabh Rawat, Peter Schulam, Sohail Bahmani, Antonio Juarez, Wei Tong, Yezhou Yang, Susanne Burger, Florian Metze, Rita Singh, Bhiksha Raj, Richard M. Stern, Teruko Mitamura, Eric Nyberg, Alexander Hauptmann, Qiang Chen, Lisa Brown, A. Datta, Quanfu Fan, R. Feris, Shuicheng Yan, Sharath Pankanti. 2015. Informedia E-Lamp@TRECVID 2012: Multimedia Event Detection and Recounting (MED and MER). Abstract: In the first part of this report we describe our system and novel approaches used in the TRECVID 2012 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. A separate section of the report (SIN) details methods and results for the Semantic Indexing task. The final section (SED) describes our approaches and results on the Surveillance Event Detection task.
- Lu Jiang, Yajie Miao, Yezhou Yang, Zhenzhong Lan, Alexander Hauptmann. 2014. Viral Video Style: A Closer Look at Viral Videos on YouTube. Abstract: Viral videos that gain popularity through the process of Internet sharing are having a profound impact on society. Existing studies on viral videos have only been on small or confidential datasets. We collect by far the largest open benchmark for viral video study called CMU Viral Video Dataset, and share it with researchers from both academia and industry. Having verified existing observations on the dataset, we discover some interesting characteristics of viral videos. Based on our analysis, in the second half of the paper, we propose a model to forecast the future peak day of viral videos. The application of our work is not only important for advertising agencies to plan advertising campaigns and estimate costs, but also for companies to be able to quickly respond to rivals in viral marketing campaigns. The proposed method is unique in that it is the first attempt to incorporate video metadata into the peak day prediction. The empirical results demonstrate that the proposed method outperforms the state-of-the-art methods, with statistically significant differences.
- Zhongwen Xu, Yi Yang, Alexander Hauptmann. 2014. A discriminative CNN video representation for event detection. Abstract: In this paper, we propose a discriminative video representation for event detection over a large scale video dataset when only limited hardware resources are available. The focus of this paper is to effectively leverage deep Convolutional Neural Networks (CNNs) to advance event detection, where only frame level static descriptors can be extracted by the existing CNN toolkits. This paper makes two contributions to the inference of CNN video representation. First, while average pooling and max pooling have long been the standard approaches to aggregating frame level static features, we show that performance can be significantly improved by taking advantage of an appropriate encoding method. Second, we propose using a set of latent concept descriptors as the frame descriptor, which enriches visual information while keeping it computationally affordable. The integration of the two contributions results in a new state-of-the-art performance in event detection over the largest video datasets. Compared to improved Dense Trajectories, which has been recognized as the best video representation for event detection, our new representation improves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVID MEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset.
- Zhenzhong Lan, Ming Lin, Xuanchong Li, Alexander Hauptmann, B. Raj. 2014. Beyond Gaussian Pyramid: Multi-skip Feature Stacking for action recognition. Abstract: Most state-of-the-art action feature extractors involve differential operators, which act as highpass filters and tend to attenuate low frequency action information. This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices. The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation. However, at the core of the Gaussian Pyramid is a convolutional smoothing operation, which makes it incapable of generating new features at coarse scales. In order to address this problem, we propose a novel feature enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space. MIFS compensates for information lost from using differential operators by recapturing information at coarse scales. This recaptured information allows us to match actions at different speeds and ranges of motion. We prove that MIFS enhances the learnability of differential-based features exponentially. The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods. Experimental results show significantly improved performance on challenging action recognition and event detection tasks. Specifically, our method exceeds the state-of-the-arts on Hollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts on HMDB51 and Olympics Sports datasets. MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost.
- Lu Jiang, Wei Tong, Deyu Meng, Alexander Hauptmann. 2014. Towards Efficient Learning of Optimal Spatial Bag-of-Words Representations. Abstract: Spatial Pyramid Matching (SPM) assumes that the spatial Bag-of-Words (BoW) representation is independent of data. However, evidence has shown that the assumption usually leads to a suboptimal representation. In this paper, we propose a novel method called Jensen-Shannon (JS) Tiling to learn the BoW representation from data directly at the BoW level. The proposed JS Tiling is especially appropriate for large-scale datasets as it is orders of magnitude faster than existing methods, but with comparable or even better classification precision. Experimental results on four benchmarks including two TRECVID12 datasets validate that JS Tiling outperforms the SPM and the state-of-the-art methods. The runtime comparison demonstrates that selecting BoW representations by JS Tiling is more than 1,000 times faster than running classifiers. Besides, JS Tiling is an important component contributing to CMU Teams' final submission in TRECVID 2012 Multimedia Event Detection.
- Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, S. Shan, Alexander Hauptmann. 2014. Self-Paced Learning with Diversity. Abstract: Self-paced learning (SPL) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training. Existing methods are limited in that they ignore an important aspect in learning: diversity. To incorporate this information, we propose an approach called self-paced learning with diversity (SPLD) which formalizes the preference for both easy and diverse samples into a general regularizes This regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks. Albeit non-convex, the optimization of the variables included in this SPLD regularization term for sample selection can be globally solved in linearithmic time. We demonstrate that our method significantly outperforms the conventional SPL on three real-world datasets. Specifically, SPLD achieves the best MAP so far reported in literature on the Hollywood2 and Olympic Sports datasets.
- Gaowen Liu, Yan Yan, Chenqiang Gao, Wei Tong, Alexander Hauptmann, N. Sebe. 2014. The Mystery of Faces: Investigating Face Contribution for Multimedia Event Detection. Abstract: Multimedia event detection (MED) is a retrieval task with the goal of finding videos of a particular event in a large scale internet video archive, given example videos and text descriptions. Nowadays, different multimodal fusion schemes of low-level and high-level features are extensively investigated and evaluated for MED. For most of events in MED, people are usually the central subjects in videos. The face of a person can be considered as the most important factor which brings a lot of information describing the video events. However, face information has not been systematically investigated in the previous research for MED. In this paper, we investigate the possibility of using the high-level face information to assist multimedia event detection. Moreover, since the labeled data in TRECVID MED dataset are limited, we propose a semi-supervised kernel ridge regression which works well in practice to explore the useful information from unlabeled data to assist the event detection. Extensive experimental results on TRECVID MED dataset show that our proposed method outperforms the state-of-the-art methods by up to 4%.
- Sen Wang, Zhigang Ma, Yi Yang, Xue Li, C. Pang, Alexander Hauptmann. 2014. Semi-Supervised Multiple Feature Analysis for Action Recognition. Abstract: This paper presents a semi-supervised method for categorizing human actions using multiple visual features. The proposed algorithm simultaneously learns multiple features from a small number of labeled videos, and automatically utilizes data distributions between labeled and unlabeled data to boost the recognition performance. Shared structural analysis is applied in our approach to discover a common subspace shared by each type of feature. In the subspace, the proposed algorithm is able to characterize more discriminative information of each feature type. Additionally, data distribution information of each type of feature has been preserved. The aforementioned attributes make our algorithm robust for action recognition, especially when only limited labeled training samples are provided. Extensive experiments have been conducted on both the choreographed and the realistic video datasets, including KTH, Youtube action and UCF50. Experimental results show that our method outperforms several state-of-the-art algorithms. Most notably, much better performances have been achieved when there are only a few labeled training samples.
- Changyu Liu, Huiling Li, Alexander Hauptmann, Cong Li. 2014. Modeling Physical and Chemical Growths of Avascular Tumor. Abstract: Due to the avascular tumor growths were a complex activities that involved a chemical process and a physical process, a Finite Element Method (FEM) based mixture model was proposed in this paper. For the chemical process, the FEM was used to solve the reaction diffusion equations of three chemicals, which are the oxygen, glucose and GIF, under several specified boundary conditions. For the physical process, the FEM was used to solve the total energy equations which can be uniquely calculated as a function of the nodal displacements in the tumor. In each simulation step, we minimize the energy increment for all the tumor cells. The experiment results demonstrated that the proposed approach is quite efficient for the modeling of avascular tumor growth.
- Changyu Liu, Shoubin Dong, Bin Lu, Alexander Hauptmann, Cong Li. 2014. Study on Adaptive and Fuzzy Weighted Image Fusion Based on Wavelet Transform in Trinocular Vision of Picking Robot. Abstract: In order to improve the adaptive recognition abilities of picking robot in complex environment, a fusion approach of trinocular vision in wavelet domain based on fuzzy reasoning weight was proposed. Firstly, membership functions of fusion rules are determined by fuzzy reasoning of picking environmental features, and membership values of fusion types are calculated according to regional energy and match degree of origin images. Based on the maximum membership degree principle, fuzzy decision is carried on to determine the fusion types and fusion weight. Secondly, the mean weighted method and regional energy feature method are adopted respectively to carry on the low frequency as well as high frequency coefficients fusion among multi-source images by using two-level 2D wavelet, and the final fusion images are attained by inverse HIS transform based on inverse wavelet transform. Four groups of experiment show that in the complex picking environment like weak illumination and strong noise, the information entropy and average gradient of fused image that obtained by using the wavelet fusion method based on fuzzy reasoning weight are higher than that of traditional mean method, pyramid algorithm and wavelet packet method, which means that the fusion effect has been improved greatly.
- N. Boujemaa, Alexander Hauptmann, S. Satoh. 2014. 湘南会議 The future of multimedia analysis and mining. Abstract: Recent explosive growth of the amount of accessible multimedia information requires far more intelligent access to multimedia data. Multimedia analysis and mining play a key role to address this problem. For instance, multimedia analysis enables semantic access to multimedia information at any description level and for any applications or needs, even though the original multimedia data may not have any prior semantic annotation. Multimedia mining helps to provide highlevel semantic and structural information to expose key information within a large-scale multimedia database. However, the development of such technologies is often severely limited due to the famous “Semantic Gap” in multimedia content analysis. This is well-known as a supremely difficult issue that is very hard to overcome. On the other hand, researchers in this field now have access to far more computational resources thanks to recent developments in GPU use, multi-core technologies or the availability of cloud computing, as well as far more data resources thanks to the explosive growth of available multimedia data especially via Web. Several research projects have already begun to take advantage of these points independently. Based on the objective, we organized shonan meeting on “The Future of Multimedia Analysis and Mining,” from 3 to 6, November, 2012. In this meeting, we aim to discuss recent research trends and their impact on multimedia research. Then we consolidate key research challenges and explore promising new research directions, hopefully toward “Bridging the semantic gap.” Following the meeting, we organized the special issue on “The Future of Multimedia Analysis and Mining” in the Progress in Informatics. Important topics include the following:
- Zhenzhong Lan, Xuanchong Li, Alexander Hauptmann. 2014. Temporal Extension of Scale Pyramid and Spatial Pyramid Matching for Action Recognition. Abstract: Historically, researchers in the field have spent a great deal of effort to create image representations that have scale invariance and retain spatial location information. This paper proposes to encode equivalent temporal characteristics in video representations for action recognition. To achieve temporal scale invariance, we develop a method called temporal scale pyramid (TSP). To encode temporal information, we present and compare two methods called temporal extension descriptor (TED) and temporal division pyramid (TDP) . Our purpose is to suggest solutions for matching complex actions that have large variation in velocity and appearance, which is missing from most current action representations. The experimental results on four benchmark datasets, UCF50, HMDB51, Hollywood2 and Olympic Sports, support our approach and significantly outperform state-of-the-art methods. Most noticeably, we achieve 65.0% mean accuracy and 68.2% mean average precision on the challenging HMDB51 and Hollywood2 datasets which constitutes an absolute improvement over the state-of-the-art by 7.8% and 3.9%, respectively.
- Zhigang Ma, Yi Yang, N. Sebe, Alexander Hauptmann. 2014. Knowledge Adaptation with PartiallyShared Features for Event DetectionUsing Few Exemplars. Abstract: Multimedia event detection (MED) is an emerging area of research. Previous work mainly focuses on simple event detection in sports and news videos, or abnormality detection in surveillance videos. In contrast, we focus on detecting more complicated and generic events that gain more users' interest, and we explore an effective solution for MED. Moreover, our solution only uses few positive examples since precisely labeled multimedia content is scarce in the real world. As the information from these few positive examples is limited, we propose using knowledge adaptation to facilitate event detection. Different from the state of the art, our algorithm is able to adapt knowledge from another source for MED even if the features of the source and the target are partially different, but overlapping. Avoiding the requirement that the two domains are consistent in feature types is desirable as data collection platforms change or augment their capabilities and we should be able to respond to this with little or no effort. We perform extensive experiments on real-world multimedia archives consisting of several challenging events. The results show that our approach outperforms several other state-of-the-art detection algorithms.
- Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, S. Shan, Alexander Hauptmann. 2014. Supplementary Materials : Self-Paced Learning with Diversity. Abstract: This is the supplementary material for the paper entitled “Self-Paced Learning with Diversity”. The material is organized as follows: Section 1 gives the proof of Theorem 1. Section 2.1 and Section 2.2 present the detailed experimental settings and results on the MED (Multimedia Event Detection) dataset. Section 2.3 and Section 2.4 present the settings and detailed results on the Hollywood2 and Olympic datasets. Section 3 briefly discusses our practical lessons and the observed deficiency of the SPL/SPLD models. 1 Proof of Theorem 1 We present the proof of Theorem 1 in the paper. Given the training dataset D = {(x1, y1), · · · , (xn, yn)}, where xi ∈ R denotes the i observed sample and yi denotes its label. Assume that the training samples X = [x1, · · · ,xn] are with b groups: X, · · · ,X, where X (j) = (x (j) 1 , · · · ,x (j) nj ) ∈ Rm×nj corresponds to samples in the j group, nj is the sample number in this group and ∑b j=1 nj = n. Accordingly, denote the weight vector as v = [v , · · · ,v], where v = (v (j) 1 , · · · , v (j) nj ) T ∈ Rj . The following theorem proves that Algorithm 1 can get the global solution of the following non-convex optimization problem: min v∈[0,1]n E(w,v;λ, γ) = n ∑ i=1 viL(yi, f(xi,w))− λ n ∑ i=1 vi − γ‖v‖2,1, (1) where L(yi, f(xi,w)) denotes the loss function which calculates the cost between the ground truth label yi and the estimated label f(xi,w), and the l2,1-norm ‖v‖2,1 is the group sparsity of v: ‖v‖2,1 = b ∑
- Shoou-I Yu, Lu Jiang, Zhongwen Xu, Zhenzhong Lan, Shicheng Xu, Xiaojun Chang, Xuanchong Li, Zexi Mao, Chuang Gan, Yajie Miao, Xingzhong Du, Yang Cai, Lara J. Martin, Nikolas Wolfe, Anurag Kumar, Huan Li, Ming Lin, Zhigang Ma, Yi Yang, Deyu Meng, S. Shan, P. D. Sahin, Susanne Burger, Florian Metze, Rita Singh, B. Raj, T. Mitamura, R. Stern, Alexander Hauptmann. 2014. Informedia@TrecVID 2014: MED and MER. Abstract: We report on our system used in the TRECVID 2014 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. On the MED task, the CMU team achieved leading performance in the Semantic Query (SQ), 000Ex, 010Ex and 100Ex settings. Furthermore, SQ and 000Ex runs are significantly better than the submissions from the other teams. We attribute the good performance to 4 main components: 1) our large-scale semantic concept detectors trained on video shots for SQ/000Ex systems, 2) better features such as improved trajectories and deep learning features for 010Ex/100Ex systems, 3) a novel Multistage Hybrid Late Fusion method for 010Ex/100Ex systems and 4) our developed reranking methods for Pseudo Relevance Feedback for 000Ex/010Ex systems. On the MER task, our system utilizes a subset of features and detection results from the MED system from which the recounting is then generated. Recounting evidence is presented by selecting the most likely concepts detected in the salient shots of a video. Salient shots are detected by searching for shots which have high response when predicted by the video level event detector.
- Lu Jiang, Deyu Meng, T. Mitamura, Alexander Hauptmann. 2014. Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search. Abstract: Reranking has been a focal technique in multimedia retrieval due to its efficacy in improving initial retrieval results. Current reranking methods, however, mainly rely on the heuristic weighting. In this paper, we propose a novel reranking approach called Self-Paced Reranking (SPaR) for multimodal data. As its name suggests, SPaR utilizes samples from easy to more complex ones in a self-paced fashion. SPaR is special in that it has a concise mathematical objective to optimize and useful properties that can be theoretically verified. It on one hand offers a unified framework providing theoretical justifications for current reranking methods, and on the other hand generates a spectrum of new reranking schemes. This paper also advances the state-of-the-art self-paced learning research which potentially benefits applications in other fields. Experimental results validate the efficacy and the efficiency of the proposed method on both image and video search tasks. Notably, SPaR achieves by far the best result on the challenging TRECVID multimedia event search task.
- J. Chiu, Alexander Hauptmann. 2014. Speech Retrieval under Limited Resources and Open Domain Conditions. Abstract: Speech Retrieval focuses on retrieving a segment of speech from a speech corpus correspond to a given query. A standard Speech Retrieval system usually composed by two systems, the Automatic Speech Recognition (ASR) system and the Information Retrieval (IR) system. The ASR system transcribes the speech and represents the transcript in different formats. The transcript is then indexed and searched by the IR system. As a result, Speech Retrieval is sensitive to the ASR, since IR system is depend on the transcript generated by ASR. The current challenge in Speech Retrieval is the limitation of ASR performance under certain conditions. Two such conditions are Limited Resources and Open Domain. Under Limited resources condition, the training data is not sufficient for creating a robust ASR system. A good example for this condition is to perform Speech Retrieval on limited resources languages such as Tagalog or Assamese. On the other hand, under Open Domain condition, the recorded speech varies in many perspectives. The high diversity of recorded speech limits the performance of a single ASR system. A good example for this condition is to perform Speech Retrieval on YouTube videos, such as online lectures. We believe Speech Retrieval under these conditions can be significantly improved from different approaches. The first one is to apply extra information, such as contexts from conversation. A context includes the other words in the same utterance or conversation. The second approach is to refine the existing IR system, by using better IR search strategy for Speech Retrieval. We analysis existing IR system and present a better search strategy, which is based on the diversity of current approaches. We have investigated how to integrate these two approaches to Speech Retrieval, and determined that the approaches can achieve improvement on Spoken Term Detection (STD) under the limited resources condition. The resulting system has been shown to be effective on multiple languages, implying that the improvement is language independent. Based on our positive result regarding the limited resources condition, we propose to extend existing approaches and develop new techniques for better Speech Retrieval under the open domain condition. We propose a new Speech Retrieval task called Spoken Snippet Retrieval (SSR), which retrieve a moderate size of speech from the speech collection with just enough context. The retrieved snippet is easier for user to listen through compare to the spoken document retrieved by Spoken Document Retrieval (SDR) systems, which has average length of 3 minutes. The snippet is more comprehensible compare to the term location detected by STD systems, since the context are given. The main contribution for the thesis is to complete SSR on the open domain data, which we believe is doing the adequate retrieval on the appropriate data.
- Zhongwen Xu, I. Tsang, Yi Yang, Zhigang Ma, Alexander Hauptmann. 2014. Event Detection Using Multi-level Relevance Labels and Multiple Features. Abstract: We address the challenging problem of utilizing related exemplars for complex event detection while multiple features are available. Related exemplars share certain positive elements of the event, but have no uniform pattern due to the huge variance of relevance levels among different related exemplars. None of the existing multiple feature fusion methods can deal with the related exemplars. In this paper, we propose an algorithm which adaptively utilizes the related exemplars by cross-feature learning. Ordinal labels are used to represent the multiple relevance levels of the related videos. Label candidates of related exemplars are generated by exploring the possible relevance levels of each related exemplar via a cross-feature voting strategy. Maximum margin criterion is then applied in our framework to discriminate the positive and negative exemplars, as well as the related exemplars from different relevance levels. We test our algorithm using the large scale TRECVID 2011 dataset and it gains promising performance.
- Shoou-I Yu, Lu Jiang, Alexander Hauptmann. 2014. Instructional Videos for Unsupervised Harvesting and Learning of Action Examples. Abstract: Online instructional videos have become a popular way for people to learn new skills encompassing art, cooking and sports. As watching instructional videos is a natural way for humans to learn, analogously, machines can also gain knowledge from these videos. We propose to utilize the large amount of instructional videos available online to harvest examples of various actions in an unsupervised fashion. The key observation is that in instructional videos, the instructor's action is highly correlated with the instructor's narration. By leveraging this correlation, we can exploit the timing of action corresponding terms in the speech transcript to temporally localize actions in the video and harvest action examples. The proposed method is scalable as it requires no human intervention. Experiments show that the examples harvested are of reasonably good quality, and action detectors trained on data collected by our unsupervised method yields comparable performance with detectors trained with manually collected data on the TRECVID Multimedia Event Detection task.
- Changyu Liu, Shoubin Dong, Huiling Li, Bin Lu, Alexander Hauptmann. 2014. A TPSAC Model and Its Application to Mechanical Cloud Simulation. Abstract: As a further development of the simulation grid, the cloud simulation platform is a new kind of network modeling as well as simulation platforms, and one of the hottest research directions in the cloud computing. While brings about the users with a lot of convenience, the cloud simulation shows also many severe security issues with its own characteristics, which can’t be solved effectively by the traditional access control strategies. According to the traditional role based access control(RBAC) model, this paper proposed a tree proxy-based and service-oriented access control(TPSAC) model. In the TPSAC model, a multilevel inherited meta permission and a multi-tree child-sibling linked list were adopted to separate the permission loading function and the permission distribution function to achieve a multi-granularity and quantized access control with the cloud simulation. A verification experiment on the CloudSim simulation platform was conducted then to demonstrate that the TPSAC model achieved the desired result.
- Yue Gao, R. Ji, Longfei Zhang, Alexander Hauptmann. 2014. Symbiotic Tracker Ensemble Toward A Unified Tracking Framework. Abstract: Tracking people and objects is a fundamental stage toward many video surveillance systems, for which various trackers have been specifically designed in the past decade. However, it comes to a consensus that there is not any specific tracker that works sufficiently well under all circumstances. Therefore, one potential solution is to deploy multiple trackers, with a tracker output fusion step to boost the overall performance. Subsequently, an intelligent fusion design, yet general and orthogonal to any specific tracker, plays a key role in successful tracking. In this paper, we propose a symbiotic tracker ensemble toward a unified tracking framework, which is based on only the output of each individual tracker, without knowing its specific mechanism. In our approach, all trackers run in parallel, without requiring any details for tracker running, which means that all trackers are treated as black boxes. The proposed symbiotic tracker ensemble framework aims at learning an optimal combination of these tracking results. Our method captures the relation among individual trackers robustly from two aspects. First, the consistency between two successive frames is calculated for each tracker. Then, the pair-wise correlation among different trackers is estimated in the new coming frame by a graph-propagation process. Experimental results on the Caremedia dataset and the Caviar dataset demonstrate the effectiveness of the proposed method, with comparisons to several state-of-the-art methods.
- N. Boujemaa, Alexander Hauptmann, S. Satoh. 2014. The future of multimedia analysis and mining Orgamizers :. Abstract: Shonan meeting on “The Future of Multimedia Analysis and Mining,” was organized from 3 to 6, November, 2012. This technical report summarizes the program of the meeting for
- Zhigang Ma, Yi Yang, N. Sebe, Alexander Hauptmann. 2014. Multiple Features But Few Labels?: A Symbiotic Solution Exemplified for Video Analysis. Abstract: Video analysis has been attracting increasing research due to the proliferation of internet videos. In this paper, we investigate how to improve the performance on internet quality video analysis. Particularly, we work on the scenario of few labeled training videos being provided, which is less focused in multimedia. To being with, we consider how to more effectively harness the evidences from the low-level features. Researchers have developed several promising features to represent videos to capture the semantic information. However, as videos usually characterize rich semantic contents, the analysis performance by using one single feature is potentially limited. Simply combining multiple features through early fusion or late fusion to incorporate more informative cues is doable but not optimal due to the heterogeneity and different predicting capability of these features. For better exploitation of multiple features, we propose to mine the importance of different features and cast it into the learning of the classification model. Our method is based on multiple graphs from different features and uses the Riemannian metric to evaluate the feature importance. On the other hand, to be able to use limited labeled training videos for a respectable accuracy we formulate our method in a semi-supervised way. The main contribution of this paper is a novel scheme of evaluating the feature importance that is further casted into a unified framework of harnessing multiple weighted features with limited labeled training videos. We perform extensive experiments on video action recognition and multimedia event recognition and the comparison to other state-of-the-art multi-feature learning algorithms has validated the efficacy of our framework.
- Lu Jiang, T. Mitamura, Shoou-I Yu, Alexander Hauptmann. 2014. Zero-Example Event Search using MultiModal Pseudo Relevance Feedback. Abstract: We propose a novel method MultiModal Pseudo Relevance Feedback (MMPRF) for event search in video, which requires no search examples from the user. Pseudo Relevance Feedback has shown great potential in retrieval tasks, but previous works are limited to unimodal tasks with only a single ranked list. To tackle the event search task which is inherently multimodal, our proposed MMPRF takes advantage of multiple modalities and multiple ranked lists to enhance event search performance in a principled way. The approach is unique in that it leverages not only semantic features, but also non-semantic low-level features for event search in the absence of training data. Evaluated on the TRECVID MEDTest dataset, the approach improves the baseline by up to 158% in terms of the mean average precision. It also significantly contributes to CMU Team's final submission in TRECVID-13 Multimedia Event Detection.
- Chenqiang Gao, Deyu Meng, Wei Tong, Yi Yang, Yang Cai, Haoquan Shen, Gaowen Liu, Shicheng Xu, Alexander Hauptmann. 2014. Interactive Surveillance Event Detection through Mid-level Discriminative Representation. Abstract: Event detection from real surveillance videos with complicated background environment is always a very hard task. Different from the traditional retrospective and interactive systems designed on this task, which are mainly executed on video fragments located within the event-occurrence time, in this paper we propose a new interactive system constructed on the mid-level discriminative representations (patches/shots) which are closely related to the event (might occur beyond the event-occurrence period) and are easier to be detected than video fragments. By virtue of such easily-distinguished mid-level patterns, our framework realizes an effective labor division between computers and human participants. The task of computers is to train classifiers on a bunch of mid-level discriminative representations, and to sort all the possible mid-level representations in the evaluation sets based on the classifier scores. The task of human participants is then to readily search the events based on the clues offered by these sorted mid-level representations. For computers, such mid-level representations, with more concise and consistent patterns, can be more accurately detected than video fragments utilized in the conventional framework, and on the other hand, a human participant can always much more easily search the events of interest implicated by these location-anchored mid-level representations than conventional video fragments containing entire scenes. Both of these two properties facilitate the availability of our framework in real surveillance event detection applications.
- Zhigang Ma, Yi Yang, Zhongwen Xu, Shuicheng Yan, N. Sebe, Alexander Hauptmann. 2013. Complex Event Detection via Multi-source Video Attributes. Abstract: Complex events essentially include human, scenes, objects and actions that can be summarized by visual attributes, so leveraging relevant attributes properly could be helpful for event detection. Many works have exploited attributes at image level for various applications. However, attributes at image level are possibly insufficient for complex event detection in videos due to their limited capability in characterizing the dynamic properties of video data. Hence, we propose to leverage attributes at video level (named as video attributes in this work), i.e., the semantic labels of external videos are used as attributes. Compared to complex event videos, these external videos contain simple contents such as objects, scenes and actions which are the basic elements of complex events. Specifically, building upon a correlation vector which correlates the attributes and the complex event, we incorporate video attributes latently as extra informative cues into the event detector learnt from complex event videos. Extensive experiments on a real-world large-scale dataset validate the efficacy of the proposed approach.
- Yi Yang, Zhigang Ma, Alexander Hauptmann, N. Sebe. 2013. Feature Selection for Multimedia Analysis by Sharing Information Among Multiple Tasks. Abstract: While much progress has been made to multi-task classification and subspace learning, multi-task feature selection has long been largely unaddressed. In this paper, we propose a new multi-task feature selection algorithm and apply it to multimedia (e.g., video and image) analysis. Instead of evaluating the importance of each feature individually, our algorithm selects features in a batch mode, by which the feature correlation is considered. While feature selection has received much research attention, less effort has been made on improving the performance of feature selection by leveraging the shared knowledge from multiple related tasks. Our algorithm builds upon the assumption that different related tasks have common structures. Multiple feature selection functions of different tasks are simultaneously learned in a joint framework, which enables our algorithm to utilize the common knowledge of multiple tasks as supplementary information to facilitate decision making. An efficient iterative algorithm is proposed to optimize it, whose convergence is guaranteed. Experiments on different databases have demonstrated the effectiveness of the proposed algorithm.
- Wei Liu, Alexander Hauptmann. 2013. A Crowdsourcing Approach to Tracker Fusion. Abstract: There are many tracking methods been proposed, using different features and algorithms, but none of them can track object correctly all the time. In this paper, we explore the idea of combining a crowd of trackers, and propose a crowdsourcing tracking method. We model the problem under the Sequential Monte Carlo framework, where we treat different trackers outputs, the bounding boxes, as weak observations, and use the wisdom-of-the-crowds to simultaneously infer both the hidden ground truth bounding box and the corresponding time-varying confidence for each tracker. We have tested our proposed method on two public surveillance video datasets and two of our own video datasets. The results show that the crowdsourcing tracking method can provide more stable and better performance.
- Ehsan Younessian, Michael Quinn, T. Mitamura, Alexander Hauptmann. 2013. Multimedia event detection using visual concept signatures. Abstract: Multimedia Event Detection (MED) is a multimedia retrieval task with the goal of finding videos of a particular event in a large-scale Internet video archive, given example videos and text descriptions. In this paper, we mainly focus on an 'ad-hoc' scenario in MED where we do not use any example video. We aim to retrieve test videos based on their visual semantics using a Visual Concept Signature (VCS) generated for each event only derived from the event description provided as the query. Visual semantics are described using the Semantic INdexing (SIN) feature which represents the likelihood of predefined visual concepts in a video. To generate a VCS for an event, we project the given event description to a visual concept list using the proposed textual semantic similarity. Exploring SIN feature properties, we harmonize the generated visual concept signature and the SIN feature to improve retrieval performance. We conduct different experiments to assess the quality of generated visual concept signatures with respect to human expectation, and in the context of the MED task to retrieve the SIN feature of videos in the test dataset when we have no or only very few training videos.
- Yi Yang, Jingkuan Song, Zi Huang, Zhigang Ma, N. Sebe, Alexander Hauptmann. 2013. Multi-Feature Fusion via Hierarchical Regression for Multimedia Analysis. Abstract: Multimedia data are usually represented by multiple features. In this paper, we propose a new algorithm, namely Multi-feature Learning via Hierarchical Regression for multimedia semantics understanding, where two issues are considered. First, labeling large amount of training data is labor-intensive. It is meaningful to effectively leverage unlabeled data to facilitate multimedia semantics understanding. Second, given that multimedia data can be represented by multiple features, it is advantageous to develop an algorithm which combines evidence obtained from different features to infer reliable multimedia semantic concept classifiers. We design a hierarchical regression model to exploit the information derived from each type of feature, which is then collaboratively fused to obtain a multimedia semantic concept classifier. Both label information and data distribution of different features representing multimedia data are considered. The algorithm can be applied to a wide range of multimedia applications and experiments are conducted on video data for video concept annotation and action recognition. Using Trecvid and CareMedia video datasets, the experimental results show that it is beneficial to combine multiple features. The performance of the proposed algorithm is remarkable when only a small amount of labeled training data are available.
- Yang Cai, Yi Yang, Alexander Hauptmann, H. Wactlar. 2013. A cognitive assistive system for monitoring the use of home medical devices. Abstract: Despite the popularity of home medical devices, serious safety concerns have been raised, because the use-errors of home medical devices have linked to a large number of fatal hazards. To resolve the problem, we introduce a cognitive assistive system to automatically monitor the use of home medical devices. Being able to accurately recognize user operations is one of the most important functionalities of the proposed system. However, even though various action recognition algorithms have been proposed in recent years, it is still unknown whether they are adequate for recognizing operations in using home medical devices. Since the lack of the corresponding database is the main reason causing the situation, at the first part of this paper, we present a database specially designed for studying the use of home medical devices. Then, we evaluate the performance of the existing approaches on the proposed database. Although using state-of-art approaches which have demonstrated near perfect performance in recognizing certain general human actions, we observe significant performance drop when applying it to recognize device operations. We conclude that the tiny action involved in using devices is one of the most important reasons leading to the performance decrease. To accurately recognize tiny actions, it's critical to focus on where the target action happens, namely the region of interest(ROI) and have more elaborate action modeling based on the ROI. Therefore, in the second part of this paper, we introduce a simple but effective approach to estimating ROI for recognizing tiny actions. The key idea of this method is to analyze the correlation between an action and the sub-regions of a frame. The estimated ROI is then used as a filter for building more accurate action representations. Experimental results show significant performance improvements over the baseline methods by using the estimated ROI for action recognition.
- Nicolas Ballas, Yi Yang, Zhenzhong Lan, Bertrand Delezoide, F. Prêteux, Alexander Hauptmann. 2013. Space-Time Robust Representation for Action Recognition. Abstract: We address the problem of action recognition in unconstrained videos. We propose a novel content driven pooling that leverages space-time context while being robust toward global space-time transformations. Being robust to such transformations is of primary importance in unconstrained videos where the action localizations can drastically shift between frames. Our pooling identifies regions of interest using video structural cues estimated by different saliency functions. To combine the different structural information, we introduce an iterative structure learning algorithm, WSVM (weighted SVM), that determines the optimal saliency layout of an action model through a sparse regularizer. A new optimization method is proposed to solve the WSVM' highly non-smooth objective function. We evaluate our approach on standard action datasets (KTH, UCF50 and HMDB). Most noticeably, the accuracy of our algorithm reaches 51.8% on the challenging HMDB dataset which outperforms the state-of-the-art of 7.3% relatively.
- Shoou-I Yu, Yi Yang, Alexander Hauptmann. 2013. Harry Potter's Marauder's Map: Localizing and Tracking Multiple Persons-of-Interest by Nonnegative Discretization. Abstract: A device just like Harry Potter's Marauder's Map, which pinpoints the location of each person-of-interest at all times, provides invaluable information for analysis of surveillance videos. To make this device real, a system would be required to perform robust person localization and tracking in real world surveillance scenarios, especially for complex indoor environments with many walls causing occlusion and long corridors with sparse surveillance camera coverage. We propose a tracking-by-detection approach with nonnegative discretization to tackle this problem. Given a set of person detection outputs, our framework takes advantage of all important cues such as color, person detection, face recognition and non-background information to perform tracking. Local learning approaches are used to uncover the manifold structure in the appearance space with spatio-temporal constraints. Nonnegative discretization is used to enforce the mutual exclusion constraint, which guarantees a person detection output to only belong to exactly one individual. Experiments show that our algorithm performs robust localization and tracking of persons-of-interest not only in outdoor scenes, but also in a complex indoor real-world nursing home environment.
- Zhigang Ma, Yi Yang, Zhongwen Xu, N. Sebe, Alexander Hauptmann. 2013. We are not equally negative: fine-grained labeling for multimedia event detection. Abstract: Multimedia event detection (MED) is an effective technique for video indexing and retrieval. Current classifier training for MED treats the negative videos equally. However, many negative videos may resemble the positive videos in different degrees. Intuitively, we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning. Aiming for this, we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event. Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation. The resulting fine-grained labels may be not accurate enough to characterize the negative videos. Hence, we propose to jointly optimize the fine-grained labels with the knowledge from the visual features and the attributes representations, which brings mutual reciprocality. Our model obtains two kinds of classifiers, one from the attributes and one from the features, which incorporate the informative cues from the fine-grained labels. The outputs of both classifiers on the testing videos are fused for detection. Extensive experiments on the challenging TRECVID MED 2012 development set have validated the efficacy of our proposed approach.
- N. Boujemaa, Alexander Hauptmann, S. Satoh. 2013. The Future of Multimedia Analysis and Mining: Visions from the Shonan Meeting. Abstract: This article summarizes the outcome of the 2012 Shonan Meeting “Future of Multimedia Analysis and Mining.”The meeting was really interesting, and the participants had a fun time with an Kamakura excursion and fine dinners, in addition to in-depth discussions on ready-to-go hot research topics (see Figure 4). We have enjoyed sharing even part of our experiences with readers here.
- Zhenzhong Lan, Lu Jiang, Shoou-I Yu, Chenqiang Gao, Shourabh Rawat, Yang Cai, Shicheng Xu, Haoquan Shen, Xuanchong Li, Yipei Wang, Waito Sze, Yan Yan, Zhigang Ma, Nicolas Ballas, Deyu Meng, Wei Tong, Yi Yang, Susanne Burger, Florian Metze, Rita Singh, Bhiksha Raj, Richard Stern, Teruko Mitamura, Eric Nyberg, Alexander Hauptmann, Alexander Hauptmann. 2013. Informedia@TRECVID 2013. Abstract: In the first part of this three-part report we describe our system and novel approaches used in the TRECVID 2013 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. A separate section of the report (SIN) details methods and results for the Semantic Indexing task. The final section (SED) describes our approaches and results on the Surveillance Event Detection task. Abstract We report on our system used in the TRECVID 2013 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. For MED, it consists of four main steps: extracting features, representing features, training detectors and fusion. In the feature extraction part, we extract more than 10 low-level, high-level, and text features. Those features are then represented in three different ways which are spatial bag-of words, Gaussian Mixture Model Super Vectors (GMM) and Fisher Vectors. In the detector training and fusion, two classifiers and weighted double fusion method are employed. The official evaluation results show that our MED full systems achieve the best scores on Ah-Hoc EK10 and EK0, our audio systems achieve the best scores in EK100 and EK10 for both Pre-specified and Ad-Hoc tasks. Our MER system utilizes a subset of features and detection results from the MED system from which the recounting is generated.
- Qiang Chen, Yang Cai, L. Brown, A. Datta, Quanfu Fan, R. Feris, Shuicheng Yan, Alexander Hauptmann, Sharath Pankanti. 2013. Spatio-temporal fisher vector coding for surveillance event detection. Abstract: We present a generic event detection system evaluated in the Surveillance Event Detection (SED) task of TRECVID 2012. We investigate a statistical approach with spatio-temporal features applied to seven event classes, which were defined by the SED task. This approach is based on local spatio-temporal descriptors, called MoSIFT and generated by pair-wise video frames. A Gaussian Mixture Model(GMM) is learned to model the distribution of the low level features. Then for each sliding window, the Fisher vector encoding [improvedFV] is used to generate the sample representation. The model is learnt using a Linear SVM for each event. The main novelty of our system is the introduction of Fisher vector encoding into video event detection. Fisher vector encoding has demonstrated great success in image classification. The key idea is to model the low level visual features as a Gaussian Mixture Model and to generate an intermediate vector representation for bag of features. FV encoding uses higher order statistics in place of histograms in the standard BoW. FV has several good properties: (a) it can naturally separate the video specific information from the noisy local features and (b) we can use a linear model for this representation. We build an efficient implementation for FV encoding which can attain a 10 times speed-up over real-time. We also take advantage of non-trivial object localization techniques to feed into the video event detection, e.g. multi-scale detection and non-maximum suppression. This approach outperformed the results of all other teams submissions in TRECVID SED 2012 on four of the seven event types.
- Zhigang Ma, Yi Yang, N. Sebe, Kai Zheng, Alexander Hauptmann. 2013. Multimedia Event Detection Using A Classifier-Specific Intermediate Representation. Abstract: Multimedia event detection (MED) plays an important role in many applications such as video indexing and retrieval. Current event detection works mainly focus on sports and news event detection or abnormality detection in surveillance videos. Differently, our research aims to detect more complicated and generic events within a longer video sequence. In the past, researchers have proposed using intermediate concept classifiers with concept lexica to help understand the videos. Yet it is difficult to judge how many and what concepts would be sufficient for the particular video analysis task. Additionally, obtaining robust semantic concept classifiers requires a large number of positive training examples, which in turn has high human annotation cost. In this paper, we propose an approach that exploits the external concepts-based videos and event-based videos simultaneously to learn an intermediate representation from video features. Our algorithm integrates the classifier inference and latent intermediate representation into a joint framework. The joint optimization of the intermediate representation and the classifier makes them mutually beneficial and reciprocal. Effectively, the intermediate representation and the classifier are tightly correlated. The classifier dependent intermediate representation not only accurately reflects the task semantics but is also more suitable for the specific classifier. Thus we have created a discriminative semantic analysis framework based on a tightly coupled intermediate representation. Extensive experiments on multimedia event detection using real-world videos demonstrate the effectiveness of the proposed approach.
- Alexia Briasouli, J. Benois-Pineau, Alexander Hauptmann. 2013. Proceedings of the 1st ACM international workshop on Multimedia indexing and information retrieval for healthcare. Abstract: It is our great pleasure to welcome you to the 2013 ACM MM Workshop on Multimedia Indexing and Information Retrieval for Healthcare -- MIIRH'13. This is the first workshop on Multimedia Information Indexing and Retrieval for Healthcare and is intended to establish a platform for the continued discussion of key research issues in multimedia for healthcare, remote monitoring and treatment. Multimodal monitoring for health can take place at home, but should be discreet, unobtrusive and personalized. Theoretical, i.e. research-oriented and practical, application-specific issues related to the extraction of lifestyle, behavior and health information from multimodal data will be examined. The setup of smart homes has become of great interest lately, as the latter need to ensure accurate, reliable, discreet and cost-efficient measurements, involving, among others, privacy protection and appropriate sensor placement. Marketing of multimodal remote monitoring options also needs to be examined carefully, as independent living at home, smart homes and remote care are forecast to gain importance in years to come. Multimedia Indexing and Retrieval research is now being oriented towards this application domain of primarily importance for the society. Workshop papers and presentations will focus on the analysis of multimodal data to obtain information pertinent to healthcare problems and also examine remote monitoring solutions. Personalization, unobtrusiveness, accuracy of analysis results, data storage, retrieval, transmission, marketability and privacy concerns are among the many topics that will be discussed. 
 
The workshop will provide a forum for researchers from all over the world to share information on their latest investigations on multimedia information retrieval and indexing with healthcare applications. MIIRH is held as a one day workshop with presentations from 9:00 am to 5:00 pm with breaks and a 1-hour lunch period. 
 
The call for papers attracted many submissions from Asia, Europe, Australia and the Americas. The program committee accepted 10 papers that cover a variety of topics, including: recognition of daily living activities, analysis of therapy exercises, fall detection, context aware recommendation, experience sharing, and medical image retrieval. In addition to paper presentations, posters and demos, MIIRH will feature an invited keynote talk by Prof. Linda Shapiro on "Image Analysis for Biomedical and Healthcare Applications" as well as an invited talk by Prof. Jean-Francois Dartigues on "Dementia and Dependency: a Major Challenge for the 21st century". We hope that these proceedings will serve as a valuable reference for healthcarerelated multimedia indexing and retrieval researchers and developers.
- Zhenzhong Lan, Lu Jiang, Shoou-I Yu, Chenqiang Gao, Shourabh Rawat, Yang Cai, Shicheng Xu, Haoquan Shen, Xuanchong Li, Yipei Wang, Waito Sze, Yan Yan, Zhigang Ma, Nicolas Ballas, Deyu Meng, Wei Tong, Yi Yang, Susanne Burger, Florian Metze, Rita Singh, B. Raj, R. Stern, T. Mitamura, Eric Nyberg, Alexander Hauptmann, A. Hauptmann. 2013. Informedia@TRECVID 2013. Abstract: In the first part of this three-part report we describe our system and novel approaches used in the TRECVID 2013 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. A separate section of the report (SIN) details methods and results for the Semantic Indexing task. The final section (SED) describes our approaches and results on the Surveillance Event Detection task.
- J. Benois-Pineau, A. Briassouli, Alexander Hauptmann. 2013. ACM MM MIIRH 2013: workshop on multimedia indexing and information retrieval for healthcare. Abstract: Healthcare systems are depending on increasingly sophisticated and ubiquitous technology, while telehealth is rapidly gaining importance with the advent of low-cost and effective technological solutions in medicine. The increase in the worldwide elderly population and the burden this is inflicting upon the workforce, societies and economies are making remote care and independent living at home a necessity. MIIRH is the first workshop on multimedia analysis for remote care of and assisted living solutions which enable people that are incapacitated in some regard to continue living independently at home and remain active members of society. The topics addressed in MIIRH are extremely timely, as multitudes of cost-effective and high quality care solutions are already being developed and used, rendering the examination of new medical, healthcare paradigms an absolute necessity.
- Chenqiang Gao, Deyu Meng, Yi Yang, Yongtao Wang, Xiaofang Zhou, Alexander Hauptmann. 2013. Infrared Patch-Image Model for Small Target Detection in a Single Image. Abstract: The robust detection of small targets is one of the key techniques in infrared search and tracking applications. A novel small target detection method in a single infrared image is proposed in this paper. Initially, the traditional infrared image model is generalized to a new infrared patch-image model using local patch construction. Then, because of the non-local self-correlation property of the infrared background image, based on the new model small target detection is formulated as an optimization problem of recovering low-rank and sparse matrices, which is effectively solved using stable principle component pursuit. Finally, a simple adaptive segmentation method is used to segment the target image and the segmentation result can be refined by post-processing. Extensive synthetic and real data experiments show that under different clutter backgrounds the proposed method not only works more stably for different target sizes and signal-to-clutter ratio values, but also has better detection performance compared with conventional baseline methods.
- Zhongwen Xu, Yi Yang, I. Tsang, N. Sebe, Alexander Hauptmann. 2013. Feature Weighting via Optimal Thresholding for Video Analysis. Abstract: Fusion of multiple features can boost the performance of large-scale visual classification and detection tasks like TRECVID Multimedia Event Detection (MED) competition [1]. In this paper, we propose a novel feature fusion approach, namely Feature Weighting via Optimal Thresholding (FWOT) to effectively fuse various features. FWOT learns the weights, thresholding and smoothing parameters in a joint framework to combine the decision values obtained from all the individual features and the early fusion. To the best of our knowledge, this is the first work to consider the weight and threshold factors of fusion problem simultaneously. Compared to state-of-the-art fusion algorithms, our approach achieves promising improvements on HMDB [8] action recognition dataset and CCV [5] video classification dataset. In addition, experiments on two TRECVID MED 2011 collections show that our approach outperforms the state-of-the-art fusion methods for complex event detection.
- Sen Wang, Zhongwen Xu, Yi Yang, Xue Li, C. Pang, Alexander Hauptmann. 2013. Fall detection in multi-camera surveillance videos: experimentations and observations. Abstract: This paper presents our study on fall detection for ageing care monitoring. We collected a choreographed multi-camera dataset that contains fall actions and other actions such as walking, standing up, sitting down and so forth. In our work, MoSIFT feature is extracted from the videos recorded by each camera. We conduct a series of experiments to show the performance variations of fall detection when different methods are used. We first compare the performance of the standard Bag-of-Words and spatial Bag-of-Words with different codebook sizes. Then, we test different fusion methods which combines the information from the videos recorded by two orthogonally deployed cameras, where a non-linear χ2 kernel Support Vector Machine (SVM) is trained to detect fall actions. In addition, we also use explicit feature maps along with linear kernel for fall detection and compare it to the standard bag of word representation with a non-linear χ2 kernel. Our experiment results show that late fusion of Bag-of-Words with a 1000 centers codebook obtains the best performance. The best result reaches 90.46% in average precision, which in turn may provide a more independent and safer living environment for the elderly.
- Yi Yang, Zhigang Ma, Zhongwen Xu, Shuicheng Yan, Alexander Hauptmann. 2013. How Related Exemplars Help Complex Event Detection in Web Videos?. Abstract: Compared to visual concepts such as actions, scenes and objects, complex event is a higher level abstraction of longer video sequences. For example, a "marriage proposal" event is described by multiple objects (e.g., ring, faces), scenes (e.g., in a restaurant, outdoor) and actions (e.g., kneeling down). The positive exemplars which exactly convey the precise semantic of an event are hard to obtain. It would be beneficial to utilize the related exemplars for complex event detection. However, the semantic correlations between related exemplars and the target event vary substantially as relatedness assessment is subjective. Two related exemplars can be about completely different events, e.g., in the TRECVID MED dataset, both bicycle riding and equestrianism are labeled as related to "attempting a bike trick" event. To tackle the subjectiveness of human assessment, our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis. Experiments demonstrate that our algorithm is able to utilize related exemplars adaptively, and the algorithm gains good performance for complex event detection.
- Zhigang Ma, Alexander Hauptmann, Yi Yang, N. Sebe. 2012. Classifier-specific intermediate representation for multimedia tasks. Abstract: Video annotation and multimedia classification play important roles in many applications such as video indexing and retrieval. To improve video annotation and event detection, researchers have proposed using intermediate concept classifiers with concept lexica to help understand the videos. Yet it is difficult to judge how many and what concepts would be sufficient for the particular video analysis task. Additionally, obtaining robust semantic concept classifiers requires a large number of positive training examples, which in turn has high human annotation cost. In this paper, we propose an approach that is able to automatically learn an intermediate representation from video features together with a classifier. The joint optimization of the two components makes them mutually beneficial and reciprocal. Effectively, the intermediate representation and the classifier are tightly correlated. The classifier dependent intermediate representation not only accurately reflects the task semantics but is also more suitable for the specific classifier. Thus we have created a discriminative semantic analysis framework based on a tightly-coupled intermediate representation. Several experiments on video annotation and multimedia event detection using real-world videos demonstrate the effectiveness of the proposed approach.
- Yang Cai, Wei Tong, Linjun Yang, Alexander Hauptmann. 2012. Constrained keypoint quantization: towards better bag-of-words model for large-scale multimedia retrieval. Abstract: Bag-of-words models are among the most widely used and successful representations in multimedia retrieval. However, the quantization error which is introduced when mapping keypoints to visual words is one of the main drawbacks of the bag-of-words model. Although some techniques, such as soft-assignment to bags [23] and query expansion [27], have been introduced to deal with the problem, the performance gain is always at the cost of longer query response time, which makes them difficult to apply to large-scale multimedia retrieval applications. In this paper, we propose a simple "constrained keypoint quantization" method which can effectively reduce the overall quantization error of the bag-of-words representation and greatly improve the retrieval efficiency at the same time. The central idea of the proposed quantization method is that if a keypoint is far away from all visual words, we simply remove it. At first glance, this simple strategy seems naive and dangerous. However, we show that the proposed method has a solid theoretical background. Our experimental results on three widely used datasets for near duplicate image and video retrieval confirm that by removing a large amount of keypoints which have high quantization error, we obtain comparable or even better retrieval performance while dramatically boosting retrieval efficiency.
- Zhigang Ma, F. Nie, Yi Yang, J. Uijlings, N. Sebe, Alexander Hauptmann. 2012. Discriminating Joint Feature Analysis for Multimedia Data Understanding. Abstract: In this paper, we propose a novel semi-supervised feature analyzing framework for multimedia data understanding and apply it to three different applications: image annotation, video concept detection and 3-D motion data analysis. Our method is built upon two advancements of the state of the art: (1) l2, 1-norm regularized feature selection which can jointly select the most relevant features from all the data points. This feature selection approach was shown to be robust and efficient in literature as it considers the correlation between different features jointly when conducting feature selection; (2) manifold learning which analyzes the feature space by exploiting both labeled and unlabeled data. It is a widely used technique to extend many algorithms to semi-supervised scenarios for its capability of leveraging the manifold structure of multimedia data. The proposed method is able to learn a classifier for different applications by selecting the discriminating features closely related to the semantic concepts. The objective function of our method is non-smooth and difficult to solve, so we design an efficient iterative algorithm with fast convergence, thus making it applicable to practical applications. Extensive experiments on image annotation, video concept detection and 3-D motion data analysis are performed on different real-world data sets to demonstrate the effectiveness of our algorithm.
- A. Velivelli, Alexander Hauptmann. 2012. Human action recognition using a Markovian conditional exponential model. Abstract: We model the sequence of human actions operating an infusion pump using a Markovian conditional exponential model. We divide each video recorded by a camera into video action units. A video action unit corresponds to the start of a unique human action operation of the infusion pump to the end of that human action operating an infusion pump. We calculate the MOSIFT features of video action units which combines the spatial and temporal dimensions from videos. We vector quantize the MOSIFT features of video action units using K means clustering as video codebook elements. We estimate the conditional exponential model parameters from a training set using maximum entropy constraint and use the video codebook elements as maximum entropy constraint features. We estimate the parameters of the Markovian conditional exponential model from a training set. This Markovian conditional exponential model has 6 states which correspond to the 6 classes of infusion pump operation. To find the optimal state sequence of the Markovian conditional exponential model we use the Viterbi algorithm. This optimal state sequence corresponds to the class label sequence. The infusion pump operation is recorded from 4 video cameras. We calculate the results of classification of 6 classes of infusion pump operation using the conditional exponential model for the 4 video cameras and also we calculate the results of of classification of 6 classes of infusion pump operation using the Markovian conditional exponential model for the 4 video cameras. The classification performance of the Markovian conditional exponential model is better than the classification performance of conditional exponential model.
- Yi Yang, Alexander Hauptmann, Ming-yu Chen, Yang Cai, Ashok Bharucha, H. Wactlar. 2012. Learning to predict health status of geriatric patients from observational data. Abstract: Data for diagnosis and clinical studies are now typically gathered by hand. While more detailed, exhaustive behavioral assessments scales have been developed, they have the drawback of being too time consuming and manual assessment can be subjective. Besides, clinical knowledge is required for accurate manual assessment, for which extensive training is needed. Therefore our great research challenge is to leverage machine learning techniques to better understand patients health status automatically based on continuous computer observations. In this paper, we study the problem of health status prediction for geriatric patients using observational data. In the first part of this paper, we propose a distance metric learning algorithm to learn a Mahalanobis distance which is more precise for similarity measures. In the second part, we propose a robust classifier based on ℓ2,1-norm regression to predict the geriatric patients' health status. We test the algorithm on a dataset collected from a nursing home. Experiment shows that our algorithm achieves encouraging performance.
- Lei Bao, Longfei Zhang, Shoou-I Yu, Zhenzhong Lan, Lu Jiang, Arnold Overwijk, Qin Jin, Shohei Takahashi, B. Langner, Yuanpeng Li, Michael Garbus, Susanne Burger, Florian Metze, Alexander Hauptmann. 2012. Informedia @ Trecvid 2011 Informedia @ Trecvid 2011 Multimedia Event Detection, Semantic Indexing 1 Multimedia Event Detection (med) 1.1 Feature Extraction. Abstract: We report on our results in the TRECVID 2011 Multimedia Event Detection (MED) and Semantic Indexing (SIN) tasks. Generally, both of these tasks consist of three main steps: extracting features, training detectors and fusing. In the feature extraction part, we extracted many low-level features, high-level features and text features. We used the Spatial-Pyramid Matching technique to represent the low-level visual local features, such as SIFT and MoSIFT, which describe the location information of feature points. In the detector training part, besides the traditional SVM, we proposed a Sequential Boosting SVM classifier to deal with the large-scale unbalanced classification problem. In the fusion part, to take the advantages from different features, we tried three different fusion methods: early fusion, late fusion and double fusion. Double fusion is a combination of early fusion and late fusion. The experimental results demonstrated that double fusion is consistently better than or at least comparable to early fusion and late fusion. 1 Multimedia Event Detection (MED) 1.1 Feature Extraction In order to encompass all aspects of a video, we extracted a wide variety of visual and audio features as shown in figure 1. Table 1: Features used for the MED task. Visual Features Audio Features Low-level Features • SIFT [19] • Color SIFT [19] • Transformed Color Histogram [19] • Motion SIFT [3] • STIP [9] Mel-Frequency Cepstral Coefficients High-level Features • PittPatt Face Detection [12] • Semantic Indexing Concepts [15] Acoustic Scene Analysis Text Features Optical Character Recognition Automatic Speech Recognition 1.1.1 SIFT, Color SIFT (CSIFT), Transformed Color Histogram (TCH) These three features describe the gradient and color information of a static image. We used the Harris-Laplace detector for corner detection. For more details, please see [19]. Instead of extracting features from all frames for all videos, we first run shot-break detection and only extract features from the keyframe of a corresponding shot. The shot-break detection algorithm detects large color histogram differences between adjacent frames and a shot-boundary is detected when the histogram difference is larger than a threshold. For the 16507 training videos, we extracted 572,881 keyframes. For the 32061 testing videos, we extracted 1,035,412 keyframes. Once we have the keyframes, we extract the three features as in [19]. Given the raw feature files, a 4096 word codebook is acquired using the K-Means clustering algorithm. According to the codebook and given a region in an image, we can create a 4096 dimensional vector representing that region. Using the Spatial-Pyramid Matching [10] technique, we extract 8 regions from an keyframe image and calculate a bag-of-words vector for each region. At the end, we get a 8× 4096 = 32768 dimensional bag-of-words vector. The 8 regions are calculated as follows. • The whole image as one region. • Split the image into 4 quadrants and each quadrant is a region. • Split the image horizontally into 3 equally sized rectangles and each rectangle is a region. Since we only have feature vectors describing a keyframe, and a video is described by many keyframes, we compute a vector representing a whole video by averaging over the feature vectors from each keyframe. The features are then provided to a classifier for classification. 1.1.2 Motion SIFT (MoSIFT) Motion SIFT [3] is a motion-based feature that combines information from SIFT and optical flow. The algorithm first extract SIFT points, and for each SIFT point, it checks whether there is a large enough optical flow near the point. If the optical flow value is larger than a threshold, a 256 dimensional feature is computed for that point. The first 128 dimensions of the feature vector is the SIFT descriptor, and the latter 128 dimensions describes the optical flow near the point. We extracted Motion SIFT by calculating the optical flow between neighboring frames, but due to speed issues, we only extract Motion SIFT for the every third frame. Once we have the raw features, a 4096 dimensional codebook is computed, and using the same process as SIFT, a 32768 dimensional vector is created for classification. 1.1.3 Space-Time Interest Points (STIP) Space-Time Interest Points are computed like in [9]. Given the raw features, a 4096 dimensional code is computed, and using the same process as SIFT, a 32768 dimensional vector is created for classification. 1.1.4 Semantic Indexing (SIN) We predicted the 346 semantic concepts from Semantic Indexing 11 onto the MED keyframes. For details on how we created the models for the 346 concepts, please refer to section 2. Once we have the prediction scores of each concept on each keyframe, we compute a 346 dimensional feature that represents a video. The value of each dimension is the mean value of the concept prediction scores on all keyframes in a given video. We tried out different kinds of score merging techniques, including mean and max, and mean had the best performance. These features are then provided to a classifier for classification.
- K. Schoeffmann, B. Mérialdo, Alexander Hauptmann, C. Ngo, Y. Andreopoulos, C. Breiteneder. 2012. Advances in Multimedia Modeling - 18th International Conference, MMM 2012, Klagenfurt, Austria, January 4-6, 2012. Proceedings. Abstract: This book constitutes the refereed proceedings of the 18th International Multimedia Modeling Conference, MMM 2012, held in Klagenfurt, Austria, in January 2012. The 38 revised regular papers, 12 special session papers, 15 poster session papers, and 6 demo session papers were carefully reviewed and selected from 142 submissions. The papers are organized in the following topical sections: annotation, annotation and interactive multimedia applications, event and activity, mining and mobile multimedia applications, search, summarization and visualization, visualization and advanced multimedia systems, and the special sessions: interactive and immersive entertainment and communication, multimedia preservation: how to ensure multimedia access over time, multi-modal and cross-modal search, and video surveillance.
- Ehsan Younessian, T. Mitamura, Alexander Hauptmann. 2012. Multimodal knowledge-based analysis in multimedia event detection. Abstract: Multimedia Event Detection (MED) is a multimedia retrieval task with the goal of finding videos of a particular event in a large-scale Internet video archive, given example videos and text descriptions. We focus on the multimodal knowledge-based analysis in MED where we utilize meaningful and semantic features such as Automatic Speech Recognition (ASR) transcripts, acoustic concept indexing (i.e. 42 acoustic concepts) and visual semantic indexing (i.e. 346 visual concepts) to characterize videos in archive. We study two scenarios where we either do or do not use the provided example videos. In the former, we propose a novel Adaptive Semantic Similarity (ASS) to measure textual similarity between ASR transcripts of videos. We also incorporate acoustic concept indexing and classification to retrieve test videos, specially with too few spoken words. In the latter 'ad-hoc' scenario where we do not have any example video, we use only the event kit description to retrieve test videos ASR transcripts and visual semantics. We also propose an event-specific fusion scheme to combine textual and visual retrieval outputs. Our results show the effectiveness of the proposed ASS and acoustic concept indexing methods and their complimentary role. We also conduct a set of experiments to assess the proposed framework for the 'ad-hoc' scenario.
- Yang Liu, Fei Wu, Yi Yang, Yueting Zhuang, Alexander Hauptmann. 2012. Spline Regression Hashing for Fast Image Search. Abstract: Techniques for fast image retrieval over large databases have attracted considerable attention due to the rapid growth of web images. One promising way to accelerate image search is to use hashing technologies, which represent images by compact binary codewords. In this way, the similarity between images can be efficiently measured in terms of the Hamming distance between their corresponding binary codes. Although plenty of methods on generating hash codes have been proposed in recent years, there are still two key points that needed to be improved: 1) how to precisely preserve the similarity structure of the original data and 2) how to obtain the hash codes of the previously unseen data. In this paper, we propose our spline regression hashing method, in which both the local and global data similarity structures are exploited. To better capture the local manifold structure, we introduce splines developed in Sobolev space to find the local data mapping function. Furthermore, our framework simultaneously learns the hash codes of the training data and the hash function for the unseen data, which solves the out-of-sample problem. Extensive experiments conducted on real image datasets consisting of over one million images show that our proposed method outperforms the state-of-the-art techniques.
- Yue Ming, Q. Ruan, Alexander Hauptmann. 2012. Activity Recognition from RGB-D Camera with 3D Local Spatio-temporal Features. Abstract: Kinect, as a 3D digital capturing device, can collect the RGB and depth information of human activities rapidly. We study fusing the depth and RGB information for activity recognition. We introduce histogram color-based image thresholding to detect skin on human body, and use a GMM model to segment human hand areas. We design a new local descriptor, called a 3D Motion Scale-Invariant Feature Transform (3D MoSIFT), which can effectively detect interesting points based on both RGB and depth information, and consequently encode the visual and motion information from both to describe the interesting points. Experiments, based on a video dataset collected by a Kinect camera, show that adding depth information in the descriptor can distinctly improve the accuracy of human activity recognition. We introduce the F1-score measurement to evaluate and compare our performance with the other algorithms.
- Shoou-I Yu, Zhongwen Xu, Duo Ding, Waito Sze, F. Vicente, Zhenzhong Lan, Yang Cai, Shourabh Rawat, Peter F. Schulam, Nisarga Markandaiah, Sohail Bahmani, A. Juárez, Wei Tong, Yi Yang, Susanne Burger, Florian Metze, Rita Singh, Bhiksha Raj, Richard Stern, Teruko Mitamura, Eric Nyberg, Lu Jiang, Qiang Chen, Lisa M. Brown, Ankur Datta, Quanfu Fan, Rogério Schmidt Feris, S. Yan, Alexander Hauptmann, Sharath Pankanti. 2012. Informedia @TRECVID 2012.. Abstract: In the first part of this report we describe our system and novel approaches used in the TRECVID 2012 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. A separate section of the report (SIN) details methods and results for the Semantic Indexing task. The final section (SED) describes our approaches and results on the Surveillance Event Detection task. Abstract We report on our system used in the TRECVID 2012 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. For MED, generally, it consists of three main steps: extracting features, training detectors and fusion. In the feature extraction part, we extract many low-level, high-level features and text features. Those features are then represented in three different ways which are spatial bag-of words with standard tiling, spatial bag-of-words with feature and event specific tiling and the Gaussian Mixture Model Super Vector. In the detector training and fusion, two classifiers and three fusion methods are employed. The results from both of the official sources and our internal evaluations show good performance of our system. For our MER system, it takes some of the features and detection results from the MED system from which the recount is then generated.
- Zhongfei Zhang, Zhengyou Zhang, R. Jain, Yueting Zhuang, N. Contractor, Alexander Hauptmann, A. Jaimes, W. Li, A. Loui, Tao Mei, N. Sebe, Yonghong Tian, V. Tseng, Qing Wang, Changsheng Xu, Huimin Yu, Shiwen Yu. 2012. Societally connected multimedia across cultures. Abstract: The advance of the Internet in the past decade has radically changed the way people communicate and collaborate with each other. Physical distance is no more a barrier in online social networks, but cultural differences (at the individual, community, as well as societal levels) still govern human-human interactions and must be considered and leveraged in the online world. The rapid deployment of high-speed Internet allows humans to interact using a rich set of multimedia data such as texts, pictures, and videos. This position paper proposes to define a new research area called ‘connected multimedia’, which is the study of a collection of research issues of the super-area social media that receive little attention in the literature. By connected multimedia, we mean the study of the social and technical interactions among users, multimedia data, and devices across cultures and explicitly exploiting the cultural differences. We justify why it is necessary to bring attention to this new research area and what benefits of this new research area may bring to the broader scientific research community and the humanity.
- Yi Yang, Fei Wu, F. Nie, Heng Tao Shen, Yueting Zhuang, Alexander Hauptmann. 2012. Web and Personal Image Annotation by Mining Label Correlation With Relaxed Visual Graph Embedding. Abstract: The number of digital images rapidly increases, and it becomes an important challenge to organize these resources effectively. As a way to facilitate image categorization and retrieval, automatic image annotation has received much research attention. Considering that there are a great number of unlabeled images available, it is beneficial to develop an effective mechanism to leverage unlabeled images for large-scale image annotation. Meanwhile, a single image is usually associated with multiple labels, which are inherently correlated to each other. A straightforward method of image annotation is to decompose the problem into multiple independent single-label problems, but this ignores the underlying correlations among different labels. In this paper, we propose a new inductive algorithm for image annotation by integrating label correlation mining and visual similarity mining into a joint framework. We first construct a graph model according to image visual features. A multilabel classifier is then trained by simultaneously uncovering the shared structure common to different labels and the visual graph embedded label prediction matrix for image annotation. We show that the globally optimal solution of the proposed framework can be obtained by performing generalized eigen-decomposition. We apply the proposed framework to both web image annotation and personal album labeling using the NUS-WIDE, MSRA MM 2.0, and Kodak image data sets, and the AUC evaluation metric. Extensive experiments on large-scale image databases collected from the web and personal album show that the proposed algorithm is capable of utilizing both labeled and unlabeled data for image annotation and outperforms other algorithms.
- Lu Jiang, Alexander Hauptmann, Guang Xiang. 2012. Leveraging high-level and low-level features for multimedia event detection. Abstract: This paper addresses the challenge of Multimedia Event Detection by proposing a novel method for high-level and low-level features fusion based on collective classification. Generally, the method consists of three steps: training a classifier from low-level features; encoding high-level features into graphs; and diffusing the scores on the established graph to obtain the final prediction. The final prediction is derived from multiple graphs each of which corresponds to a high-level feature. The paper investigates two graph construction methods using logarithmic and exponential loss functions, respectively and two collective classification algorithms, i.e. Gibbs sampling and Markov random walk. The theoretical analysis demonstrates that the proposed method converges and is computationally scalable and the empirical analysis on TRECVID 2011 Multimedia Event Detection dataset validates its outstanding performance compared to state-of-the-art methods, with an added benefit of interpretability.
- E. Chang, Shih-Fu Chang, Alexander Hauptmann, Thomas S. Huang. 2012. Web-Scale Multimedia Processing and Applications. Abstract: This special issue provides a timely and comprehensive review of recent progress, applications, and challenges spurred by the introduction of web-scale data resources, involving text, image, video, speech, user recommendation, and their combinations. The first group of papers covers fundamental crosscutting techniques critical for developing web-scale systems, including large-scale linear classification, optimal hashing, robust graph-based semisupervised learning, and classifier adaptation. The second group reviews techniques for discovering knowledge, extracting information, and combining knowledge with multimedia content networks. The third group surveys representative work demonstrating the unique impact of web-scale data in practical applications, such as image annotation, media recommendation, enterprise social networks, and social media networks. This special issue is intended for broad audience interested in gaining comprehensive understanding of technical issues and opportunities in this important field. I . INTRODUCTION
- Duo Ding, Florian Metze, Shourabh Rawat, Peter F. Schulam, Susanne Burger, Ehsan Younessian, Lei Bao, Michael G. Christel, Alexander Hauptmann. 2012. Beyond audio and video retrieval: towards multimedia summarization. Abstract: Given the deluge of multimedia content that is becoming available over the Internet, it is increasingly important to be able to effectively examine and organize these large stores of information in ways that go beyond browsing or collaborative filtering. In this paper we review previous work on audio and video processing, and define the task of Topic-Oriented Multimedia Summarization (TOMS) using natural language generation: given a set of automatically extracted features from a video (such as visual concepts and ASR transcripts) a TOMS system will automatically generate a paragraph of natural language ("a recounting"), which summarizes the important information in a video belonging to a certain topic area, and provides explanations for why a video was matched and retrieved. We see this as a first step towards systems that will be able to discriminate visually similar, but semantically different videos, compare two videos and provide textual output or summarize a large number of videos at once. In this paper, we introduce our approach of solving the TOMS problem. We extract visual concept features and ASR transcription features from a given video, and develop a template-based natural language generation system to produce a textual recounting based on the extracted features. We also propose possible experimental designs for continuously evaluating and improving TOMS systems, and present results of a pilot evaluation of our initial system.
- Zhigang Ma, Yi Yang, Yang Cai, N. Sebe, Alexander Hauptmann. 2012. Knowledge adaptation for ad hoc multimedia event detection with few exemplars. Abstract: Multimedia event detection (MED) has a significant impact on many applications. Though video concept annotation has received much research effort, video event detection remains largely unaddressed. Current research mainly focuses on sports and news event detection or abnormality detection in surveillance videos. Our research on this topic is capable of detecting more complicated and generic events. Moreover, the curse of reality, i.e., precisely labeled multimedia content is scarce, necessitates the study on how to attain respectable detection performance using only limited positive examples. Research addressing these two aforementioned issues is still in its infancy. In light of this, we explore Ad Hoc MED, which aims to detect complicated and generic events by using few positive examples. To the best of our knowledge, our work makes the first attempt on this topic. As the information from these few positive examples is limited, we propose to infer knowledge from other multimedia resources to facilitate event detection. Experiments are performed on real-world multimedia archives consisting of several challenging events. The results show that our approach outperforms several other detection algorithms. Most notably, our algorithm outperforms SVM by 43% and 14% comparatively in Average Precision when using Gaussian and Χ2 kernel respectively.
- B. Huet, Tat-Seng Chua, Alexander Hauptmann. 2012. Large-Scale Multimedia Data Collections. Abstract: The widespread adoption of smartphones equipped with high-quality image-capturing capabilities coupled with the prevalent use of social networks have resulted in an explosive growth of social media content. People now routinely capture the scenes around them and instantly share the multimedia content with their friends over a variety of social networks. The social network functions also ensure that much of this content comes with some form of social annotations. This environment sets the stage for advances in large-scale media research. This special issue hopes to address these challenges. The articles in this issue cover identification of use cases and task design, dataset development, and basic research over existing datasets.
- Jun Yang, Wei Tong, Alexander Hauptmann. 2012. A Framework for Classifier Adaptation for Large-Scale Multimedia Data. Abstract: Machine learning techniques have been used extensively to build models for the analysis and retrieval of multimedia data. The explosion of multimedia data on the Web poses a great challenge to such techniques not simply because of the sheer data volume, but also because of the heterogeneity of the data. With data from a wide variety of domains, models trained from one domain do not generalize well to other domains, while at the same time it is prohibitively expensive to build new models for each and every domain due to the high cost for labeling training examples. In this paper, we tackle the heterogeneity challenge in large-scale multimedia data using cross-domain model adaptation for better performance and reduced human cost. Specifically, we investigate the problem of adapting supervised classifiers trained from one or more source domains to a new classifier for a target domain that has only limited labeled examples. The foundation of our work is a general framework for function-level classifier adaptation based on the regularized loss minimization principle, which adapts a classifier by directly modifying its decision function. Under this framework, one can derive concrete adaptation algorithms by plugging in any loss and regularization functions, among which we elaborate on adaptive support vector machines (a-SVM). We further extend this framework for multiclassifier adaptation, namely adapting multiple existing classifiers into a classifier for the target domain, in a way that the contributions of these existing classifiers are automatically determined. We evaluate the proposed approaches in cross-domain semantic concept detection based on TRECVID corpora. The results show that our approaches outperform existing (adaptation and nonadaptation) methods in terms of accuracy and/or efficiency, and adaptation from multiple classifiers offers further benefits.
- E. Chang, Shih-Fu Chang, Alexander Hauptmann, Thomas S. Huang, M. Slaney. 2012. Web-Scale Multimedia Processing and Applications [Scanning the Issue]. Abstract: The articles in this special issue focus on web-scale multimedia processing as well as applications for its use.
- Sen Wang, Yi Yang, Zhigang Ma, Xue Li, C. Pang, Alexander Hauptmann. 2012. Action recognition by exploring data distribution and feature correlation. Abstract: Human action recognition in videos draws strong research interest in computer vision because of its promising applications for video surveillance, video annotation, interactive gaming, etc. However, the amount of video data containing human actions is increasing exponentially, which makes the management of these resources a challenging task. Given a database with huge volumes of unlabeled videos, it is prohibitive to manually assign specific action types to these videos. Considering that it is much easier to obtain a small number of labeled videos, a practical solution for organizing them is to build a mechanism which is able to conduct action annotation automatically by leveraging the limited labeled videos. Motivated by this intuition, we propose an automatic video annotation algorithm by integrating semi-supervised learning and shared structure analysis into a joint framework for human action recognition. We apply our algorithm on both synthetic and realistic video datasets, including KTH [20], CareMedia dataset [1], Youtube action [12] and its extended version, UCF50 [2]. Extensive experiments demonstrate that the proposed algorithm outperforms the compared algorithms for action recognition. Most notably, our method has a very distinct advantage over other compared algorithms when we have only a few labeled samples.
- Lei Bao, Shoou-I Yu, Zhenzhong Lan, Arnold Overwijk, Qin Jin, B. Langner, Michael Garbus, Susanne Burger, Florian Metze, Alexander Hauptmann. 2011. Informedia @ TRECVID 2011. Abstract: The Informedia group participated in three tasks this year, including: Multimedia Event Detection (MED), Semantic Indexing (SIN) and Surveillance Event Detection. Generally, all of these tasks consist of three main steps: extracting feature, training detector and fusing. In the feature extraction part, we extracted a lot of low-level features, high-level features and text features. Especially, we used the Spatial-Pyramid Matching technique to represent the low-level visual local features, such as SIFT and MoSIFT, which describe the location information of feature points. In the detector training part, besides the traditional SVM, we proposed a Sequential Boosting SVM classifier to deal with the large-scale unbalance classification problem. In the fusion part, to take the advantages from different features, we tried three different fusion methods: early fusion, late fusion and double fusion. Double fusion is a combination of early fusion and late fusion. The experimental results demonstrated that double fusion is consistently better, or at least comparable than early fusion and late fusion.
- Huan Li, Chao Li, Yuan Shi, Z. Xiong, Alexander Hauptmann. 2011. Cross-domain active learning for video concept detection. Abstract: As video data from a variety of different domains (e.g., news, documentaries, entertainment) have distinctive data distributions, cross-domain video concept detection becomes an important task, in which one can reuse the labeled data of one domain to benefit the learning task in another domain with insufficient labeled data. In this paper, we approach this problem by proposing a cross-domain active learning method which iteratively queries labels of the most informative samples in the target domain. Traditional active learning assumes that the training (source domain) and test data (target domain) are from the same distribution. However, it may fail when the two domains have different distributions because querying informative samples according to a base learner that initially learned from source domain may no longer be helpful for the target domain. In our paper, we use the Gaussian random field model as the base learner which has the advantage of exploring the distributions in both domains, and adopt uncertainty sampling as the query strategy. Additionally, we present an instance weighting trick to accelerate the adaptability of the base learner, and develop an efficient model updating method which can significantly speed up the active learning process. Experimental results on TRECVID collections highlight the effectiveness.
- Huan Li, Lei Bao, Arnold Overwijk, Wei Liu, Longfei Zhang, Shoou-I Yu, Ming-yu Chen, Florian Metze, Alexander Hauptmann. 2010. Informedia @ TRECVID 2010. Abstract: The Informedia group participated in four tasks this year, including Semantic indexing, Known-item search, Surveillance event detection and Event detection in Internet multimedia pilot. For semantic indexing, except for training traditional SVM classifiers for each high level feature by using different low level features, a kind of cascade classifier was trained which including four layers with different visual features respectively. For Known Item Search task, we built a text-based video retrieval and a visual-based video retrieval system, and then query-class dependent late fusion was used to combine the runs from these two systems. For surveillance event detection, we especially put our focus on analyzing motions and human in videos. We detected the events by three channels. Firstly, we adopted a robust new descriptor called MoSIFT, which explicitly encodes appearance features together with motion information. And then we trained event classifiers in sliding windows using a bag-of-video-word approach. Secondly, we used the human detection and tracking algorithms to detect and track the regions of human, and then just focus on the MoSIFT points in the human regions. Thirdly, after getting the decision, we also borrow the results of human detection to filter the decision. In addition, to reduce the number of false alarms further, we aggregated short positive windows to favor long segmentation and applied a cascade classifier approach. The performance shows dramatic improvement over last year on the event detection task. For event detection in internet multimedia pilot, our system is purely based on textual information in the form of Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR). We submitted three runs; a run based on a simple combination of three different ASR transcripts, a run based on OCR only and a run that combines ASR and OCR. We noticed that both ASR and OCR contribute to the goals of this task. However the video collection is very challenging for those features, resulting in a low recall but high precision.
- Zan Gao, Marcin Detyniecki, Ming-yu Chen, Wen Wu, Alexander Hauptmann, H. Wactlar, A. Cai. 2010. Multi-camera recognition of people operating home medical devices. Abstract: We perform action recognition with a robust approach to recognize action information based on explicitly encoding motion information. This algorithm detects interest points and encodes not only their local appearance but also explicitly models local motion. Our goal is to recognize individual human actions in the operations of a home medical device to see if the patient has correctly performed the required actions. Using a specific infusion pump as a test case, requiring 22 operation steps from 6 action classes, our resulting classifier fused information from 4 cameras, to obtain an average class recognition exceeding 50%.
- Yu-Gang Jiang, Jun Yang, C. Ngo, Alexander Hauptmann. 2010. Representations of Keypoint-Based Semantic Concept Detection: A Comprehensive Study. Abstract: Based on the local keypoints extracted as salient image patches, an image can be described as a ¿bag-of-visual-words (BoW)¿ and this representation has appeared promising for object and scene classification. The performance of BoW features in semantic concept detection for large-scale multimedia databases is subject to various representation choices. In this paper, we conduct a comprehensive study on the representation choices of BoW, including vocabulary size, weighting scheme, stop word removal, feature selection, spatial information, and visual bi-gram. We offer practical insights in how to optimize the performance of BoW by choosing appropriate representation choices. For the weighting scheme, we elaborate a soft-weighting method to assess the significance of a visual word to an image. We experimentally show that the soft-weighting outperforms other popular weighting schemes such as TF-IDF with a large margin. Our extensive experiments on TRECVID data sets also indicate that BoW feature alone, with appropriate representation choices, already produces highly competitive concept detection performance. Based on our empirical findings, we further apply our method to detect a large set of 374 semantic concepts. The detectors, as well as the features and detection scores on several recent benchmark data sets, are released to the multimedia community.
- Huan Li, Yuan Shi, Ming-yu Chen, Alexander Hauptmann, Z. Xiong. 2010. Joint-AL: Joint Discriminative and Generative Active Learning for Cross-Domain Semantic Concept Classification. Abstract: As multimedia data come from a wide variety of domains, each having its distinctive data distributions, cross-domain video semantic concept classification becomes an important task in semantic computing. Its challenge arises from the different distribution (in feature space) of the concept between the source and the target domain, which makes a classifier trained on a source domain perform poorly on a target domain. Active learning can be employed to reuse the existing classifier in order to avoid expensively labeling target domain data for building a new classifier, which queries the labels for a number of most ambiguous samples in target domain and uses these samples to refine the source domain classifier. This discriminative query strategy, used by many traditional active learning methods, could fail if the difference in the feature space distribution of the concept is too large. A generative query strategy is proposed by us in this paper, to deal with large differences between two domains of one semantic concept, which queries samples that are most unlikely to be generated from current distribution. We then present a joint active learning method by adaptively combining the dis-criminative and the generative query strategies. This method dynamically adapts to the distribution differences and results a hybrid strategy that performs more robustly compared to either single strategy. We evaluate the proposed approaches in cross-domain semantic classification based on TRECVID corpora. The results show the effectiveness of our joint method.
- Zan Gao, Marcin Detyniecki, Ming-yu Chen, Alexander Hauptmann, H. Wactlar, A. Cai. 2010. The Application of Spatio-temporal Feature and Multi-Sensor in Home Medical Devices. Abstract: Medical devices, such as infusion pumps that deliver life-critical medication, are frequently used at home by patient's themselves as a cost saving measure. As individuals age, many are impacted by cognitive decline such that the proper sequencing of steps of a task is forgotten. Since the danger of making an error can be quite high. So, to detect errors when patients operate a home medical device, we observe them with multiple cameras and record pump sensor information. We then use a robust approach to recognize actions based on explicitly encoding motion information using MoSIFT, Which detects interest points and encodes not only their local appearance but also explicitly models local motion. Our goal is to see if the patient has correctly performed the required actions in the prescribed sequence for the device. Thus, firstly, we will evaluate how to group the requiring 22 operation steps. Secondly, we will analysis the duration of actions, and consider the user adaption. Thirdly, the order of actions is introduced, and HMM is used in our algorithm. Fourthly, as some actions are very difficult to recognize by computer vision, so the physical sensor information is borrowed in our system. Subsequently, we also consider how to fuse the results from different cameras. Finally, we also evaluate the performances when we add different numbers of videos from test subjects. From the experiments, we can see that the improvement of our system performance changes from 49% to over 80%. And if we can obtain 6-7 videos from the patients, and add them into the training dataset, the performance of our system with our simplest way can be 83.7%.
- Ming-yu Chen, Huan Li, Alexander Hauptmann. 2010. Combining motion understanding and keyframe image analysis for broadcast video information extraction. Abstract: We describe a robust new approach to extract semantic concept information based on explicitly encoding static image appearance features together with motion information. For high-level semantic concept identification detection in broadcast video, we trained multi-modality classifiers which combine the traditional static image features and a new motion feature analysis method (MoSIFT). The experimental result show that the combined features have solid performance for detecting a variety of motion related concepts and provide a large improvement over static image analysis features in video.
- Zan Gao, Ming-yu Chen, Marcin Detyniecki, Wen Wu, Alexander Hauptmann, H. Wactlar, A. Cai. 2010. Multi-camera Monitoring of Infusion Pump Use. Abstract: When patients operate a home infusion pump, they maybe make some mistakes, and it will be dangerous. To detect potentially life threatening errors, we design an assistance system based on observation by multiple cameras and robust spatio-temporal algorithm. Firstly, we record the video by multiple cameras when people use the infusion pump. Secondly, we use a robust MoSIFT algorithm, which detects interest points and encodes not only their local appearance but also explicitly models local motion, to describe the action. Thirdly, we recognize each individual human operating step in the use of an infusion pump to see if the patient has correctly performed the required actions in a safe sequence. The specific infusion pump used for evaluation requires 22 operation steps from 12 action classes. From the experiments show that our best classifier can obtains an average rate of 56%, and MoSIFT algorithm is robust and stable.
- Huan Li, Lei Bao, Zan Gao, Arnold Overwijk, Wei Liu, Longfei Zhang, Shoou-I Yu, Ming-yu Chen, Florian Metze, Alexander Hauptmann. 2010. Informedia @ TRECVID2010. Abstract: The Informedia group participated in four tasks this year, including Semantic indexing, Known-item search, Surveillance event detection and Event detection in Internet multimedia pilot. For semantic indexing, except for training traditional SVM classifiers for each high level feature by using different low level features, a kind of cascade classifier was trained which including four layers with different visual features respectively. For Known Item Search task, we built a text-based video retrieval and a visual-based video retrieval system, and then query-class dependent late fusion was used to combine the runs from these two systems. For surveillance event detection, we especially put our focus on analyzing motions and human in videos. We detected the events by three channels. Firstly, we adopted a robust new descriptor called MoSIFT, which explicitly encodes appearance features together with motion information. And then we trained event classifiers in sliding windows using a bag-of-video-word approach. Secondly, we used the human detection and tracking algorithms to detect and track the regions of human, and then just focus on the MoSIFT points in the human regions. Thirdly, after getting the decision, we also borrow the results of human detection to filter the decision. In addition, to reduce the number of false alarms further, we aggregated short positive windows to favor long segmentation and applied a cascade classifier approach. The performance shows dramatic improvement over last year on the event detection task. For event detection in internet multimedia pilot, our system is purely based on textual information in the form of Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR). We submitted three runs; a run based on a simple combination of three different ASR transcripts, a run based on OCR only and a run that combines ASR and OCR. We noticed that both ASR and OCR contribute to the goals of this task. However the video collection is very challenging for those features, resulting in a low recall but high precision. 1 Semantic Indexing (SIN) In SIN task, we submit 4 runs this year. The first, the second and the forth runs are the full submissions whose results include all the 130 high level features. The third run is the light submission which submits the results for 10 high level features predifined. 1.1 Description of submissions • CMU1 1: MoSIFT feature only, trained with χ kernel for each high level feature. • CMU2 2: Select the low level feature which has best performance on training data and then train a classifier based on it. • CMU3 3: Cascade classifier is trained with four layers, and different layer is trained by using different visual feature. • CMU4 4: Linearly combine the prediction results of the classifiers trained on MoSIFT feature, SIFT feature, color feature, audio feature and face feature. 1.2 Details of submissions
- Lei Bao, Juan Cao, Yongdong Zhang, Jintao Li, Ming-yu Chen, Alexander Hauptmann. 2010. Explicit and implicit concept-based video retrieval with bipartite graph propagation model. Abstract: The major scientific problem for content-based video retrieval is the semantic gap. Generally speaking, there are two appropriate ways to bridge the semantic gap: the first one is from human perspective (top-down) and the other one is from computer perspective (bottom-up). The top-down method defines a concept lexicon from human perspective, trains the detector for each concept based on supervised learning, and then indexes the corpus with concept detectors. Since each concept has an explicit semantic meaning, we call this concept as an explicit concept. The bottom-up approach directly discovers the underlying latent topics from video corpus by machine perspective using an unsupervised learning. The video corpus is indexed subsequently by these latent topics. As opposite to explicit concepts, we name latent topics as implicit concepts. Given the explicit concept set is pre-defined and independent of the corpus, it is impossible to completely describe corpus and users' queries. On the other hand, the implicit concepts are dynamic and dependent on the corpus, which is able to fully describe corpus and users' queries. Therefore, combining explicit and implicit concepts could be a promising way to bridge the semantic gap effectively. In this paper, a Bipartite Graph Propagation Model (BGPM) is applied to automatically balance influences from explicit and implicit concepts. Concept nodes with strong connections to queries are reinforced no matter explicit or implicit. Demonstrated by the experiments on TREVID 2008 video dataset, BGPM successfully fuses explicit and implicit concepts to achieve a significant improvement on 48 search tasks.
- B. Huet, Tat-Seng Chua, Alexander Hauptmann. 2010. ACM international workshop on very-large-scale multimedia corpus, mining and retrieval (VLS-MCMR'10). Abstract: The purpose of this workshop is to bring together researchers interested in the construction and analysis of Very Large Scale Multimedia Corpus, as well as the methodologies to Mine and Retrieve information from them. The Workshop will provide a forum to consolidate key issues related to research on very large scale multimedia dataset such as the construction of dataset, creation of ground truth, sharing and extension of such resources in terms of ground truth, features, algorithms and tools etc. The Workshop will discuss and formulate action plan towards these goals.
- Zan Gao, Marcin Detyniecki, Ming-yu Chen, Wen Wu, Alexander Hauptmann, H. Wactlar. 2010. Towards automated assistance for operating home medical devices. Abstract: To detect errors when subjects operate a home medical device, we observe them with multiple cameras. We then perform action recognition with a robust approach to recognize action information based on explicitly encoding motion information. This algorithm detects interest points and encodes not only their local appearance but also explicitly models local motion. Our goal is to recognize individual human actions in the operations of a home medical device to see if the patient has correctly performed the required actions in the prescribed sequence. Using a specific infusion pump as a test case, requiring 22 operation steps from 6 action classes, our best classifier selects high likelihood action estimates from 4 available cameras, to obtain an average class recognition rate of 69%.
- Ming-yu Chen, L. Mummert, P. Pillai, Alexander Hauptmann, R. Sukthankar. 2010. Exploiting multi-level parallelism for low-latency activity recognition in streaming video. Abstract: Video understanding is a computationally challenging task that is critical not only for traditionally throughput-oriented applications such as search but also latency-sensitive interactive applications such as surveillance, gaming, videoconferencing, and vision-based user interfaces. Enabling these types of video processing applications will require not only new algorithms and techniques, but new runtime systems that optimize latency as well as throughput. In this paper, we present a runtime system called Sprout that achieves low latency by exploiting the parallelism inherent in video understanding applications. We demonstrate the utility of our system on an activity recognition application that employs a robust new descriptor called MoSIFT, which explicitly augments appearance features with motion information. MoSIFT outperforms previous recognition techniques, but like other state-of-the-art techniques, it is computationally expensive -- a sequential implementation runs 100 times slower than real time. We describe the implementation of the activity recognition application on Sprout, and show that it can accurately recognize activities at full frame rate (25 fps) and low latency on a challenging airport surveillance video corpus.
- Ming-yu Chen, L. Mummert, P. Pillai, Alexander Hauptmann, R. Sukthankar. 2010. Controlling your TV with gestures. Abstract: Vision-based user interfaces enable natural interaction modalities such as gestures. Such interfaces require computationally intensive video processing at low latency. We demonstrate an application that recognizes gestures to control TV operations. Accurate recognition is achieved by using a new descriptor called MoSIFT, which explicitly encodes optical flow with appearance features. MoSIFT is computationally expensive - a sequential implementation runs 100 times slower than real time. To reduce latency sufficiently for interaction, the application is implemented on a runtime system that exploits the parallelism inherent in video understanding applications.
- Lu Jiang, Zhongwen Xu, Zhenzhong Lan, Shicheng Xu, Xiaojun Chang, Xuanchong Li, Zexi, Mao, Chuang Gan, Yajie Miao, Xingzhong Du, Yang Cai, Lara Martin, Nikolas Wolfe, Anurag Kumar, Huan Li, Ming Lin, Yezhou Yang, Deyu Meng, S. Shan, P. D. Sahin, Susanne, Burger, Florian Metze, Rita Singh, B. Raj, T. Mitamura, R. Stern, Alexander, Hauptmann, Pinar Duygulu-Sahin, Alexander Hauptmann, Yicheng Zhao. 2010. MMM-TJU at TRECVID 2010. Abstract: Surveillance Event Detection Semantic event detection in the huge amount of surveillance video in both retrospective and real-time styles is essential to a variety of higher-level applications in the public security. In TRECVID 2010, to overcome the limitations of the traditional human action analysis method with human detection/tracking and domain knowledge, we evaluate the general framework for multiple human behaviors modeling with the philosophy of bag of spatiotemporal feature (BoSTF). The brief
- Huan Li, Yuan Shi, Ming-yu Chen, Alexander Hauptmann, Z. Xiong. 2010. Hybrid active learning for cross-domain video concept detection. Abstract: Cross-domain video concept detection is a challenging task due to the distribution difference between the source domain and target domain. In order to avoid expensive labeling the target-domain data, Active Learning can be used to incrementally learn a target classifier by reusing the one in the source domain. It uses a discriminative query strategy and picks the most ambiguous samples to label, which could fail if the distribution difference is too large. In this paper, to deal with large difference in data distributions, we propose a generative query strategy which is then combined with the existing discriminative one to yield a hybrid method. This method adaptively fits the distribution differences and gives a mixture strategy that performs more robustly compared to both single strategies. Experimental results on TRECVID semantic concept detection task demonstrate superior performance of our hybrid method.
- B. Huet, Tat-Seng Chua, Alexander Hauptmann. 2010. Proceedings of the international workshop on Very-large-scale multimedia corpus, mining and retrieval. Abstract: Welcome to the International Workshop on Very-Large-Scale Multimedia Corpus, Mining and Retrieval (VLS-MCMR'10). The purpose of this workshop is to bring together researchers interested in the construction and analysis of Very Large Scale Multimedia Corpus, as well as the methodologies to Mine and Retrieve information from them. The Workshop will provide a forum to consolidate key issues related to research on very large scale multimedia dataset such as the construction of dataset, creation of ground truth, sharing and extension of such resources in terms of ground truth, features, algorithms and tools etc. The Workshop will discuss and formulate action plan towards these goals. 
 
This workshop welcomes contributions on the following topics: 
Construction, Unification and Evolution of Corpus 
Framework for sharing of dataset, ground truth, features, algorithms and tools 
Indexing and retrieval for large multimedia collections (including images, video, audio and other multi-modal systems) 
Large-scale video event and temporal analysis over diverse sources 
Automatic machine tagging, semantic annotation and object recognition on massive multimedia collections 
Interfaces for exploring, browsing and visualizing large multimedia collections 
Scalable and distributed machine learning and data mining methods for multimedia data 
Performance evaluation methodologies and standards 
Large-scale copy detection and near-duplicate detection 
Web-scale combined analysis of social and content networks 
Scalable and distributed systems for multimedia content analysis 
 
 
 
Large-Scale multimedia applications are among the potential topics for the ACM multimedia 2010 hosts the "Multimedia Grand Challenge." The availability of Large-Scale Corpus would effectively boost research in this direction and foster many new applications for the years to come. 
 
The call for papers attracted 26 submissions from Asia, North-America, Europe and Africa. The program committee accepted 10 high quality papers. In addition, the program includes a panel on the topics addressed by the workshop and a keynote speech. We hope that the proceedings will serve as a valuable reference for multimedia researchers and developers as well as encourage new research direction and results. 
 
Looking over the papers accepted for the workshop, we observe three major trends. First there are approaches that attempt to benefit from the user-contributed data in order to facilitate modeling, mining and retrieval. Second, there are studies that focus on the algorithmic issues related to the use of massively parallel computing facilities. Finally, there is work that addresses the scalability issues when going very-large-scale. 
 
Tong tackles large scale image annotation using user contributed annotation (tags etc.) provided from social media network; where scalability is achieved through the use of the GRID'5000 computation mresources. Zhou et al. identify relevant text terms from text blocks that surround the web images in order to improve the accuracy of web image annotation on a 5 million image dataset. Wang et al. propose a deep model-based and data-driven hybrid architecture for annotating images. It is shown that DMD can scale-up well, thanks to its sparse regularization and scalable supervised learning steps. Creating a corpus is both expensive and time consuming. Liu and Huet propose a technique to automatically augment the training set for concept detector refinement. Two kinds of information is used to select the training data, one is visual feature, where video shots with high confidence scores are selected, the other is tags, in which tags are used to filter out video shots not tagged with the concept. 
 
Wu et al. present an unsupervised fully automatic algorithm for detecting commercials in broadcast TV. Their solution is scalable and efficient for fast, large scale, unsupervised commercial detection. Gudmund et al. revisit a cluster pruning algorithm, considering factors such as CPU/IO cost and memory constraint for large-scale copy detection. The method shows interesting clustering and retrieval computational cost when scaling up. In Kosh, processing and optimizing strategies are presented along with a cost model for integrating a similarity-based image join in a multimedia database. 
 
Nagy et al. addresses the scalability issues for visual vocabulary based image annotation algorithms as new object categories are added. To this end, a hierarchical approach is proposed based on classspecific vocabulary and a scoring function. Fan et al. reported an extensive analysis of user behavior in online video streaming based on a large-scale trace database of online video access sessions. The study of the statistical characteristics of user behavior patterns shows that user behavior in a video access session is not only related to the content of the video, but also has strong correlation with the behaviors of previous access sessions. Wang and Merialdo propose an approach to boost the performance of video concept detection based on the Bag-of-Words through the assignment different weights to the visual words according to their informativeness for the detection of different concepts.
- Alexander Hauptmann, Jun Yang. 2009. A general framework for classifier adaptation and its applications in multimedia. Abstract: For the analysis and retrieval of multimedia data, machine learning techniques have been extensively applied to build models that map various feature vectors of the data into semantic labels. As multimedia data come from a wide variety of domains (e.g., genres, sources), each having its distinctive data characteristics, models trained from one domain do not usually generalize well to other domains. For example, the performance of semantic concept detectors trained from news video drops 60-70% when they are applied on documentary video. Meanwhile, it is prohibitively expensive to build new models for each and every domain due to the high cost for labeling training examples. Therefore, techniques for adapting models across different, domains are desirable for better performance and reduced human cost. 
In this thesis, we investigate a generic adaptation problem in multimedia and other areas, which is to adapt supervised classifiers trained from one or more source domains to a new classifier for a target domain that has only limited labeled examples. The foundation of our work is a general framework for function-level classifier adaptation based on the regularized loss minimization principle. Fundamentally different from existing adaptation techniques, this framework adapts a classifier by directly modifying its decision function rather than re-training over the data in source domains, making it highly efficient and applicable to any type of classifier. Under tins framework, one can derive concrete adaptation algorithms by plugging-in any loss and regularization functions, among which we elaborate on adaptive support vector machines (a-SVM) and adaptive kernel logistic regression (a-KLR). We further extend this framework for multi classifier adaptation, namely adapting multiple existing classifiers into a classifier for the target domain, in a way that the contributions of these existing classifiers are automatically determined. We evaluate the proposed approaches in cross-domain semantic concept detection based on TRECVID corpora. The results show that our approaches outperform existing (adaptation and non-adaptation) methods in terms of accuracy and/or efficiency, and adaptation Irons multiple classifiers offers further benefits. We also demonstrate the effectiveness of our approaches in adapting classifiers of teat documents and of EEG data. 
We then focus on improving the cost-efficiency of adaptation by selecting and prioritizing adaptation tasks involving multiple classifiers. We approach this problem by first conducting a comprehensive analysis of the generalizability of concept classifiers, which is related to the cost-efficiency of adapting a classifier. This analysis reveals strong correlations between generalizability and various meta-features of a classifier; ranging from model structure to the distribution of its output. We show that generalizability can be predicted quantitatively from these model meta-features using regression models. Based on the predictions of generalizability, we propose several selective adaptation methods for selecting the classifiers to be adapted and allocating their training examples such that they achieve higher overall post-adaptation performance than equally adapting every classifier.
- Xinghua Sun, Ming-yu Chen, Alexander Hauptmann. 2009. Action recognition via local descriptors and holistic features. Abstract: In this paper we propose a unified action recognition framework fusing local descriptors and holistic features. The motivation is that the local descriptors and holistic features emphasize different aspects of actions and are suitable for the different types of action databases. The proposed unified framework is based on frame differencing, bag-of-words and feature fusion. We extract two kinds of local descriptors, i.e. 2D and 3D SIFT feature descriptors, both based on 2D SIFT interest points. We apply Zernike moments to extract two kinds of holistic features, one is based on single frames and the other is based on motion energy image. We perform action recognition experiments on the KTH and Weizmann databases, using Support Vector Machines. We apply the leave-one-out and pseudo leave-N-out setups, and compare our proposed approach with state-of-the-art results. Experiments show that our proposed approach is effective. Compared with other approaches our approach is more robust, more versatile, easier to compute and simpler to understand.
- Wei-Hao Lin, Alexander Hauptmann. 2009. Identifying news videos' ideological perspectives using emphatic patterns of visual concepts. Abstract: Television news has become the predominant way of understanding the world around us, but individual news broadcasters can frame or mislead an audience's understanding of political and social issues. We are developing a computer system that can automatically identify highly biased television news and encourage audiences to seek news stories from contrasting viewpoints. But can computers identify the ideological perspective from which a news video was produced? We propose a method based on an empathic pattern of visual concepts: news broadcasters holding contrasting ideological beliefs appear to emphasize different subsets of visual concepts. We formalize the emphatic patterns and propose a statistical model. We evaluate the proposed model on a large broadcast news video archive with promising experimental results.
- Xiao Wu, C. Ngo, Alexander Hauptmann, Hung-Khoon Tan. 2009. Real-Time Near-Duplicate Elimination for Web Video Search With Content and Context. Abstract: With the exponential growth of social media, there exist huge numbers of near-duplicate web videos, ranging from simple formatting to complex mixture of different editing effects. In addition to the abundant video content, the social Web provides rich sets of context information associated with web videos, such as thumbnail image, time duration and so on. At the same time, the popularity of Web 2.0 demands for timely response to user queries. To balance the speed and accuracy aspects, in this paper, we combine the contextual information from time duration, number of views, and thumbnail images with the content analysis derived from color and local points to achieve real-time near-duplicate elimination. The results of 24 popular queries retrieved from YouTube show that the proposed approach integrating content and context can reach real-time novelty re-ranking of web videos with extremely high efficiency, where the majority of duplicates can be rapidly detected and removed from the top rankings. The speedup of the proposed approach can reach 164 times faster than the effective hierarchical method proposed in , with just a slight loss of performance.
- Ming-yu Chen, Huan Li, Alexander Hauptmann. 2009. Informedia @ TRECVID2009: Analyzing Video Motions. Abstract: The Informedia team participated in the tasks of high-level feature extraction and event detection in surveillance video. This year, we especially put our focus on analyzing motions in videos. We developed a robust new descriptor called MoSIFT, which explicitly encodes appearance features together with motion information. For the high-level feature detection, we trained multi-modality classifiers which include traditional static features and MoSIFT. The experimental result shows that MoSIFT has solid performance on motion related concepts and is complementary to static features. For event detection, we trained event classifiers in sliding windows using a bag-of-video-word approach. To reduce the number of false alarms, we aggregated short positive windows to favor long segmentation and applied a cascade classifier approach. The performance shows dramatic improvement over last year on the event detection task. 1 MoSIFT This section presents our MoSIFT[7] algorithm to detect and describe spatio-temporal interest points. In part-based methods, there are three major steps: detecting interest points, constructing a feature descriptor, and building a classifier. Detecting interest points reduces the whole video from a volume of pixels to compact but descriptive interest points. Therefore, we desire to develop a detection method, which detects a sufficient number of interest points containing the necessary information to recognize a human action. The MoSIFT algorithm detects spatially distinctive interest points with substantial motion. We first apply the well-known SIFT algorithm to find visually distinctive components in the spatial domain and detect spatiotemporal interest points with (temporal) motion constraints. The motion constraint consists of a 'sufficient' amount of optical flow around the distinctive points. Details of our algorithm are described in the following sections.
- B. Huet, Jinhui Tang, Alexander Hauptmann. 2009. Proceedings of the 1st workshop on Web-scale multimedia corpus. Abstract: Welcome to the 1st International Workshop on Web-Scale Multimedia Corpus (WSMC'09). The purpose of the workshop is to bring together researchers interested in the construction and analysis of Web Scale multimedia datasets and resources. The Workshop will provide a forum to consolidate key factors related to research on very large scale multimedia dataset such as the construction of dataset, creation of ground truth, sharing and extension of such resources in terms of ground truth, features, algorithms and tools etc. The Workshop will discuss and formulate action plan towards these goals. 
 
This workshop welcomes contributions from renowned multimedia researchers on the following topics: 
Construction, Unification and Evolution of Corpus 
Framework for sharing of dataset, ground truth, features, algorithms and tools 
Web-scale Corpus analysis techniques 
Knowledge mining from web-scale multimedia corpus. 
Optimization techniques on web-scale multimedia data for efficiency. 
Techniques for web-scale content-based multimedia retrieval. 
Other related techniques. 
Performance evaluation methodologies and standards 
 
 
 
ACM multimedia 2009 hosts the "Multimedia Grand Challenge" for the 1st time. Web- Scale multimedia applications are among the potential competitors for the Grand Challenge. The availability of a Web-Scale Corpus would effectively boost research in this direction and foster many new applications to the grand challenge for the years to come. 
 
The call for papers attracted 13 submissions from Asia, North-America, Europe and Africa. The program committee accepted 6 high quality papers. In addition, the program includes a panel on the topics addressed by the workshop and a keynote speech. We hope that these proceedings will serve as a valuable reference for multimedia researchers and developers as well as foster new research direction and results.
- Ming-yu Chen, Alexander Hauptmann. 2009. MoSIFT : Recognizing Human Actions in Surveillance Videos CMU-CS-09-161. Abstract: The goal of this paper is to build robust human action recognition for real world surveillance videos. Local spatio-temporal features around interest points provide compact but descriptive representations for video analysis and motion recognition. Current approaches tend to extend spatial descriptions by adding a temporal component for the appearance descriptor, which only implicitly captures motion information. We propose an algorithm called MoSIFT, which detects interest points and encodes not only their local appearance but also explicitly models local motion. The idea is to detect distinctive local features through local appearance and motion. We construct MoSIFT feature descriptors in the spirit of the well-known SIFT descriptors to be robust to small deformations through grid aggregation. We also introduce a bigram model to construct a correlation between local features to capture the more global structure of actions. The method advances the state of the art result on the KTH dataset to an accuracy of 95.8%. We also applied our approach to 100 hours of surveillance data as part of the TRECVID Event Detection task with very promising results on recognizing human actions in the real world surveillance videos.
- Alexander Hauptmann, Wei-Hao Lin. 2009. Identifying ideological perspectives in text and video. Abstract: Polarizing opinions about political and social controversies take place commonly in mass and more recently user-generated media. A functional democratic society builds on civic discussions among people holding different beliefs on an issue. However, so far, few computer technologies have been devoted to facilitate mutual understanding, and arguably could have worsened the situation. 
We envision a computer system that can automatically understand different ideological viewpoints on an issue and identify biased news stories, blog posts, and television news. Such a computer system will raise news readers' awareness of individual sources' biases and encourage them to seek news stories from different viewpoints. 
· Computer understanding of ideological perspectives, however, has been long considered almost impossible. In this thesis, we show that ideology, although very abstract, exhibits a concrete pattern when it is communicated among a group of people who share similar beliefs in written text, spoken text, television news production, and web video folksonomies. This emphatic pattern in ideological discourse opens up a new field of automatic ideological analysis, and enables a large amount of ideological text and video to be automatically analyzed. 
· We develop a new statistical model, called Joint Topic and Perspective Models, based on the emphatic pattern in ideological discourse. The model combines two essential aspects of ideological discourse: topic matters and ideological biases. The simultaneous inference on topics and ideological emphasis, however, poses a computational challenge. We thus develop an approximate inference algorithm for the model based on variational methods. 
· The emphatic pattern in ideological discourse and the Joint Topic and Perspective Model enable many interesting applications in text analysis and multimedia content understanding. At the corpus level, we show that ideological discourse can be reliably distinguished from non-ideological discourse. At the document level, we show that the perspective from which a document is written or a video is produced can be identified with high accuracy. At the sentence level, we extend the model to summarize an ideological document by selecting sentences that strongly express a particular perspective.
- Ming-yu Chen, Alexander Hauptmann. 2009. MoSIFT: Recognizing Human Actions in Surveillance Videos. Abstract: The goal of this paper is to build robust human action recognition for real world surveillance videos. Local spatio-temporal features around interest points provide compact but descriptive representations for video analysis and motion recognition. Current approaches tend to extend spatial descriptions by adding a temporal component for the appearance descriptor, which only implicitly captures motion information. We propose an algorithm called MoSIFT, which detects interest points and encodes not only their local appearance but also explicitly models local motion. The idea is to detect distinctive local features through local appearance and motion. We construct MoSIFT feature descriptors in the spirit of the well-known SIFT descriptors to be robust to small deformations through grid aggregation. We also introduce a bigram model to construct a correlation between local features to capture the more global structure of actions. The method advances the state of the art result on the KTH dataset to an accuracy of 95.8%. We also applied our approach to 100 hours of surveillance data as part of the TRECVID Event Detection task with very promising results on recognizing human actions in the real world surveillance videos.
- B. Huet, Jinhui Tang, Alexander Hauptmann. 2009. ACM SIGMM the first workshop on web-scale multimedia corpus (WSMC09). Abstract: The purpose of this workshop is to bring together researchers interested in the construction and analysis of Web Scale multimedia datasets and resources. The Workshop will provide a forum to consolidate key factors related to research on very large scale multimedia dataset such as the construction of dataset, creation of ground truth, sharing and extension of such resources in terms of ground truth, features, algorithms and tools etc. The Workshop will discuss and formulate action plan towards these goals.
- Wei-Hao Lin, Alexander Hauptmann. 2008. Do These News Videos Portray a News Event from Different Ideological Perspectives?. Abstract: Television news has been the predominant way of understanding the world around us, but individual news broadcasters can frame or mislead audience's understanding about political and social issues. We aim to develop a computer system that can automatically identify highly biased television news and encourages audience to seek news stories from contrasting viewpoints. But can computers identify news videos produced by broadcasters holding differing ideological beliefs? We developed a method of identifying differing ideological perspectives based on a large-scale visual concept ontology, and the experimental results were promising.
- Alexander Hauptmann, R. Baron, Ming-yu Chen, Michael G. Christel, Wei-Hao Lin, Xinghua Sun, Víctor Valdés, Jun Yang, L. Mummert, S. Schlosser. 2008. Informedia @ TRECVID2008: Exploring New Frontiers. Abstract: The Informedia team participated in the tasks of Rushes summarization, high-level feature extraction and event detection in surveillance video. For the rushes summarization, our basic idea was to use subsampled video at the appropriate rate, showing almost the whole video faster, and then modify the result to remove garbage frames. Sinply subsampling the frames proved to be the best method for summarizing BBC rushes video, with other improvements not improving the basic inclusion rate, nor appreciably affecting the other subjective metrics. For the high-level feature detection, we trained exclusively on TRECVID’05 data and trying to assess and predict the reliability of the detectors. The voting scheme for combining multiple classifiers performed best, marginally better than trying to predict the best classifier based on a robustness calculation from within dataset cross-domain performance. For event detection, we found that the overall approach was effective at characterizing a presegmented event in the training data, but lack of event segmentation (information about the duration of an event and the existence of a known event resulted in a dramatically lower score in the official evaluation.
- Cees G. M. Snoek, M. Worring, O. de Rooij, K. V. D. Van De Sande, Rong Yan, Alexander Hauptmann, John R Standards, Smith. 2008. VideOlympics : Real-Time Evaluation of Multimedia Retrieval Systems. Abstract: Alexander G. Hauptmann Carnegie Mellon University Interactive prototypes are often the best way to convince an audience of a new multimedia technology’s possible impact. Because of its dynamic audiovisual nature, a multimedia application demonstration communicates applied science more effectively than a static description in a journal publication would. Ideally, a multimedia demonstrator grasps the audience’s attention by presenting effective results, advanced multimodal interfaces, novel means of user interaction, or combinations thereof. An interactive demo offers researchers a means to engage their audience in a way that they could never achieve with a written manuscript alone. All major multimedia conferences have adopted demo sessions where researchers, equipped with their laptop-installed system, show their demo to the conference attendees on a one-on-one basis. While effective, these demo sessions lack a common denominator. Individual systems provide solutions for different tasks on different data sets, and the conference attendees have to become familiar with the peculiarities of each individual demo. Moreover, a one-on-one demo session makes it infeasible to present a system to the entire conference audience. In addition, due to their lack of focus, the impact of demo sessions on the audience is suboptimal. Establishing focus A good way to establish focus is letting several demo systems solve the same problem on a similar data set. A notable example of such a focused effort takes place at the demo session at the National Institute for Standards and Technology’s (NIST’s) Trecvid workshop (see the ‘‘Trecvid Interactive Video Retrieval Task’’ sidebar). Trecvid promotes progress in video retrieval by providing a large video collection, common retrieval tasks, uniform evaluation procedures, and a forum for researchers interested in comparing their results. In the months preceding the workshop, researchers work on a common retrieval problem. Results are submitted offline and then presented to the benchmark participants during the workshop. In the Trecvid demo session, participants showcase their video-retrieval systems. Because the audience is knowledgeable in the problem area and the task at hand, the result is a lively demo session where the audience gains deep insight in various video-retrieval systems. Unfortunately, the audience at this event is limited to researchers who participate in the evaluation campaign. Of course, the videoretrieval systems are shown at regular demo sessions, but they’re never exposed simultaneously to an audience. A common data set and a common task aid in establishing a focused demo session, but these aren’t sufficient to engage an uninformed audience.
- Wei-Hao Lin, Alexander Hauptmann. 2008. Vox Populi Annotation: Measuring Intensity of Ideological Perspectives by Aggregating Group Judgments. Abstract: Polarizing discussions about political and social issues are common in mass media. Annotations on the degree to which a sentence expresses an ideological perspective can be valuable for evaluating computer programs that can automatically identify strongly biased sentences, but such annotations remain scarce. We annotated the intensity of ideological perspectives expressed in 250 sentences by aggregating judgments from 18 annotators. We proposed methods of determining the number of annotators and assessing reliability, and showed the annotations were highly consistent across different annotator groups.
- H. Wactlar, Robert Walters, John Bertoty, Alexander Hauptmann. 2008. The Aware Community. Abstract: The McKIZ Aware Community will enable us to move the paradigm of an aware and assistive home to the development of an aware and assistive community infrastructure by incorporating devices and methods into a small urban community of homes, recreation facilities, retail and service providers, on city streets with vehicular traffic and public transportation. This broadens our research, data collection and evaluation of persons with disabilities and aging residents to include instrumental activities of daily living and quality of life that extend beyond the confines of their home.
- Alexander Hauptmann, Jonathan J. Wang, Wei-Hao Lin, Jun Yang, Michael G. Christel. 2008. Efficient search: the informedia video retrieval system. Abstract: We introduce an interface for efficient video search that exploits the human ability to quickly scan visual content, after an automatic system has done its best to arrange the images in order of relevance. While extreme video retrieval is taxing to the human, it has also been shown to be very effective. The system will demonstrate several ways to rapidly scan images, change search queries, and employ different types of relevance feedback.
- Jun Yang, Alexander Hauptmann. 2008. (Un)Reliability of video concept detection. Abstract: Great effort has been made to improve video concept detection and continuous progress has been reported. With the current evaluation method being confined to carefully annotated domains and thus quite forgiving, the reliability of the state-of-the-art concept classifiers remains in question. Adopting a more rigorous evaluation approach, we find that most concept classifiers built using the mainstream approach are unreliable because they generalize poorly to domains other than their training domain. Moreover, evidences show that SVM-based concept classifiers learn little beyond memorizing most of the positive training data, and behave close to memory-based models such as kNN indicated by comparable performance between the two models. Examining the properties of the reliable concept classifiers, we find that the classifiers of frequent concepts, "bloated" classifiers, and classifiers capable of learning the pattern of data, tend to be more reliable. This paper contributes to a better understanding of concept detection, suggests heuristics to identify reliable concept classifiers, and discusses solutions to improving concept detection reliability.
- Alexander Hauptmann. 2008. Multimedia Information Extraction Roadmap. Abstract: The broad challenge in my view is to exploit multi-lingual, multimedia information from both web and TV video to allow broader understanding of different ideological, social, and cultural perspectives in different sources, for a wide variety of applications. This will involve the judicious analysis of the text and video features using a variety of machine learning and language analysis methods, as well as understanding of the video editing structure, as well as the context in which the media appears. Other challenges involve dealing with the flood of data through mechanisms for intelligent, context-driven summarization, as our brains remain limited in the amount of information they can process. Yet different challenges concern the mobile use of multimedia data, considering limited bandwidth, small displays, limited multimedia input mechanisms as well as the social network that provides the context of the media – however, the latter challenge won’t be addressed in the following paragraphs.
- Xiao Wu, C. Ngo, Alexander Hauptmann. 2008. Multimodal News Story Clustering With Pairwise Visual Near-Duplicate Constraint. Abstract: Story clustering is a critical step for news retrieval, topic mining, and summarization. Nonetheless, the task remains highly challenging owing to the fact that news topics exhibit clusters of varying densities, shapes, and sizes. Traditional algorithms are found to be ineffective in mining these types of clusters. This paper offers a new perspective by exploring the pairwise visual cues deriving from near-duplicate keyframes (NDK) for constraint-based clustering. We propose a constraint-driven co-clustering algorithm (CCC), which utilizes the near-duplicate constraints built on top of text, to mine topic-related stories and the outliers. With CCC, the duality between stories and their underlying multimodal features is exploited to transform features in low-dimensional space with normalized cut. The visual constraints are added directly to this new space, while the traditional DBSCAN is revisited to capitalize on the availability of constraints and the reduced dimensional space. We modify DBSCAN with two new characteristics for story clustering: 1) constraint-based centroid selection and 2) adaptive radius. Experiments on TRECVID-2004 corpus demonstrate that CCC with visual constraints is more capable of mining news topics of varying densities, shapes and sizes, compared with traditional k-means, DBSCAN, and spectral co-clustering algorithms.
- Wei-Hao Lin, Alexander Hauptmann. 2008. Identifying Ideological Perspectives of Web Videos Using Folksonomies. Abstract: We are developing a classifier that can automatically identify a web video’s ideological perspective on a political or social issue (e.g., pro-life or pro-choice on the abortion issue). The problem has received little attention, possibly due to inherent difficulties in content-based approaches. We propose to develop such a classifier based on the pattern of tags emerging from folksonomies. The experimental results are positive and encouraging.
- Alexander Hauptmann, Michael G. Christel, Rong Yan. 2008. Video Retrieval Based on Semantic Concepts Results of retrieval experiments are analyzed and evaluated to explore the usefulness of descriptive-language to accurately retrieve video and audio material.. Abstract: An approach using many intermediate semantic concepts is proposed with the potential to bridge the semantic gap between what a color, shape, and texture-based Blow- level( image analysis can extract from video and what users really want to find, most likely using text descriptions of their information needs. Semantic concepts such as cars, planes, roads, people, animals, and different types of scenes (outdoor, night time, etc.) can be automatically detected in the video with reasonable accuracy. This leads us to ask how can they be used automatically and how does a user (or a retrieval system) translate the user's information need into a selection of related concepts that would help find the relevant video clips, from the large list of available concepts. We illustrate how semantic concept retrieval can be automatically exploited by mapping queries into query classes and through pseudo-relevance feedback. We also provide evidence how a semantic concept can be utilized by users in interactive retrieval, through interfaces that provide affordances of explicit concept selec- tion and search, concept filtering, and relevance feedback. How many concepts we actually need and how accurately they need to be detected and linked through various relationships is specified in the ontology structure.
- Wei-Hao Lin, Alexander Hauptmann. 2008. Identifying News Broadcasters’ Ideological Perspectives Using a Large-Scale Video Ontology. Abstract: Television news has been the predominant way of understanding the world around us, but individual news broadcasters can frame or mislead audience’s understanding about political and social issues. We aim to develop a computer system that can automatically identify highly biased television news, which may prompt audience to seek news stories from contrasting viewpoints. But can computers determine if news videos were produced by broadcasters holding differing ideological beliefs? We developed a method of identifying differing ideological perspectives based on a large-scale visual concept ontology, and the experimental results were promising.
- Wei-Hao Lin, Alexander Hauptmann. 2008. Identifying Ideological Perspectives of Web Videos using Patterns Emerging from Folksonomies. Abstract: We are developing a classier that can automatically identify a web video's ideological perspective on a political or social issue (e.g., pro-life or pro-choice on the abortion issue). The problem has received little attention, possibly due to inherent diculties in content-based ap- proaches. We propose to develop such a classier based on the pattern of tags emerging from folksonomies. The experimental results are positive and encouraging.
- Cees G. M. Snoek, M. Worring, O. D. Rooij, K. V. D. Sande, Rong Yan, Alexander Hauptmann. 2008. VideOlympics: Real-Time Evaluation of Multimedia Retrieval Systems. Abstract: The first VideOlympics brings content-based analysis to the archive and allows for many-to- many communication between video search engines and their audience It was a great Success. The VideOlympics provided the excitement of a competition without the associated stress on the participants. For the first time, the audience was able to compare different multimedia retrieval systems on the same tasks and see how they performed with unrehearsed topics. Many audience members felt they understood the technology's capabilities after seeing it in live action and in several system variations.
- Dipanjan Das, Datong Chen, Alexander Hauptmann. 2008. Improving multimedia retrieval with a video OCR. Abstract: We present a set of experiments with a video OCR system (VOCR) tailored for video information retrieval and establish its importance in multimedia search in general and for some specific queries in particular. The system, inspired by an existing work on text detection and recognition in images, has been developed using techniques involving detailed analysis of video frames producing candidate text regions. The text regions are then binarized and sent to a commercial OCR resulting in ASCII text, that is finally used to create search indexes. The system is evaluated using the TRECVID data. We compare the system's performance from an information retrieval perspective with another VOCR developed using multi-frame integration and empirically demonstrate that deep analysis on individual video frames result in better video retrieval. We also evaluate the effect of various textual sources on multimedia retrieval by combining the VOCR outputs with automatic speech recognition (ASR) transcripts. For general search queries, the VOCR system coupled with ASR sources outperforms the other system by a very large extent. For search queries that involve named entities, especially people names, the VOCR system even outperforms speech transcripts, demonstrating that source selection for particular query types is extremely essential.
- Alexander Hauptmann, Michael G. Christel, Rong Yan. 2008. Video Retrieval Based on Semantic Concepts. Abstract: An approach using many intermediate semantic concepts is proposed with the potential to bridge the semantic gap between what a color, shape, and texture-based ldquolow-levelrdquo image analysis can extract from video and what users really want to find, most likely using text descriptions of their information needs. Semantic concepts such as cars, planes, roads, people, animals, and different types of scenes (outdoor, night time, etc.) can be automatically detected in the video with reasonable accuracy. This leads us to ask how can they be used automatically and how does a user (or a retrieval system) translate the user's information need into a selection of related concepts that would help find the relevant video clips, from the large list of available concepts. We illustrate how semantic concept retrieval can be automatically exploited by mapping queries into query classes and through pseudo-relevance feedback. We also provide evidence how a semantic concept can be utilized by users in interactive retrieval, through interfaces that provide affordances of explicit concept selection and search, concept filtering, and relevance feedback. How many concepts we actually need and how accurately they need to be detected and linked through various relationships is specified in the ontology structure.
- Michael G. Christel, Alexander Hauptmann, Wei-Hao Lin, Ming-yu Chen, Jun Yang, Bryan Maher, R. Baron. 2008. Exploring the utility of fast-forward surrogates for bbc rushes. Abstract: This paper discusses in detail our approaches for producing the video summaries submitted to the TRECVID 2008 BBC rushes summarization task, including the baseline method. Empirical work produced during and after the TRECVID 2007 rushes summarization task gave strong evidence that a simple 50x method (sampling every 50th frame) provides excellent coverage (text inclusion performance). Our submissions for TRECVID 2008 investigated the effects of junk frame removal, including a comprehensible audio track, and emphasizing pans and zooms when backfilling to reclaim the space removed with the noise shots from the original 50x set. Results show that 50x based methods provide excellent coverage as expected. There were limited effects for the other strategies to improve user satisfaction, with the discussion providing some insights for future video summary development and evaluation work.
- Jun Yang, Alexander Hauptmann. 2008. A framework for classifier adaptation and its applications in concept detection. Abstract: There is often a need to adapt supervised classifiers such as semantic concept detectors across different domains of data. This paper describes a generic framework for function-level classifier adaptation based on regularized loss minimization. It directly modifies the decision function of an existing classifier of any type into a classifier for a new domain, based on limited labeled data in the new domain and no "old data", which makes it an efficient and flexible framework. We then extend this framework to adapt multiple classifiers into one classifier, with the weights of existing classifiers learned automatically to reflect their utility. We elaborate on two concrete adaptation algorithms derived from the framework, namely adaptive SVM and multi-adaptive SVM, for one-to-one and many-to-one adaptation respectively. In the experiments of adapting semantic concept detectors across video channels/types, our adaptation approach is proven to be superior to using original (unadapted) classifiers or building new ones in terms of accuracy and labeling effort.
- Jun Yang, Yu-Gang Jiang, Alexander Hauptmann, C. Ngo. 2007. Evaluating bag-of-visual-words representations in scene classification. Abstract: Based on keypoints extracted as salient image patches, an image can be described as a "bag of visual words" and this representation has been used in scene classification. The choice of dimension, selection, and weighting of visual words in this representation is crucial to the classification performance but has not been thoroughly studied in previous work. Given the analogy between this representation and the bag-of-words representation of text documents, we apply techniques used in text categorization, including term weighting, stop word removal, feature selection, to generate image representations that differ in the dimension, selection, and weighting of visual words. The impact of these representation choices to scene classification is studied through extensive experiments on the TRECVID and PASCAL collection. This study provides an empirical basis for designing visual-word representations that are likely to produce superior classification performance.
- Alexander Hauptmann, Ming-yu Chen, Michael G. Christel, Wei-Hao Lin, Jun Yang. 2007. A Hybrid Approach to Improving Semantic Extraction of News Video. Abstract: In this paper we describe a hybrid approach to improving semantic extraction from news video. Experiments show the value of careful parameter tuning, exploiting multiple feature sets and multilingual linguistic resources, applying text retrieval approaches for image features, and establishing synergy between multiple concepts through undirected graphical models. No single approach provides a consistently better result for every concept detection, which suggests that extracting video semantics should exploit multiple resources and techniques rather than a single approach.
- Alexander Hauptmann, Rong Yan, Wei-Hao Lin. 2007. How many high-level concepts will fill the semantic gap in news video retrieval?. Abstract: A number of researchers have been building high-level semantic concept detectors such as outdoors, face, building, etc., to help with semantic video retrieval. Using the TRECVID video collection and LSCOM truth annotations from 300 concepts, we simulate performance of video retrieval under different assumptions of concept detection accuracy. Even low detection accuracy provides good retrieval results, when sufficiently many concepts are used. Considering this extrapolation under reasonable assumptions, this paper arrives at the conclusion that "concept-based" video retrieval with fewer than 5000 concepts, detected with minimal accuracy of 10% mean average precision is likely to provide high accuracy results, comparable to text retrieval on the web, in a typical broadcast news collection. We also derive evidence that it should be feasible to find sufficiently many new, useful concepts that would be helpful for retrieval.
- Jun Yang, Rong Yan, Alexander Hauptmann. 2007. Adapting SVM Classifiers to Data with Shifted Distributions. Abstract: Many data mining applications can benefit from adapt- ing existing classifiers to new data with shifted distribu- tions. In this paper, we present Adaptive Support Vector Machine (Adapt-SVM) as an efficient model for adapting a SVM classifier trained from one dataset to a new dataset where only limited labeled examples are available. By in- troducing a new regularizer into SVM's objective function, Adapt-SVM aims to minimize both the classification error over the training examples, and the discrepancy between the adapted and original classifier. We also propose a selective sampling strategy based on the loss minimization principle to seed the most informative examples for classifier adap- tation. Experiments on an artificial classification task and on a benchmark video classification task shows that Adapt- SVM outperforms several baseline methods in terms of ac- curacy and/or efficiency.
- Alexander Hauptmann, Rong Yan, Wei-Hao Lin, Michael G. Christel, H. Wactlar. 2007. Can High-Level Concepts Fill the Semantic Gap in Video Retrieval? A Case Study With Broadcast News. Abstract: A number of researchers have been building high-level semantic concept detectors such as outdoors, face, building, to help with semantic video retrieval. Our goal is to examine how many concepts would be needed, and how they should be selected and used. Simulating performance of video retrieval under different assumptions of concept detection accuracy, we find that good retrieval can be achieved even when detection accuracy is low, if sufficiently many concepts are combined. We also derive suggestions regarding the types of concepts that would be most helpful for a large concept lexicon. Since our user study finds that people cannot predict which concepts will help their query, we also suggest ways to find the best concepts to use. Ultimately, this paper concludes that "concept-based" video retrieval with fewer than 5000 concepts, detected with a minimal accuracy of 10% mean average precision is likely to provide high accuracy results in broadcast news retrieval.
- Alexander Hauptmann, Michael G. Christel, Wei-Hao Lin, Bryan Maher, Jun Yang, R. Baron, Guang Xiang. 2007. Summarizing BBC Rushes the Informedia Way. Abstract: For the first time in 2007, TRECVID considered structured evaluation of automated video summarization, utilizing BBC rushes video. This paper discusses in detail our approaches for producing the submitted summaries to TRECVID, including the two baseline methods. The cluster method performed well in terms of coverage, and adequately in terms of user satisfaction, but did take longer to review. We conducted additional evaluations using the same TRECVID assessment interface to judge 2 additional methods for summary generation: 25x (simple speed-up by 25 times), and pz (emphasizing pans and zooms). Data from 4 human assessors shows significant differences between the cluster, pz, and 25x approaches. The best coverage (text inclusion performance) is obtained by 25x, but at the expense of taking the most time to evaluate and perceived as the most redundant. Method pz was easier to use than cluster and had better performance on pan/zoom recall tasks, leading into discussions on how summaries can be improved with more knowledge of the anticipated users and tasks.
- Alexander Hauptmann, Michael G. Christel, Wei-Hao Lin, Bryan Maher, Jun Yang, R. Baron, Guang Xiang. 2007. Clever Clustering vs . Simple Speed-Up for Summarizing BBC Rushes. Abstract: This paper discusses in detail our approaches for producing the submitted summaries to TRECVID, including the two baseline methods. The cluster method performed well in terms of coverage, and adequately in terms of user satisfaction, but did take longer to review. We conducted additional evaluations using the same TRECVID assessment interface to judge 2 additional methods for summary generation: 25x (simple speed-up by 25 times), and pz (emphasizing pans and zooms). Human assessors show significant differences between the cluster, pz, and 25x approaches. The best coverage (text inclusion performance) is obtained by 25x, but at the expense of taking the most time to evaluate and perceived as the most redundant. Method pz was easier to use than cluster and had better performance on pan/zoom recall tasks, leading into discussions on how summaries can be improved with more knowledge of the anticipated users and tasks.
- Yan Liu, Jun Yang, Alexander Hauptmann. 2007. Undirected Graphical Models for Video Analysis and Classification. Abstract: Accurate and efficient video classification and retrieval demands the fusion of multimodal information and the use of intermediate representations. This paper describes an undirected graphical model based on exponential-family harmonium, which derives intermediate semantic representations of video data by jointly modeling the textual and image information in the video. We propose an extension of the model to derive category-specific video representation and integrate video classification as a part of the modeling process. We report satisfactory classification performance on a set of 15 video categories from TRECVID collection as well as comparison on the effectiveness of different inference algorithms.
- Jun Yang, Yan Liu, E. Xing, Alexander Hauptmann. 2007. Harmonium Models for Semantic Video Representation and Classification. Abstract: Accurate and efficient video classification demands the fusion of multimodal information and the use of intermediate representations. Combining the two ideas into the same framework, we propose a probabilistic approach for video classification using intermediate semantic representations derived from the multi-modal features. Based on a class of bipartite undirected graphical models named harmonium, our approach represents video data as latent semantic topics derived by jointly modeling the transcript keywords and color-histogram features, and perform classification using these latent topics under a unified framework. We show satisfactory classification performance of our approach on a benchmark dataset, and some interesting insights of the data provided by this approach.
- Jun Yang, Rong Yan, Alexander Hauptmann. 2007. Cross-domain video concept detection using adaptive svms. Abstract: Many multimedia applications can benefit from techniques for adapting existing classifiers to data with different distributions. One example is cross-domain video concept detection which aims to adapt concept classifiers across various video domains. In this paper, we explore two key problems for classifier adaptation: (1) how to transform existing classifier(s) into an effective classifier for a new dataset that only has a limited number of labeled examples, and (2) how to select the best existing classifier(s) for adaptation. For the first problem, we propose Adaptive Support Vector Machines (A-SVMs) as a general method to adapt one or more existing classifiers of any type to the new dataset. It aims to learn the "delta function" between the original and adapted classifier using an objective function similar to SVMs. For the second problem, we estimate the performance of each existing classifier on the sparsely-labeled new dataset by analyzing its score distribution and other meta features, and select the classifiers with the best estimated performance. The proposed method outperforms several baseline and competing methods in terms of classification accuracy and efficiency in cross-domain concept detection in the TRECVID corpus.
- Ming-yu Chen, Alexander Hauptmann. 2007. Discriminative Fields for Modeling Semantic Concepts in Video. Abstract: A current trend in video analysis research hypothesizes that a very large number of semantic concepts could provide a novel way to characterize, retrieve and understand video. These semantic concepts do not appear in isolatation to each other and thus it could be very useful to exploit the relationships between multiple semantic concepts to enhance the concept detection performance in video. In this paper we present a discriminative learning framework called Multi-concept Discriminative Random Field (MDRF) for building probabilistic models of video semantic concept detectors by incorporating related concepts as well as the low-level observations. The proposed model exploits the power of discriminative graphical models to simultaneously capture the associations of concept with observed data and the interactions between related concepts. Compared with previous methods, this model not only captures the co-occurrence between concepts but also incorporates the raw data observations into a unified framework. We also present an approximate parameter estimation algorithm and apply it to the TRECVID 2005 data. Our experiments show promising results compared to the single concept learning approach for semantic concept detection in video.
- Alexander Hauptmann, Rong Yan, Wei-Hao Lin. 2007. How many high-level concepts will fill the semantic gap in video retrieval ?. Abstract: A number of researchers have been building high-level seman tic concept detectors such as outdoors, face, building, etc., t o help with semantic video retrieval. Using the TRECVID video collecti on and LSCOM truth annotations from 300 concepts, we simulate perf ormance of video retrieval under different assumptions of con cept detection accuracy. Even low detection accuracy provides g ood retrieval results, when sufficiently many concepts are used. C onsidering this extrapolation under reasonable assumptions, th is paper arrives at the conclusion that “concept-based” video retri eval with fewer than 5000 concepts, detected with minimal accuracy of 10% mean average precision is likely to provide high accuracy re sults, comparable to text retrieval on the web, in a typical broadca st news collection. We also derive evidence that it should be feasib le to find sufficiently many new, useful concepts that would be help ful for retrieval.
- Rong Yan, Alexander Hauptmann. 2007. Query expansion using probabilistic local feedback with application to multimedia retrieval. Abstract: As one of the most effective query expansion approaches, local feedback is able to automatically discover new query terms and improve retrieval accuracy for different retrieval models. However, the performance of local feedback is heavily dependent on the assumption that most top-ranked documents are relevant to the query topic. Although this assumption might be sensible for ad-hoc text retrieval, it is usually violated in many other retrieval tasks such as multimedia retrieval. In this paper, we develop a robust local analysis approach called probabilistic local feedback (PLF) based on a discriminative probabilistic retrieval framework. The proposed model is effective for improving retrieval accuracy without assuming the most top-ranked documents are relevant. It also provides a sound probabilistic interpretation and a convergence guarantee on the iterative result updating process. Although derived from variational techniques, this approach only involves an iterative process of simple operations on ranking features and thus can be computed efficiently in practice. Our multimedia retrieval experiments on TRECVID'03-'05 collections have demonstrated the advantage of the proposed PLF approaches which can achieve noticeable gains in terms of mean average precision over various baseline methods and PRF-augmented results.
- Michael G. Christel, Alexander Hauptmann. 2007. Exploring Concept Selection Strategies for Interactive Video Search. Abstract: Ranked shot lists from 39 automated LSCOM-Lite concept classifiers are investigated with respect to 24 TRECVID 2006 topics. Selecting the best fitting concept or pair of concepts produces the shot set with greatest utility, rather than drawing fewer shots from a larger set of concepts. Mean average precision measures show concept-based shot sets have great utility for topics when perfectly traversed by a user. Using empirical data, however, shows that realistic ability to separate relevant shots from irrelevant ones and recall all the relevant ones is topic-dependent and far from perfect. Concept-based strategies including user-driven selection strategies not using idealized oracle prioritization are also discussed, with implications for query-by-concept in interactive video retrieval as concept spaces grow from tens to thousands.
- Alexander Hauptmann. 2007. UNDIRECTEDGRAPHICALMODELS FORVIDEOANALYSISAND CLASSIFICATION. Abstract: Accurate andefficient videoclassification andretrieval demandsthefusion ofmultimodal information andtheuseof intermediate representations. Thispaperdescribes anundirected graphical modelbasedonexponential-family harmonium, whichderives intermediate semantic representations of video databyjointly modeling thetextual andimageinformation inthevideo. Wepropose anextension ofthemodel toderive category-specific video representation andintegrate video classification asapartofthemodeling process. We report satisfactory classification performance onasetof15 video categories fromTRECVIDcollection aswellascomparison ontheeffectiveness ofdifferent inference algorithms.
- Xiao Wu, Alexander Hauptmann, C. Ngo. 2007. Novelty detection for cross-lingual news stories with visual duplicates and speech transcripts. Abstract: An overwhelming volume of news videos from different channels and languages is available today, which demands automatic management of this abundant information. To effectively search, retrieve, browse and track cross-lingual news stories, a news story similarity measure plays a critical role in assessing the novelty and redundancy among them. In this paper, we explore the novelty and redundancy detection with visual duplicates and speech transcripts for cross-lingual news stories. News stories are represented by a sequence of keyframes in the visual track and a set of words extracted from speech transcript in the audio track. A major difference to pure text documents is that the number of keyframes in one story is relatively small compared to the number of words and there exist a large number of non-near duplicate keyframes. These features make the behavior of similarity measures different compared to traditional textual collections. Furthermore, the textual features and visual features complement each other for news stories. They can be further combined to boost the performance. Experiments on the TRECVID-2005 cross-lingual news video corpus show that approaches on textual features and visual features demonstrate different performance, and measures on visual features are quite effective. Overall, the cosine distance on keyframes is still a robust measure. Language models built on visual features demonstrate promising performance. The fusion of textual and visual features improves overall performance.
- Alexander Hauptmann, T. Kanade, Scott Stevens, Ashok Bharucha. 2007. CareMedia: Automated Video and Sensor Analysis for Geriatric Care. Abstract: In the absence of objective, reliable assessment and outcomes measurement methodologies in a nursing home, effectiveness of behavioral and pharmacological interventions cannot be determined. Pervasive technology holds the promise of developing objective, real-time, continuous assessment and outcomes measurement methodologies that were previously unfeasible. Such technologies can contribute greatly to a deeper understanding of the activity and behavior patterns of individual residents, and the physical, environmental and psychosocial correlates of these patterns. Bharucha, A., Allin, S. and Stevens, S., “CareMedia: Towards Automated Behavior Analysis in the Nursing Home Setting,” in The International Psychogeriatric Association Eleventh International Conference, Aug. 17-22, 2003. Several hours of surveillance-type video were captured in a nursing home. The task of data reduction and extraction of highlevel activity information was approached through both automated and manual techniques. For the manual encoding, 4 undergraduate students were trained by a geriatric psychiatrist to code the data frame-by-frame. A computer interface allowed coders to annotate behaviors of interest, as well as physical pose and ambulatory status. Behaviors of interest were identified with the CohenMansfield Agitation Inventory and grouped into 4 sub-categories: CareMedia Carnegie Mellon University 8 CareMedia: Automated Video and Sensor Analysis for Geriatric Care March 2003 Annual Progress Report physically aggressive, physically non-aggressive, verbally aggressive, and verbally non-aggressive. These manual encodings are currently forming the development of automated techniques at Carnegie Mellon University to extract information relevant to the detection of anomalous and disruptive physical activities. This includes automated tracking and extraction of navigational patterns. Gao, J., Hauptmann, A.G., Barucha, A. and Wactlar, H.D., “Dining Activity Analysis Using Hidden Markov Models,” accepted to The 17th International Conference on Pattern Recognition (ICPR’04), Cambridge, United Kingdom, Aug. 23-26, 2004. Abstract: We describe an algorithm for dining activity analysis in a nursing home. Based on several features, including motion vectors and distance between moving regions in the subspace of an individual person, a hidden Markov model is proposed to characterize different stages in dining activities with certain temporal order. Using HMM model, we are able to identify the start (and ending) of individual dining events with high accuracy and low false positive rate. This approach could be successful in assisting caregivers in assessments of resident's activity levels over time. We describe an algorithm for dining activity analysis in a nursing home. Based on several features, including motion vectors and distance between moving regions in the subspace of an individual person, a hidden Markov model is proposed to characterize different stages in dining activities with certain temporal order. Using HMM model, we are able to identify the start (and ending) of individual dining events with high accuracy and low false positive rate. This approach could be successful in assisting caregivers in assessments of resident's activity levels over time. Gao, J., Hauptmann, A.G. and Wactlar, H.D., “Combining Motion Segmentation with Tracking for Activity Analysis,” submitted to The Sixth International Conference on Automatic Face and Gesture Recognition (FG’04), Seoul, Korea, May 17-19, 2004. Abstract: We explore a novel motion feature as the appropriate basis for classifying or describing a number of fine motor human activities. Our approach not only estimates motion directions and magnitudes in different image regions, but also provides accurate segmentation of moving regions. Through a combination of motion segmentation and region tracking techniques, while filtering for temporal consistency, we achieve a balance between accuracy and reliability of motion feature extraction. To identify specific activities, we characterize the dominant directions of relative motions. Experimental results show that this approach to motion feature analysis could be successful in assisting caregivers at a nursing home in assessments of patient's activity levels over time. We explore a novel motion feature as the appropriate basis for classifying or describing a number of fine motor human activities. Our approach not only estimates motion directions and magnitudes in different image regions, but also provides accurate segmentation of moving regions. Through a combination of motion segmentation and region tracking techniques, while filtering for temporal consistency, we achieve a balance between accuracy and reliability of motion feature extraction. To identify specific activities, we characterize the dominant directions of relative motions. Experimental results show that this approach to motion feature analysis could be successful in assisting caregivers at a nursing home in assessments of patient's activity levels over time. Hauptmann, A.G., Gao, J., Yan, R., Qi, Y., Yang, J., and Wactlar, H.D., “Aiding Geriatric Patients and Caregivers through Automated Analysis of Nursing Home Observations,” to be published in IEEE Pervasive Computing, April-June special issue: Pervasive Computing for Successful Aging. Abstract: Through pervasive activity monitoring in a skilled nursing facility, a continuous audio and video record is captured. Through pervasive activity monitoring in a skilled nursing facility, a continuous audio and video record is captured. CareMedia Carnegie Mellon University 9 CareMedia: Automated Video and Sensor Analysis for Geriatric Care March 2003 Annual Progress Report Our CareMedia Project research analyzes this video information by automatically tracking people, assisting in efficiently labeling individuals, and characterizing selected activities and actions. Special emphasis is given to detecting eating activity in the dining hall and to personal hygiene. Through this work, the video record is transformed into an information asset that can provide geriatric care specialists with greater insights and evaluation of behavioral problems for the elderly. Evaluations of the effectiveness of analyzing such a large video record illustrate the feasibility of our approach. Hauptmann, A.G., Jin, R. and Wactlar, H.D., “Data Analysis for a Multimedia Library, in Text and Speech-Triggered Information Access,” Renals, S and Grefenstette, G. (eds)., Springer, Berlin, pp. 6-37, 2003. Abstract: This book section describes the indexing, search and retrieval of various combinations of audio, video, text and image media and the automated content processing that enables it. The intent is to provide a framework for data analysis in multimedia digital libraries. The introduction briefly distinguishes the digital from traditional libraries and touches on the specific issues important to searching the content of multimedia libraries. The second section introduces the Informedia Digital Video Library as an example of a multimedia library, including a quick tour of the functionality. The next section discusses the processing of audio and image information, as it relates to a multimedia library. Section four illustrates the interplay between audio and video information using a video information retrieval experiment as an example. Section five discusses the exporting and sharing of metadata in a digital library using MPEG-7. Finally, section 6 provides one vision of a future digital library, where all personal memory can be recorded and accessed. This book section describes the indexing, search and retrieval of various combinations of audio, video, text and image media and the automated content processing that enables it. The intent is to provide a framework for data analysis in multimedia digital libraries. The introduction briefly distinguishes the digital from traditional libraries and touches on the specific issues important to searching the content of multimedia libraries. The second section introduces the Informedia Digital Video Library as an example of a multimedia library, including a quick tour of the functionality. The next section discusses the processing of audio and image information, as it relates to a multimedia library. Section four illustrates the interplay between audio and video information using a video information retrieval experiment as an example. Section five discusses the exporting and sharing of metadata in a digital library using MPEG-7. Finally, section 6 provides one vision of a future digital library, where all personal memory can be recorded and accessed. Jin, R., Hauptmann, A., Carbonell, J., Si, L., Liu, Y., “A New Boosting Algorithm Using Input Dependent Regularizer,” 20th International Conference on Machine Learning (ICML'03), Washington, DC, August 21-24, 2003. Abstract: AdaBoost has proved to be an effective method to improve the performance of base classifiers both theoretically and empirically. However, previous studies have shown that AdaBoost might suffer from the overfitting problem, especially for noisy data. In addition, most current work on boosting assumes that the combination weights are fixed constants and therefore does not take particular input patterns into consideration. In this paper, we present a new boosting algorithm, “WeightBoost”, which tries to solve these two problems by introducing an input-dependent regularization factor to the combination weight. Similarly to AdaBoost, we derive a learning procedure for WeightBoost, which AdaBoost has proved to be an effective method to improve the performance of base classifiers both theoretically and empirically. However, previous studies have shown that AdaBoost might suffer from the overfitting problem, especially for noisy data. In addition, most current work on boosting assumes that the combination weights are fixed constants and therefore does not take particular input p
- Alexander Hauptmann, Michael G. Christel, Wei-Hao Lin, Bryan Maher, Jun Yang, R. Baron, Guang Xiang. 2007. Clever clustering vs. simple speed-up for summarizing rushes. Abstract: This paper discusses in detail our approaches for producing the submitted summaries to TRECVID, including the two baseline methods. The cluster method performed well in terms of coverage, and adequately in terms of user satisfaction, but did take longer to review. We conducted additional evaluations using the same TRECVID assessment interface to judge 2 additional methods for summary generation: 25x (simple speed-up by 25 times), and pz (emphasizing pans and zooms). Human assessors show significant differences between the cluster, pz, and 25x approaches. The best coverage (text inclusion performance) is obtained by 25x, but at the expense of taking the most time to evaluate and perceived as the most redundant. Method pz was easier to use than cluster and had better performance on pan/zoom recall tasks, leading into discussions on how summaries can be improved with more knowledge of the anticipated users and tasks.
- Alexander Hauptmann, Rong Yan. 2006. Probabilistic models for combining diverse knowledge sources in multimedia retrieval. Abstract: In recent years, the multimedia retrieval community is gradually shifting its emphasis from analyzing one media source at a time to exploring the opportunities of combining diverse knowledge sources from correlated media types and context. In order to combine multimedia knowledge sources, two basic issues must be addressed: what to combine and how to combine. While considerable effort has been expended to generate a wide range of ranking features from knowledge sources, relatively less attention has been given to the problem of finding a suitable strategy to combine them. It has always been a significant challenge to develop principled combination approaches and capture useful factors such as query information and context information in the retrieval process. 
This thesis presents a conditional probabilistic retrieval model as a principled framework to combine diverse knowledge sources. This model can integrate multiple forms of ranking features (query dependent and query independent features) as well as query information and context information in a unified framework with a solid probabilistic foundation. Under this retrieval framework, we overview and develop a number of state-of-the-art approaches for extracting ranking features from multimedia knowledge sources. In order to deal with heterogenous features, a discriminative learning approach is suggested for estimating the combination parameters. Moreover, an efficient rank learning approach has been developed to explicitly model the ranking relations in the learning process with much less training time. 
To incorporate query information in the combination model, this thesis develops a number of query analysis models that can automatically discover mixing structure of the query space based on previous retrieval results, and predict combination parameters for unseen queries. In more detail, we propose the query-class based analysis model which needs to manually define the query classes and a series of probabilistic latent query analysis(pLQA) models which can automatically discover latent query classes from the development data by unifying the combination weight optimization and query class categorization into a discriminative learning framework. To adapt the combination function on a per query basis, this thesis also presents a probabilistic local context analysis(pLCA) model to automatically leverage additional retrieval sources to improve initial retrieval outputs. A pLCA variant is proposed to utilize human feedback to adjust combination parameters. 
All the proposed approaches are evaluated on multimedia retrieval tasks with large-scale video collections. Beyond multimedia collections, we also evaluate our approaches on meta-search tasks with large-scale text collections. Experi mental evaluations demonstrate the promising performance of the probabilistic retrieval framework with query analysis and context analysis in the task of knowledge source combination. The applicability of the proposed methods can be extended to many other areas, such as question answering, web IR, cross-lingual IR, multi-sensor fusion, human tracking, and so forth.
- Cees G. M. Snoek, M. Worring, Alexander Hauptmann. 2006. Learning rich semantics from news video archives by style analysis. Abstract: We propose a generic and robust framework for news video indexing which we founded on a broadcast news production model. We identify within this model four production phases, each providing useful metadata for annotation. In contrast to semiautomatic indexing approaches which exploit this information at production time, we adhere to an automatic data-driven approach. To that end, we analyze a digital news video using a separate set of multimodal detectors for each production phase. By combining the resulting production-derived features into a statistical classifier ensemble, the framework facilitates robust classification of several rich semantic concepts in news video; rich meaning that concepts share many similarities in their production process. Experiments on an archive of 120 hours of news video from the 2003 TRECVID benchmark show that a combined analysis of production phases yields the best results. In addition, we demonstrate that the accuracy of the proposed style analysis framework for classification of several rich semantic concepts is state-of-the-art.
- Wei-Hao Lin, Alexander Hauptmann. 2006. Are These Documents Written from Different Perspectives? A Test of Different Perspectives Based on Statistical Distribution Divergence. Abstract: In this paper we investigate how to automatically determine if two document collections are written from different perspectives. By perspectives we mean a point of view, for example, from the perspective of Democrats or Republicans. We propose a test of different perspectives based on distribution divergence between the statistical models of two collections. Experimental results show that the test can successfully distinguish document collections of different perspectives from other types of collections.
- Jun Yang, Alexander Hauptmann. 2006. Exploring temporal consistency for video analysis and retrieval. Abstract: Temporal consistency is ubiquitous in video data, where temporally adjacent video shots usually share similar visual and semantic content.This paper presents a thorough study of temporal consistency defined with respect to semantic concepts and query topics using quantitative measures,and discusses its implications to video analysis and retrieval tasks. We further show that,in interactive settings, using temporal consistency leads to considerable improvement on the performance of semantic concept detection and retrieval of video data.Speci fically,an active learning method with temporal sampling strategy is proposed for building classifiers of semantic concepts,and a temporal reranking method is proposed for improving the efficiency of interactive video search.Both methods outperform existing methods by considerable margins on the TRECVID dataset.
- James Ze Wang, N. Boujemaa, A. Bimbo, D. Geman, Alexander Hauptmann, Jelena Tešić. 2006. Diversity in multimedia information retrieval research. Abstract: Multimedia information retrieval is a highly diverse field. A variety of data types, research problems,methodologies are involved.Researchers in the field come from very different disciplines, ranging from mathematical and physical sciences, computational sciences and engineering, to application domains. The panel, consisting of highly visible active researchers from both academia and the industry,opens a discussion on the importance of diversity to the healthy growth of the field. This paper records their opinions expressed at the panel.
- A. Velivelli, Thomas S. Huang, Alexander Hauptmann. 2006. Video shot retrieval using a kernel derived from a continuous HMM. Abstract: In this paper, we propose a discriminative approach for retrieval of video shots characterized by a sequential structure. The task of retrieving shots similar in content to a few positive example shots is more close to a binary classification problem. Hence, this task can be solved by a discriminative learning approach. For a content-based retrieval task the twin characteristics of rare positive example occurrence and a sequential structure in the positive examples make it attractive for us to use a learning approach based on a generative model like HMM. To make use of the positive aspects of both discriminative and generative models, we derive Fisher and Modified score kernels for a Continuous HMM and incorporate them into SVM classification framework. The training set video shots are used to learn SVM classifier. A test set video shot is ranked based on its proximity to the positive class side of hyperplane. We evaluate the performance of the derived kernels by retrieving video shots of airplane takeoff. The retrieval performance using the derived kernels is found to be much better compared to linear and RBF kernels.
- Wei-Hao Lin, Theresa Wilson, J. Wiebe, Alexander Hauptmann. 2006. Which Side are You on? Identifying Perspectives at the Document and Sentence Levels. Abstract: In this paper we investigate a new problem of identifying the perspective from which a document is written. By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans. Can computers learn to identify the perspective of a document? Not every sentence is written strongly from a perspective. Can computers learn to identify which sentences strongly convey a particular perspective? We develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the Israeli-Palestinian conflict. The results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy.
- Rong Yan, Ming-yu Chen, Alexander Hauptmann. 2006. Mining Relationship Between Video Concepts using Probabilistic Graphical Models. Abstract: For large scale automatic semantic video characterization, it is necessary to learn and model a large number of semantic concepts. These semantic concepts do not exist in isolation to each other and exploiting this relationship between multiple video concepts could be a useful source to improve the concept detection accuracy. In this paper, we describe various multi-concept relational learning approaches via a unified probabilistic graphical model representation and propose using numerous graphical models to mine the relationship between video concepts that have not been applied before. Their performances in video semantic concept detection are evaluated and compared on two TRECVID'05 video collections
- Scott Stevens, Ashok Bharucha, Michael G. Christel, Alexander Hauptmann, H. Wactlar, Datong Chen. 2006. Automatic collection, analysis, access, and archiving of individual and group psycho-social behavior. Abstract: CareMedia is a collaborative effort that to date has captured more than 13,000 hours of video and audio recordings of life in the shared spaces of a nursing home dementia unit, by using 23 ceiling-mounted cameras, 24 hours a day for 25 days, ensuring an un-occluded view of every point in the recorded space. Computer machine learning techniques are being applied to the resulting 25 Terabytes of data, automatically processing the record for efficient use by analytical observers (e.g., social and behavioral scientists, geriatricians, and healthcare policy makers) to monitor and understand residents' well-being, and enhance their quality of life. This truly interdisciplinary effort bridges the psychological, social and behavioral sciences, and clinical medicine with multiple engineering and computer science disciplines to establish a clinical evidence base to guide rational therapeutics, an elusive goal ardently articulated by the Institute of Medicine. This paper discusses early foundation work being conducted with the data.
- M. Naphade, John R. Smith, Jelena Tešić, Shih-Fu Chang, Winston H. Hsu, L. Kennedy, Alexander Hauptmann, Jon Curtis. 2006. Large-scale concept ontology for multimedia. Abstract: As increasingly powerful techniques emerge for machine tagging multimedia content, it becomes ever more important to standardize the underlying vocabularies. Doing so provides interoperability and lets the multimedia community focus ongoing research on a well-defined set of semantics. This paper describes a collaborative effort of multimedia researchers, library scientists, and end users to develop a large standardized taxonomy for describing broadcast news video. The large-scale concept ontology for multimedia (LSCOM) is the first of its kind designed to simultaneously optimize utility to facilitate end-user access, cover a large semantic space, make automated extraction feasible, and increase observability in diverse broadcast news video data sets
- Wei-Hao Lin, Alexander Hauptmann. 2006. Structuring continuous video recordings of everyday life using time-constrained clustering. Abstract: As personal wearable devices become more powerful and ubiquitous, soon everyone will be capable to continuously record video of everyday life. The archive of continuous recordings need to be segmented into manageable units so that they can be efficiently browsed and indexed by any video retrieval systems. Many researchers approach the problem in two-pass methods: segmenting the continuous recordings into chunks, followed by clustering chunks. In this paper we propose a novel one-pass algorithm to accomplish both tasks at the same time by imposing time constraints on the K-Means clustering algorithm. We evaluate the proposed algorithm on 62.5 hours of continuous recordings, and the experiment results show that time-constrained clustering algorithm substantially outperforms the unconstrained version.
- Scott Stevens, Ashok Bharucha, Mike Christel, Alexander Hauptmann, H. Wactlar, Datong Chen. 2006. Automatic Collection, Analysis, Access, and Archiving of Psycho/Social Behavior by Individuals and Groups. Abstract: Nearly 2.5 million Americans currently reside in nursing homes and assisted living facilities in the United States, accounting for approximately 5% of persons 65 years and older. 1 The aging of the “Baby Boomer” generation is expected to lead to an exponential growth in the need for some form of long-term care (LTC) for this segment of the population within the next 25 years. In light of these sobering demographic shifts, there is an urgency to address the profound concerns that exist about the quality-of-care (QoC) and quality-of-life (QoL) of this frailest segment of our population.
- Wei-Hao Lin, Alexander Hauptmann. 2006. Label Disambiguation and Sequence Modeling for Identifying Human Activities from Wearable Physiological Sensors. Abstract: Wearable physiological sensors can provide a faithful record of a patient's physiological states without constant attention of caregivers. A computer program that can infer human activities from physiological recordings will be an valuable tool for physicians. In this paper we investigate to what extent current machine learning algorithms can correctly identify human activities from physiological sensors. We further identify two challenges that developers need to address. The first problem is that the labels of training data are inevitably noisy due to difficulties of annotating thousands hours of data. The second problem lies in the continuous nature of human activities, which violates the independence assumption made by many learning algorithms. We approach the first problem of noisy labeling in the multiple-label framework, and develop a conditional Markov models to take temporal context into consideration. We evaluate the proposed methods on 12,000 hours of the physiological recordings. The results show that support vector machines are effective to identify human activities from physiological signals, and efforts of disambiguating noisy labels are worthwhile
- Alexander Hauptmann, Wei-Hao Lin, Rong Yan, Jun Yang, Ming-yu Chen. 2006. Extreme video retrieval: joint maximization of human and computer performance. Abstract: We present an efficient system for video search that maximizes the use of human bandwidth, while at the same time exploiting the machine's ability to learn in real-time from user selected relevant video clips. The system exploits the human capability for rapidly scanning imagery augmenting it with an active learning loop, which attempts to always present the most relevant material based on the current information. Two versions of the human interface were evaluated, one with variable page sizes and manual paging, the other with a fixed page size and automatic paging. Both require absolute attention and focus of the user for optimal performance. In either case, as users search and find relevant results, the system can invisibly re-rank its previous best guesses using a number of knowledge sources, such as image similarity, text similarity, and temporal proximity. Experimental evidence shows a significant improvement using the combined extremes of human and machine power over either approach alone.
- Wei-Hao Lin, Alexander Hauptmann. 2006. Which Thousand Words are Worth a Picture? Experiments on Video Retrieval using a Thousand Concepts. Abstract: In contrast to traditional video retrieval that represents visual content with low-level features (e.g. color and texture), emerging concept-based video retrieval allows users to search video archives by specifying a limited number of high-level concepts (e.g. outdoors and car). Recent studies have demonstrated the feasibility of concept-based retrieval, but a fundamental question remains: what kinds of concepts should we index? We analyze a large video archive annotated with more than a thousand high-level concepts, and develop guidelines for choosing concepts of high utility to video retrieval
- Rong Yan, Alexander Hauptmann. 2006. Probabilistic latent query analysis for combining multiple retrieval sources. Abstract: Combining the output from multiple retrieval sources over the same document collection is of great importance to a number of retrieval tasks such as multimedia retrieval, web retrieval and meta-search. To merge retrieval sources adaptively according to query topics, we propose a series of new approaches called probabilistic latent query analysis (pLQA), which can associate non-identical combination weights with latent classes underlying the query space. Compared with previous query independent and query-class based combination methods, the proposed approaches have the advantage of being able to discover latent query classes automatically without using prior human knowledge, to assign one query to a mixture of query classes, and to determine the number of query classes under a model selection principle. Experimental results on two retrieval tasks, i.e., multimedia retrieval and meta-search, demonstrate that the proposed methods can uncover sensible latent classes from training data, and can achieve considerable performance gains.
- Jun Yang, Alexander Hauptmann. 2006. 3WNews: who, where, and when in news video. Abstract: We describe 3WNews as a novel system for browsing news video by the people (who) and locations (where) appearing in the footage as well as the time (when) of news events. The people names, locations, and time expressions are recognized from video transcript and their ambiguous references are resolved. As a key advantage, 3WNews distinguishes the people and locations that actually appear in the video from those merely mentioned in the transcript, and uses them as (better) indexes for browsing. It also supports browsing of news video by event time instead of broadcasting time.
- Scott Stevens, Ashok Bharucha, Michael G. Christel, Alexander Hauptmann, H. Wactlar, Datong Chen. 2006. Automatic Collection Anaylsis, Access, and Archiving of Psycho/Social Behavior by Individuals and Groups. Abstract: Nearly 2.5 million Americans currently reside in nursing homes and assisted living facilities in the United States, accounting for approximately 5% of persons 65 years and older.The aging of the “Baby Boomer” generation is expected to lead to an exponential growth in the need for some form of long-term care (LTC) for this segment of the population within the next 25 years. In light of these sobering demographic shifts, there is an urgency to address the profound concerns that exist about the quality-of-care (QoC) and quality-of-life (QoL) of this frailest segment of our population.
- L. Kennedy, Alexander Hauptmann. 2006. LSCOM Lexicon Definitions and Annotations (Version 1.0). Abstract: The DTO sponsored LSCOM workshop is developing an expanded multimedia concept lexicon on the order of 1000. Concepts related to events, objects, locations, people, and programs have been selected following a multi-step process involving input solicitation, expert critiquing, comparison with related ontologies, and performance evaluation. Participants of the process include representatives from intelligence community users, ontology specialists, and multimedia analytics researchers. In addition, each concept has been qualitatively assessed according to some criteria, such as utility (usefulness), observability (by humans), and feasibility (by automatic detection). An annotation process was completed in late 2005 by student annotators at Columbia University and CMU, over the entire development set of TRECVID 2005 videos. Human subjects judge the presence or absence of each concept in the key frame of each subshot, resulting in a total of 61901 labels for each concept. Out of the 834 initial selected concepts, the first version of LSCOM annotations consists of 449 unique concepts (39 LSCOM-Lite concepts included) over the entire TRECVID 2005 development set (61901 subshots).
- Jun Yang, Alexander Hauptmann. 2006. Categorization Approach to Video Scene Classification Using Keypoint Features. Abstract: Scene classification based on local keypoint features has emerged as a promising research direction. Each image is represented by a “bag of visual words” as high-dimensional, vector-quantized keypoint features, which is analogous to the “bag of words” representation of text documents. Based on such representation, we take a fully text categorization approach to the scene classification problem, with an emphasis on comparatively evaluating various implementation choices related to this visual-word representation, including vocabulary size, feature weighting and normalization, feature selection, etc. We intend to practical insights as to the optimal representation choices that maximize the classification performance. Scene classification experiments based on a large video corpus lead to many important observations: 1) using our approach, carefully engineered visual-word features achieve comparable performance to that of traditional color/texture features, and their combination significantly enhances the performance; 2) the visual word distribution in the video corpus bears many similarities yet important differences to the word distribution in a text corpus; 3) a vocabulary much larger than the ones currently used is preferred; 4) frequent visual words are not “stop words” but more informative than rare words; 5) binary features are as effective as tf or tf-idf features, and normalization always hurts the classification performance; 6) feature selection can reduce the vocabulary size by half without loss of performance; 7) spatial information is much more useful with a small vocabulary than with a large one.
- M. Naphade, Alexander Hauptmann, John R. Smith, Shih-Fu Chang. 2006. Revision of LSCOM Event/Activity Annotations. Abstract: To gather these video-based judgments, we designed a simple tool for providing binary positive/negative labels based on viewing a video clip of a subshot. The tool is web-based and can be used remotely with any modern web browser. It allows the user to work on only one subshot and concept at a time. The user can input the labels using a mouse or keyboard commands and is rapidly shown the next subshot to label as soon as a label is entered. A snapshot of the interface for the tool is shown in Figure 1.
- Alexander Hauptmann, Ming-yu Chen, Michael G. Christel, Dipanjan Das, Wei-Hao Lin, Rong Yan, Jie Yang, G. Backfried, Xiao Wu. 2006. Multi-Lingual Broadcast News Retrieval. Abstract: In this notebook paper we describe the technical details of the submissions to TRECVID 2006 from CMU Informedia team. We participated in the high-level feature extraction and the search (automatic and interactive) tasks. Our emphasis is on various techniques used for the search task, where our interactive runs won the first place in the interactive track and our automatic runs are also among the top performers in the automatic track. 1 High-level feature extraction We submitted 6 runs for TRECVID 2006 high level feature evaluation, as shown in Table 1. There were 61901 labeled shots for the 39 concepts of the LSCOM-Lite set. We split those labeled shots into a training set (45963 shots) and a fusion set (15938 shots). We use the training set to train our baseline classifiers based on various combinations of low-level features. Support vector machines (SVM) with radial basis kernel function (RBF) are used in the training of baseline classifiers. Based on our experience, the parameter setting of SVM is critical to the performance. Therefore, we perform linear search of the parameter space using cross-validation to find the optimal parameters for each concept in the training set, particularly the gamma parameter in the kernel function and the cost parameter. In Run 1, using the optimal parameter setting achieves an average of 27% improvement (0.2633 to 0.3352) over the default setting in terms of the mean average precision (MAP) metric on the 39 concepts in the cross-validation experiment.
- Alexander Hauptmann. 2006. TRECVID: the utility of a content-based video retrieval evaluation. Abstract: TRECVID, an annual retrieval evaluation benchmark organized by NIST, encourages research in information retrieval from digital video. TRECVID benchmarking covers both interactive and manual searching by end users, as well as the benchmarking of some supporting technologies including shot boundary detection, extraction of semantic features, and the automatic segmentation of TV news broadcasts. Evaluations done in the context of the TRECVID benchmarks show that generally, speech transcripts and annotations provide the single most important clue for successful retrieval. However, automatically finding the individual images is still a tremendous and unsolved challenge. The evaluations repeatedly found that none of the multimedia analysis and retrieval techniques provide a significant benefit over retrieval using only textual information such as from automatic speech recognition transcripts or closed captions. In interactive systems, we do find significant differences among the top systems, indicating that interfaces can make a huge difference for effective video/image search. For interactive tasks efficient interfaces require few key clicks, but display large numbers of images for visual inspection by the user. The text search finds the right context region in the video in general, but to select specific relevant images we need good interfaces to easily browse the storyboard pictures. In general, TRECVID has motivated the video retrieval community to be honest about what we don't know how to do well (sometimes through painful failures), and has focused us to work on the actual task of video retrieval, as opposed to flashy demos based on technological capabilities.
- A. Smeaton, P. Over, Cash Costello, A. D. Vries, David Doermanni, Alexander Hauptmann, M. Rorvig, '. JohnR.Smith, Lide Wu. 2005. The TREC 2001 Video Track : Information Retrieval on Digital Video lnf ormation. Abstract: 3 Johns Hopkins University Applied Physics Laboratory, Laurel, Md., USA. •cwr, Amsterdam, The Netherlands. 5 Laboratory for Language and Media Processing, University of Maryland, College Park, MD. USA 6School of Computer Science, Carnegie Mellon University, USA 7School of Library Information Sciences, University of North Texas, Tx., USA 'IBM T. J. Watson Research Center, Hawthorne, NY, USA. 'Dept. of Computer Science, Fudan University, Shanghai, China ..
- Jun Yang, Alexander Hauptmann. 2005. Multi-modal analysis for person type classification in news video. Abstract: Classifying the identities of people appearing in broadcast news video into anchor, reporter, or news subject is an im-portant topic in high-level video analysis. Given the visual resemblance of different types of people, this work explores multi-modal features derived from a variety of evidences, such as the speech identity, transcript clues, temporal video structure, named entities, and uses a statistical learning approach to combine all the features for person type classifica-tion. Experiments conducted on ABC World News Tonight video have demonstrated the effectiveness of the approach, and the contributions of different categories of features have been compared.
- E. Xing, Rong Yan, Alexander Hauptmann. 2005. Mining Associated Text and Images with Dual-Wing Harmoniums. Abstract: We propose a multi-wing harmonium model for mining multimedia data that extends and improves on earlier models based on two-layer random fields, which capture bidirectional dependencies between hidden topic aspects and observed inputs. This model can be viewed as an undirected counterpart of the two-layer directed models such as LDA for similar tasks, but bears significant difference in inference/learning cost tradeoffs, latent topic representations, and topic mixing mechanisms. In particular, our model facilitates efficient inference and robust topic mixing, and potentially provides high flexibilities in modeling the latent topic spaces. A contrastive divergence and a variational algorithm are derived for learning. We specialized our model to a dual-wing harmonium for captioned images, incorporating a multivariate Poisson for word-counts and a multivariate Gaussian for color histogram. We present empirical results on the applications of this model to classification, retrieval and image annotation on news video collections, and we report an extensive comparison with various extant models.
- R. Manmatha, S. Rüger, Alexander Hauptmann. 2005. Multimedia information retrieval: workshop report. Abstract: The Multimedia Information Retrieval workshop at SIGIR'05 was well attended with about 28 participants. The workshop format consisted of 8 papers on a variety of different topics from features and representations for image retrieval to a complete retrieval system for media monitoring. The list of papers may be seen at http://mmis.doc.ic.ac.uk/mmir2005/, where many of the papers are available online. A substantial amount of time was set aside for discussion of issues in multimedia retrieval, and this report will focus on the discussion.
- Norman Papernick, Alexander Hauptmann. 2005. Summarization of Broadcast News Video through Link Analysis of Named Entities. Abstract: This paper describes the use of connections between named entities for summarization of broadcast news. We fi rst extract named entities from a transcript of a news story, and find related entities nearby. In the context of a q uery, a link graph of relevant entities is rendered in an intera ctive display, allowing the user to manipulate, browse an d examine the components, including the ability to pl ay back video clips that mention with interesting relations hips. An evaluation of the approach shows that completely au tomatic summaries from a year of broadcast news can reflect a most 50% of the entities in a manually created reference summary found on the web. Locations are the most accurate a sp ct of summarization.
- Jun Yang, Rong Yan, Alexander Hauptmann. 2005. Multiple instance learning for labeling faces in broadcasting news video. Abstract: Labeling faces in news video with their names is an interesting research problem which was previously solved using supervised methods that demand significant user efforts on labeling training data. In this paper, we investigate a more challenging setting of the problem where there is no complete information on data labels. Specifically, by exploiting the uniqueness of a face's name, we formulate the problem as a special multi-instance learning (MIL) problem, namely exclusive MIL or eMIL problem, so that it can be tackled by a model trained with partial labeling information as the anonymity judgment of faces, which requires less user effort to collect. We propose two discriminative probabilistic learning methods named Exclusive Density (ED) and Iterative ED for eMIL problems. Experiments on the face labeling problem shows that the performance of the proposed approaches are superior to the traditional MIL algorithms and close to the performance achieved by supervised methods trained with complete data labels.
- Wei-Hao Lin, Alexander Hauptmann. 2005. Revisiting the effect of topic set size on retrieval error. Abstract: Evaluating retrieval systems in a controlled environment with a large set of topics has been the core paradigm in the information retrieval community. Voorhees and Buckley proposed to estimate the reliability of retrieval experiments by calculating the probability of making wrong effectiveness judgments between two retrieval systems over two retrieval experiments[2], which is called Retrieval Experiment Error Rate (REER) in this paper. They have successfully shown how the topic set sizes affect the retrieval experiment reliability. However, the REER model in the previous work was empirically justified without providing a derivation based on statistical principles. We fill this gap and show that REER can indeed be derived from statistical principles. Based on the derived model we can explain why a successful experiment design depends on factors including a sufficient number of topics, large enough measurement score difference between systems, and a homogeneous distribution of retrieval scores for topics and systems, which reduces the variance of the score differences.
- Ming-yu Chen, Michael G. Christel, Alexander Hauptmann, H. Wactlar. 2005. Putting active learning into multimedia applications: dynamic definition and refinement of concept classifiers. Abstract: The authors developed an extensible system for video exploitation that puts the user in control to better accommodate novel situations and source material. Visually dense displays of thumbnail imagery in storyboard views are used for shot-based video exploration and retrieval. The user can identify a need for a class of audiovisual detection, adeptly and fluently supply training material for that class, and iteratively evaluate and improve the resulting automatic classification produced via multiple modality active learning and SVM. By iteratively reviewing the output of the classifier and updating the positive and negative training samples with less effort than typical for relevance feedback systems, the user can play an active role in directing the classification process while still needing to truth only a very small percentage of the multimedia data set. Examples are given illustrating the iterative creation of a classifier for a concept of interest to be included in subsequent investigations, and for a concept typically deemed irrelevant to be weeded out in follow-up queries. Filtering and browsing tools making use of existing and iteratively added concepts put the user further in control of the multimedia browsing and retrieval process.
- Ming-yu Chen, Alexander Hauptmann. 2005. Active Learning in Multiple Modalities for Semantic Feature Extraction from Video. Abstract: Active learning has been demonstrated to be a useful tool to reduce human labeling effort for many multimedia applications. However, most of the previous work on multimedia active learning has gloss the multi-modality problem very much. From several experimental results, multi-modality fusion plays an important role to boost performance of multimedia classification. In this paper, we present a multi-modality active learning approach which enhances the process of active learning approach from single-modality to multi-modality. The experimental results on the TRECVID 2004 semantic feature extraction task show that the proposed active learning approach works more effectively than single-modality approach and also demonstrate a significantly reduced amount of labeled data.
- Rong Yan, Alexander Hauptmann. 2004. Multi-class active learning for video semantic feature extraction. Abstract: Active learning has been demonstrated to be a useful tool to reduce human labeling effort for many multimedia applications, especially for those handling large video collections. However, most of the previous work on active learning has focused on only binary classification, which greatly limits the applicability of active learning. We present a multi-class active learning approach which extends active learning from binary classification to multi-class classification using a unified representation with margin-based loss functions. The experimental results on the TREC03 semantic feature extraction task shows that the proposed active learning approach works effectively even with a significantly reduced amount of labeled data.
- Ming-yu Chen, Alexander Hauptmann. 2004. Towards robust face recognition from multiple views. Abstract: The paper presents a novel approach to aid face recognition: Using multiple views of a face, we construct a 3D model instead of directly using the 2D images for recognition. Our framework is designed for videos, which contain many instances of a target face from a sequence of slightly differing views, as opposed to a single static picture of the face. Specifically, we reconstruct the 3D face shapes from two orthogonal views and select features based on pairwise distances between landmark points on the model using Fisher's linear discriminant. While 3D face shape reconstruction is sensitive to the quality of the feature point localization, our experiments show that 3D reconstruction together with the regularized Fisher's linear discriminant can provide highly accurate face recognition from multiple facial views. Experiments on the Carnegie Mellon PIE (pose, illumination and expressions) database containing the faces of 68 people, with at least 3 expressions under varying lighting conditions, demonstrate vastly improved performance
- Alexander Hauptmann, Michael G. Christel. 2004. Successful approaches in the TREC video retrieval evaluations. Abstract: This paper reviews successful approaches in evaluations of video retrieval over the last three years. The task involves the search and retrieval of shots from MPEG digitized video recordings using a combination of automatic speech, image and video analysis and information retrieval technologies. The search evaluations are grouped into interactive (with a human in the loop) and non-interactive (where the human merely enters the query into the system) submissions. Most non-interactive search approaches have relied extensively on text retrieval, and only recently have image-based features contributed reliably to improved search performance. Interactive approaches have substantially outperformed all non-interactive approaches, with most systems relying heavily on the user's ability to refine queries and reject spurious answers. We will examine both the successful automatic search approaches and the user interface techniques that have enabled high performance video retrieval.
- Jiang Gao, R. Collins, Alexander Hauptmann, H. Wactlar. 2004. Articulated Motion Modeling for Activity Analysis. Abstract: We propose an algorithm for articulated human motion segmentation that estimates parametric motions of body parts and segments images into moving regions accordingly. Our approach combines robust optical flow estimation, RANSAC, and region segmentation using color and Gaussian shape priors. This combination results in an algorithm that can robustly estimate and segment multiple motions, even for moving regions with small support and in low-resolution images. Based on the raw motion segmentation, consistent body motions are detected over time to characterize human activity. The effectiveness of this approach is demonstrated in a real scenario: characterizing dining activities of patients at a nursing home.
- Cees G. M. Snoek, M. Worring, Alexander Hauptmann. 2004. Detection of TV news monologues by style analysis. Abstract: We propose a method for detection of semantic concepts in produced video based on style analysis. Recognition of concepts is done by applying a classifier ensemble to the detected style elements. As a case study we present a method for detecting the concept of news subject monologues. Our approach had the best average precision performance amongst 26 submissions in the 2003 TRECVID benchmark
- Rong Yan, Jun Yang, Alexander Hauptmann. 2004. Learning query-class dependent weights in automatic video retrieval. Abstract: Combining retrieval results from multiple modalities plays a crucial role for video retrieval systems, especially for automatic video retrieval systems without any user feedback and query expansion. However, most of current systems only utilize query independent combination or rely on explicit user weighting. In this work, we propose using query-class dependent weights within a hierarchial mixture-of-expert framework to combine multiple retrieval results. We first classify each user query into one of the four predefined categories and then aggregate the retrieval results with query-class associated weights, which can be learned from the development data efficiently and generalized to the unseen queries easily. Our experimental results demonstrate that the performance with query-class dependent weights can considerably surpass that with the query independent weights.
- Jun Yang, Alexander Hauptmann. 2004. Video grammar for locating named people. Abstract: Finding a named person in broadcast news video is important to video retrieval. Relying on the text information such as video transcript and OCR text, this task suffers from the temporal mismatch between a person's visual appearance and his/her name occurred in text. By exploring video grammar on the concurrence pattern between faces and names, we propose an extended text-based IR method to overcome this problem and yield superior performance.
- P. D. Sahin, Ming-yu Chen, Alexander Hauptmann. 2004. Comparison and combination of two novel commercial detection methods. Abstract: Detection and removal of commercials plays an important role when searching for important broadcast news video material. Two novel approaches are proposed based on two distinctive characteristics of commercials, namely, repetitive use of commercials over time and distinctive color and audio features. Furthermore, proposed strategies for combining the results of the two methods yield even better performance. Experiments show over 90% recall and precision on a test set of 5 hours of ABC and CNN broadcast news data
- Jiang Gao, Alexander Hauptmann, H. Wactlar. 2004. Combining motion segmentation with tracking for activity analysis. Abstract: We explore a motion feature as the appropriate basis for classifying or describing a number of fine motor human activities. Our approach not only estimates motion directions and magnitudes in different image regions, but also provides accurate segmentation of moving regions. Through a combination of motion segmentation and region tracking techniques, while filtering for temporal consistency, we achieve a balance between accuracy and reliability of motion feature extraction. To identify specific activities, we characterize the dominant directions of relative motions. Experimental results show that this approach to motion feature analysis could be successful in assisting caregivers at a nursing home in assessments of patient's activity levels over time.
- Alexander Hauptmann, Yan Chen, Michael G. Christel, C. J. Huang, Wei-Hao Lin, T. Ng, Norman Papernick, A. Velivelli, Jeongsam Yang, Ruohe Yan, Hui Yang, H. Wactlar. 2004. Confounded Expectations: Informedia at TRECVID 2004. Abstract: For TRECVID 2004, CMU participated in the semantic feature extraction task, and the manual, interactive and automatic search tasks. For the semantic features classifiers, we tried unimodal, multi-modal and multi-concept classifiers. In interactive search, we compared a visual-only vs a complete video retrieval system using visual AND text data, and also contrasted expert vs novice users; The manual runs were similar to 2003, but they did not work comparably to last year, Additionally, we shared our low-level features with the TRECVID community. Overview We first describe the low-level features, which formed the input for all our analysis and which were distributed to other participants. Then we sketch out the experiments done for the semantic features, followed by the search task experiments (manual, automatic and interactive). Low-level ‘raw’ features Low-level features are extracted for each shot. “Low-level” means the features which are directly extracted from the source, videos. We use the term ‘low-level’ to distinguish them from the TRECVID high-level semantic feature extraction task. The low-level features are derived from several different sources: visual, audio, text and ‘semantic’ detectors, such as face detection and Video OCR detection. In TRECVID 2004, we extract 16 low-level raw features for the whole data set. This data was provided to all participating groups to encourage other researchers to use or compare their approaches with a standardized feature set. Image features A shot is the basic unit in our system; therefore, we extract one key-frame within each shot as a representative image. Image features are then based on the features extracted from that representative image. There are 3 different types of image features: color histograms, textures and edges. For all image features, we split the image into a 5 by 5 grid that tries to capture some spatial locality of information. The distributed data lists the features for each grid cell by rows, starting at the top. HSV, RGB, HVC and HCSqr Color Histograms Three different color spaces are used to construct color histogram features: HSV, HVC and RGB. Each grid presents its color histogram in 125 dimensions. Each channel is represented by 5 dimensions and plotted in a 3D histogram. Therefore, for each image, the dimension of the color histogram is 3125 (5*5*125). Due to this high dimensionality, we also provide the mean and variance for each grid and reduce the dimension to 50 (5*5*2). We also add an alternative feature called hcsqr, which is derived from the HVC color histogram, but removes variance and linearizes Hue and Chroma into a 2D histogram. Texture Images are first gray-scaled. Each image is convolved with six orientated Gabor filters. For each filter, the image is divided by 5 by 5 grids. The resulting filtered grids are then threshold and reduced to 16 bins histogram. The dimension is 2400 (6*16*5*5). Edge The edge detection is done using Canny edge detection. The result of Canny edge detection is convolved with 8 orientations. For each grid, there are 8 dimensions which show the mean magnitude for the 8 orientations. The dimensionality is thus 200 (5*5*8). Audio features We extract audio signal every 20 msecs (512 windows at 44100 HZ sampling rate). However, the basic unit of analysis is a shot which has variable length. We therefore calculate the mean and variance for each shot. FFT FFT is based on the Short Time Fourier Transform (STFT). The features are the means and variances of the spectral centroid, rolloff, flux and zerocrossings. Another feature called low energy is added. Therefore, they are 9 (4*2+1) dimensions. MFCC MFCC features are based on 10 Mel-Frequency cepstral coefficients. SFFT SFFT is a simplified FFT. It only lists the mean for the spectral centroid, rolloff, flux and zerocrossings. Motion features Motion features try to capture the movement within the shot. Although they very noisy, motion features potentially allow us to move from still image analysis to analysis of the moving video. Since the video was encoded with different MPEG encoders using different motion block numbers, we did not use the MPEG P-frame motion blocks. Kinetic Energy Kinetic energy measures the pixel variation within the shot. We convert the image to gray level and calculate the frame by frame differences for every shot. The value is the mean of the difference within the shot. Although we could still split it into 5 by 5 grids, we utilized this feature as a measurement of the stability of a shot and did not split the image. Optical Flow The optical flow motion is calculated for every 5 frames. It’s 5 by 5 and each grid contains 3 dimensions. The 3 dimensions are the mean x direction, mean y direction and variance of the magnitude. Text features The text feature is derived from the audio transcript. Semantic Detector features Two of our features are not quite “low”-level features. However, they are very basic measurements that discover peculiar characteristics in the video. Since people play important roles in news video, face detection gives us useful information. VOCR (video optical character reader) often shows people names and locations. Faces The face result collects the information of the most confident face detection result within the shot. The 5 dimensions are confidence, face size, face pose (1 is front, 2 is left, 3 is right), x coordinate of center point, and y coordinate of center point. Center point is the center location of the face detection box. VOCR VOCR result generates the information about VOCR detection boxes. The VOCR detection box is the detection result which shows the possible places to contain VOCR. The four dimensions are the number of boxes, average size of boxes, mean of x and mean of y coordinate. The timestamped contents of the recognized text are listed in a separate file. High-level semantic features To classify the 10 high-level semantic features (Boat/Ship, Madeleine Albright, Bill Clinton, Train, Beach, Basketball scored, Airplane takeoff, People walking/running, physical violence and Road), our baseline was a single modality classification approach. For each, we chose one single feature set of our standard low-level feature classes and built a classifier for that feature. Separate runs used a multi-modality classification strategy. Here, we built classifiers for each low-level feature and then combined them with a meta-classifier (stacking). The main challenge for semantic feature extraction from video is the large diversity. Low level features tend not to capture a complete semantic class well. For example, ‘outdoors’ contains many different concepts, as well as colors, textures and shapes. It may be an urban scene, a rural scene, a beach scene or another natural scene, each with different colors, textures, etc. Therefore, we attempted to utilize other semantic features to boost the performance of a specific classifier (or detector). For example, if we want to build a classifier for outdoor scenes, we can build (hopefully easier) classifiers, like sky, ocean, tree, grassland, road, building and other outdoor-related concepts. Ideally, the outdoor classifier can gain power from other easy and strong detectors, like sky, ocean and grassland, and thus the outdoor detector will be able to correctly classify many different scenes. Our approach in TRECVID 2004 was to find other concepts related to the classification task. Then, using the principles of causation and inference, we developed two classification strategies. ID Target Concept Causally related concepts 28 Boat/Ship Boat, Water_Body, Sky, Cloud 31 Train Car_Crash, Man_Made_scene, Smoke, Road 32 Beach Sky, Water_Body, Nature_Non-Vegetation, Cloud 33 Basket Scored Crowd, People, Running, Non-Studio_Setting 34 Airplane Takeoff Airplane, Sky, Smoke, Space_Vehicle_Launch 35 People Walking/running Walking, Running, People, Person Table 1. Causal relationships for the TRECVID 2004 concepts. 36 Physical violence Gun_Shot, Building, Gun, Explosion 37 Road Car, Road_Traffic, Truck, Vehicle_Noise Causation The common annotation set, distributed in TRECVID 2003, labeled several hundred semantic concepts in TRECVID 2003 development set of 47322 shots. Among those concepts, there are 190 concepts which have a frequency higher than 10. We analyzed the causal relationship of these 190 concepts to 8 high-level semantic features concepts, listed above We excluded Madeleine Albright and Bill Clinton, because the latter were more suitable for specific person x search strategies describe below. We selected the top 4 causal origins for each concept and grouped them together. Table 1 shows the respective 4 concepts which were determined to cause the evaluated target concepts. Inference Using each group of 5 concepts (4 causal ones and the target concept), we built a multi-modality classifier for each concept. To train the combination parameters, we split our training data into two sets. The first set is used to build the classifiers for each individual concept. The second set is used to validate the combination. The next step is to infer the causational concept classifier results into target concept. We then experimented with two approaches (A and B) to combine the classifier results. AWe use the confidence of causal relationship (form 0 to 1) and the error rate obtained on the training set to combine the results.
- Alexander Hauptmann, Jiang Gao, Rong Yan, Yanjun Qi, Jie Yang, H. Wactlar. 2004. Automated analysis of nursing home observations. Abstract: Pervasive activity monitoring in a skilled-nursing facility helps capture a continuous audio and video record. The CareMedia project analyzes this video information by automatically tracking people, helping to efficiently label individuals, and characterizing selected activities and actions.
- Wayne H. Ward, Alexander Hauptmann, R. Stern, T. Chanak. 2004. Parsing Spoken Phrases Page 1 PARSING SPOKEN PHRASES DESPITE MISSING. Abstract: This paper compares the recognition accuracy obtained in forming sentence hypotheses using islanddriven sentence parsers with parsers that hypothesize sentences in left-to-right fashion. Island-driven parsing algorithms are especially valuable in speech recognition systems because they can function more gracefully when not all of the correct words of an utterance were produced by the word hypothesizer. The inputs to both types of parsers consist of a lattice of candidate words, which are identified by their begin and end times, and the quality of the acoustic-phonetic match. Grammatical constraints are expressed by trigram models of sequences of lexical and semantic labels. We found that the island-driven parser produces parses with a higher percentage of correct words than the left-to-right parser in all cases considered. When the quality of the input lattices is extremely high, differences in parsing accuracy can be directly attributed to the superior ability of the island-driven parser to handle lattices with missing words.
- H. Wactlar, Michael G. Christel, Alexander Hauptmann, Datong Chen, Jie Yang. 2004. Infrastructure for Machine Understanding of Video Observations in Skilled Care Facilities – Implications of Early Results from CareMedia Case Studies. Abstract: CareMedia captures and analyzes a continuous audio and video record of behavior and activity in a skilled nursing facility. Through computer vision and machine learning we automatically identify individuals, classify activities, recognize behaviors, and extract relevant events. Two extensive field trials have been undertaken which produced meaningful but sometimes limited clinical results. Based on an analysis of this experience, combined with the development of new approaches and algorithms, we describe a radically improved audiovisual recording and computing infrastructure to be implemented, enabling longitudinal studies with comprehensive video and audio coverage provided with a mix of resolution, frame-rate, compression and storage requirements. Changes in the amount or rate of social interaction, eating, walking, gait, arm swing, and other attributes of motion from each patient’s baseline behavior will also be made easy to flag for professional review and diagnosis through the recording and analysis infrastructure, as such changes are key indicators to the benefits and possibly detrimental side effects of pharmacological interventions.
- Wei-Hao Lin, Alexander Hauptmann. 2004. Modeling timing features in broadcast news video classification. Abstract: Broadcast news programs are well-structured video, and timing can be a strong predictor for specific types of news reports. However, learning a classifier using timing features may not be an easy task when training data are noisy. We approach the problem from the generative model perspective, and approximate the class density in a non-parametric fashion. The results show that timing is a simple but extremely effective feature, and our method can achieve significantly better performance than a discriminative classifier.
- Wei-Hao Lin, Alexander Hauptmann. 2004. Informedia at PDMC. Abstract: Our Digial Human Memory project (Lin & Hauptmann, 2002) aims to collect and index every aspect of human daily experiences in digital form. By wearing a spy camera, microphones, and a BodyMedia armband, the wearer can collect rich records in a unobtrutive fashion, and many applications can build on top of such multimodal collections. For example, digital human memory can serve as a memory prosthesis to help the wearer recall past events; the habits or anomalies of the wearer can be analyzed from digital human memory. The physiological recordings recorded by a Bodymedia armband provides complementary dimensions of the wearer’s experiences, and play an important role in identifying wearer’s context and activities.
- Alexander Hauptmann, Alexander I. Rudnicky. 2004. A Comparison of Speech vs Typed Input. Abstract: We conducted a series of empirical experiments in which users were asked to enter digit strings into the computer by voice or keyboard. Two different ways of verifying and correcting the spoken input were examined. Extensive timing analyses were performed to determine which aspects of the interface were critical to speedy completion of the task. The results show that speech is preferable for strings that require more than a few keystrokes. The results emphasize the need for fast and accurate speech recognition, but also demonstrate how error correction and input validation are crucial for an effective speech interface.
- Rong Yan, Jian Zhang, Jie Yang, Alexander Hauptmann. 2004. A discriminative learning framework with pairwise constraints for video object classification. Abstract: In video object classification, insufficient labeled data may at times be easily augmented with pairwise constraints on sample points, i.e, whether they are in the same class or not. In this paper, we proposed a discriminative learning approach, which incorporates pairwise constraints into a conventional margin-based learning framework. The proposed approach offers several advantages over existing approaches dealing with pairwise constraints. First, as opposed to learning distance metrics, the new approach derives its classification power by directly modeling the decision boundary. Second, most previous work handles labeled data by converting them to pairwise constraints and thus leads too much more computation. The proposed approach can handle pairwise constraints together with labeled data so that the computation is greatly reduced. The proposed approach is evaluated on a people classification task with two surveillance video datasets.
- Ming-yu Chen, Alexander Hauptmann. 2004. Multi-modal classification in digital news libraries. Abstract: This paper describes a comprehensive approach to construct robust multimodal video classification on a specific digital source, broadcast news. Broadcast news has a very stable structure and every segment has its specific purpose. Video classification can support fundamental understanding of the structure of the video and the content. The variety of video content makes it hard to classify; however, it also provides multimodal information. Our approach tries to solve two important issues of multimodal classification. The first one is to select few discriminative features from many raw features and the second one is to efficiently combine multiple sources. We applied Fisher's Linear Discriminant (FLD) for feature selection and concatenated the projections into a single synthesized feature vector as the combination strategy. Experimental results on the 2003 TRECVID news video archive show that our approach achieves very robust and accurate performance.
- Jiang Gao, Alexander Hauptmann, Ashok Bharucha, H. Wactlar. 2004. Dining activity analysis using a hidden Markov model. Abstract: We describe an algorithm for dining activity analysis in a nursing home. Based on several features, including motion vectors and distance between moving regions in the subspace of an individual person, a hidden Markov model is proposed to characterize different stages in dining activities with certain temporal order. Using HMM model, we are able to identify the start (and ending) of individual dining events with high accuracy and low false positive rate. This approach could be successful in assisting caregivers in assessments of resident's activity levels over time.
- Ming-yu Chen, Alexander Hauptmann. 2004. Searching for a specific person in broadcast news video. Abstract: People as news subjects play an important role in broadcast news and finding a specific person is a major challenge for multimedia retrieval. Beyond mere content-based general retrieval, this task requires exploitation of the structure, time sequence and meaning of news content. We introduce a comprehensive approach to discovering clues for finding a specific person in broadcast news video. Various information aspects are investigated, including text information, timing information, scene detection, and face recognition. Experimental results on the TREC 2003 video search task show that our approach can achieve surprisingly high performance by exploiting broadly diverse information to find specific named people.
- Wei-Hao Lin, Alexander Hauptmann. 2004. Merging rank lists from multiple sources in video classification. Abstract: Multimedia corpora increasingly consist of data from multiple sources, with different characteristics that can be exploited by specialized applications. This paper focuses on video classification over multiple-source collections, and addresses the question whether classifiers should train from individual sources or from a full data set across all sources. If training separately, how can rank lists from different sources be merged effectively? We formulate the problem of merging ranked lists as learning a function mapping from local scores to global scores, and propose a learning method based on logistic regression. In our experiments we find that source characteristics are very important for video classification. Moreover, our method of learning mapping functions performs significantly better than merging methods without explicitly learning the mapping junctions
- Jun Yang, Alexander Hauptmann. 2004. Naming every individual in news video monologues. Abstract: Naming every individual person appearing in broadcast news videos with names detected from the video transcript leads to better access of the news video content. In this paper, we approach this challenging problem with a statistical learning method. Two categories of information extracted from multiple video modalities have been explored, namely <i>features</i>, which help distinguish the true name of every person, as well as <i>constraints</i>, which reveal the relationships among the names of different persons. The person-naming problem is formulated into a learning framework which predicts the most likely name for each person based on the features, and refines the predictions using the constraints. Experiments conducted on ABC World New Tonight and CNN Headline News videos demonstrate that this approach outperforms a non-learning alternative by a large amount.
- H. Wactlar, Michael G. Christel, Alexander Hauptmann, S. Stevens, Ashok Bharucha. 2003. A system of video information capture, indexing and retrieval for interpreting human activity. Abstract: This system creates a manageable information resource that enables more complete and accurate interpretation, assessment and diagnosis of human behavior in constrained physical spaces. Through activity and environmental monitoring, a continuous, voluminous audio and video record is captured. Through work in information extraction, behavior analysis and synthesis, this record is transformed into an information asset whose efficient, secure presentation empowers specialists with greater insights into problems, effectiveness of treatments, and determination of environmental and social influences. Application environments range from nursery schools to nursing homes. The foundation for this work, the informedia digital video library (H.D. Wactlar et al., 1999), has demonstrated the successful integration of speech, image, and natural language processing in automatically creating an indexed, searchable multimedia information resource for broadcast-quality video, upon which this system builds.
- Wei-Hao Lin, Rong Jin, Alexander Hauptmann. 2003. Web image retrieval re-ranking with relevance model. Abstract: Web image retrieval is a challenging task that requires efforts from image processing, link structure analysis, and Web text retrieval. Since content-based image retrieval is still considered very difficult, most current large-scale Web image search engines exploit text and link structure to "understand" the content of the Web images. However, local text information, such as caption, filenames and adjacent text, is not always reliable and informative. Therefore, global information should be taken into account when a Web image retrieval system makes relevance judgment. We propose a re-ranking method to improve Web image retrieval by reordering the images retrieved from an image search engine. The re-ranking process is based on a relevance model, which is a probabilistic model that evaluates the relevance of the HTML document linking to the image, and assigns a probability of relevance. The experiment results showed that the re-ranked image retrieval achieved better performance than original Web image retrieval, suggesting the effectiveness of the re-ranking method. The relevance model is learned from the Internet without preparing any training data and independent of the underlying algorithm of the image search engines. The re-ranking process should be applicable to any image search engines with little effort.
- Rong Yan, Alexander Hauptmann. 2003. The combination limit in multimedia retrieval. Abstract: Combining search results from multimedia sources is crucial for dealing with heterogeneous multimedia data, particularly in multimedia retrieval where a final ranked list of items of interest is returned sorted by confidence or relevance. However, relatively little attention has been given to combination functions, especially their upper bound performance limits. This paper presents a theoretical framework for studying upper bounds for two types of combination functions. A general upper bound and two approximations are proposed for monotonic combination functions. We also studied the upper bounds for linear combination functions using a global optimization technique. Our experimental results show that the choice of combination functions has a considerable influence to retrieval performance.
- Photina Jaeyun Jang, Alexander Hauptmann. 2003. TIVA Learning to Recognize Speech by Watching Television. Abstract: Imagine a computer plugged into your television at home or in a foreign hotel. In the morning, it can barely transcribe one out of two words correctly, but by evening, it can provide a largely correct transcript of the evening news show, based on what it learned during the day from the TV. This seems like the core of familiar science fiction shows, but the research described here brings this vision closer to reality by exploiting the multimodal nature of the broadcast signals and the variety of sources available for broadcast data. Our proposed technique gathers large amounts of speech from open broadcast sources and combines it with automatically obtained text or closed captioning to identify suitable speechtraining material. George Zavaliagkos and Thomas Colthurst worked on a different approach to this method that uses confidence scoring on the acoustic data itself to improve performance in the absence of any transcribed data, but their approach only yielded marginal results. Our initial efforts also provided only limited success with small amounts of data. In this article, we describe our approach to collecting almost unlimited amounts of accurately transcribed speech data. This information serves as training data for the acoustic-model component of most high-accuracy speaker-independent speech-recognition systems. The errorridden closed-captioned text aligns with the similarly error-ridden speech-recognizer output. We assume matching segments of sufficient length are reliable transcriptions of the corresponding speech. We then use these segments as the training data for an improved speech recognizer.
- Alexander Hauptmann. 2003. Machine Learning for Video Classification and Retrieval. Abstract: Alexander Hauptmann Dept. of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA Video analysis and retrieval from video collection is a difficult task. One would like to characterize the video in terms of various types and styles, understand what objects are in the video, divide it into camera shots and group those into coherent scenes. Ultimately we want to make video as tractable and ‘searchable’ as current text collections that are indexed through the web. Video analysis is orders of magnitude more complex than speech recognition, where the data stream is merely a one-dimensional signal and suitable intermediate level representations such as sentences, words, and phonemes permit a divide-and-conquer approach utilizing machine learning.
- T. D. Ng, Michael G. Christel, Alexander Hauptmann, H. Wactlar. 2003. Collages as Dynamic Summaries of Mined Video Content for Intelligent Multimedia Knowledge Management. Abstract: The video collage is a novel and effective interface for dynamically summarizing and presenting mined multimedia information from video collections. It provides a flexible mechanism for inferring knowledge and trailing analytic thoughts, which are the essential tasks in multimedia knowledge management. We will describe multimedia knowledge management, discuss how collages are automatically produced, illustrate their use for multimedia knowledge management, and evaluate their effectiveness as summaries across news stories. Collages are presentations of text and images extracted from multiple video sources. They provide an interactive visualization for a set of analyzed video documents, summarizing their contents and offering a navigation aid for further exploration. The dynamic creation of collages is based on user context, e.g., an originating query, coupled with automatic processing to refine the candidate imagery. Named entity identification and common phrase extraction provides associative textual description. The dynamic manipulation of collages allows user-directed filtering as well as revealing additional details and previously undiscovered content. The utility of collages as summaries is examined with respect to other published news summaries.
- Alexander Hauptmann, Michael G. Christel, Pinar Duygulu, Chang Huang, Rong Jin, T. D. Ng, N. Moraveji, Norman Papernick, G. Tzanetakis, J. Yang, R. Yan. 2003. Informedia at TRECVID 2003 : Analyzing and Searching Broadcast News Video. Abstract: This year, we submitted two runs using different versions of the Informedia systems. In one run, a version identical to last year's interactive system was used by five researchers, who split up the topics between themselves. The system interface emphasizes text queries, allowing search across ASR, closed captions and OCR text. The result set can then be manipulated through: • storyboards of images spanning across video story segments • emphasizing matching shots to a user’s query to reduce the image count to a manageable size • resolution and layout under user control • additional filtering provided through shot classifiers such as outdoors, and shots with people, etc. • display of filter count and distribution to guide their use in manipulating storyboard views. In the best-performing interactive run, for all topics a single researcher used an improved version of the system, which allowed more effective browsing and visualization of the results of text queries using a variety of filter strategies. The improvements made included a magnifying lens on the keyframe under mouse focus in the storyboard, simplified classifier filter access and use, and a browsing interface to browse the top-ranked shots according to the different classifiers. Color and texture based image search engines were also optimized for better performance.
- Rong Yan, Alexander Hauptmann, Rong Jin. 2003. Negative pseudo-relevance feedback in content-based video retrieval. Abstract: Video information retrieval requires a system to find information relevant to a query which may be represented simultaneously in different ways through a text description, audio, still images and/or video sequences. We present a novel approach that uses pseudo-relevance feedback from retrieved items that are NOT similar to the query items without further inquiring user feedback. We provide insight into this approach using a statistical model and suggest a score combination scheme via posterior probability estimation. An evaluation on the 2002 TREC Video Track queries shows that this technique can improve video retrieval performance on a real collection. We believe that negative pseudo-relevance feedback shows great promise for very difficult multimedia retrieval tasks, especially when combined with other different retrieval algorithms.
- Rong Jin, Alexander Hauptmann, Rong Yan. 2003. Image Classification Using a Bigram Model. Abstract: representation and classification algorithm are two important aspects for the task of image classification. Previous studies focused on using color histogram and/or texture as the features for representing an image. Support vector machines (SVM) have been among the most successful classification algorithms for image classification. In this paper, we examine a new type of representational feature, namely the 'bigram' feature, which computes a distribution for pixel pairs. Unlike color histogram based features, which treats each pixel as independent from others, the 'bigram' feature scheme is able to take the correlations between pairs of pixels into account. In experiments over six different image categories, the 'bigram' feature scheme appeared to be a better representation for image classification and achieved a better classification accuracy than either color histogram features, texture features or color correlograms.
- Rong Jin, Alexander Hauptmann. 2003. Learning to identify video shots with people based on face detection. Abstract: We examine how to identify video shots with at least two humans using only detected face information. While face detection is much more reliable than shape based people classification in broadcast video, one particular difficulty is that, when there are several humans in an image, the accuracy of face detection is usually significantly degraded, which leads to poor performance in identifying shots of 'people'. Furthermore, while our standard face detector works from individual still images, we propose using the statistics of face information of images within a whole shot as additional evidence in deciding whether or not a video shot belongs to the 'people' category. Empirically, we studied which statistics of face information are more informative than others and how to combine different statistics together in order to achieve better prediction.
- Jian Zhang, Rong Jin, Yiming Yang, Alexander Hauptmann. 2003. Modified Logistic Regression: An Approximation to SVM and Its Applications in Large-Scale Text Categorization. Abstract: Logistic Regression (LR) has been widely used in statistics for many years, and has received extensive study in machine learning community recently due to its close relations to Support Vector Machines (SVM) and AdaBoost. In this paper, we use a modified version of LR to approximate the optimization of SVM by a sequence of unconstrained optimization problems. We prove that our approximation will converge to SVM, and propose an iterative algorithm called "MLR-CG" which uses Conjugate Gradient as its inner loop. Multiclass version "MMLR-CG" is also obtained after simple modifications. We compare the MLR-CG with SVMlight over different text categorization collections, and show that our algorithm is much more efficient than SVMlight when the number of training examples is very large. Results of the multiclass version MMLR-CG is also reported.
- Rong Yan, Yan Liu, Rong Jin, Alexander Hauptmann. 2003. On predicting rare classes with SVM ensembles in scene classification. Abstract: Scene classification is an important technique to infer high-level semantic scene categories from low-level visual features. However, in the real world the positive data for many scenes may be rare, which degrades the performance of many classifiers. In this paper, we propose SVM ensembles to address the rare class problem. Various classifier combination strategies are investigated, including majority voting, sum rule, neural network gater and hierarchical SVMs. We also compare our method with two other common approaches for dealing with the rare class problem. Our experimental results show that hierarchical SVMs can achieve significantly better and more stable performance than other strategies, as well as high computational efficiency.
- Mark Derthik, Michael G. Christel, Alexander Hauptmann, D. Ng, S. Stevens, H. Wactlar. 2003. A Cityscape Visualization of Video Perspectives. Abstract: CMU’s Informedia project has collected and automatically processed a multi-terabyte video corpus containing 8 years of CNN broadcasts and other video sources [5]. Previous work has demonstrated multi-modal querying by text, image, time, and location, and the ability to summarize a single document or a set of documents matching a query. We now plan to organize the corpus or a subset along multiple dimensions, or perspectives, adding relevant background material, significantly expanding and accelerating the viewer’s comprehension and integration of knowledge. A perspective can provide factual background information, a history of an issue, the view of a biased source, a technical or medical perspective, or any of dozens of others. This abstract proposes a cityscape metaphor for organizing visual context in terms of perspectives.
- Mark Derthick, Michael G. Christel, Alexander Hauptmann, H. Wactlar. 2003. Constant density displays using diversity sampling. Abstract: The Informedia Digital Video Library user interface summarizes query results with a collage of representative keyframes. We present a user study in which keyframe occlusion caused difficulties. To use the screen space most efficiently to display images, both occlusion and wasted whitespace should be minimized. Thus optimal choices will tend toward constant density displays. However, previous constant density algorithms are based on global density, which leads to occlusion and empty space if the density is not uniform. We introduce an algorithm that considers the layout of individual objects and avoids occlusion altogether. Efficiency concerns are important for dynamic summaries of the Informedia Digital Video Library, which has hundreds of thousands of shots. Posting multiple queries that take into account parameters of the visualization as well as the original query reduces the amount of work required. This greedy algorithm is then compared to an optimal one. The approach is also applicable to visualizations containing complex graphical objects other than images, such as text, icons, or trees.
- Rong Jin, Yan Liu, Luo Si, J. Carbonell, Alexander Hauptmann. 2003. A New Boosting Algorithm Using Input-Dependent Regularizer. Abstract: AdaBoost has proved to be an eective method to improve the performance of base classifiers both theoretically and empirically. However, previous studies have shown that AdaBoost might suer from the overfitting problem, especially for noisy data. In addition, most current work on boosting assumes that the combination weights are fixed constants and therefore does not take particular input patterns into consideration. In this paper, we present a new boosting algorithm, “WeightBoost”, which tries to solve these two problems by introducing an inputdependent regularization factor to the combination weight. Similarly to AdaBoost, we derive a learning procedure for WeightBoost, which is guaranteed to minimize training errors. Empirical studies on eight dierent UCI data sets and one text categorization data set show that WeightBoost almost always achieves a considerably better classification accuracy than AdaBoost. Furthermore, experiments on data with artificially controlled noise indicate that the WeightBoost is more robust to noise than AdaBoost.
- Yanjun Qi, Alexander Hauptmann, Ting Liu. 2003. Supervised classification for video shot segmentation. Abstract: In this paper, we explore supervised classification methods for video shot segmentation. We transform the temporal segmentation problem into a multi-class categorization issue. This approach provides a uniform framework for using different kinds of features extracted from the video and for detecting various types of shot boundaries. The approach utilizes manual labeled training data and a simple classification structure, which eliminates arbitrary thresholds and achieves more reliable estimation than previous threshold-based methods. Contrastive experiments on 13 videos (/spl sim/4 hours) show excellent performance on the 2001 TREC video track shot classification task in terms of precision and recall.
- Cees G. M. Snoek, Alexander Hauptmann. 2003. Learning to Identify TV News Monologues by Style and Context. Abstract: We focus on the problem of learning semantics from multimedia data associated with broadcast video documents. In this paper we propose to learn semantic concepts from multimodal sources based on style and context detectors, in combination with statistical classier ensembles. As a case study we present our method for detecting the concept of news subject monologues. This approach had the best average precision performance amongst 26 submissions in the 2003 video track of the Text Retrieval Conference benchmark. Experiments were conducted with respect to individual detector contribution, ensemble size, and ranking mechanism. It was found that the combination of detectors is decisive for the nal result, although some detectors might appear useless in isolation. Moreover, by using a probabilistic ranking, in combination with a large classier ensemble, results can be improved even further.
- Rong Jin, ChengXiang Zhai, Alexander Hauptmann. 2003. Information retrieval for OCR documents: a content-based probabilistic correction model. Abstract: The difficulty with information retrieval for OCR documents lies in the fact that OCR documents contain a significant amount of erroneous words and unfortunately most information retrieval techniques rely heavily on word matching between documents and queries. In this paper, we propose a general content-based correction model that can work on top of an existing OCR correction tool to “boost” retrieval performance. The basic idea of this correction model is to exploit the whole content of a document to supplement any other useful information provided by an existing OCR correction tool for word corrections. Instead of making an explicit correction decision for each erroneous word as typically done in a traditional approach, we consider the uncertainties in such correction decisions and compute an estimate of the original “uncorrupted” document language model accordingly. The document language model can then be used for retrieval with a language modeling retrieval approach. Evaluation using the TREC standard testing collections indicates that our method significantly improves the performance compared with simple word correction approaches such as using only the top ranked correction.
- Rong Jin, Rong Yan, Jian Zhang, Alexander Hauptmann. 2003. A Faster Iterative Scaling Algorithm for Conditional Exponential Model. Abstract: Conditional exponential model has been one of the widely used conditional models in machine learning field and improved iterative scaling (IIS) has been one of the major algorithms for finding the optimal parameters for the conditional exponential model. In this paper, we proposed a faster iterative algorithm named FIS that is able to find the optimal parameters faster than the IIS algorithm. The theoretical analysis shows that the proposed algorithm yields a tighter bound than the traditional IIS algorithm. Empirical studies on the text classification over three different datasets showed that the new iterative scaling algorithm converges substantially faster than both the IIS algorithm and the conjugate gradient algorithm (CG). Furthermore, we examine the quality of the optimal parameters found by each learning algorithm in the case of incomplete training. Experiments have shown that, when only a limited amount of computation is allowed (e.g. no convergence is achieved), the new algorithm FIS is able to obtain lower testing errors than both the IIS method and the CG method.
- Rong Yan, Jie Yang, Alexander Hauptmann. 2003. Automatically labeling video data using multi-class active learning. Abstract: Labeling video data is an essential prerequisite for many vision applications that depend on training data, such as visual information retrieval, object recognition, and human activity modelling. However, manually creating labels is not only time-consuming but also subject to human errors, and eventually, becomes impossible for a very large amount of data (e.g. 24/7 surveillance video). To minimize the human effort in labeling, we propose a unified multiclass active learning approach for automatically labeling video data. We include extending active learning from binary classes to multiple classes and evaluating several practical sample selection strategies. The experimental results show that the proposed approach works effectively even with a significantly reduced amount of labeled data. The best sample selection strategy can achieve more than a 50% error reduction over random sample selection.
- Alexander Hauptmann, Rong Jin, T. D. Ng. 2003. Video retrieval using speech and image information. Abstract: Video contains multiple types of audio and visual information, which are difficult to extract, combine or trade-off in general video information retrieval. This paper provides an evaluation on the effects of different types of information used for video retrieval from a video collection. A number of different sources of information are present in most typical broadcast video collections and can be exploited for information retrieval. We will discuss the contributions of automatically recognized speech transcripts, image similarity matching, face detection and video OCR in the contexts of experiments performed as part of 2001 TREC Video Retrieval Track evaluation performed by the National Institute of Standards and Technology. For the queries used in this evaluation, image matching and video OCR proved to be the deciding aspects of video information retrieval.
- Michael G. Christel, Alexander Hauptmann, H. Wactlar, T. D. Ng. 2002. Collages as dynamic summaries for news video. Abstract: This paper introduces the video collage, a novel effective interface for browsing and interpreting video collections. The paper discusses how collages are automatically produced, illustrates their use, and evaluates their effectiveness as summaries across news stories. Collages are presentations of text and images derived from multiple video sources, which provide an interactive visualization for a set of video documents, summarizing their contents and providing a navigation aid for further exploration. The dynamic creation of collages is based on user context, e.g., an originating query, coupled with automatic processing to refine the candidate imagery. Named entity identification and common phrase extraction provides descriptive text. The dynamic manipulation of collages allows user-directed browsing and reveals additional detail. The utility of collages as summaries is examined with respect to other published news summaries.
- Rong Jin, Alexander Hauptmann. 2002. A New Probabilistic Model for Title Generation. Abstract: Title generation is a complex task involving both natural language understanding and natural language synthesis. In this paper, we propose a new probabilistic model for title generation. Different from the previous statistical models for title generation, which treat title generation as a generation process that converts the 'document representation' of information directly into a 'title representation' of the same information, this model introduces a hidden state called 'information source' and divides title generation into two steps, namely the step of distilling the 'information source' from the observation of a document and the step of generating a title from the estimated 'information source'. In our experiment, the new probabilistic model outperforms the previous model for title generation in terms of both automatic evaluations and human judgments.
- Wei-Hao Lin, Rong Jin, Alexander Hauptmann. 2002. Meta-Classification of Multimedia Classifiers. Abstract: Combining multiple classifiers is of particular interest in the multimedia systems, since there is usually data of very different types/modalities that should be mined or analyzed. Our wearable ‘experience collection’ system unobtrusively records the wearer’s conversation, recognizes the face of the dialog partner and remembers his/her voice. When the system sees the same person’s face or hears the same voice it can then use a summary of the last conversation with this person to remind the wearer. To correctly identify a person from a mixture of video and audio stream, classification judgments from individual modality classifiers must be combined effectively to yield a more accurate decision. To address the problems of combination strategy in previous studies, a meta-classification strategy using Support Vector Machine is proposed. Preliminary results show that combining different face recognition and speaker identification technology by meta-classification is dramatically more effective than weighted interpolation. Meta-classification is general enough to be applied to any application that needs to combine multiple classifiers without much modification.
- Alexander Hauptmann, Rong Jin, T. D. Ng. 2002. Multi-modal information retrieval from broadcast video using OCR and speech recognition. Abstract: We examine multi-modal information retrieval from broadcast video where text can be read on the screen through OCR and speech recognition can be performed on the audio track. OCR and speech recognition are compared on the 2001 TREC Video Retrieval evaluation corpus. Results show that OCR is more important that speech recognition for video retrieval. OCR retrieval can further improve through dictionary-based post-processing. We demonstrate how to utilize imperfect multi-modal metadata results to benefit multi-modal information retrieval.
- Alexander Hauptmann, Rong Jin. 2002. Video Information Retrieval: Lessons Learned with the Informedia Digital Video Library. Abstract: Video contains multiple types of audio and visual information, which are difficult to extract, combine or trade-off in general video information retrieval. This paper provides an evaluation on the effects of different types of information used for video retrieval from a video collection. A number of different sources of information are present in most typical broadcast video collections and can be exploited for information retrieval. We will discuss the contributions of automatically recognized speech transcripts, image similarity matching, and video OCR in the contexts of experiments performed as part of 2001 TREC Video Retrieval Track evaluation performed by the National Institute of Standards and Technology. For the queries used in this evaluation, image matching and video OCR proved to be the most important aspects of video information retrieval.
- Alexander Hauptmann, Rong Yan, Yanjun Qi, Rong Jin, Michael G. Christel, Mark Derthick, Ming-yu Chen, R. Baron, Wei-Hao Lin, T. D. Ng. 2002. Video Classification and Retrieval with the Informedia Digital Video Library System. Abstract: This paper is organized in three parts. The first part details some of the lower level shot classification work, the second part describes the ‘manual’ retrieval systems while the last section details the interactive retrieval system for the Carnegie Mellon University TREC Video Retrieval Track runs. The description of the data can be found elsewhere in the proceedings of the 2002 TREC conference video track overview.
- Wei-Hao Lin, Alexander Hauptmann. 2002. News video classification using SVM-based multimodal classifiers and combination strategies. Abstract: Video classification is the first step toward multimedia content understanding. When video is classified into conceptual categories, it is usually desirable to combine evidence from multiple modalities. However, combination strategies in previous studies were usually ad hoc. We investigate a meta-classification combination strategy using Support Vector Machine, and compare it with probability-based strategies. Text features from closed-captions and visual features from images are combined to classify broadcast news video. The experimental results show that combining multimodal classifiers can significantly improve recall and precision, and our meta-classification strategy gives better precision than the approach of taking the product of the posterior probabilities.
- Rong Jin, Alexander Hauptmann. 2002. Using a probabilistic source model for comparing images. Abstract: We propose a probabilistic model for image retrieval. To obtain the similarity between the query image IQ and any image I' in the collection, the model computes the probability of generating the image I' given the observation of the query image I/sub Q/. We compare our probabilistic model for image retrieval with a color histogram based image retrieval method and the IBM QBIC image search engine. The evaluation used the 11-hour video retrieval collection (80,000 extracted images) and associated queries from the 2001 TREC-10 information retrieval evaluations. The experimental results show that the probabilistic model dramatically outperforms the color histogram based image retrieval method and the IBM QBIC image search engine by 40%.
- Alexander Hauptmann, Norman Papernick. 2002. Video-cuebik: adapting image search to video shots. Abstract: We propose a new analysis for searching images in video libraries that goes beyond simple image search, which compares one still image frame to another. The key idea is to expand the definition of an image to account for the variability in the sequence of video frames that comprise a shot. A first implementation of this method for a QBIC-like image search engine shows a clear improvement over still image search. A combination of the traditional still image search and the new video image search provided the overall best results on the TREC video retrieval evaluation data.
- Rong Jin, Yanjun Qi, Alexander Hauptmann. 2002. A probabilistic model for camera zoom detection. Abstract: Camera motion detection is essential for automated video analysis. We propose a new probabilistic model for detecting zoom-in/zoom-out operations. The model uses EM to estimate the probability of a zoom versus a non-zoom operation from standard MPEG motion vectors. Traditional methods usually set an empirical threshold after deriving parameters proportional to zoom, pan, rotate and tilt. In contrast, our probabilistic model has a solid probabilistic foundation and a clear, simple probability threshold. Experiments show that this probabilistic model significantly out-performs a baseline parametric method for zoom detection in both precision and recall.
- Rong Jin, Luo Si, Alexander Hauptmann, Jamie Callan. 2002. Language model for IR using collection information. Abstract: Information retrieval using meta data can be traced back to the early age of IR where documents are represented by the controlled vocabulary. In this paper, we explore the usage of meta-data information under the framework of language model. We present a new language model that is able to take advantage of the category information for documents to improve the retrieval accuracy. We compare the new language model with the traditional language model over the TREC4 dataset where the collection information for documents is obtained using the k-means clustering method. The new language model outperforms the traditional language model, which verifies our statement.
- Rong Jin, Alexander Hauptmann, ChengXiang Zhai. 2002. Title language model for information retrieval. Abstract: In this paper, we propose a new language model, namely, a title language model, for information retrieval. Different from the traditional language model used for retrieval, we define the conditional probability P(Q|D) as the probability of using query Q as the title for document D. We adopted the statistical translation model learned from the title and document pairs in the collection to compute the probability P(Q|D). To avoid the sparse data problem, we propose two new smoothing methods. In the experiments with four different TREC document collections, the title language model for information retrieval with the new smoothing method outperforms both the traditional language model and the vector space model for IR significantly.
- Wei-Hao Lin, Rong Jin, Alexander Hauptmann. 2002. Triggering Memories of Conversations using Multimodal Classifiers. Abstract: Our personal conversation memory agent is a wearable ‘experience collection’ system, which unobtrusively records the wearer’s conversation, recognizes the face of the dialog partner and remembers his/her voice. When the system sees the same person’s face or hears the same voice it uses a summary of the last conversation with this person to remind the wearer. To correctly identify a person and help remember the earlier conversation, the system must be aware of the current situation, as analyzed from audio and video streams, and classify the situation by combining these modalities. Multimodal classifiers, however, are relatively unstable in the uncontrolled real word environments, and a simple linear interpolation of multiple classification judgments cannot effectively combine multimodal classifiers. We propose a meta-classification strategy using a Support Vector Machine as a new combination strategy. Experimental results show that combining face recognition and speaker identification by meta-classification is dramatically more effective than a linear combination. This meta-classification approach is general enough to be applied to any situation-aware application that needs to combine multiple classifiers.
- Wei-Hao Lin, Alexander Hauptmann. 2002. A wearable digital library of personal conversations. Abstract: We have developed a wearable, personalized digital library system, which unobtrusively records the wearer's part of a conversation, recognizes the face of the current dialog partner and remembers his/her voice. The next time the system sees the same person and hears the same voice, it can replay parts of the last conversation in compressed form. Results from a prototype system show the effectiveness of combining of face recognition and speaker identification for retrieving conversations.
- Rong Jin, Alexander Hauptmann. 2001. Title Generation for Machine-Translated Documents. Abstract: In this paper, we present and compare automatically generated titles for machine-translated documents using several different statistics-based methods. A Naive Bayesian, a K-Nearest Neighbour, a TF-IDF and an iterative Expectation-Maximization method for title generation were applied to 1000 original English news documents and again to the same documents translated from English into Portuguese, French or German and back to English using SYSTRAN. The AutoSummarization function of Microsoft Word was used as a base line. Results on several metrics show that the statistics-based methods of title generation for machine-translated documents are fairly language independent and title generation is possible at a level approaching the accuracy of titles generated for the original English documents.
- Mike Christel, T. Kanade, C. Faloutsos, J. Lafferty, Alexander Hauptmann, Yiming Yang. 2001. Informedia-II : Auto-Summarization and Visualization Over Multiple Video. Abstract: 1 Overview The Informedia-II Project will change the paradigm for accessing digital video libraries through meaningful, manipulable overviews of video document sets, multimodal queries, and adaptive summarizations of very large amounts of video from heterogeneous distributed sources. Video information collages are the key technology in Informedia-II and will be built by advancing information visualization research to effectively deal with multiple video documents. A video information collage is a presentation of text, images, audio, and video derived from multiple video sources in order to summarize, provide context, and communicate aspects of the content for the originating set of sources. The collages to be investigated include chrono-collages emphasizing time, geo-collages emphasizing spatial relationships, and auto-documentaries which preserve video's temporal nature. Users will be able to interact with the video collages to generate multimodal queries across time, space, and sources. Video collages are made adaptive by giving preference to the concepts and query terms in the user's interaction history. The synthesis and summarization functions underlying these collages will be made possible through extensions of text clustering and expectation maximization algorithms to video and audio features. 2.1 Video Information Collages: Adaptive Visualization and Summarization 2.1.1 Interactive visualization of digital library data and metadata Much current research on digital libraries focuses on named entity extraction and transformation into structured metadata. Examples include entities like events, people, and places, and attributes like birth date or latitude. Unfortunately, this extraction process is not very reliable, and in any case a digital library may contain references to hundreds of thousands of entities. We have been integrating Visage and Visage functionality for visualization and semantic zooming within the Informedia library. Information visualization is a powerful tool for summarizing large sets of structured data about entities and their relationships to uncover overall patterns, and then drilling down into interesting subsets. We have applied this technique to metadata extracted from the Informedia Digital Video Library, and demonstrated examples of conclusions that can be drawn from metadata patterns alone at JCDL'01. Currently, text attributes are handled poorly in terms of both query semantics and interaction speed. Our goal is to overcome these difficulties, so that integrated data and metadata library browsing becomes a continuous interactive activity. The visual query language allows database-style joins between entities of different types, Dynamic Query filtering of attribute values, visualization of conditional attribute value distributions with histograms, and drill down to individual entities [Derthick1997]. For instance, the …
- Alexander Hauptmann, Rong Jin, Norman Papernick, T. D. Ng, Yanjun Qi, Ricky Houghton, Sue Thornton. 2001. Video Retrieval with the Informedia Digital Video Library System. Abstract: Background: The Informedia Digital Video Library System. The Informedia Digital Video Library [1] was the only NSF DLI project focusing specifically on information extraction from video and audio content. Over a terabyte of online data was collected, with automatically generated metadata and indices for retrieving videos from this library. The architecture for the project was based on the premise that real-time constraints on library and associated metadata creation could be relaxed in order to realize increased automation and deeper parsing and indexing for identifying the library contents and breaking it into segments. Library creation was an offline activity, with library exploration by users occurring online and making use of the generated metadata and segmentation. The goal of the Informedia interface was to enable quick access to relevant information in a digital video library, leveraging from derived metadata and the partitioning of the video into small segments. Figure 1 shows the IDVLS interface following a query. In this figure, a set of results is displayed at the bottom. The display includes a window containing a headline, and a pictorial menu of video segments each represented with a thumbnail image at approximately 1⁄4 resolution of the video in the horizontal and vertical dimensions. The headline window automatically pops up whenever the mouse is positioned over a result item; the headline window for the first result is shown. IDVLS also supports other ways of navigating and browsing the digital video library. These interface features were essential to deal with the ambiguity of the derived data generated by speech recognition, image processing, and natural language processing. Consider the filmstrip and video playback IDVLS window shown in Figure 2. For this actual video in the IDVLS library, the segmentation process failed, resulting in a thirty-minute segment. This long segment was one of the returned results for the query “Mir collision.” The filmstrip in Figure 2 shows that the segment is more than just a story on the Russian space station, but rather begins with a commercial, then the weather, and then coverage of Hong Kong before addressing Mir. By overlaying the filmstrip and video playback windows with match location information, the user can quickly see that matches don’t occur until later in the segment, after these other stories that were irrelevant to the query. The match bars are optionally color-coded to specific query words; in Figure 2 “Mir” matches are in red and “collision” matches in purple. When the user moved the mouse over the match bars in the filmstrip, a text window displayed the actual matching word from the transcript or Video OCR metadata for that particular match; “Mir” is shown in one such text window in Figure 2. By investigating the distribution of match locations on the filmstrip, the user can determine the relevance of the returned result and the location of interest within the segment. The user can click on a match bar to jump directly to that point in the video segment. Hence, clicking the mouse as shown in Figure 2 would start playing the video at this mention of “Mir” with the overhead shot of people at desks. Similarly, IDVLS provided “seek to next match” and “seek to previous match” buttons in the video player allowing the user to quickly jump from one match to the next. In the example of Figure 2, these interface features allowed the user to bypass problems in segmentation and jump directly to the “Mir” story without having to first watch the opening video on other topics.
- Rong Jin, Alexander Hauptmann. 2001. Automatic Title Generation for Spoken Broadcast News. Abstract: In this paper, we implemented a set of title generation methods using training set of 21190 news stories and evaluated them on an independent test corpus of 1006 broadcast news documents, comparing the results over manual transcription to the results over automatically recognized speech. We use both F1 and the average number of correct title words in the correct order as metric. Overall, the results show that title generation for speech recognized news documents is possible at a level approaching the accuracy of titles generated for perfect text transcriptions.
- Rong Jin, Christos Falusos, Alexander Hauptmann. 2001. Meta-scoring: automatically evaluating term weighting schemes in IR without precision-recall. Abstract: In this paper, we present a method that can automatically evaluate performance of different term weighting schemes in information retrieval without resorting to precision-recall based on human relevance judgments. Specifically, the problem is: given two document-term matrixes generated from two different term weighting schemes, can we tell which term weighting scheme will performance better than the other? We propose a meta-scoring function, which takes as input the document-term matrix generated by some term weighting scheme and computes a goodness score from the document-term matrix. In our experiments, we found out that this score is highly correlated with the precision-recall measurement for all the collections and term weighting schema we tried. Thus, we conclude that our meta-scoring function can be a substitute for the precision-recall measurement that needs relevance judgments of human subject. Furthermore, this meta-scoring function is not limited only to text information retrieval can be applied to fields such as image and DNA retrieval.
- Alexander Hauptmann, Wei-Hao Lin. 2001. Beyond the Informedia digital video library: video and audio analysis for remembering conversations. Abstract: The Informedia Project digital video library pioneered the automatic analysis of television broadcast news and its retrieval on demand. Building on that system, we have developed a wearable, personalized Informedia system, which listens to and transcribes the wearer's part of a conversation, recognizes the face of the current dialog partner and remembers his/her voice. The next time the system sees the same person's face and hears the same voice, it can retrieve the audio from the last conversation, replaying in compressed form the names and major issues that were mentioned. All of this happens unobtrusively, somewhat like an intelligent assistant who whispers to you: "That's Bob Jones from Tech Solutions; two weeks ago in London you discussed solar panels". This paper outlines the general system components as well as interface considerations. Initial implementations showed that both face recognition methods and speaker identification technology have serious shortfalls that must be overcome.
- Rong Jin, Alexander Hauptmann. 2001. Learning to Select Good Title Words: An New Approach based on Reverse Information Retrieval. Abstract: In this paper, we show how we can learn to select good words for a document title. We view the problem of selecting good title words for a document as a variant of an Information Retrieval problem. Each title word is treated as a “document” and selection of appropriate title words as finding relevant “documents”. Based on our training collection consisting of 40,000 document and title pairs, we learn the “document” representations for all the title words and apply these learned representations to select appropriate title words over 10,000 test documents. Compared to other learning approaches, namely K nearest neighbor approach, a Naive Bayesian approach and a variant of a machine translation model, we find that our approach is significantly better as indicated by the F1 metric.
- Rong Jin, Alexander Hauptmann. 2001. Headline Generation using a Training Corpus. Abstract: . This paper discusses fundamental issues involved in word selection for title generation. We review several common methods that have been used for title generation and compare the performance of those methods using an F1 metric. Both a KNN (k nearest neighbor) method, which we are the first to apply to title generation, and a limited-vocabulary Naïve Bayesian method outperform other evaluated methods with an F1 score of over 20%. We conclude that KNN (k nearest neighbor) is a simple and promising method in title generation under the assumption that strong content overlap exists between the training corpus and the test collection. We also point out ways to improve the performance both from the learning side and from the generation side.
- Michael G. Christel, H. Wactlar, Alexander Hauptmann. 2001. Improving Access to Digital Video Archives through Informedia Technology. Abstract: Informedia research at Carnegie Mellon University combines speech recognition, image processing, and natural language processing to automatically index a digital video library. This engineering report focuses on the contribution of speech analysis for transcript generation and alignment, and the use of these features in library interface development. By deepening the automated analysis, such as using named entity extraction to identify people and place names in the audio transcript, better summaries and visualizations can be produced to navigate through video libraries holding thousands of hours of material.
- Alexander Hauptmann, Michael G. Christel, Ricky Houghton, Andreas M. Olligschlaeger. 2000. Complementary Video AND Audio Analysis FOR Broadcast. Abstract: 1994, uniquely utilizes integrated speech and image and natural language understanding to process broadcast video. The project's goal is to allow search and retrieval in the video medium, similar to what is available today for text only. To enable this access to video, fast, high-accuracy automatic transcriptions of broadcast news stories are generated through
- Rong Jin, Alexander Hauptmann. 2000. Title generation for spoken broadcast news using a training corpus. Abstract: The problem of title generation involves finding the essence of a document and expressing it in only a few words. The results of a query to the Informedia Digital Video Library are summarized through an automatically generated title for each retrieved news story. When the document is errorful, as with speech-recognized broadcast news stories, the title creation challenge becomes even greater. We implemented a set of title word selection strategies and evaluated them on an independent test corpus of 579 broadcast news documents, comparing manual transcription results to automatically recognized speech using the CMU Sphinx speech recognition system with a 64000-word broadcast news language model. Using a training collection of 21190 transcribed broadcast news stories, we trained several systems to produce appropriate title words, i.e. Naive Bayesian approach with full vocabulary, Naive Bayesian approach with limited vocabulary, nearest neighbor approach and extractive approach. The F1 results shows that the nearest neighbor approach is a quick and easy way of generating good titles for speech recognized documents (F1 = 15.2%), while a Nave Bayesian approach with limited vocabulary also does well on our F1 measure (F1 = 21.6%), which ignores word order in the titles. Overall, the results show that title generation for speech recognized news documents is possible at a level approaching the accuracy of titles generated for perfect text transcriptions. One surprising phenomenon is that extractive approach performances slightly better for speech recognized documents than for manual transcripts.
- Paul E. Kennedy, Alexander Hauptmann. 2000. Automatic Title Generation using EM. Abstract: Our prototype automatic title generation system inspired by statistical machine-translation approaches [1] treats the document title like a translation of the document. Titles can be generated without extracting words from the document. A large corpus of documents with human-assigned titles is required for training title “translation” models. On an f1 evaluation score our approach outperformed another approach based on Bayesian probability estimates [7].
- Paul E. Kennedy, Alexander Hauptmann. 2000. Automatic title generation for EM. Abstract: Our prototype automatic title generation system inspired by statistical machine-translation approaches [1] treats the document title like a translation of the document. Titles can be generated without extracting words from the document. A large corpus of documents with human-assigned titles is required for training title "translation" models. On an f1 evaluation score our approach outperformed another approach based on Bayesian probability estimates [7].
- H. Wactlar, Alexander Hauptmann, Michael G. Christel, Ricky Houghton, Andreas M. Olligschlaeger. 2000. Complementary video and audio analysis for broadcast news archives. Abstract: Abstract The Informedia Digital Video Library system extracts information from digitized video sources and allows full content search and retrieval over all extracted data. This extracted 'metadata' enables users to rapidly find interesting news stories and to quickly identify whether a retrieved TV news story is indeed relevant to their query. This article highlights two unique features: named faces and location analysis . Named faces automatically associate a name with a face, while location analysis allows the user to visually follow the action in the news story on a map and also allows queries for news stories by graphically selecting a region on the map. 1 The Informedia Digital Video Library Project The Informedia Digital Video Library project [1], initiated in 1994, uniquely utilizes integrated speech, image and natural language understanding to process broadcast video. The project’s goal is to allow search and retrieval in the video medium, similar to what is available today for text only. To enable this access to video, fast, high-accuracy automatic transcriptions of broadcast news stories are generated through Carnegie Mellon’s Sphinx speech recognition system and closed captions are incorporated where available. Image processing determines scene boundaries, recognizes faces and allows for image similarity comparisons. Text visible on the screen is recognized through video OCR and can be searched. Everything is indexed into a searchable digital video library [2], where users can ask queries and retrieve relevant news stories as results. The
- Michael G. Christel, H. Wactlar, Alexander Hauptmann. 1999. Informedia Digital Video Library Accomplishments and Future Directions. Abstract: The Informedia Digital Video Library Project (IDVL), launched in mid-1994, was one of six Digital Library Initiative (DLI) Phase 1 projects funded jointly by NSF, DARPA and NASA. IDVL was the only DLI project focusing specifically on information extraction from video and audio content, successfully pioneering the automated indexing and retrieval of multimedia documents from over a terabyte of online data. IDVL integrated speech recognition, image processing, and natural language processing to deal with the size and temporal characteristics specific to video.
- H. Wactlar, Michael G. Christel, Yihong Gong, Alexander Hauptmann. 1999. Lessons Learned from Building a Terabyte Digital Video Library. Abstract: The Informedia Project at Carnegie Mellon University has created a terabyte digital video library in which automatically derived descriptors for the video are used for indexing, segmenting and accessing the library contents. Begun in 1994, the project presented numerous challenges for library creation and deployment, valuable information covered in this article. The authors, developers of the project at Carnegie Mellon University, addressed these challenges by: automatically extracting information from digitized video; creating interfaces that allowed users to search for and retrieve videos based on extracted information; and validating the system through user testbeds. Through speech, image, and natural language processing, the Informedia Project has demonstrated that previously inaccessible data can be derived automatically and used to describe and index video segments.
- Photina Jaeyun Jang, Alexander Hauptmann. 1999. Improving acoustic models with captioned multimedia speech. Abstract: Speech recognition can be used to create searchable transcripts for audio indexing in digital video libraries. Large amounts of hand-transcribed speech training data are required to build or improve acoustic models of highly accurate speech recognition systems using current technologies. We present a technique to use television broadcasts with closed-captions as a source for large amounts of automatically extracted and accurately transcribed speech for improving acoustic models. The errorful closed captioned text is aligned with the also errorful speech recognition output and matching segments are used with each corresponding audio segment as acoustic training data to improve the speech recognition system. Our technique automatically extracted 131.4 hours of transcribed speech and improved the word error rate of our currently best speech recognition system (Sphinx-III) from 32.82% to 31.19%. A speech recognizer trained exclusively on 70.7 hours of this automatically transcribed speech produced a word error rate of 32.7%.
- Alexander Hauptmann. 1999. Semantic Interpretation ((frame *move) (form Ques) (agent ((frame *human) (pro +) (number Sing) (person 2))) (object ((frame *body-part) (name *thumb) (possessive ((frame *human) (number Sg) (person 2) (pro +) ) 1) ) ). Abstract: The development of larger scale natural language systems has been hampered by the need to manually create mappings from syntactic structures into meaning representations. A new approach to semantic interpretation is proposed, which uses partial syntactic structures as the main unit of abstraction for interpretation rules. This approach can work for a variety of syntactic representations corresponding to directed acyclic graphs. It is designed to map into meaning representations based on frame hierarchies with inheritance. We define semantic interpretation rules in a compact format. The format is suitable for automatic rule extension or rule generalization, when existing hand-coded rules do not cover the current input. Furthermore, automatic discovery of semantic interpretation rules from input/output examples is made possible by this new rule format. The principles of the approach are validated in a comparison to other methods on a separately developed domain. Instead of relying purely on painstaking human effort, this paper combines human expertise with computer learning strategies to successfully overcome the bottleneck of semantic interpretation. Semantic Interpretation An important step in the language understanding process is constructing a representation of the meaning of a sentence, given the syntactic structure. Mapping from syntactic structures into a meaning representation is referred to as semantic interpretation or semantic mapping. To do this, we need a set of interpretation rules, which tell us how to create a meaning representation from the syntax representation. Creating semantic interpretations can be difficult for many reasons. Consider, for example, a machine translation system with N languages and M different domains. Each domain describes a distinct world of conversational topics and concepts. While we only need to write one syntactic grammar to understand each language and only one frame representation for each domain, we must write N * M different sets of semantic interpretation rules to interpret and map from each syntactic representation into ((FRAME *MOVE) (FORM QUES) (AGENT ((FRAME *HUMAN) (PRO +) (NUMBER SING) (PERSON 2))) (OBJECT ((FRAME *BODY-PART) (NAME *THUMB) (POSSESSIVE ((FRAME *HUMAN) (NUMBER SG) (PERSON 2)
- Alexander Hauptmann, Andreas M. Olligschlaeger. 1999. USING LOCATION INFORMATION FROM SPEECH RECOGNITION OF TELEVISION NEWS BROADCASTS. Abstract: The Informedia Digital Video Library system extracts information from digitized video sources and allows full content search and retrieval over all extracted data. This extracted ’metadata’ enables users to rapidly find interesting news stories and to quickly identify whether a retrieved TV news story is indeed relevant to their query. Through the extraction of named entity information from broadcast news we can determine what people, organizations, dates, times and monetary amounts are mentioned in the broadcast. With respect to location data, we have been able to use location analysis derived from the speech transcripts to allow the user to visually follow the action in the news story on a map and also allow queries for news stories by graphically selecting a region on the map. 1. The Informedia Digital Video Library Project The Informedia Digital Video Library project [1], initiated in 1994, uniquely utilizes integrated speech, image and natural language understanding to process broadcast video. The project’s goal is to allow search and retrieval in the video medium, similar to what is available today for text only. To enable this access to video, fast, high-accuracy automatic transcriptions of broadcast news stories are generated through Carnegie Mellon’s Sphinx speech recognition system and closed captions are incorporated where available. Image processing determines scene boundaries, recognizes faces [4][12] and allows for image similarity comparisons. Text visible on the screen is recognized through video OCR [5] and can be searched. Everything is indexed into a searchable digital video library [2][3], where users can ask queries and retrieve relevant news stories as results. The News-on-Demand collection in the Informedia Digital Library serves as a testbed for automatic library creation techniques of continuously captured television and radio news content from multiple countries in a variety of languages. As of January 1998, the Informedia project had about 1.5 terabytes of news video indexed and accessible online, with over 1600 news broadcasts containing about 40,000 news stories dating back to 1996 The Informedia system allows information retrieval in both spoken language and video or image domains. Queries for relevant news stories may be made with words, images or maps. Faces are detected in the video and can be searched. Information summaries can be displayed at varying detail, both visually and textually. Text summaries are displayed for each news story through topics and titles. Visual summaries are given through thumbnail images, filmstrips and dynamic video skims. Every location referenced in the news stories is labeled for geographic display on a map and the corresponding news item can be retrieved through a map area selection. The system also provides for extraction and reuse of video documents encoded in MPEG-1 format for web-based access and presentation. A multi-lingual component, currently implemented for Spanish and Serb/Croatian corpora, translates English language queries for text search into the target language. English language topics are also assigned to news stories. A user can add spoken or typed annotations to any news story, which are immediately searchable. News clips can be cut and pasted into HTML or PowerPoint presentations. 1.1 Speech Recognition Speech recognition in the Informedia Digital Video Library is done in two different passes. To get an initial transcript into the library as quickly as possible, a 20,000-word vocabulary version of the Sphinx-II recognizer is applied [6]. In a second pass, we use the slower, but more accurate Sphinx-III speech recognition system. In the 1997 DARPA broadcast news evaluations, the CMU Sphinx-III system achieved overall word error rate of 24% when multiple passes are applied [7]. However, this result was obtained at processing speeds of several hundred times real time. To make the speech recognition reasonably fast, we restrict the beam of the recognizer, resulting in a word error rate of about 34% on the broadcast news evaluation data. To obtain better performance, we use additional training data extracted from closed-captioned news transcripts for which the speech recognizer has a high confidence that the data is correct [9][10]. In addition, we also build a new language model every day, which interpolates a standard broadcast news transcription corpus [7][8] with current online web news reports and actual transcripts available on the CNN website (cnn.com). With help from both the improved acoustic models and by using the daily language model, we obtain a word error below 20%. Since we can parallelize the processing of a news story, the actual transcript data appears in our library within 2.5 hours from the broadcast time. 2. Location Analysis A named-entity extraction process is used to provide location data derived from the speech transcripts for geocoding and map displays and searches. Using a named entity tagger implemented as described in [13][14], we extract possible location phrases from the audio transcript. We also take the output of the video optical character recognition extraction [5], and extract possible location phrases from the video OCR. All these possible locations are then cross-referenced against a gazetteer of about 80,000 places and their locations [16], which include countries, cities, villages and states or provinces from all over the world. Currently excluded from this list are water areas, mountains and other non-political geographical data. Since we will ignore all locations that cannot be found in the gazetteer with a latitude and longitude, the accuracy of the named entity extraction process per se is not as critical. What is more relevant is the question whether we have identified the correct coordinates for a location. If the location is not in the gazetteer, then we have no chance of providing the coordinates of this location entity. If the words from the phrase describing the location were not in the speech recognition vocabulary, we again have no chance of providing the proper coordinates, even if we have properly identified the words as denoting a location. However, often locations are ambiguous in their coordinates. We then need to disambiguate different references to locations, e.g. “Washington” may refer to a number of cities in the United States, or it can refer to a state. A simple hierarchical disambiguation scheme has been implemented to distinguish among candidate coordinates: 1. If a location is determined to be ambiguous, then we first check if other location references within the current news story disambiguate among the alternate locations. 2. If the location is still ambiguous, we then see if other location references within a state or province favor a particular state. This way we can, for example, distinguish between different, initially ambiguous, references to Memphis in different states within the United States. 3. If there is no disambiguating evidence at the state/province level, we check to see if the location can be distinguished on the basis of country reference elsewhere in the news story transcripts. The mention of France in conjunction with Paris would, for example, distinguish Paris, Texas from Paris, France, under the assumption that either Texas or France would be mentioned elsewhere in the news story. 4. If the location is still ambiguous, we check for reference to locations within continents that might disambiguate the locations. 5. If the information is still ambiguous, we either discard it or choose the first of the alternative location interpretations. It is obvious that a more sophisticated approach would use a large amount of manually coded training data to help distinguish ambiguous locations. With sufficient amounts of training data, a HMM based approach along the lines of [14] would be feasible. Since we are not in a position to afford manual geocoding of large amounts of data, our simple approach appears to distinguish between different ambiguous locations. One notable exception is the disambiguation of " New York", as either a city or a state. Our approach frequently fails to distinguish between to two, for example in "The governor of New York drove to New York to meet with the mayor." Only if we have cues through the mention of "New York City" vs. "New York State" can we distinguish between these two alternative ambiguous location references. A big factor in the geocoding of information is the quality of the gazetteer. At present we have no automatic process for adding new locations and coordinates to our gazetteer, and any errors, misspellings or erroneous data must be corrected by hand. The location information is stored in a relational database, which associates the geocoded locations with specific news stories and time periods with the news story. This database can be queried textually by looking for all the location names as text strings, geographically by the coordinates of the locations and also through a lookup of the ID of a news story. In each case, a set of location names, coordinates and news stories identification tags are returned. Having the coordinates identified allows us to use the location database information in three ways: 1. Locations that occur more than once in a news story are added to the title information. Since automatically created titles [11] from speech transcripts are quite noisy, this helps identify major locations in the news story. 2. Locations can be dynamically displayed on a map, allowing the user to “follow along” with the geographic focus of a news story. As the video or audio for the relevant paragraph selection is playing, the locations in focus are highlighted. The dynamic map also serves as a static summary of the locations in the news story, by identifying all the locations in the news story at one glance. 3. We can use a query that graphically specifies a rectangle on the map to find all news stories that re
- Michael G. Christel, Alexander Hauptmann, Adrienne Warmack, Scott A. Crosby. 1999. Adjustable filmstrips and skims as abstractions for a digital video library. Abstract: Filmstrips and video skims are two presentation schemes for abstracting information in a digital video segment. Filmstrips present information all at once in a static form, while video skims are played and disclose information temporally. The paper discusses the evolution of the filmstrip and skim interfaces in the Informedia Digital Video Library. Filmstrips are commonly deployed as interfaces for video and image libraries, but we found initial Informedia filmstrips and skims received little use. We discuss the interface considerations motivating the redesign of filmstrips and skims to adjust their presentations dynamically based on user context and preference.
- M. Siegler, Rong Jin, Alexander Hauptmann. 1999. CMU Spoken Document Retrieval in Trec-8: Analysis of the role of Term Frequency TF. Abstract: The participation of Carnegie Mellon University in the TREC-8 Spoken Document Retrieval Track used the basic same Sphinx speech recognition system as in TREC-7. Due to some unfortunate defaults in the parameter setup files, the speech recognizer did not perform in a reasonable manner. We will not analyze the results of the speech recognizer runs, as we believe the results contained abnormal types of errors, and insights or improvements on these errors would not generalize. A thorough examination of the speech recognition condition is given in [3]. However, we did evaluate a slightly modified weighting scheme in the reference (R1) and baseline (B1) conditions, which is described below.
- Paul E. Kennedy, Alexander Hauptmann. 1999. Laughter extracted from television closed captions as speech recognizer training data. Abstract: Closed captions in television broadcasts, intended to aid the hearing impaired, also have potential as training data for speech-recognition software. Use of closed captions for automatic extraction of virtually unlimited training data has already been demonstrated [1]. This paper reports some preliminary work on the use of non-speech sound tokens included in closed captions to extract training data to augment a speech recognizer’s repertoire of non-speech phonemes. A small experiment was performed to pinpoint laughter sounds in television news broadcasts using the Informedia Digital Video Library’s retrieval capabilities, which automatically exploit closed captions. The snippets found were used to retrain a speech recognizer. A small test showed a small but significant gain in performance. In the future we plan to develop this approach into a fully automatic procedure for extracting training data for non-speech sounds.
- H. Wactlar, Michael G. Christel, Alexander Hauptmann, Yihong Gong. 1999. Informedia Experience-on-Demand: capturing, integrating and communicating experiences across people, time and space. Abstract: The Informedia Experience-on-Demand system uses speech, image, and natural language processing combined with GPS information to capture, integrate, and communicate personal multimedia experiences. This paper discusses in initial prototype of the EOD system.
- Alexander Hauptmann, Danny Lee, Paul E. Kennedy. 1999. Topic Labeling of Multilingual Broadcast News in the Informedia Digital Video Library. Abstract: The Informedia Digital Video Library Project includes a multilingual component for retrieval of video documents in multiple languages and a topic-labeling component for English video documents. We now extend this capability to English topic labeling of foreign-language broadcast-news stories. News stories are coarsely machine-translated into English, then assigned to a topic category using a K-nearest-neighbor algorithm. In preliminary tests on Croatian television news, topic assignment based on the best available machine translation technology showed performance only 8% worse (on a standard F-measure of performance) than that based on manual document translation. Using a phrase-based MT module the performance degradation was 31%. 1 The Informedia Digital Video Library The Informedia Digital Library Project [1,2] allows full content indexing and retrieval of text, audio and video material, similar to what is available today for text only. To enable this access to video, speech recognition is used to provide a text transcript for the audio track, image processing determines scene boundaries, recognizes faces and allows for image similarity comparisons. Everything is indexed into a searchable digital video library [4,6], where users can submit queries and retrieve relevant news stories as results. News-on-Demand is a particular collection in the Informedia Digital Library that has served as a test-bed for automatic library creation techniques. As of July 1998, the Informedia project had about 1.3 terabytes of news video indexed and accessible online, with 1200 news broadcasts containing 24000 news stories. The Informedia digital video library system has two distinct subsystems: the Library Creation System and the Library Exploration Client. The library creation system runs every night, automatically capturing, processing and adding current news shows to the library. It is during the library creation phase, that topics for news stories are automatically assigned to incoming stories. In [17], we described and evaluated tested a topic labeling component for the English language version of the Informedia Digital Video Library. During library exploration, the user can browse or search these stories and topics using the library exploration client. At 5 topics, the KNN-based system’s recall was 0.49; and relevance was 0.48, with an F-measure at equal recall and precision of about 0.48.
- Alexander Hauptmann. 1999. Integrating and using large databases of text, images, video, and audio. Abstract: WITH THE ADVENT OF RELAtively cheap, large online storage capacities and advances in digital compression, comprehensive sources of text, image, video, and audio (TIVA) can be stored and made available for research and applications. The processing of a single medium has seen significant progress, especially for pure text sources. Also, images are frequently processed and made available through a queryby-example procedure (that is, find another image that has similar colors, textures, and shapes as this one). However, the processing of a combination of multiple types of data has not been explored as thoroughly. Most TIVA sources were not produced with computer processing in mind. In contrast with text processing, few effective methods exist for understanding or even searching the content of combined TIVA sources. Intelligent, content-understanding systems can greatly improve the usefulness of the huge quantities of existing material from these sources. Collecting and intelligently integrating several of these media sources open up opportunities for novel applications of existing AI techniques and for further development of intelligent technologies. Unfortunately, there is no clear categorization or organization of the various research efforts concerning mixed-media databases.
- Photina Jaeyun Jang, Alexander Hauptmann. 1999. Selection for acoustic coverage from unlimited speech extracted from closed-captioned TV. Abstract: Given unlimited amounts of speech training data, it is desirable to predict informative subsets that will still improve the resulting acoustic model. We present a triphone frequency threshold measure for predicting informative subsets from vast amounts of speech. Results with single pass decoding show that acoustic models built from our selection-based speech set perform better than when trained on similar amounts of non-selected speech, and perform similar to models built from the original, larger amount of speech.
- Photina Jaeyun Jang, Alexander Hauptmann. 1999. Learning to Recognize Speech by Watching Television. Abstract: Our proposed technique gathers large amounts of speech from open broadcast sources and combines it with automatically obtained text or closed captioning to identify suitable speech-training material. George Zavaliagkos and Thomas Colthurst worked on a different approach to this method that uses confidence scoring on the acoustic data itself to improve performance in the absence of any transcribed data, but their approach only yielded marginal results. Our initial efforts also provided only limited success with small amounts of data. We describe our approach to collecting almost unlimited amounts of accurately transcribed speech data. This information serves as training data for the acoustic model component of most high-accuracy speaker-independent speech-recognition systems. The error-ridden closed-captioned text aligns with the similarly error-ridden speech recognizer output. We assume matching segments of sufficient length are reliable transcriptions of the corresponding speech. We then use these segments as the training data for an improved speech recognizer.
- H. Wactlar, Alexander Hauptmann, M. Witbrock. 1998. INFORMEDIATM: NEWS-ON-DEMAND EXPERIMENTS IN SPEECH RECOGNITION. Abstract: In theory, speech recognition technology can make any spoken words in video or audio media usable for text indexing, search and retrieval. This article describes the News-on-Demand application created within the InformediaTM Digital Video Library project and discusses how speech recognition is used in transcript creation from video, alignment with closed-captioned transcripts, audio paragraph segmentation and a spoken query interface. Speech recognition accuracy varies dramatically depending on the quality and type of data used. Informal information retrieval test show that reasonable recall and precision can be obtained with only moderate speech recognition accuracy. 1. INFORMEDIA: NEWS-ON-DEMAND The InformediaTM digital video library project [1,2,3,4] at Carnegie Mellon University is creating a digital library of text, images, videos and audio data available for full content search and retrieval. News-on-Demand is an application within Informedia that monitors news from TV, radio and text sources and allows the user to retrieve news stories of interest. A compelling application of the Informedia project is the indexing and retrieval of television, radio and text news. The Informedia: News-on-Demand application [5,6] is an innovative example of indexing and searching broadcast news video and news radio material by text content. News-on-Demand is a fullyautomatic system that monitors TV, radio and text news and allows selective retrieval of news stories based on spoken queries. The user may choose among the retrieved stories and play back news stories of interest. The system runs on a Pentium PC using MPEGI video compression. Speech recognition is done on a separate platform using the Sphinx-II continuous speech recognition system [7]. The News-on-Demand application forces us to consider the limits of what can be done automatically and in limited time. News events happen daily and it is not feasible to process, segment and label news through manual or “human-assisted” methods. Timeliness of the library information is important, as is the ability to continuously update the contents. Thus we are forced to fully exploit the potential of computer speech recognition without the benefit of human corrections and editing. INFORMEDIATM: NEWS-ON-DEMAND EXPERIMENTS IN SPEECH RECOGNITION Howard D. Wactlar, Alexander G. Hauptmann and Michael J. Witbrock School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3890 Even though our work is centered around processing news stories from TV broadcasts, the system exemplifies an approach that can make any video, audio or text data accessible. Similar methods can help to index and search other streamed multi-media data by content in other Informedia applications. Other attempts at solutions have been obliged to restrict the data to only text material, as found in most news databases. Video-ondemand allows a user to select (and pay for) a complete program, but does not allow selective retrieval. The closest approximation to News-on-Demand can be found in the “CNN-AT-WORK” system offered to businesses by a CNN/Intel venture. At the heart of the CNN-AT-WORK solution is a digitizer that encodes the video into the INDEO compression format and transmits it to workstations over a local area network. Users can store headlines together with video clips and retrieve them at a later date. However, this service depends entirely on the separately transmitted “headlines” and does not include other news sources. In addition, CNN-AT-WORK does not feature an integrated multimodal query interface [8]. Preliminary investigations on the use of speech recognition to analyze a news story were made by [9]. Without a powerful speech recognizer, their approach used a phonetic engine that transformed the spoken text into an (errorful) phoneme string. The query was also transformed into a phoneme string and the database searched for the best approximate match. Errors in recognition, as well as word prefix and suffix differences did not severely affect the system since they scattered equally over all documents and wellmatching search scores dominate the retrieval. Another news processing systems that includes video materials is the MEDUSA system [10]. The MEDUSA news broadcast application can digitize and record news video and teletext, which is equivalent to closed-captions. Instead of segmenting the news into stories, the system uses overlapping windows of adjacent text lines for indexing and retrieval. During retrieval the system responds to typed requests returning an ordered list of the most relevant news broadcasts. Query words are stripped of suffixes before search and the relevance ranking takes word frequency in the segment and over all the corpus into account, as well as the ability of words to discriminate between stories. Within a news broadcast, it is up to the user to select and play a region using information given by the system about the location of the matched keywords. The focus of MEDUSA is in the system architecture and the information retrieval component. No image processing and no speech recognition is performed. 1.1. Component Technologies There are three broad categories of technologies we can bring This material is based upon work supported by the National Science Foundation under Cooperative Agreement No. IRI-9411299. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. to bear to create and search a digital video library from broadcast video and audio materials [11]: Text processing looks at the textual (ASCII) representation of the words that were spoken, as well as other text annotations. These may be derived from the transcript, from the production notes or from the closed-captioning that might be available. Text analysis can work on an existing transcript to help segment the text into paragraphs [12]. An analysis of keyword prominence allows us to identify important sections in the transcript [13]. Other more sophisticated language based criteria are under investigation. We currently use two main techniques for text analysis: 1. If we have a complete time aligned transcript available from the closed-captioning or through a human-generated transcription, we can exploit natural “structural” text markers such as punctuation to identify news story boundaries 2. To identify and rank the contents of one news segment, we use the well-known technique of TF/IDF (term frequency/ inverse document frequency) to identify critical keywords and their relative importance for the video document[13]. Image analysis looks at the images in the video portion of the MPEG stream. This analysis is primarily used for the identification of scene breaks and to select static frame icons that are representative of a scene. Image statistics are computed for Library Creation Library Exploration Offline Online TV News Radio News Text News
- Photina Jaeyun Jang, Alexander Hauptmann. 1998. Hierarchical cluster language modeling with statistical rule extraction for rescoring n-best hypotheses during speech decoding. Abstract: We propose an unsupervised learning algorithm that learns hierar- chical patterns of word sequences in spoken language utterances. It extracts cluster rules from training data based on high n-gram prob- abilities to cluster words or segment a sentence. Cluster trees, simi-lar to parse trees, are constructed from the learned cluster rules. Through hierarchical clustering we are adding grammatical structure onto the traditional trigram language model. The learned clus- ter rules are used to improve the n-best utterance hypothesis list which is output by the Sphinx III speech recognizer. Our hierarchi- cal cluster language model is used to rescore and filter these n-best utterance hypotheses. It assigns confidence scores to segments of hypotheses that can be clustered hierarchically with the learned cluster rules. Rescoring the original n-best hypothesis list, which is based on acoustic and trigram language model scores, with our hierarchical cluster language model results in a set of hypotheses with lower word error rate. Our cluster language model was trained on TREC broadcast news data from 1995 and 1996, and tested on the HUB-4 ‘97 development test broadcast news data. Compared to manually created grammar rules, the cluster trees more accurately reflect the speech data since their cluster rules are automatically learned based on empirical n-gram probabilities from the training data, whereas manually written grammar rules can introduce human bias, and are expensive to develop. Prior symbolic knowledge in the form of rules can also be incorporated by simply applying the rules to the training data before the earliest applicable learning iteration. Our algorithm is also able to learn clusters reflecting various styles of data: whether the language is formal, strictly grammatical or loose conversational speech.
- Alexander Hauptmann, R. E. Jones, K. Seymore, Seán Slattery, M. Witbrock, M. Siegler. 1998. EXPERIMENTS IN INFORMATION RETRIEVAL FROM SPOKEN DOCUMENTS. Abstract: This paper describes the experiments performed as part of the TREC-97 Spoken Document Retrieval Track. The task was to pick the correct document from 35 hours of recognized speech documents, based on a text query describing exactly one document. Among the experiments we described here are: Vocabulary size experiments to assess the effect of words missing from the speech recognition vocabulary; experiments with speech recognition using a stemmed language model; using confidence annotations that estimate of the correctness of each recognized word; using multiple hypotheses from the recognizer. And finally we also measured the effects of corpus size on the SDR task. Despite fairly high word error rates, information retrieval performance was only slightly degraded for speech recognizer transcribed documents.
- Alexander Hauptmann, P. Scheytt, H. Wactlar, Paul E. Kennedy. 1998. MULTI-LINGUAL INFORMEDIA: A DEMONSTRATION OF SPEECH RECOGNITION AND INFORMATION RETRIEVAL ACROSS MULTIPLE LANGUAGES. Abstract: The Multilingual Informedia Project demonstrates a seamless extension of the Informedia approach to search and discovery across video documents in multiple languages. Previously, we successfully demonstrated that current speech recognizers allow accurate information retrieval for automatically processed English news TV broadcasts. The new system performs speech recognition on foreign language news broadcasts, segments it into stories and indexes the foreign data together with existing English news data. This first multi-lingual prototype could easily be extended to other languages.
- H. Wactlar, Alexander Hauptmann, M. Witbrock. 1998. Informedia News-On Demand: Using Speech Recognition to Create a Digital Video Library. Abstract: Abstract : In theory, speech recognition technology can make any spoken words in video or audio media usable for text indexing, search and retrieval. This article describes the News-on-Demand application created within the Informedia(TM) Digital Video Library project and discusses how speech recognition is used in transcript creation from video, alignment with closed-captioned transcripts, audio paragraph segmentation and a spoken query interface. Speech recognition accuracy varies dramatically depending on the quality and type of data used. Informal information retrieval tests show that reasonable recall and precision can be obtained with only moderate speech recognition accuracy.
- Christine Lichti, C. Faloutsos, H. Wactlar, Michael G. Christel, Alexander Hauptmann. 1998. Informedia: Lessons from a Terabyte+, Operational, Digital Video Database System. Abstract: We describe the design decisions and lessons learned from Informedia, a digital video database system developed at Carnegie Mellon University and currently hosting over one terabyte of video data. Informedia is a research prototype for video retrieval by content, as well as a production system. The system has been implemented on a relational database, and it currently achieves less than five-second response times for ad hoc queries by video content on mid-range Pentium ® workstations.
- Alexander Hauptmann, M. Witbrock. 1998. Story segmentation and detection of commercials in broadcast news video. Abstract: The Informedia Digital Library Project allows full content indexing and retrieval of text, audio and video material. Segmentation is an integral process in the Informedia digital video library. The success of the Informedia project hinges on two critical assumptions: that we can extract sufficiently accurate speech recognition transcripts from the broadcast audio and that we can segment the broadcast into video paragraphs, or stories, that are useful for information retrieval. In previous papers we have shown that speech recognition is sufficient for information retrieval of pre-segmented video news stories. We now address the issue of segmentation and demonstrate that a fully automatic system can extract story boundaries using available audio, video and closed-captioning cues. The story segmentation step for the Informedia Digital Video Library splits full-length news broadcasts into individual news stories. During this phase the system also labels commercials as separate "stories". We explain how the Informedia system takes advantage of the closed captioning frequently broadcast with the news, how it extracts timing information by aligning the closed-captions with the result of the speech recognition, and how the system integrates closed-caption cues with the results of image and audio processing.
- Alexander Hauptmann, Danny Lee. 1998. Topic labeling of broadcast news stories in the informedia digital video library. Abstract: This paper describes the implementation of a topic labeling component for the Informedia Digital Video Library. Each news story recorded from the evening news is assigned to one of 3178 topic categories using a K-nearest neighbor classification algorithm. In preliminary tests, the system achieved recall of 0.49 1 with relevance of 0.482 when up to 5 topics could be assigned to a news story.
- Alexander Hauptmann, Richard Jones, K. Seymore, Seán Slattery, M. Witbrock, M. Siegler. 1998. EXPERIMENTS IN INFORMATION RETRIEVAL FROM SPOKEN DOCUMENTS1. Abstract: This paper describes the experiments performed as p art of the TREC-97 Spoken Document Retrieval Track. The task w as to pick the correct document from 35 hours of recogniz ed speech documents, based on a text query describing exactly one document. Among the experiments we described here a re: Vocabulary size experiments to assess the effect of w rds missing from the speech recognition vocabulary; exp eriments with speech recognition using a stemmed language mo del; using confidence annotations that estimate of the correct ness of each recognized word; using multiple hypotheses from the recognizer. And finally we also measured the effects of corpus size on the SDR task. Despite fairly high word error rates, inf ormation retrieval performance was only slightly degraded fo r speech recognizer transcribed documents.
- M. Witbrock, Alexander Hauptmann. 1998. Improving Acoustic Models by Watching Television. Abstract: Abstract : Obtaining sufficient labelled training data is a persistent difficulty for speech recognition research. Although well transcribed data is expensive to produce, there is a constant stream of challenging speech data and poor transcription broadcast as closed-captioned television. We describe a reliable unsupervised method for identifying accurately transcribed sections of these broadcasts, and show how these segments can be used to train a recognition system. Starting from acoustic models trained on the Wall Street Journal database, a single iteration of our training method reduced the word error rate on an independent broadcast television news test set from 62.2% to 59.5%.
- M. Witbrock, Alexander Hauptmann. 1998. Speech Recognition for a Digital Video Library. Abstract: The standard method for making the full content of audio and video material searchable is to annotate it with human-generated meta-data that describes the content in a way that the search can understand, as is done in the creation of multimedia CD-ROMs. However, for the huge amounts of data that could usefully be included in digital video and audio libraries, the cost of producing this meta-data is prohibitive. In the Informedia Digital Video Library, the production of the meta-data supporting the library interface is automated using techniques derived from artificial intelligence (AI) research. By applying speech recognition together with natural language processing, information retrieval, and image analysis, an interface has been produced that helps users locate the information they want, and navigate or browse the digital video library more effectively. Specific interface components include automatic titles, filmstrips, video skims, word location marking, and representative frames for shots. Both the user interface and the information retrieval engine within Informedia are designed for use with automatically derived meta-data, much of which depends on speech recognition for its production. Some experimental information retrieval results will be given, supporting a basic premise of the Informedia project: That speech recognition generated transcripts can make multimedia material searchable. The Informedia project emphasizes the integration of speech recognition, image processing, natural language processing, and information retrieval to compensate for deficiencies in these individual technologies. © 1998 John Wiley & Sons, Inc.
- Alexander Hauptmann, M. Witbrock, Michael G. Christel. 1997. Artificial intelligence techniques in the interface to a Digital Video Library. Abstract: For the huge amounts of audio and video material that could usefully be included in digital libraries, the cost of producing human-generated annotations and meta-data is prohibitive. In the Informedia Digital Video Library, the production of meta-data supporting the library interface is automated using techniques from Artificial Intelligence (AI). By applying speech recognition, natural language processing and image analysis, the interface helps users locate the information they want and navigate or browse the digital video library more effectively. Specific AI-based interface components include automatic titles, filmstrips, video skims, word location marking and representative frames for shots.
- H. Wactlar, Alexander Hauptmann, M. Smith, K. Pendyala, D. Garlington. 1997. Automated video indexing of very large video libraries. Abstract: The Informedia Digital Video Library project is implementing full content search and retrieval from digital video, audio, and text libraries through the utilization of integrated speech, image, and language understanding technologies for automated creation and exploration. Image processing analyzes scenes, speech processing transcribes the audio signal, and natural language processing determines word relevance. Together, these generate a meaningful index into the video content. Segment breaks produced by image processing are examined, along with the boundaries identified by the natural language processing of the transcript to partition the video library into sets of segments, or video paragraphs. Automating these techniques into a unified collaborative system uniquely enables us to include and search through vast amounts of video data in the library with little to no human intervention.
- B. Gaines, M. Musen, R. Uthurusamy, S. Haller, S. McRoy, Douglas W. Oard, David Hull, Alexander Hauptmann, Michael Witbrock, Kevin Mahesh, A. Farquhar, M. Gruninger, J. Doyle, R. H. Thomason. 1997. AAAI 1997 Spring Symposium Reports. Abstract: The Association for the Advancement of Artificial Intelligence (AAAI) held its 1997 Spring Symposium Series on 24 to 26 March at Stanford University in Stanford, California. This article contains summaries of the seven symposia that were conducted: (1) Artificial Intelligence in Knowledge Management; (2) Computational Models for Mixed-Initiative Interaction; (3) Cross-Language Text and Speech Retrieval; (4) Intelligent Integration and Use of Text, Image, Video, and Audio Corpora; (5) Natural Language Processing for the World Wide Web; (6) Ontological Engineering; and (7) Qualitative Preferences in Deliberation and Practical Reasoning.
- Alexander Hauptmann, H. Wactlar. 1997. Indexing and search of multimodal information. Abstract: The Informedia Digital Library Project allows full content indexing and retrieval of text, audio and video material. The integration of speech recognition, image processing, natural language processing and information retrieval overcomes limits in each technology to create a useful system. In order to answer the question how good speech recognition has to be in order to be useful and usable for indexing and retrieving speech recognizer generated transcripts, some empirical evidence is presented that illustrates the degradation of information retrieval at different levels of speech accuracy. In our experiments, word error rates up to 25% did not significantly impact information retrieval and error rates of 50% still provided 85 to 95% of the recall and precision relative to fully accurate transcripts in the same retrieval system.
- M. Witbrock, Alexander Hauptmann. 1997. Using words and phonetic strings for efficient information retrieval from imperfectly transcribed spoken documents. Abstract: Digital Compression Text Library Creation
- Alexander Hauptmann, M. Witbrock. 1997. Informedia: news-on-demand multimedia information acquisition and retrieval. Abstract: In theory, speech recognition technology can make any spoken words in video or audio media subject to text indexing, search and retrieval. This article describes the News-on-Demand application created within the InformediaTM Digital Video Library project and discusses how speech recognition is used for transcript creation from video, time alignment of closed-captioned transcripts, a speech query interface, and audio paragraph segmentation. Our results show that speech recognition accuracy varies dramatically depending on the quality and type of data used, but the system is quite useable with only moderate speech recognition accuracy. 1. What is Informedia: News-on-Demand The InformediaTM digital video library project [Informedia95, Wactlar96] at Carnegie Mellon University is creating a digital library in which text, image, video and audio data are available for full content retrieval. News-on-Demand is an application within Informedia which monitors news from TV, radio and text sources and allows the user to retrieve news stories of interest. This paper gives a brief overview of the Informedia digital video library project [Christel94a, Stevens94, Christel94b, Informedia95] followed by a detailed description of the News-on-Demand application [Hauptmann95]. Both the automated library creation process for News-on-Demand and the news library exploration process will be explained. We show how speech recognition fits into the various digital news library processing steps. Results are presented for speech recognition on actual broadcast news data. Finally we discuss some active areas of research relevant to the multimedia information acquisition and retrieval problem. 1.1 An Overview of the Informedia Digital Video Library Project Vast digital libraries of information will soon become available on the World Wide Web as a result of emerging multimedia computing technologies. However, it is not enough simply to store and play back information as many commercial video-on-demand services apparently intend to do. New technology is needed to organize and search these vast data collections, retrieve the most relevant selections, and permit the to be effectively reused. Through the integration of technologies from the fields of natural language understanding, image processing, speech recognition and video compression, the Informedia project [Christel-94a] allows a user to explore multimedia data in depth as well as in breadth. The Informedia digital video library project goes far beyond the current paradigm of video-on-demand, where a user can Intelligent Multimedia Information Retrieval, Mark T. Maybury, Ed.. AAAI Press, pps. 213-239, 1997. 2 Informedia: News-on-Demand — Multimedia Information Acquisition and Retrieval select one video from a limited set and view that video after a delay of a perhaps a few minutes. The computer adds no substantial benefit to this video-on-demand model over a VCR with each video on a tape; the user remains a passive observer of someone else’s produced material. By contrast, the Informedia Project segments hours of video into logical pieces and indexes these pieces according to their raw content (dialog, images, narration). The users can actively explore the information by finding sections of content relevant to their search, rather than by following someone else’s path through the material (as one does when using the current generation of educational CD-ROMs) or by viewing a large chunk of pre-produced material (as with video on demand). Through the active, dynamic exploration supported by a deep, rich library and the indexing and retrieval capabilities of the computer, the user is more motivated and may learn more from the data set. Using such a library, a large body of video material can be searched with very little effort. Users are able to explore Informedia libraries through an interface that allows them to search using typed or spoken natural language queries, to select relevant documents retrieved from the library and to play or display the material on their PC workstations. The library retrieval system can effectively process natural spoken queries and deliver relevant video data in small video paragraphs, based on information associated with the video during library creation. Video and other data may be explored in depth for related content. During retrieval based on keyword searches by a user, only the query-relevant video segments are displayed. The Informedia project is developing new technologies and embedding them in a video library system primarily for use in education and training. The Informedia project will establish an on-line digital video library consisting of over 1000 hours of video material. In order to be able to process this volume of data, practical, effective and efficient tools are essential. In the United States, schools and industry together spend between $400 and $600 billion per year on education and training, an activity that is 93% labor-intensive, with little change in teacher productivity ratios since the 1800s. The new digital video library technology will bring about a revolutionary improvement in the way education and training are delivered and received. The initial Informedia test-bed system has been installed in a K-12 school, where students use the Informedia System to explore multimedia data for educational purposes. We plan to extend this test-bed to other Pittsburgh schools. During library creation for the test-bed, video material obtained from our Informedia Project Partners such as WQED/Pittsburgh and the British Open University is used. Our project plan calls for four test-bed installations with users ranging from grade school children to university faculty. In addition, we will provide networked access to the primary test bed, and export portions of the system and data to other sites for their local exploration and experimentation. The user tests will be conducted at Carnegie Mellon University, the Winchester Thurston School in Pittsburgh, the Fairfax County (VA.) public school system, and with the Open University in the UK. Users will be of many different types, as we test the practicality of the concept of multimedia library search and the usability of the user interface for various age and interest groups. Universal access to large amounts of low-cost digital information and entertainment will significantly affect the conduct of business, professional, and personal activity. The initial impact of the Informedia project’s activity will be by enabling broad accessibility and reuse of existing Hauptmann and Witbrock 3 video materials (e.g., documentaries, news, vocational, training) previously generated for public broadcast, public and professional education, and vocational, military and business training. 1.2 The Informedia: News-on-Demand Application One compelling application branch of the Informedia project is the indexing and retrieval of television, radio and text news. The Informedia: News-on-Demand application [Hauptmann95] is an innovative example of indexing and searching broadcast news video and news radio material by its text content. News-on-Demand is a fully-automatic system that monitors TV, radio and text news and allows selective retrieval of news stories based on spoken queries. The user may choose among the retrieved stories and play back the news stories of interest. The system runs on a Pentium PC using MPEG-I video compression. Speech recognition is currently done on a separate platform using the Sphinx-II continuous speech recognition system [CMU-Speech95]. The News-on-Demand application forces us to consider the limits of what can be done automatically and in limited time. Since news events happen daily, it is not feasible to process, segment and label news through manual or “human-assisted” methods. Immediate availability of the library information is important, as is continuous updating of the contents. 4 Informedia: News-on-Demand — Multimedia Information Acquisition and Retrieval Digital Compression Text Library Creation
