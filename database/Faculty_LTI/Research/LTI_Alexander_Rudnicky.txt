Alexander Rudnicky
Paper count: 264
- Koichiro Yoshino, Yun-Nung (Vivian) Chen, Paul A. Crook, Satwik Kottur, Jinchao Li, Behnam Hedayatnia, Seungwhan Moon, Zhengcong Fei, Zekang Li, Jinchao Zhang, Yang Feng, Jie Zhou, Seokhwan Kim, Yang Liu, Di Jin, A. Papangelis, Karthik Gopalakrishnan, Dilek Z. Hakkani-Tür, B. Damavandi, A. Geramifard, Chiori Hori, Ankit Shah, Chen Zhang, Haizhou Li, João Sedoc, L. F. D’Haro, Rafael E. Banchs, Alexander I. Rudnicky. 2024. Overview of the Tenth Dialog System Technology Challenge: DSTC10. Abstract: This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.
- Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky. 2023. Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. Abstract: In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.
- Ta-Chung Chi, Alexander I. Rudnicky. 2023. Structured Dialogue Discourse Parsing. Abstract: Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse links and corresponding relations. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model’s robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).
- Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky. 2023. A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech. Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.
- Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky. 2023. Tartan: an LLM Driven SocialBot. Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users
- Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, L. F. D’Haro, Alexander I. Rudnicky. 2023. Overview of Robust and Multilingual Automatic Evaluation Metrics

for Open-Domain Dialogue Systems at DSTC 11 Track 4. Abstract: The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics’ correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.
- Diogo Tavares, David Semedo, Alexander I. Rudnicky, João Magalhães. 2023. Learning to Ask Questions for Zero-shot Dialogue State Tracking. Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.
- Ta-Chung Chi, Ting-Han Fan, Alexander I. Rudnicky. 2022. Receptive Field Alignment Enables Transformer Length Extrapolation. Abstract: Length extrapolation is a desirable property that permits training a transformer language model on short sequences and retaining similar perplexities when the model is tested on substantially longer sequences. A relative positional embedding mechanism applied on the transformer self-attention matrix, ALiBi, demonstrates the length extrapolation property with the widest usage to date. In this paper, we show that ALiBi surprisingly does not utilize tokens further than the training sequence length, which can be explained by its implicit windowed attention effect that aligns the receptive ﬁeld during training and testing stages. Inspired by ALiBi and the receptive ﬁled alignment hypothesis, we propose another transformer positional embedding design named Sandwich that uses longer than training sequence length information, and it is a greatly simpliﬁed formulation of the earliest proposed Sinusoidal positional embedding. Finally, we show that both ALiBi and Sandwich enable efﬁcient inference thanks to their implicit windowed attention effect.
- Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky, P. Ramadge. 2022. Training Discrete Deep Generative Models via Gapped Straight-Through Estimator. Abstract: While deep generative models have succeeded in image processing, natural language processing, and reinforcement learning, training that involves discrete random variables remains challenging due to the high variance of its gradient estimation process. Monte Carlo is a common solution used in most variance reduction approaches. However, this involves time-consuming resampling and multiple function evaluations. We propose a Gapped Straight-Through (GST) estimator to reduce the variance without incurring resampling overhead. This estimator is inspired by the essential properties of Straight-Through Gumbel-Softmax. We determine these properties and show via an ablation study that they are essential. Experiments demonstrate that the proposed GST estimator enjoys better performance compared to strong baselines on two discrete deep generative modeling tasks, MNIST-VAE and ListOps.
- Vinayshekhar Bannihatti Kumar, Vaibhav Kumar, Mukul Bhutani, Alexander I. Rudnicky. 2022. An Empirical study to understand the Compositional Prowess of Neural Dialog Models. Abstract: In this work, we examine the problems associated with neural dialog models under the common theme of compositionality. Specifically, we investigate three manifestations of compositionality: (1) Productivity, (2) Substitutivity, and (3) Systematicity. These manifestations shed light on the generalization, syntactic robustness, and semantic capabilities of neural dialog models. We design probing experiments by perturbing the training data to study the above phenomenon. We make informative observations based on automated metrics and hope that this work increases research interest in understanding the capacity of these models.
- Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky. 2022. A Unified One-Shot Prosody and Speaker Conversion System with Self-Supervised Discrete Speech Units. Abstract: We present a unified system to realize one-shot voice conversion (VC) on the pitch, rhythm, and speaker attributes. Existing works generally ignore the correlation between prosody and language content, leading to the degradation of naturalness in converted speech. Additionally, the lack of proper language features prevents these systems from accurately preserving language content after conversion. To address these issues, we devise a cascaded modular system leveraging self-supervised discrete speech units as language representation. These discrete units provide duration information essential for rhythm modeling. Our system first extracts utterance-level prosody and speaker representations from the raw waveform. Given the prosody representation, a prosody predictor estimates pitch, energy, and duration for each discrete unit in the utterance. A synthesizer further reconstructs speech based on the predicted prosody, speaker representation, and discrete units. Experiments show that our system outperforms previous approaches in naturalness, intelligibility, speaker transferability, and prosody transferability. Code and samples are publicly available.1
- Ta-Chung Chi, Ting-Han Fan, P. Ramadge, Alexander I. Rudnicky. 2022. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. Abstract: Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at~\url{https://github.com/chijames/KERPLE.git}.
- Li-Wei Chen, Alexander I. Rudnicky. 2021. Exploring Wav2vec 2.0 Fine Tuning for Improved Speech Emotion Recognition. Abstract: While Wav2Vec 2.0 has been proposed for speech recognition (ASR), it can also be used for speech emotion recognition (SER); its performance can be significantly improved using different fine-tuning strategies. Two baseline methods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are first presented. We show that V-FT is able to outperform state-of-the-art models on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy, further improves the performance on SER. We also introduce a novel fine-tuning method termed P-TAPT, which modifies the TAPT objective to learn contextualized emotion representations. Experiments show that P-TAPT performs better than TAPT, especially under low-resource settings. Compared to prior works in this literature, our top-line system achieved a 7.4% absolute improvement in unweighted accuracy (UA) over the state-of-the-art performance on IEMOCAP. Our code is publicly available.1
- Chen Zhang, João Sedoc, L. F. D’Haro, Rafael E. Banchs, Alexander I. Rudnicky. 2021. Automatic Evaluation and Moderation of Open-domain Dialogue Systems. Abstract: The development of Open-Domain Dialogue Systems (ODS)is a trending topic due to the large number of research challenges, large societal and business impact, and advances in the underlying technology. However, the development of these kinds of systems requires two important characteristics:1) automatic evaluation mechanisms that show high correlations with human judgements across multiple dialogue evaluation aspects (with explainable features for providing constructive and explicit feedback on the quality of generative models' responses for quick development and deployment)and 2) mechanisms that can help to control chatbot responses,while avoiding toxicity and employing intelligent ways to handle toxic user comments and keeping interaction flow and engagement. This track at the 10th Dialogue System Technology Challenge (DSTC10) is part of the ongoing effort to promote scalable and toxic-free ODS. This paper describes the datasets and baselines provided to participants, as well as submission evaluation results for each of the two proposed subtasks.
- Takashi Maekaku, Xuankai Chang, Yuya Fujita, Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky. 2021. Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021. Abstract: We present a system for the Zero Resource Speech Challenge 2021, which combines a Contrastive Predictive Coding (CPC) with deep cluster. In deep cluster, we first prepare pseudo-labels obtained by clustering the outputs of a CPC network with k-means. Then, we train an additional autoregressive model to classify the previously obtained pseudo-labels in a supervised manner. Phoneme discriminative representation is achieved by executing the second-round clustering with the outputs of the final layer of the autoregressive model. We show that replacing a Transformer layer with a Conformer layer leads to a further gain in a lexical metric. Experimental results show that a relative improvement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3% in a syntactic metric are achieved compared to a baseline method of CPC-small which is trained on LibriSpeech 460h data. We achieve top results in this challenge with the syntactic metric.
- Ta-Chung Chi, Alexander I. Rudnicky. 2021. Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection. Abstract: Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all reply-to links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a zero-shot dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10% of the data, we achieve nearly the same performance of using the full dataset.
- Li-Wei Chen, Alexander I. Rudnicky. 2021. Fine-Grained Style Control In Transformer-Based Text-To-Speech Synthesis. Abstract: In this paper, we present a novel architecture to realize fine-grained style control on the transformer-based text-to-speech synthesis (TransformerTTS). Specifically, we model the speaking style by extracting a time sequence of local style tokens (LST) from the reference speech. The existing content encoder in TransformerTTS is then replaced by our designed cross-attention blocks for fusion and alignment between content and style. As the fusion is performed along with the skip connection, our cross-attention block provides a good inductive bias to gradually infuse the phoneme representation with a given style. Additionally, we prevent the style embedding from encoding linguistic content by randomly truncating LST during training and using wav2vec 2.0 features. Experiments show that with fine-grained style control, our system performs better in terms of naturalness, intelligibility, and style transferability. Our code and samples are publicly available.1
- Yangyang Xia, Li-Wei Chen, Alexander I. Rudnicky, R. Stern. 2021. Temporal Context in Speech Emotion Recognition. Abstract: We investigate the importance of temporal context for speech emotion recognition (SER). Two SER systems trained on traditional and learned features, respectively, are developed to predict categorical labels of emotion. For traditional acoustical features, we study the combination of ﬁlterbank features and prosodic features and the impact on SER when the temporal context of these features is expanded by learnable spectro-temporal receptive ﬁelds (STRFs). Experiments show that the system trained on learnable STRFs outperforms other reported systems evalu-ated with a similar setup. We also demonstrate that the wav2vec features, pretrained with long temporal context, are superior to traditional features. We then introduce a novel segment-based learning objective to constrain our classiﬁer to extract local emotion features from the large temporal context. Combined with the learning objective and ﬁne-tuning strategy, our top-line system using wav2vec features reaches state-of-the-art performance on the IEMOCAP dataset.
- R. Sousa, Pedro Ferreira, Pedro Costa, Pedro Azevedo, J. Costeira, Carlos Santiago, João Magalhães, David Semedo, Rafael Ferreira, Alexander I. Rudnicky, Alexander Hauptmann. 2021. iFetch: Multimodal Conversational Agents for the Online Fashion Marketplace. Abstract: Most of the interaction between large organizations and their users will be mediated by AI agents in the near future. This perception is becoming undisputed as online shopping dominates entire market segments, and the new "digitally-native" generations become consumers. iFetch is a new generation of task-oriented conversational agents that interact with users seamlessly using verbal and visual information. Through the conversation, iFetch provides targeted advice and a "physical store-like" experience while maintaining user engagement. This context entails the following vital components: 1) highly complex memory models that keep track of the conversation, 2) extraction of key semantic features from language and images that reveal user intent, 3) generation of multimodal responses that will keep users engaged in the conversation and 4) an interrelated knowledge base of products from which to extract relevant product lists.
- R. Cole, Alexander I. Rudnicky, V. Zue, B. Scott. 2020. Perception and Production of Fluent Speech (Psychology Library Editions: Cognitive Science Book 4) pdf. Abstract: book (electronic).... an international music psychology conference across four locations part of the Asia-Pacific Society for the Cognitive Sciences of Music (APSCOM).. musical and speech sounds, convey meaning and foster interaction in... perception and production. Implications for educational practice of the science of learning Killexams Preparation Pack contains Real IBM 000-M77 Questions and Answers is to achieve a highly reliable speech and visual recognition as required for a valid Maybe extra cognitive assessment is to test for fluency in hausa These are the. who spend most of their time in searching relevant content in the books. Baars: A Cognitive Theory of Consciousness Perception and Production of Fluent Speech CRC Press Book. Series: Psychology Library Editions: Cognitive Science. Routledge Published June 27, 2016 Implications for educational practice of the science of learning PSYCHOLOGY LIBRARY EDITIONS: COGNITIVE SCIENCE. Volume 4.. psychologists), we turn to the final section of the book, Production o f Fluent. Speech. Becoming Fluent: How Cognitive Science Can Help Adults Explicit instruction in social, emotional, and cognitive skills, such as. Another set of tubs includes books labeled by type, all connected to current. Students need a sense of physical and psychological safety for... The National Academy of Sciences is currently producing a second edition of How People Wong Kk Nyu webdesign-kiel-hamburg.de In psychology and cognitive sciences, social perception is the process of acquiring, The author wrote this book to help students organize their thinking about. Skinner, These terms are matched to Myers 8th edition of Psychology chapter 4. Communication accommodation theory Extension of speech accommodation Research in Education This book is gratefully dedicated to the pioneers in cognitive science, who made it. In cognitive psychology, conscious and unconscious events have the same status. consciousness as the "glue" for combining different perceptual features,... word perception, imagery, memory retrieval, speech production, and the like Art of problem solving youtube In spite of scientific consensus about the overall structure of speech present some of these models along with evidence from experimental psychology. In the section treating speech production, the reader is introduced to the Although we perceive speech sounds as phonemes, it is wrong to assume.. Third Edition. Essentials of a Theory of Language Cognition ELLIS 2019 ... Light Dark Fantasy A Grayscale Coloring Book Collection With Beautiful Women Diary And Experience Sampling Research Methodology In The Social Sciences KALLIS SAT Writing And Language Pattern Workbook Study Guide For The Dewey Decimal Classification Edition 22 First North American Edition Library Ftb interactions stuttering Technology Exchange 4 Other Languages The eSpeak speech synthesizer does text to speech for Speech Production: Models, Phonetic Processes and Techniques brings phonetics, physiology, psychology all with a special interest in how speech is produced. a principal investigator in the Communicative and Cognitive Sciences
- Tzu-Hsiang Lin, Alexander I. Rudnicky, Trung Bui, Doo Soon Kim, Jean Oh. 2020. Adjusting Image Attributes of Localized Regions with Low-level Dialogue. Abstract: Natural Language Image Editing (NLIE) aims to use natural language instructions to edit images. Since novices are inexperienced with image editing techniques, their instructions are often ambiguous and contain high-level abstractions which require complex editing steps. Motivated by this inexperience aspect, we aim to smooth the learning curve by teaching the novices to edit images using low-level command terminologies. Towards this end, we develop a task-oriented dialogue system to investigate low-level instructions for NLIE. Our system grounds language on the level of edit operations, and suggests options for users to choose from. Though compelled to express in low-level terms, user evaluation shows that 25% of users found our system easy-to-use, resonating with our motivation. Analysis shows that users generally adapt to utilizing the proposed low-level language interface. We also identified object segmentation as the key factor to user satisfaction. Our work demonstrates advantages of low-level, direct language-action mapping approach that can be applied to other problem domains beyond image editing such as audio editing or industrial design.
- M. Marge, Alexander I. Rudnicky. 2019. Miscommunication Detection and Recovery in Situated Human–Robot Dialogue. Abstract: Even without speech recognition errors, robots may face difficulties interpreting natural-language instructions. We present a method for robustly handling miscommunication between people and robots in task-oriented spoken dialogue. This capability is implemented in TeamTalk, a conversational interface to robots that supports detection and recovery from the situated grounding problems of referential ambiguity and impossible actions. We introduce a representation that detects these problems and a nearest-neighbor learning algorithm that selects recovery strategies for a virtual robot. When the robot encounters a grounding problem, it looks back on its interaction history to consider how it resolved similar situations. The learning method is trained initially on crowdsourced data but is then supplemented by interactions from a longitudinal user study in which six participants performed navigation tasks with the robot. We compare results collected using a general model to user-specific models and find that user-specific models perform best on measures of dialogue efficiency, while the general model yields the highest agreement with human judges. Our overall contribution is a novel approach to detecting and recovering from miscommunication in dialogue by including situated context, namely, information from a robot’s path planner and surroundings.
- George Larionov, Zachary Kaden, Hima Varsha Dureddy, G. B. Kalejaiye, Mihir Kale, Srividya Pranavi Potharaju, Ankit Shah, Alexander I. Rudnicky. 2018. Tartan: A retrieval-based socialbot powered by a dynamic finite-state machine architecture. Abstract: This paper describes the Tartan conversational agent built for the 2018 Alexa Prize Competition. Tartan is a non-goal-oriented socialbot focused around providing users with an engaging and fluent casual conversation. Tartan's key features include an emphasis on structured conversation based on flexible finite-state models and an approach focused on understanding and using conversational acts. To provide engaging conversations, Tartan blends script-like yet dynamic responses with data-based generative and retrieval models. Unique to Tartan is that our dialog manager is modeled as a dynamic Finite State Machine. To our knowledge, no other conversational agent implementation has followed this specific structure.
- V. Logacheva, M. Burtsev, Valentin Malykh, V. Poluliakh, Alexander I. Rudnicky, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, A. Black, Yoshua Bengio. 2018. A Dataset of Topic-Oriented Human-to-Chatbot Dialogues. Abstract: This document contains the description of dataset collected during the first round of Conversational Intelligence Challenge (ConvAI) which took place in July 2017. During this evaluation round we collected over 2,500 dialogues from 10 chatbots and 500 volunteers. Here we provide the analysis of dataset statistics and outline some possible improvements for future data collection experiments.
- Ryu Takeda, Kazunori Komatani, Alexander I. Rudnicky. 2018. Word Segmentation From Phoneme Sequences Based On Pitman-Yor Semi-Markov Model Exploiting Subword Information. Abstract: Word segmentation from phoneme sequences is essential to identify unknown words -of-vocabulary; OOV) in spoken dialogues. The Pitman-Yor semi-Markov model (PYSMM) is used for word segmentation that handles dynamic increase in vocabularies. The obtained vocabularies, however, still include meaningless entries due to insufficient cues for phoneme sequences. We focus here on using subword information to capture patterns as “words.” We propose 1) a model based on subword N-gram and subword estimation using a vocabulary set, and 2) posterior fusion of the results of a PYSMM and our model to take advantage of both. Our experiments showed 1) the potential of using subword information for OOV acquisition, and 2) that our method outperformed the PYSMM by 1.53 and 1.07 in terms of the F-measure of the obtained OOV set for English and Japanese corpora, respectively.
- Huiting Liu, Tao Lin, Hanfei Sun, Weijian Lin, C. Chang, Teng Zhong, Alexander I. Rudnicky. 2017. RubyStar: A Non-Task-Oriented Mixture Model Dialog System. Abstract: RubyStar is a dialog system designed to create "human-like" conversation by combining different response generation strategies. RubyStar conducts a non-task-oriented conversation on general topics by using an ensemble of rule-based, retrieval-based and generative methods. Topic detection, engagement monitoring, and context tracking are used for managing interaction. Predictable elements of conversation, such as the bot's backstory and simple question answering are handled by separate modules. We describe a rating scheme we developed for evaluating response generation. We find that character-level RNN is an effective generation model for general responses, with proper parameter settings; however other kinds of conversation topics might benefit from using other models.
- Arjun Bhardwaj, Alexander I. Rudnicky. 2017. User Intent Classification using Memory Networks: A Comparative Analysis for a Limited Data Scenario. Abstract: In this report, we provide a comparative analysis of different techniques for user intent classification towards the task of app recommendation. We analyse the performance of different models and architectures for multi-label classification over a dataset with a relative large number of classes and only a handful examples of each class. We focus, in particular, on memory network architectures, and compare how well the different versions perform under the task constraints. Since the classifier is meant to serve as a module in a practical dialog system, it needs to be able to work with limited training data and incorporate new data on the fly. We devise a 1-shot learning task to test the models under the above constraint. We conclude that relatively simple versions of memory networks perform better than other approaches. Although, for tasks with very limited data, simple non-parametric methods perform comparably, without needing the extra training data.
- Toby Jia-Jun Li, I. Labutov, B. Myers, A. Azaria, Alexander I. Rudnicky, Tom Michael Mitchell. 2017. An End User Development Approach for Failure Handling in Goal-oriented Conversational Agents. Abstract: This chapter introduces an end user development (EUD) approach for handling common types of failures encountered by goal-oriented conversational agents. We start with identifying three common sources of failures in human-agent conversations: unknown concepts , out-of-domain tasks and wrong fulﬁllment means or level of generalization in task execution . To handle these failures, it is useful to enable the end user to program the agent and to “teach” the agent what to do as a fallback strategy. Showing examples for this approach, we walk through our two integrated systems: S UGILITE and L IA . S UGILITE uses the programming by demonstration (PBD) technique, allowing the user to program the agent by demonstrating new tasks or new means for completing a task using the GUIs of third-party smart-phone apps, while L IA learns new tasks from verbal instructions, enabling the user to teach the agent through breaking down the procedure verbally. L IA also supports the user to verbally deﬁne unknown concepts used in the commands and adds those concepts into the agent’s ontology. Both S UGILITE and L IA can generalize what they have learned from the user across related entities and perform a task with new parameters in a different context.
- Zhou Yu, Alexander I. Rudnicky, A. Black. 2017. Learning Conversational Systems that Interleave Task and Non-Task Content. Abstract: Task-oriented dialog systems have been applied in various tasks, such as automated personal assistants, customer service providers and tutors. These systems work well when users have clear and explicit intentions that are well-aligned to the systems' capabilities. However, they fail if users intentions are not explicit. To address this shortcoming, we propose a framework to interleave non-task content (i.e. everyday social conversation) into task conversations. When the task content fails, the system can still keep the user engaged with the non-task content. We trained a policy using reinforcement learning algorithms to promote long-turn conversation coherence and consistency, so that the system can have smooth transitions between task and non-task content. To test the effectiveness of the proposed framework, we developed a movie promotion dialog system. Experiments with human users indicate that a system that interleaves social and task content achieves a better task success rate and is also rated as more engaging compared to a pure task-oriented system.
- Yun-Nung (Vivian) Chen, Ming Sun, Alexander I. Rudnicky, A. Gershman. 2016. Unsupervised user intent modeling by feature-enriched matrix factorization. Abstract: Spoken language interfaces are being incorporated into various devices such as smart phones and TVs. However, dialogue systems may fail to respond correctly when users' request functionality is not supported by currently installed apps. This paper proposes a feature-enriched matrix factorization (MF) approach to model open domain intents, which allows a system to dynamically add unexplored domains according to users' requests. First we leverage the structured knowledge from Wikipedia and Freebase to automatically acquire domain-related semantics to enrich features of input utterances, and then MF is applied to model automatically acquired knowledge, published app textual descriptions and users' spoken requests in a joint fashion; this generates latent feature vectors for utterances and user intents without need of prior annotations. Experiments show that the proposed MF models incorporated with rich features significantly improve intent prediction, achieving about 34% of mean average precision (MAP) for both ASR and manual transcripts.
- R. Banchs, Ryuichiro Higashinaka, W. Minker, J. Mariani, D. Traum, Miroslav Vodolán, Filip Jurcícek, Ondřej Dušek, David Nilsson, Magnus Sahlgren, Jussi Karlgren, Zhou Yu, Ziyu Xu, A. Black, Alexander I. Rudnicky, M. Kaleem, Omar Alobadi, James O’Shea, Keeley A. Crockett, D. Novick, Adriana I. Camacho, I. Gris, Laura M. Rodriguez, B. AbuShawar, E. Atwell, Luis Fernando D'Haro, G. D. Duplessis, Vincent Letard, Anne-Laure Ligozat, S. Rosset, Haizhou Li. 2016. RE-WOCHAT : Workshop on Collecting and Generating Resources for Chatbots and Conversational Agents-Development and Evaluation Workshop Programme ( May 28. Abstract: This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.
- Zhou Yu, Ziyu Xu, A. Black, Alexander I. Rudnicky. 2016. Chatbot Evaluation and Database Expansion via Crowdsourcing. Abstract: Chatbots use a database of responses often culled from a corpus of text generated for a different purpose, for example ﬁlm scripts or interviews. One consequence of this approach is a mismatch between the data and the inputs generated by participants. We describe an approach that while starting from an existing corpus (of interviews) makes use of crowdsourced data to augment the response database, focusing on responses that people judge as inappropriate. The long term goal is to create a data set of more appropriate chat responses; the short term consequence appears to be the identiﬁcation and replacement of particularly inappropriate responses. We found the version with the expanded database was rated signiﬁcantly better in terms of the response level appropriateness and the overall ability to engage users. We also describe strategies we developed that target certain breakdowns discovered during data collection. Both the source code of the chatbot, TickTock, and the data collected are publicly available.
- Ming Sun, Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2016. An Intelligent Assistant for High-Level Task Understanding. Abstract: People are able to interact with domain-specific intelligent assistants (IAs) and get help with tasks. But sometimes user goals are complex and may require interactions with multiple applications. However current IAs are limited to specific applications and users have to directly manage execution spanning multiple applications in order to engage in more complex activities. An ideal personal agent would be able to learn, over time, about tasks that span different resources. This paper addresses the problem of cross-domain task assistance in the context of spoken dialogue systems. We propose approaches to discover users' high-level intentions and using this information to assist users in their task. We collected real-life smartphone usage data from 14 participants and investigated how to extract high-level intents from users' descriptions of their activities. Our experiments show that understanding high-level tasks allows the agent to actively suggest apps relevant to pursuing particular user goals and reduce the cost of users' self-management.
- Zhou Yu, Leah Nicolich-Henkin, A. Black, Alexander I. Rudnicky. 2016. A Wizard-of-Oz Study on A Non-Task-Oriented Dialog Systems That Reacts to User Engagement. Abstract: In this paper, we describe a system that re-acts to both possible system breakdowns and low user engagement with a set of conversational strategies. These general strategies reduce the number of inappropriate responses and produce better user engagement. We also found that a system that reacts to both possible system break-downs and low user engagement is rated by both experts and non-experts as having better overall user engagement compared to a system that only reacts to possible system breakdowns. We argue that for non-task-oriented systems we should optimize on both system response appropriateness and user engagement. We also found that apart from making the system response appropriate, funny and provocative responses can also lead to better user engagement. On the other hand, short appropriate responses, such as “Yes” or “No” can lead to decreased user engagement. We will use these ﬁndings to further improve our system.
- K. Brown, Yael Shmueli-Friedland, M. Lawo, Brion van Over, Alexander I. Rudnicky, A. Degani, D. Dahl, N. Shaked, Ute Winter. 2016. Design of Multimodal Mobile Interfaces. Abstract: The book discusses the new world of mobile multimodality, focusing on innovative technologies and design which create a state-of-the-art user interface. It examines the practical challenges entailed in meeting commercial deployment goals, and offers new approaches to designing such interfaces.
- Ming Sun, Aasish Pappu, Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2016. Messenger } “ arrange lunch ” examples “ schedule meeting ” “ book travel tickets ”. Abstract: Users interact with mobile apps with certain intents such as finding a restaurant. Some intents and their corresponding activities are complex and may involve multiple apps; for example, a restaurant app, a messenger app and a calendar app may be needed to plan a dinner with friends. However, activities may be quite personal and third-party developers would not be building apps to specifically handle complex intents (e.g., a DinnerPlanner). Instead we want our intelligent agent to actively learn to understand these intents and provide assistance when needed. This paper proposes a framework to enable the agent to learn an inventory of intents from a small set of taskoriented user utterances. The experiments show that on previously unseen user activities, the agent is able to reliably recognize user intents using graph-based semi-supervised learning methods. The dataset, models, and the system outputs are available to research community.
- Ming Sun, Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2016. Learning User Intentions Spanning Multiple Domains. Abstract: People are able to interact with domain-specific applications in smart environments and get assistance with specific tasks. Current intelligent agents (IAs) tend to be limited to specific applications. In order to engage in more complex activities users have to directly manage a task that may span multiple applications. An ideal personal IA would be able to learn, over time, about these tasks that span different resources. This paper addresses the problem of multi-domain task assistance in the context of spoken dialog systems. We propose approaches to discover users’ high-level intentions and using this information to assist users in their task. We collected real-life smart phone usage data from 14 participants and investigated how to extract high-level intents from users’ descriptions of their activities. Our experiments show that understanding high-level tasks allows the agent to actively suggest apps relevant to pursuing particular user goals and reduce the cost of users’ self-management. Author
- Alexander I. Rudnicky, Alexander Hauptmann. 2016. Errors , Repetition , and Contrastive Emphasis in Speech Recognition. Abstract: In repeating an utterance for the benefit of a listener, talkers can use contrastive stress to mark those parts of an utterance that were misrecognized. In this study, we found that talkers use a consistent set of markers to indicate contrastive stress: Listeners unaware of the nature of the misrecognition can readily identify a contrastively stressed word. Based on an analysis of these markers, we have implemented an automatic algorithm that identifies contrastive stress with about the same accuracy as humans, using amplitude, duration, silence, and pitch cues. This ability to detect contrastive stress can be effectively exploited, as part of error recovery strategies, by a recognition system.
- Ming Sun, Yun-Nung (Vivian) Chen, Zhenhao Hua, Yulian Tamres-Rudnicky, Arnab Dash, Alexander I. Rudnicky. 2016. AppDialogue: Multi-App Dialogues for Intelligent Assistants. Abstract: Users will interact with an individual app on smart devices (e.g., phone, TV, car) to fulfill a specific goal (e.g. find a photographer), but users may also pursue more complex tasks that will span multiple domains and apps (e.g. plan a wedding ceremony). Planning and executing such multi-app tasks are typically managed by users, considering the required global context awareness. To investigate how users arrange domains/apps to fulfill complex tasks in their daily life, we conducted a user study on 14 participants to collect such data from their Android smart phones. This document 1) summarizes the techniques used in the data collection and 2) provides a brief statistical description of the data. This data guilds the future direction for researchers in the fields of conversational agent and personal assistant, etc. This data is available at http://AppDialogue.com.
- Zhou Yu, Ziyu Xu, A. Black, Alexander I. Rudnicky. 2016. Strategy and Policy Learning for Non-Task-Oriented Conversational Systems. Abstract: We propose a set of generic conversational strategies to handle possible sys-tem breakdowns in non-task-oriented dialog systems. We also design dialog policies to select among these strategies with respect to different dialog contexts. We combine expert knowledge and the statistical ﬁndings that derived from previous collected data in designing these policies. The dialog policy learned via reinforcement learning outperforms the random selection policy and the locally greedy policy in both the simulated and the real-world settings. In addition, we propose three metrics, which consider both the lo-cal and global quality of the conversation, to evaluate conversation quality.
- Ming Sun, Aasish Pappu, Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2016. Weakly supervised user intent detection for multi-domain dialogues. Abstract: Users interact with mobile apps with certain intents such as finding a restaurant. Some intents and their corresponding activities are complex and may involve multiple apps; for example, a restaurant app, a messenger app and a calendar app may be needed to plan a dinner with friends. However, activities may be quite personal and third-party developers would not be building apps to specifically handle complex intents (e.g., a DinnerPlanner). Instead we want our intelligent agent to actively learn to understand these intents and provide assistance when needed. This paper proposes a framework to enable the agent to learn an inventory of intents from a small set of task-oriented user utterances. The experiments show that on previously unseen user activities, the agent is able to reliably recognize user intents using graph-based semi-supervised learning methods. The dataset, models, and the system outputs are available to research community.
- Ming Sun, Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2015. Learning OOV through semantic relatedness in spoken dialog systems. Abstract: • Speech recognition and language understanding performance can be improved through an OOV expectand-learn procedure. • A limited domain vocabulary can be utilized to effectively acquire OOVs by the word relatedness theory through web knowledge bases. • With data-driven semantic relatedness, both the global and local learning procedures are able to successfully harvest more than 50% of OOVs, leading to better recognition and understanding performance. • This work demonstrates that o OOV learning may benefit dialog system o the proposed expect-and-learn strategy outperforms the traditional detect-and-learn in both higher effectiveness and no human involvement. 1. Linguistically semantic relatedness o Defined by linguistics, e.g., WordNet (WN), Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) 2. Data-driven semantic relatedness o Distributional semantics, e.g., continuous bag-ofword embeddings (CBOW) (Mikolov et al., 2013)  Detect-and-Learn (Qin et al., 2011; 2012): o Discover OOV words during the conversation o Example: S: “I heard something like SELF, can you repeat it?” U: “It’s SELFIE.” o Drawbacks • Limited number of new words • Required human efforts to correct spellings and pronunciations  Expect-and-Learn (proposed): o Use semantic relatedness to automatically enrich the vocabulary and language model beforehand
- A. Chotimongkol, Alexander I. Rudnicky, R. Frederking, Roni Rosenfeld, Rong Zhang. 2015. Improving Speech Recognizer Performance in a Dialog System Using N-best Hypotheses Reranking. Abstract: This thesis investigates N-best hypotheses reranking techniques for improving speech recognition accuracy. We have focused on improving the accuracy of a speech recognizer used in a dialog system. Our post-processing approach uses a linear regression model to predict the error rate of each hypothesis from hypothesis features, and then outputs the one that has the lowest (recomputed) error rate. We investigated 15 different features sampled from 3 components of a dialog system: a decoder, a parser and a dialog manager. These features are speech recognizer score, acoustic model score, language model score, N-best word rate, N-best homogeneity with speech recognizer score, N-best homogeneity with language model score, N-best homogeneity with acoustic model score, unparsed words, gap number, fragmentation transitions, highest-in-coverage, slot bigram, conditional slot, expected slots and conditional slot bigram. We also used a linear rescaling with clipping technique to normalize feature values to deal with differences in order of magnitude. A searching strategy was used to discover the optimal feature set for reordering; three search algorithms were examined: stepwise regression, greedy search and brute force search. To improve reranking accuracy and reduce computation we examined techniques for selecting utterances likely to benefit from reranking then applying reranking only to utterances so identified. Besides the conventional performance metric, word error rate, we also proposed concept error rate as an alternative metric. An experiment with human subjects revealed that concept error rate is the metric that better conforms to the criteria used by humans when they evaluated hypotheses quality. The reranking model, that performed the best, combined 6 features together to predict error rate. These 6 features are speech recognizer score, language model score, acoustic model score, slot bigram, N-best homogeneity with speech recognizer score and N-best word rate. This optimal set of features was obtained using greedy search. This model can improve the word error rate significantly beyond the speech recognizer baseline. The reranked word error rate is 11.14%, which is a 2.71% relative improvement from the baseline. The reranked concept error rate is 9.68%, which is a 1.22% relative improvement from the baseline. Adding an utterance selection module into a reranking process did not improve the reranking performance beyond the number achieved by reranking every utterance. However, some selection criteria achieved the same overall error rate by reranking just a small number (8.37%) of the utterances. When comparing the performance of the proposed reranking technique to the performance of a human on the same reranking task, the proposed method did as well as a native speaker, suggesting that an automatic reordering process is quite competitive.
- Zhou Yu, A. Papangelis, Alexander I. Rudnicky. 2015. TickTock: A Non-Goal-Oriented Multimodal Dialog System with Engagement Awareness. Abstract: We describe TickTock, a conversational agent designed to engage humans on topics of its choosing and to carry on an interaction for as long as possible. Our prototype uses a database of talk show transcripts featuring guests from the film industry. To be an interesting companion Tick Tock uses immediate context from the last two turns to formulate queries into a database of utterances. The process is automatic. TickTock monitors user engagement and performs certain moves, such as topic shifts, based on its assessment of user state. Initially we used utterance content for monitoring and subsequently we begun to investigate non-language cues, such as prosody and visual cues to create a more robust engagement model based on multiple human communication channels.
- Yun-Nung (Vivian) Chen, William Yang Wang, Alexander I. Rudnicky. 2015. Learning semantic hierarchy with distributed representations for unsupervised spoken language understanding. Abstract: We study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems, in particular, learning the hierarchical semantic structure from the data. Given unlabelled conversations, we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots (e.g., both slots “direction” and “locale” convey location-related information) for building a coherent semantic hierarchy, and then estimate the slot importance at different levels. The high-level semantic estimation involves not only within-slot but also crossslot relations. The experiments show that high-level semantic information can accurately estimate the prominence of slots, significantly improving the slot induction performance; furthermore, a semantic decoder trained on the data with automatically extracted slots achieves about 68% F-measure, which is close to the one from hand-crafted grammars.
- Yun-Nung (Vivian) Chen, William Yang Wang, Alexander I. Rudnicky. 2015. Jointly Modeling Inter-Slot Relations by Random Walk on Knowledge Graphs for Unsupervised Spoken Language Understanding. Abstract: A key challenge of designing coherent semantic ontology for spoken language understanding is to consider inter-slot relations. In practice, however, it is difficult for domain experts and professional annotators to define a coherent slot set, while considering various lexical, syntactic, and semantic dependencies. In this paper, we exploit the typed syntactic dependency theory for unsupervised induction and filling of semantics slots in spoken dialogue systems. More specifically, we build two knowledge graphs: a slot-based semantic graph, and a word-based lexical graph. To jointly consider word-to-word, word-toslot, and slot-to-slot relations, we use a random walk inference algorithm to combine the two knowledge graphs, guided by dependency grammars. The experiments show that considering inter-slot relations is crucial for generating a more coherent and compete slot set, resulting in a better spoken language understanding model, while enhancing the interpretability of semantic slots.
- M. Marge, Alexander I. Rudnicky. 2015. Miscommunication Recovery in Physically Situated Dialogue. Abstract: We describe an empirical study that crowdsourced human-authored recovery strategies for various problems encountered in physically situated dialogue. The purpose was to investigate the strategies that people use in response to requests that are referentially ambiguous or impossible to execute. Results suggest a general preference for including specific kinds of visual information when disambiguating referents, and for volunteering alternative plans when the original instruction was not possible to carry out.
- Ming Sun, Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2015. Understanding User ’ s Cross-Domain Intentions in Spoken Dialog Systems. Abstract: Spoken language based intelligent assistants (IAs) have been developed for a number of domains but their functionality has mostly been confined to the scope of a given app. One reason is that it’s is difficult for IAs to infer a user’s intent without access to relevant context and unless explicitly implemented, context is not available across app boundaries. We describe context-aware multi-app dialog systems that can learn to 1) identify meaningful user intents; 2) produce natural language representation for the semantics of such intents; and 3) predict user intent as they engage in multi-app tasks. As part of our work we collected data from the smartphones of 14 users engaged in real-life multi-app tasks. We found that it is reasonable to group tasks into high-level intentions. Based on the dialog content, IA can generate useful phrases to describe the intention. We also found that, with readily available contexts, IAs can effectively predict user’s intents during conversation, with accuracy at 58.9%.
- J. Chiu, Yajie Miao, A. Black, Alexander I. Rudnicky. 2015. Distributed representation-based spoken word sense induction. Abstract: Spoken Term Detection (STD) or Keyword Search (KWS) techniques can locate keyword instances but do not differentiate between meanings. Spoken Word Sense Induction (SWSI) differentiates target instances by clustering according to context, providing a more useful result. In this paper we present a fully unsupervised SWSI approach based on distributed representations of spoken utterances. We compare this approach to several others, including the state-of-the-art Hierarchical Dirichlet Process (HDP). To determine how ASR performance affects SWSI, we used three different levels of Word Error Rate (WER), 40%, 20% and 0%; 40% WER is representative of online video, 0% of text. We show that the distributed representation approach outperforms all other approaches, regardless of the WER. Although LDA-based approaches do well on clean data, they degrade significantly with WER. Paradoxically, lower WER does not guarantee better SWSI performance, due to the influence of common locutions.
- Michelene Kalinyak-Fliszar, N. Martin, E. Keshner, Alexander I. Rudnicky, Justin Y. Shi, G. Teodoro. 2015. Using Virtual Technology to Promote Functional Communication in Aphasia: Preliminary Evidence From Interactive Dialogues With Human and Virtual Clinicians.. Abstract: PURPOSE
We investigated the feasibility of using a virtual clinician (VC) to promote functional communication abilities of persons with aphasia (PWAs). We aimed to determine whether the quantity and quality of verbal output in dialogues with a VC would be the same or greater than those with a human clinician (HC).


METHOD
Four PWAs practiced dialogues for 2 sessions each with a HC and VC. Dialogues from before and after practice were transcribed and analyzed for content. We compared measures taken before and after practice in the VC and HC conditions.


RESULTS
Results were mixed. Participants either produced more verbal output with the VC or showed no difference on this measure between the VC and HC conditions. Participants also showed some improvement in postpractice narratives.


CONCLUSION
Results provide support for the feasibility and applicability of virtual technology to real-life communication contexts to improve functional communication in PWAs.
- Yun-Nung (Vivian) Chen, William Yang Wang, A. Gershman, Alexander I. Rudnicky. 2015. Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding. Abstract: Spoken dialogue systems (SDS) typically require a predefined semantic ontology to train a spoken language understanding (SLU) module. In addition to the annotation cost, a key challenge for designing such an ontology is to define a coherent slot set while considering their complex relations. This paper introduces a novel matrix factorization (MF) approach to learn latent feature vectors for utterances and semantic elements without the need of corpus annotations. Specifically, our model learns the semantic slots for a domain-specific SDS in an unsupervised fashion, and carries out semantic parsing using latent MF techniques. To further consider the global semantic structure, such as inter-word and inter-slot relations, we augment the latent MF-based model with a knowledge graph propagation model based on a slot-based semantic graph and a word-based lexical graph. Our experiments show that the proposed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner.
- Yun-Nung, Ming Sun, Alexander I. Rudnicky, A. Gershman. 2015. Matrix Factorization with Domain Knowledge and Behavioral Patterns for Intent Modeling. Abstract: Spoken language interfaces are being incorporated into various devices such as smart-phones and TVs. However, dialog systems will fail to respond correctly when users request functionality not supported by currently installed apps. We propose a feature-enriched matrix factorization (MF) approach to model open domain intents that allow a system to dynamically add app-relevant domains according to users’ requests. We use MF to jointly model published app descriptions and users’ spoken requests; this generates latent feature vectors for utterances and user intents without need for prior annotation. The matrix can further incorporate user behavioral patterns found in their activity logs to learn user-specific intent prediction models. We show that the MF models enriched with multimodality significantly improve the intent prediction, achieving 34% and 55% of mean average precision (MAP) for unsupervised single-turn requests and for supervised multi-turn interactions on ASR transcripts respectively.
- Yun-Nung (Vivian) Chen, Ming Sun, Alexander I. Rudnicky, A. Gershman. 2015. Leveraging Behavioral Patterns of Mobile Applications for Personalized Spoken Language Understanding. Abstract: Spoken language interfaces are appearing in various smart devices (e.g. smart-phones, smart-TV, in-car navigating systems) and serve as intelligent assistants (IAs). However, most of them do not consider individual users' behavioral profiles and contexts when modeling user intents. Such behavioral patterns are user-specific and provide useful cues to improve spoken language understanding (SLU). This paper focuses on leveraging the app behavior history to improve spoken dialog systems performance. We developed a matrix factorization approach that models speech and app usage patterns to predict user intents (e.g. launching a specific app). We collected multi-turn interactions in a WoZ scenario; users were asked to reproduce the multi-app tasks that they had performed earlier on their smart-phones. By modeling latent semantics behind lexical and behavioral patterns, the proposed multi-model system achieves about 52% of turn accuracy for intent prediction on ASR transcripts.
- J. Chiu, Yun Wang, J. Trmal, Daniel Povey, Guoguo Chen, Alexander I. Rudnicky. 2014. Combination of FST and CN search in spoken term detection. Abstract: Spoken Term Detection (STD) focuses on finding instances of a particular spoken word or phrase in an audio corpus. Most STD systems have a two-step pipeline, ASR followed by search. Two approaches to search are common, Confusion Network (CN) based search and Finite State Transducer (FST) based search. In this paper, we examine combination of these two different search approaches, using the same ASR output. We find that the CN search performs better on shorter queries, and FST search performs better on longer queries. By combining the different search results from the same ASR decoding, we achieve better performance compared to either search approach on its own. We also find that this improvement is additive to the usual combination of decoder results using different modeling techniques.
- Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2014. Two-Stage Stochastic Natural Language Generation for Email Synthesis by Modeling Sender Style and Topic Structure. Abstract: This paper describes a two-stage process for stochastic generation of email, in which the first stage structures the emails according to sender style and topic structure (high-level generation), and the second stage synthesizes text content based on the particulars of an email element and the goals of a given communication (surface-level realization). Synthesized emails were rated in a preliminary experiment. The results indicate that sender style can be detected. In addition we found that stochastic generation performs better if applied at the word level than at an original-sentence level (“template-based”) in terms of email coherence, sentence fluency, naturalness, and preference.
- Aasish Pappu, Ming Sun, Seshadri Sridharan, Alexander I. Rudnicky. 2014. Conversational Strategies for Robustly Managing Dialog in Public Spaces. Abstract: Open environments present an attention management challenge for conversational systems. We describe a kiosk system (based on Ravenclaw‐Olympus) that uses simple auditory and visual information to interpret human presence and manage the system’s attention. The system robustly differentiates intended interactions from unintended ones at an accuracy of 93% and provides similar task completion rates in both a quiet room and a public space.
- Aasish Pappu, Alexander I. Rudnicky. 2014. Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems. Abstract: Many goal-oriented dialog agents are expected to identify slot-value pairs in a spoken query, then perform lookup in a knowledge base to complete the task. When the agent encounters unknown slotvalues, it may ask the user to repeat or reformulate the query. But a robust agent can proactively seek new knowledge from a user, to help reduce subsequent task failures. In this paper, we propose knowledge acquisition strategies for a dialog agent and show their effectiveness. The acquired knowledge can be shown to subsequently contribute to task completion.
- Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2014. Two-Stage Stochastic Email Synthesizer. Abstract: This paper presents the design and implementation details of an email synthesizer using two-stage stochastic natural language generation, where the first stage structures the emails according to sender style and topic structure, and the second stage synthesizes text content based on the particulars of an email structure element and the goals of a given communication for surface realization. The synthesized emails reflect sender style and the intent of communication, which can be further used as synthetic evidence for developing other applications.
- Longlu Qin, Alexander I. Rudnicky. 2014. Building a vocabulary self-learning speech recognition system. Abstract: This paper presents initial studies on building a vocabulary selflearning speech recognition system that can automatically learn unknown words and expand its recognition vocabulary. Our recognizer can detect and recover out-of-vocabulary (OOV) words in speech, then incorporate OOV words into its lexicon and language model (LM). As a result, these unknown words can be correctly recognized when encountered by the recognizer in future. Specifically, we apply the word-fragment hybrid system framework to detect the presence of OOV words. We propose a better phoneme-to-grapheme (P2G) model so as to correctly recover the written form for more OOV words. Furthermore, we estimate LM scores for OOV words using their syntactic and semantic properties. The experimental results show that more than 40% OOV words are successfully learned from the development data, and about 60% learned OOV words are recognized in the testing data. Index Terms: Vocabulary learning, OOV word detection and recovery, lexicon expansion
- Yun-Nung (Vivian) Chen, Alexander I. Rudnicky. 2014. Dynamically supporting unexplored domains in conversational interactions by enriching semantics with neural word embeddings. Abstract: Spoken language interfaces are being incorporated into various devices (e.g. smart-phones, smart TVs, etc). However, current technology typically limits conversational interactions to a few narrow predefined domains/topics. For example, dialogue systems for smartphone operation fail to respond when users ask for functions not supported by currently installed applications. We propose to dynamically add application-based domains according to users' requests by using descriptions of applications as a retrieval cue to find relevant applications. The approach uses structured knowledge resources (e.g. Freebase, Wikipedia, FrameNet) to induce types of slots for generating semantic seeds, and enriches the semantics of spoken queries with neural word embeddings, where semantically related concepts can be additionally included for acquiring knowledge that does not exist in the predefined domains. The system can then retrieve relevant applications or dynamically suggest users install applications that support unexplored domains. We find that vendor descriptions provide a reliable source of information for this purpose.
- M. Kalinyak-Fliszar, N. Martin, E. Keshner, Alexander I. Rudnicky, Justin Y. Shi, G. Teodoro. 2014. Using Virtual Clinicians to Promote Functional Communication Skills in Aphasia. Abstract: Persons with aphasia (PWA) re-enter their community after their rehabilitation program is ended. Thus it is incumbent on rehabilitation specialists to incorporate training in using residual language skills for functional communication [1]. Evidence indicates that language abilities improve with continued treatment, even during chronic stages of aphasia (refs) For optimal generalization, PWA need to practice language in everyday living situations.

Virtual reality technology is a method of providing home-based therapeutic interventions. A valuable potential of virtual reality technology is that it supports the successful generalization of residual language skills to functional communication situations. Traditionally, role-playing [2] and script training [3] have been used to improve functional communication in PWA. A more recent approach has been the adaptation of scripts through the implementation of virtual technology. [4].

We report progress on a project that aims to develop a virtual clinician that is capable of recognizing a variety of potential responses in the context of functional communication scenarios. Our goal is to develop a virtual clinician-human interaction system that can be used independently by PWA to practice and improve communication skills. This involves development of software that will support a spoken dialog system (SDS) that can interact autonomously with an individual and can be configured to personalize treatment [5].

As use of virtual technology in aphasia rehabilitation increases, questions about the physical and psychosocial factors that influence successful use of residual communication skills need to be resolved. Thus, a second aim of this project, the topic of this paper, is to determine whether interactive dialogues between a client and virtual clinician differ in the quantity and quality of the client’s language output compared to dialogues between client and human clinician. Although the potential of using virtual clinicians is promising, it must be determined if individuals with aphasia (or other language disorder) will be responsive to the virtual clinician and produce as much language in this context as they would during dialogues with human clinicians.

We addressed two hypotheses in this study:

1. For PWA, practice with dialogues that focus on everyday activities will improve quality and quantity of verbal output in those dialogues.

2. For PWA, verbal output practiced in dialogues with a virtual clinician and a human clinician will yield similar amounts of verbal output as measured by information units in the dialogues.
- Aasish Pappu, Alexander I. Rudnicky. 2014. Learning situated knowledge bases through dialog. Abstract: To respond to a user’s query, dialog agents can use a knowledge base that is either domain specific, commonsense (e.g., NELL, Freebase) or a combination of both. The drawback is that domain-specific knowledge bases will likely be limited and static; commonsense ones are dynamic but contain general information found on the web and will be sparse with respect to a domain. We address this issue through a system that solicits situational information from its users in a domain that provides information on events (seminar talks) to augment its knowledge base (covering an academic field). We find that this knowledge is consistent and useful and that it provides reliable information to users. We show that, in comparison to a base system, users find that retrievals are more relevant when the system uses its informally acquired knowledge to augment their queries.
- J. Chiu, Alexander I. Rudnicky. 2014. LACS System Analysis on Retrieval Models for the MediaEval 2014 Search and Hyperlinking Task. Abstract: We describe the LACS submission to the Search sub-task of the Search and Hyperlinking Task at MediaEval 2014. Our experiments investigate how different retrieval models interact with word stemming and stopword removal. On the development data, we segment the subtitle and Automatic Speech Recognition (ASR) transcripts into fixed length time units, and examine the effect of different retrieval models. We find that stemming provides consistent improvement; stopword removal is more sensitive to the retrieval models on the subtitles. These manipulations do not contribute to stable improvement on the ASR transcripts. Our experiments on test data focus on the subtitle. The gap in performance for different retrieval models is much less compared to the development data. We achieved 0.477 MAP on the test data.
- Yun-Nung (Vivian) Chen, William Yang Wang, Alexander I. Rudnicky. 2014. Leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems. Abstract: Distributional semantics and frame semantics are two representative views on language understanding in the statistical world and the linguistic world, respectively. In this paper, we combine the best of two worlds to automatically induce the semantic slots for spoken dialogue systems. Given a collection of unlabeled audio files, we exploit continuous-valued word embeddings to augment a probabilistic frame-semantic parser that identifies key semantic slots in an unsupervised fashion. In experiments, our results on a real-world spoken dialogue dataset show that the distributional word representations significantly improve the adaptation of FrameNet-style parses of ASR decodings to the target semantic space; that comparing to a state-of-the-art baseline, a 13% relative average precision improvement is achieved by leveraging word vectors trained on two 100-billion words datasets; and that the proposed technology can be used to reduce the costs for designing task-oriented spoken dialogue systems.
- Longlu Qin, Alexander I. Rudnicky. 2013. Learning better lexical properties for recurrent OOV words. Abstract: Out-of-vocabulary (OOV) words can appear more than once in a conversation or over a period of time. Such multiple instances of the same OOV word provide valuable information for learning the lexical properties of the word. Therefore, we investigated how to estimate better pronunciation, spelling and part-of-speech (POS) label for recurrent OOV words. We first identified recurrent OOV words from the output of a hybrid decoder by applying a bottom-up clustering approach. Then, multiple instances of the same OOV word were used simultaneously to learn properties of the OOV word. The experimental results showed that the bottom-up clustering approach is very effective at detecting the recurrence of OOV words. Furthermore, by using evidence from multiple instances of the same word, the pronunciation accuracy, recovery rate and POS label accuracy of recurrent OOV words can be substantially improved.
- A. Nanavati, Nitendra Rajput, Saurabh Srivastava, Cumhur Erkut, A. Jylhä, Alexander I. Rudnicky, S. Serafin, M. Turunen. 2013. SiMPE: 8th workshop on speech and sound in mobile and pervasive environments. Abstract: The SiMPE workshop series started in 2006 with the goal of enabling speech processing on mobile and embedded devices. The SiMPE 2012 workshop extended the notion of audio to non-speech "Sounds" and thus the expansion became "Speech and Sound". SiMPE 2010 and 2011 brought together researchers from the speech and the HCI communities. Speech User interaction in cars was a focus area in 2009. Multimodality got more attention in SiMPE 2008. In SiMPE 2007, the focus was on developing regions.
 With SiMPE 2013, the 8th in the series, we continue to explore the area of speech along with sound. Akin to language processing and text-to-speech synthesis in the voice-driven interaction loop, sensors can track continuous human activities such as singing, walking, or shaking the mobile phone, and non-speech audio can facilitate continuous interaction. The technologies underlying speech processing and sound processing are quite different and these communities have been working mostly independent of each other. And yet, for multimodal interactions on the mobile, it is perhaps natural to ask whether and how speech and sound can be mixed and used more effectively and naturally.
- Aasish Pappu, Alexander I. Rudnicky. 2013. Predicting Tasks in Goal-Oriented Spoken Dialog Systems using Semantic Knowledge Bases. Abstract: Goal-oriented dialog agents are expected to recognize user-intentions from an utterance and execute appropriate tasks. Typically, such systems use a semantic parser to solve this problem. However, semantic parsers could fail if user utterances contain out-of-grammar words/phrases or if the semantics of uttered phrases did not match the parser’s expectations. In this work, we have explored a more robust method of task prediction. We define task prediction as a classification problem, rather than “parsing” and use semantic contexts to improve classification accuracy. Our classifier uses semantic smoothing kernels that can encode information from knowledge bases such as Wordnet, NELL and Freebase.com. Our experiments on two spoken language corpora show that augmenting semantic information from these knowledge bases gives about 30% absolute improvement in task prediction over a parserbased method. Our approach thus helps make a dialog agent more robust to user input and helps reduce number of turns required to detected intended tasks.
- Longlu Qin, Alexander I. Rudnicky. 2013. Finding recurrent out-of-vocabulary words. Abstract: Out-of-vocabulary (OOV) words can appear more than once in a conversation or over a period of time. Such multiple instances of the same OOV word provide valuable information for estimating the pronunciation or the part-of-speech (POS) tag of the word. But in a conventional OOV word detection system, each OOV word is recognized and treated individually. We therefore investigated how to identify recurrent OOV words in speech recognition. Specifically, we propose to cluster multiple instances of the same OOV word using a bottom-up approach. Phonetic, acoustic and contextual features were collected to measure the distance between OOV candidates. The experimental results show that the bottom-up clustering approach is very effective at detecting the recurrence of OOV words. We also found that the phonetic feature is better than the acoustic and contextual features, and the best performance is achieved when combining all features.
- J. Chiu, Alexander I. Rudnicky. 2013. Using conversational word bursts in spoken term detection. Abstract: We describe a language independent word burst feature based on the structure of conversational speech that can be used to improve spoken term detection (STD) performance. Word burst refers to a phenomenon in conversational speech in which particular content words tend to occur in close proximity of each other as a byproduct of the topic under discussion. To take advantage of bursts, we describe a rescoring procedure that can be applied to lattice and confusion network outputs to improve STD performance. This approach is particularly effective when acoustic models are built with limited training data (and ASR performance is relatively poor). We find that word bursts appear in the four languages we examined and that STD performance can be improved for three of them; the remaining language is agglutinative.
- Aasish Pappu, Alexander I. Rudnicky. 2013. Deploying speech interfaces to the masses. Abstract: Speech systems are typically deployed either over phones, e.g. IVR agents, or on embodied agents, e.g. domestic robots. Most of these systems are limited to a particular platform i.e., only accessible by phone or in situated interactions. This limits scalability and potential domain of operation. Our goal is to make speech interfaces more widely available, and we are proposing a new approach for deploying such interfaces on the internet along with traditional platforms. In this work, we describe a lightweight speech interface architecture built on top of Freeswitch, an open source softswitch platform. A softswitch enables us to provide users with access over several types of channels (phone, VOIP, etc.) as well as support multiple users at the same time. We demonstrate two dialog applications developed using this approach: 1) Virtual Chauffeur: a voice based virtual driving experience and 2) Talkie: a speech-based chat bot.
- Ankur Gandhe, Longlu Qin, Florian Metze, Alexander I. Rudnicky, Ian Lane, Matthias Eck. 2013. Using web text to improve keyword spotting in speech. Abstract: For low resource languages, collecting sufficient training data to build acoustic and language models is time consuming and often expensive. But large amounts of text data, such as online newspapers, web forums or online encyclopedias, usually exist for languages that have a large population of native speakers. This text data can be easily collected from the web and then used to both expand the recognizer's vocabulary and improve the language model. One challenge, however, is normalizing and filtering the web data for a specific task. In this paper, we investigate the use of online text resources to improve the performance of speech recognition specifically for the task of keyword spotting. For the five languages provided in the base period of the IARPA BABEL project, we automatically collected text data from the web using only Limited LP resources. We then compared two methods for filtering the web data, one based on perplexity ranking and the other based on out-of-vocabulary (OOV) word detection. By integrating the web text into our systems, we observed significant improvements in keyword spotting accuracy for four out of the five languages. The best approach obtained an improvement in actual term weighted value (ATWV) of 0.0424 compared to a baseline system trained only on LimitedLP resources. On average, ATWV was improved by 0.0243 across five languages.
- M. Marge, Alexander I. Rudnicky. 2013. Towards evaluating recovery strategies for situated grounding problems in human-robot dialogue. Abstract: Robots can use information from their surroundings to improve spoken language communication with people. Even when speech recognition is correct, robots face challenges when interpreting human instructions. These situated grounding problems include referential ambiguities and impossible-to-execute instructions. We present an approach to resolving situated grounding problems through spoken dialogue recovery strategies that robots can invoke to repair these problems. We describe a method for evaluating these strategies in human-robot navigation scenarios.
- Yun-Nung (Vivian) Chen, William Yang Wang, Alexander I. Rudnicky. 2013. An empirical investigation of sparse log-linear models for improved dialogue act classification. Abstract: Previous work on dialogue act classification have primarily focused on dense generative and discriminative models. However, since the automatic speech recognition (ASR) outputs are often noisy, dense models might generate biased estimates and overfit to the training data. In this paper, we study sparse modeling approaches to improve dialogue act classification, since the sparse models maintain a compact feature space, which is robust to noise. To test this, we investigate various element-wise frequentist shrinkage models such as lasso, ridge, and elastic net, as well as structured sparsity models and a hierarchical sparsity model that embed the dependency structure and interaction among local features. In our experiments on a real-world dataset, when augmenting N-best word and phone level ASR hypotheses with confusion network features, our best sparse log-linear model obtains a relative improvement of 19.7% over a rule-based baseline, a 3.7% significant improvement over a traditional non-sparse log-linear model, and outperforms a state-of-the-art SVM model by 2.2%.
- Yun-Nung (Vivian) Chen, William Yang Wang, Alexander I. Rudnicky. 2013. Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing. Abstract: Spoken dialogue systems typically use predefined semantic slots to parse users' natural language inputs into unified semantic representations. To define the slots, domain experts and professional annotators are often involved, and the cost can be expensive. In this paper, we ask the following question: given a collection of unlabeled raw audios, can we use the frame semantics theory to automatically induce and fill the semantic slots in an unsupervised fashion? To do this, we propose the use of a state-of-the-art frame-semantic parser, and a spectral clustering based slot ranking model that adapts the generic output of the parser to the target semantic space. Empirical experiments on a real-world spoken dialogue dataset show that the automatically induced semantic slots are in line with the reference slots created by domain experts: we observe a mean averaged precision of 69.36% using ASR-transcribed data. Our slot filling evaluations also indicate the promising future of this proposed approach.
- A. Nanavati, Nitendra Rajput, Alexander I. Rudnicky, M. Turunen, Thomas Sandholm, Cosmin Munteanu, Gerald Penn. 2012. SiMPE: 7th workshop on speech and sound in mobile and pervasive environments. Abstract: The SiMPE workshop series started in 2006 [2] with the goal of enabling speech processing on mobile and embedded devices to meet the challenges of pervasive environments (such as noise) and leveraging the context they offer (such as location). SiMPE 2010 and 2011 brought together researchers from the speech and the HCI communities. Multimodality got more attention in SiMPE 2008 than it had received in the previous years. In SiMPE 2007, the focus was on developing regions. Speech User interaction in cars was a focus area in 2009. With SiMPE 2012, the 7th in the series, we hope to explore the area of speech along with sound. When using the mobile in an eyes-free manner, it is natural and convenient to hear about notifications and events. The arrival of an SMS has used a very simple sound based notification for a long time now. The technologies underlying speech processing and sound processing are quite different and these communities have been working mostly independent of each other. And yet, for multimodal interactions on the mobile, it is perhaps natural to ask whether and how speech and sound can be mixed and used more effectively and naturally.
- Elijah Mayfield, David Adamson, Alexander I. Rudnicky, Carolyn Penstein Rosé. 2012. Computational representation of discourse practices across populations in task-based dialogue. Abstract: In this work, we employ quantitative methods to describe the discourse practices observed in a direction giving task. We place a special emphasis on comparing differences in strategies between two separate populations and between successful and unsuccessful groups. We isolate differences in these strategies through several novel representations of discourse practices. We find that information sharing, instruction giving, and social feedback strategies are distinct between subpopulations in empirically identifiable ways.
- Seshadri Sridharan, Yun-Nung (Vivian) Chen, K. Chang, Alexander I. Rudnicky. 2012. NeuroDialog: an EEG-enabled spoken dialog interface. Abstract: Understanding user intent is a difficult problem in Dialog Systems, as they often need to make decisions under uncertainty. Using an inexpensive, consumer grade EEG sensor and a Wizard-of-Oz dialog system, we show that it is possible to detect system misunderstanding even before the user reacts vocally. We also present the design and implementation details of NeuroDialog, a proof-of-concept dialog system that uses an EEG based predictive model to detect system misrecognitions during live interaction.
- Longlu Qin, Ming Sun, Alexander I. Rudnicky. 2012. System combination for out-of-vocabulary word detection. Abstract: This paper presents a method to improve the out-of-vocabulary (OOV) word detection performance by combining multiple speech recognition systems' outputs. Three different fragment-word hybrid systems, the phone, subword, and graphone systems, were built for detecting OOV words. Then outputs from each individual system were combined using ROVER. Two combination metrics were explored in ROVER, voting by word frequency and voting by both word frequency and word confidence score. The experimental results show that the OOV word detection performance of the ROVER system with confidence scores is better than the ROVER system with only word frequency, as well as any of the individual hybrid systems.
- Longlu Qin, Alexander I. Rudnicky. 2012. OOV Word Detection using Hybrid Models with Mixed Types of Fragments. Abstract: This paper presents initial studies to improve the out-ofvocabulary (OOV) word detection performance by using mixed types of fragment units in one hybrid system. Three types of fragment units, subwords, syllables, and graphones, were combined in two different ways to build the hybrid lexicon and language model. The experimental results show that hybrid systems with mixed types of fragment units perform better than hybrid systems using only one type of fragment unit. After comparing the OOV word detection performance with the number and length of fragment units of each system, we proposed future work to better utilize mixed types of fragment units in a hybrid system.
- Aasish Pappu, Alexander I. Rudnicky. 2012. The Structure and Generality of Spoken Route Instructions. Abstract: A robust system that understands route instructions should be able to process instructions generated naturally by humans. Also desirable would be the ability to handle repairs and other modifications to existing instructions. To this end, we collected a corpus of spoken instructions (and modified instructions) produced by subjects provided with an origin and a destination. We found that instructions could be classified into four categories, depending on their intent such as imperative, feedback, or meta comment. We asked a different set of subjects to follow these instructions to determine the usefulness and comprehensibility of individual instructions. Finally, we constructed a semantic grammar and evaluated its coverage. To determine whether instruction-giving forms a predictable sub-language, we tested the grammar on three corpora collected by others and determined that this was largely the case. Our work suggests that predictable sub-languages may exist for well-defined tasks.
- B. Miller, C. H. Hwang, Yugyung Lee, Janet Roberts, Alexander I. Rudnicky. 2011. The I 3 S Project : A Mixed , Behavioral and Semantic Approach to Discourse / Dialogue Systems. Abstract: The Intuitive Interfaces to Information Systems (I3S) project at MCC investigated novel approaches to simplifying the construction of spoken dialogue systems. Our goals included designing a system architecture that allows domain independent strategies, such as appropriate conversational gambits, to be separated from domain dependent strategies, such as the most effective prompt to accomplish the immediate task at hand. The system uses plan-based representations for dialogue and domain, and includes such components as a problem solver and plan recognizer. In addition, a new representation called Meta Problem Solving Actions that provides the rationale for problem solving behavior has been introduced to improve overall system behavior and coherency. Other important contributions include the development of a conceptual layer called Interaction Plans that relate Meta Problem Solving Actions to discourse phenomena. We have used these representations to develop an innovative interpretation strategy for user speech acts, using a combination of behavioral and semantic rules and representations to determine the most reasonable interpretation while maintaining real-time response. Our new representations lead to reduced application development and maintenance time. Prototypes have been implemented in an information service-based domain (City Resources). Overview of Current Technology As the amount and complexity of interaction between humans and computers increase, the role of the computer is becoming that of collaboration with humans. A key aspect of this role is the support of mixed-initiative interaction (Allen 1999). In order to properly support such interaction, as well as capture the rationale behind communicative and domain actions, an extensive and flexible framework is required. Common frameworks for dialogue systems built to date include graph-based, frame-based and plan-based. We will first look at the advantages and disadvantages of these systems to motivate our design decisions. * Authors’ current addresses are: Miller– Cycorp, Inc. 3721 Exec. Cntr. Dr., Austin, TX 78731. miller@cyc.com; Lee– CS/Telecom. U of Missouri Kansas City, Kansas City, MO 64110. yugi@cstp.umkc.edu; Roberts – BMC Software, Inc., 10431 Morado Circle Austin, TX 78759. janet_roberts@bmc.com; Rudnicky– School of CS, CMU, Pittsburgh PA 15213, air@cs.cmu.edu; Hwang–MCC, Austin, TX. hwang@mcc.com. First generation spoken dialogue management systems typically involve the construction of conversational flowcharts linking possible dialogue states. Each state specifies a prompt, and enumerates possible user responses that transition the dialogue to a new state (e.g., see (McTear 1998)). A practical difficulty with this approach is that conversational context must be explicitly encoded in the conversational graph. Thus, graph-based systems are exponentially hard in the input domain––i.e., every possible state must be explicitly encoded. Second generation approaches (for example, (Ward and Issar 1994)) substantially simplify this development work by making an assumption that possible interactions with the application can be expressed as a set of frames, e.g., see (Hayes and Reddy 1983), and that interaction in the domain can be driven by the filling out a frame. Given that the parser also outputs semantic frames, the application frame can be filled in with a few rules (e.g., what to do when the parsed frame specifies an already filled in slot) as well as rules for generating a prompt back to the user. Frame-and-Rule systems have a number of advantages over the graph-based model. For one, it is relatively simple to allow for a kind of mixed-initiative interaction when the domain supports keyword or phrase-based parsing. A keyword in the input stream allows the parser to make a fairly good guess as to the corresponding semantic frame, and a deep parse is not required. Prompting is simply based on unfilled slots in the application semantic frame. This allows the following sort of interaction: (S) Where would you like to go? (U) I need to leave sometime after 4pm today. (S) Leaving today after 4pm; where would you like to go? Here the system prompt may have been generated on the basis of having a slot for arrival-city in the application semantic frame. If it is currently unfilled, the rule system may select it as the next thing to ask the user. The user, on the other hand, ignores the prompt and supplies a departure time. So long as the parser can recognize the keyword “leave” and realize that a time has been supplied, it can guess that a departure time has been specified regardless of the deeper meaning of the sentence. If this fills in another unfilled slot in the application frame, the system may enter
- M. Marge, Alexander I. Rudnicky. 2011. Towards Overcoming Miscommunication in Situated Dialogue by Asking Questions. Abstract: Situated dialogue is prominent in the robot navigation task, where a human gives route instructions (i.e., a sequence of navigation commands) to an agent. We propose an approach for situated dialogue agents whereby they use strategies such as asking questions to repair or recover from unclear instructions, namely those that an agent misunderstands or considers ambiguous. Most immediately in this work we study examples from existing human-human dialogue corpora and relate them to our proposed approach.
- Longlu Qin, Ming Sun, Alexander I. Rudnicky. 2011. OOV Detection and Recovery Using Hybrid Models with Different Fragments. Abstract: In this paper, we address the out-of-vocabulary (OOV) detection and recovery problem by developing three different fragmentword hybrid systems. A fragment language model (LM) and a word LM were trained separately and then combined into a single hybrid LM. Using this hybrid model, the recognizer can recognize any OOVs as fragment sequences. Different types of fragments, such as phones, subwords, and graphones were tested and compared on the WSJ 5k and 20k evaluation sets. The experiment results show that the subword and graphone hybrid systems perform better than the phone hybrid system in both 5k and 20k tasks. Furthermore, given less training data, the subword hybrid system is more preferable than the graphone hybrid system.
- M. Marge, Alexander I. Rudnicky. 2011. The TeamTalk Corpus: Route Instructions in Open Spaces. Abstract: This paper describes the TeamTalk corpus, a new corpus of route instructions consisting of directions given to a robot. Participants provided instructions to a robot that needed to move to a marked location. The environment contained two robots and a symbolic destination marker, all within an open space. The corpus contains the collected speech, speech transcriptions, stimuli, and logs of all participant interactions from the experiment. Route instruction transcriptions are divided into steps and annotated as either metric-based or landmarkbased instructions. This corpus captured variability in directions for robots represented in 2-dimensional schematic, 3-dimensional virtual, and natural environments, all in the context of open space navigation.
- A. Nanavati, Nitendra Rajput, Alexander I. Rudnicky, M. Turunen, A. Kun, Tim Paek, I. Tashev. 2011. SiMPE: 6th Workshop on Speech in Mobile and Pervasive Environments. Abstract: With the proliferation of pervasive devices and the increase in their processing capabilities, client-side speech processing has been emerging as a viable alternative. The SiMPE workshop series started in 2006 [5] with the goal of enabling speech processing on mobile and embedded devices to meet the challenges of pervasive environments (such as noise) and leveraging the context they offer (such as location). SiMPE 2010, the latest in the series brought together, very successfully, researchers from the speech and the HCI communities. We believe this is the beginning.
 SiMPE 2011, the 6th in the series, will continue to explore issues, possibilities, and approaches for enabling speech processing as well as convenient and effective speech and multimodal user interfaces. Over the years, SiMPE has been evolving too, and since last year, one of our major goals has been to increase the participation of speech/multimodal HCI designers, and increase their interactions with speech processing experts.
 Multimodality got more attention in SiMPE 2008 than it has received in the previous years. In SiMPE 2007 [4], the focus was on developing regions. Given the importance of speech in developing regions, SiMPE 2008 had "SiMPE for developing regions" as a topic of interest. Speech User interaction in cars was a focus area in 2009 [2].
- 李 清宰, 河原 達也, Alexander I. Rudnicky. 2011. Collecting Speech Data using Amazon's Mechanical Turk for Evaluating Voice Search System. Abstract: This paper describes a crowd-sourcing method to collect speech data using Amazon’s Mechanical Turk (MTurk). We designed a task (HIT) to collect speech data as an evaluation set for voice search and another task to verify the quality of the collected speech data. More than a thousand utterances are collected very efficiently. It turned out that more than 90% of them are valid with correct transcript, and reasonable recognition accuracy is achieved. Using the data, we conducted evaluation of the voice book search system, and confirmed that the combination of slot-based vector space models provides higher search accuracy than the conventional single vector space model.
- M. Marge, Alexander I. Rudnicky. 2010. Comparing Spoken Language Route Instructions for Robots across Environment Representations. Abstract: Spoken language interaction between humans and robots in natural environments will necessarily involve communication about space and distance. The current study examines people's close-range route instructions for robots and how the presentation format (schematic, virtual or natural) and the complexity of the route affect the content of instructions. We find that people have a general preference for providing metric-based instructions. At the same time, presentation format appears to have less impact on the formulation of these instructions. We conclude that understanding of spatial language requires handling both landmark-based and metric-based expressions.
- Cheongjae Lee, Alexander I. Rudnicky, G. G. Lee. 2010. Let's Buy Books: Finding eBooks using voice search. Abstract: We describe Let's Buy Books, a dialog system that helps users search for eBook titles. In this paper we compare different vector space approaches to voice search and find that a hybrid approach using a weighted sub-space model smoothed with a general model provides the best performance over different conditions and evaluated using both synthetic queries and queries collected from users through questionnaires.
- M. Marge, João Miranda, A. Black, Alexander I. Rudnicky. 2010. Towards Improving the Naturalness of Social Conversations with Dialogue Systems. Abstract: We describe an approach to improving the naturalness of a social dialogue system, Talkie, by adding disfluencies and other content-independent enhancements to synthesized conversations. We investigated whether listeners perceive conversations with these improvements as natural (i.e., human-like) as human-human conversations. We also assessed their ability to correctly identify these conversations as between humans or computers. We find that these enhancements can improve the perceived naturalness of conversations for observers "overhearing" the dialogues.
- M. Marge, S. Banerjee, Alexander I. Rudnicky. 2010. Using the Amazon Mechanical Turk to Transcribe and Annotate Meeting Speech for Extractive Summarization. Abstract: Due to its complexity, meeting speech provides a challenge for both transcription and annotation. While Amazon's Mechanical Turk (MTurk) has been shown to produce good results for some types of speech, its suitability for transcription and annotation of spontaneous speech has not been established. We find that MTurk can be used to produce high-quality transcription and describe two techniques for doing so (voting and corrective). We also show that using a similar approach, high quality annotations useful for summarization systems can also be produced. In both cases, accuracy is comparable to that obtained using trained personnel.
- Alexander I. Rudnicky, Aasish Pappu, Peng Li, M. Marge, Benjamin Frisch. 2010. Instruction Taking in the TeamTalk System. Abstract: TeamTalk is a dialog framework that supports multiparticipant spoken interaction between humans and robots in a task-oriented setting that requires cooperation and coordination among team members. We describe two new features to the system, the ability for robots to accept and remember location labels and the ability to learn action sequences. These capabilities were made possible by incorporating an ontology and an instruction understanding component into the system.
- M. Marge, S. Banerjee, Alexander I. Rudnicky. 2010. Using the Amazon Mechanical Turk for transcription of spoken language. Abstract: We investigate whether Amazon's Mechanical Turk (MTurk) service can be used as a reliable method for transcription of spoken language data. Utterances with varying speaker demographics (native and non-native English, male and female) were posted on the MTurk marketplace together with standard transcription guidelines. Transcriptions were compared against transcriptions carefully prepared in-house through conventional (manual) means. We found that transcriptions from MTurk workers were generally quite accurate. Further, when transcripts for the same utterance produced by multiple workers were combined using the ROVER voting scheme, the accuracy of the combined transcript rivaled that observed for conventional transcription methods. We also found that accuracy is not particularly sensitive to payment amount, implying that high quality results can be obtained at a fraction of the cost and turnaround time of conventional methods.
- Longlu Qin, Alexander I. Rudnicky. 2010. Implementing and Improving MMIE Training in SphinxTrain. Abstract: Discriminative training schemes, such as Maximum Mutual Information Estimation (MMIE), have been used to improve the accuracy of speech recognition systems trained using Maximum Likelihood Estimation (MLE). In this paper, we present the implementation details of MMIE training in SphinxTrain and baseline results for MMIE training on the Wall Street Journal (WSJ) SI84 and SI284 data sets. This paper also introduces an efficient lattice pruning technique that both speeds up the process and increases the impact of MMIE training on recognition accuracy. The proposed pruning technique, based on posterior probability pruning, is shown to provide better performance than MMIE using standard pruning techniques.
- Longlu Qin, Alexander I. Rudnicky. 2010. The effect of lattice pruning on MMIE training. Abstract: In discriminative training, such as Maximum Mutual Information Estimation (MMIE) training, a word lattice is usually used as a compact representation of many different sentence hypotheses and hence provides an efficient representation of the confusion data. However, in a large vocabulary continuous speech recognition (LVCSR) system trained from hundreds or thousands hours training data, the extended Baum-Welch (EBW) computation on the word lattice is still very expensive. In this paper, we investigated the effect of lattice pruning on MMIE training, where we tested the MMIE performance trained with different lattice complexity. A beam pruning and a posterior probability pruning method were applied to generate different sizes of word lattices. The experimental results show that using the posterior probability lattice pruning algorithm, we can save about 40% of the total computation and get the same or more improvement compared to the baseline MMIE result.
- A. Nanavati, Nitendra Rajput, Alexander I. Rudnicky, M. Turunen, A. Kun, Tim Paek, I. Tashev. 2010. SiMPE: 5th workshop on speech in mobile and pervasive environments. Abstract: With the proliferation of pervasive devices and the increase in their processing capabilities, client-side speech processing has been emerging as a viable alternative. The SiMPE workshop series started in 2006 [5] with the goal of enabling speech processing on mobile and embedded devices to meet the challenges of pervasive environments (such as noise) and leveraging the context they offer (such as location).
 SiMPE 2010, the 5th in the series, will continue to explore issues, possibilities, and approaches for enabling speech processing as well as convenient and effective speech and multimodal user interfaces. Over the years, SiMPE has been evolving too, and since last year, one of our major goals has been to increase the participation of speech/multimodal HCI designers, and increase their interactions with speech processing experts.
 Multimodality got more attention in SiMPE 2008 than it has received in the previous years. In SiMPE 2007 [4], the focus was on developing regions. Given the importance of speech in developing regions, SiMPE 2008 had "SiMPE for developing regions" as a topic of interest. Speech User interaction in cars was a focus area in 2009 [2].
 Given the multi-disciplinary nature of our goal, we hope that SiMPE will become the prime meeting ground for experts in these varied fields to bring to fruition, novel, useful and usable mobile speech applications.
- S. Banerjee, Alexander I. Rudnicky. 2009. Detecting the Noteworthiness of Utterances in Human Meetings. Abstract: Our goal is to make note-taking easier in meetings by automatically detecting noteworthy utterances in verbal exchanges and suggesting them to meeting participants for inclusion in their notes. To show feasibility of such a process we conducted a Wizard of Oz study where the Wizard picked automatically transcribed utterances that he judged as noteworthy, and suggested their contents to the participants as notes. Over 9 meetings, participants accepted 35% of these suggestions. Further, 41.5% of their notes at the end of the meeting contained Wizard-suggested text. Next, in order to perform noteworthiness detection automatically, we annotated a set of 6 meetings with a 3-level noteworthiness annotation scheme, which is a break from the binary "in summary"/"not in summary" labeling typically used in speech summarization. We report Kappa of 0.44 for the 3-way classification, and 0.58 when two of the 3 labels are merged into one. Finally, we trained an SVM classifier on this annotated data; this classifier's performance lies between that of trivial baselines and inter-annotator agreement.
- Roni Rosenfeld, Alexander I. Rudnicky, J. Sherwani. 2009. Speech interfaces for information access by low literate users. Abstract: In the developing world, critical information, such as in the field of healthcare, can often mean the difference between life and death. While information and communications technologies enable multiple mechanisms for information access by literate users, there are limited options for information access by low literate users. 
In this thesis, I investigate the use of spoken language interfaces by low literate users in the developing world, specifically health information access by community health workers in Pakistan. I present results from five user studies comparing a variety of information access interfaces for these users. I first present a comparison of audio and text comprehension by users of varying literacy levels and with diverse linguistic backgrounds. I also present a comparison of two telephony-based interfaces with different input modalities: touch-tone and speech. Based on these studies, I show that speech interfaces outperform equivalent touch-tone interfaces for both low literate and literate users, and that speech interfaces outperform text interfaces for low literate users. 
A further contribution of the thesis is a novel approach for the rapid generation of speech recognition capability in resource-poor languages. Since most languages spoken in the developing world have limited speech resources, it is difficult to create speech recognizers for such languages. My approach leverages existing off-the-shelf technology to create robust, speaker-independent, small-vocabulary speech recognition capability with minimal training data requirements. I empirically show that this method is able to reach recognition accuracies of greater than 90% with very little effort and, even more importantly, little speech technology skill. 
The thesis concludes with an exploration of orality as a lens with which to analyze and understand low literate users, as well as recommendations on the design and testing of user interfaces for such users, such as an appreciation for the role of dramatic narrative in content creation for information access systems.
- Mohit Kumar, Dipanjan Das, Sachin Agarwal, Alexander I. Rudnicky. 2009. Non-textual Event Summarization by Applying Machine Learning to Template-based Language Generation. Abstract: We describe a learning-based system that creates draft reports based on observation of people preparing such reports in a target domain (conference replanning). The reports (or briefings) are based on a mix of text and event data. The latter consist of task creation and completion actions, collected from a wide variety of sources within the target environment. The report drafting system is part of a larger learning-based cognitive assistant system that improves the quality of its assistance based on an opportunity to learn from observation. The system can learn to accurately predict the briefing assembly behavior and shows significant performance improvements relative to a non-learning system, demonstrating that it's possible to create meaningful verbal descriptions of activity from event streams.
- Kazunori Komatani, Alexander I. Rudnicky. 2009. Predicting Barge-in Utterance Errors by using Implicitly-Supervised ASR Accuracy and Barge-in Rate per User. Abstract: Modeling of individual users is a promising way of improving the performance of spoken dialogue systems deployed for the general public and utilized repeatedly. We define "implicitly-supervised" ASR accuracy per user on the basis of responses following the system's explicit confirmations. We combine the estimated ASR accuracy with the user's barge-in rate, which represents how well the user is accustomed to using the system, to predict interpretation errors in barge-in utterances. Experimental results showed that the estimated ASR accuracy improved prediction performance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling.
- A. Nanavati, Nitendra Rajput, Alexander I. Rudnicky, M. Turunen, A. Kun, Tim Paek, I. Tashev. 2009. SiMPE: Fourth Workshop on Speech in Mobile and Pervasive Environments. Abstract: With the proliferation of pervasive devices and the increase in their processing capabilities, client-side speech processing has been emerging as a viable alternative.
 SiMPE 2009, the fourth in the series, will continue to explore issues, possibilities, and approaches for enabling speech processing as well as convenient and effective speech and multimodal user interfaces. One of our major goals for SiMPE 2009 is to increase the participation of speech/multimodal HCI designers, and increase their interactions with speech processing experts.
 Multimodality got more attention in SiMPE 2008 than it has received in the previous years. In SiMPE 2007 [3], the focus was on developing regions. Given the importance of speech in developing regions, SiMPE 2008 had "SiMPE for developing regions" as a topic of interest. We think of this as a key emerging area for mobile speech applications, and will continue this in 2009 as well.
- M. Marge, Aasish Pappu, Benjamin Frisch, T. Harris, Alexander I. Rudnicky. 2009. Exploring Spoken Dialog Interaction in Human-Robot Teams. Abstract: We describe TeamTalk: A human-robot interface capable of interpreting spoken dialog interactions between humans and robots in consistent real-world and virtual-world scenarios. The system is used in real environments by humanrobot teams to perform tasks associated with treasure hunting. In order to conduct research exploring spoken human-robot interaction, we have developed a virtual platform using USARSim. We describe the system, its use as a high-fidelity simulator with USARSim, and current experiments that benefit from a simulated environment and that would be difficult to implement in real-world scenarios.
- David Huggins-Daines, Alexander I. Rudnicky. 2009. Combining mixture weight pruning and quantization for small-footprint speech recognition. Abstract: Semi-continuous acoustic models, where the output distributions for all Hidden Markov Model states share a common codebook of Gaussian density functions, are a well-known and proven technique for reducing computation in automatic speech recognition. However, the size of the parameter files, and thus their memory footprint at runtime, can be very large. We demonstrate how non-linear quantization can be combined with a mixture weight distribution pruning technique to halve the size of the models with minimal performance overhead and no increase in error rate.
- A. Nanavati, Nitendra Rajput, Alexander I. Rudnicky, M. Turunen. 2008. SiMPE: third workshop on speech in mobile and pervasive environments. Abstract: In the past, voice-based applications have been accessed using unintelligent telephone devices through Voice Browsers that reside on the server. The proliferation of pervasive devices and the increase in their processing capabilities, clientside speech processing has been emerging as a viable alternative. In SiMPE 2008, the third in the series, we will continue to explore the various possibilities and issues that arise while enabling speech processing on resource-constrained, possibly mobile devices.
 In SiMPE 2007 [2], the focus was on developing regions. Given the importance of speech in developing regions, SiMPE 2008 will include "SiMPE for developing regions" as a topic of interest. As a result of discussions in SiMPE 2007, we plan to invite and encourage Speech UI designers to participate in SiMPE 2008. We will also review the progress made over the last two years, in the areas and key problems identified in SiMPE 2006 [3].
- S. Banerjee, Alexander I. Rudnicky. 2008. An extractive-summarization baseline for the automatic detection of noteworthy utterances in multi-party human-human dialog. Abstract: Our goal is to reduce meeting participants' note-taking effort by automatically identifying utterances whose contents meeting participants are likely to include in their notes. Though note-taking is different from meeting summarization, these two problems are related. In this paper we apply techniques developed in extractive meeting summarization research to the problem of identifying noteworthy utterances. We show that these algorithms achieve an f-measure of 0.14 over a 5-meeting sequence of related meetings. The precision - 0.15 - is triple that of the trivial baseline of simply labeling every utterance as noteworthy. We also introduce the concept of ldquoshow-worthyrdquo utterances - utterances that contain information that could conceivably result in a note. We show that such utterances can be recognized with an 81% accuracy (compared to 53% accuracy of a majority classifier). Further, if non-show-worthy utterances are filtered out, the precision of noteworthiness detection improves by 33% relative.
- David Huggins-Daines, Alexander I. Rudnicky. 2008. Interactive ASR Error Correction for Touchscreen Devices. Abstract: We will demonstrate a novel graphical interface for correcting search errors in the output of a speech recognizer. This interface allows the user to visualize the word lattice by "pulling apart" regions of the hypothesis to reveal a cloud of words simlar to the "tag clouds" popular in many Web applications. This interface is potentially useful for dictation on portable touchscreen devices such as the Nokia N800 and other mobile Internet devices.
- Dipanjan Das, Mohit Kumar, Alexander I. Rudnicky. 2008. Automatic Extraction of Briefing Templates. Abstract: An approach to solving the problem of automatic briefing generation from non-textual events can be segmenting the task into two major steps, namely, extraction of briefing templates and learning aggregators that collate information from events and automatically fill up the templates. In this paper, we describe two novel unsupervised approaches for extracting briefing templates from human written reports. Since the problem is non-standard, we define our own criteria for evaluating the approaches and demonstrate that both approaches are effective in extracting domain relevant templates with promising accuracies.
- David Huggins-Daines, Alexander I. Rudnicky. 2008. Mixture Pruning and Roughening for Scalable Acoustic Models. Abstract: In an automatic speech recognition system using a tied-mixture acoustic model, the main cost in CPU time and memory lies not in the evaluation and storage of Gaussians themselves but rather in evaluating the mixture likelihoods for each state output distribution. Using a simple entropy-based technique for pruning the mixture weight distributions, we can achieve a significant speedup in recognition for a 5000-word vocabulary with a negligible increase in word error rate. This allows us to achieve real-time connected-word dictation on an ARM-based mobile device.
- A. Chotimongkol, Alexander I. Rudnicky. 2008. Acquiring Domain-Specific Dialog Information from Task-Oriented Human-Human Interaction through an Unsupervised Learning. Abstract: We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data. The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs. To represent a dialog for a learning purpose, we based our representation, the form-based dialog structure representation, on an observable structure. We show that this representation is sufficient for modeling phenomena that occur regularly in several dissimilar task-oriented domains, including information-access and problem-solving. With the goal of ultimately reducing human annotation effort, we examine the use of unsupervised learning techniques in acquiring the components of the form-based representation (i.e. task, subtask, and concept). These techniques include statistical word clustering based on mutual information and Kullback-Liebler distance, TextTiling, HMM-based segmentation, and bisecting K-mean document clustering. With some modifications to make these algorithms more suitable for inferring the structure of a spoken dialog, the unsupervised learning algorithms show promise.
- Alexander I. Rudnicky. 2008. Improving Automatic Meeting- Understanding by Leveraging Meeting Participant Behavior. Abstract: Most office workers participate in multiple meetings on a daily basis. Although surveys show that large parts of these meetings are often not useful to all the participants, it has been shown (Banerjee, Rose, & Rudnicky, 2005) that participants do sometimes need to retrieve information discussed at previous meetings, and that this is usually a difficult task. The human-impact goal of this thesis is to help humans retrieve the information they need from past meetings. Several approaches have been explored in the past to help humans with this retrieval task. These approaches include meeting recording and browsing systems (Cutler, et al., 2002), and systems that automatically detect and extract pieces of useful information from the speech such as action items (Purver, Ehlen, & Niekrasz, 2006). These approaches are often examples of either classic supervised learning (with offline data collection, annotation and model training) or unsupervised learning with some adaptation to the meeting participants. While these approaches make use of the expertise of offline human annotators, we believe that little effort has been made to effectively harness the knowledge that the meeting participants have. Specifically, meeting participants will be the best judges of what information is important in a meeting. This judgment, if properly leveraged, can provide high quality information with which to improve automatic meeting-understanding systems. The challenge of leveraging meeting participant knowledge, however, is that they may have little motivation to provide labeled data to the system without some perceptible and immediate benefit. Moreover providing such information may be distracting and thus undesirable. Our hypothesis is that despite this challenge, it is possible to motivate the human users of an interactive system to provide supervision. We propose to extract this supervision by designing services that provide the user with immediate benefit, but that are designed in such a way that as the user interacts with the system, his actions can be interpreted as labeled data. Given this labeled data, the system can improve its performance over time. We propose two mechanisms: passive and active supervision extraction. In the passive approach, the system cannot select data points to query labels for, and data acquisition from user actions occurs entirely due to the design of the interface. In the active approach, the system selects data points and queries the user for their labels. Although this is similar to active learning, we are interested in motivating ordinary users to provide data (as opposed to giving the data to labelers). We create this motivation by embedding the queries in an interactive service that gives the user immediate benefit every time a query is made. The user’s responses are then interpreted as labels. We apply these
- David Huggins-Daines, Alexander I. Rudnicky. 2007. Implicitly Supervised Language Model Adaptation for Meeting Transcription. Abstract: We describe the use of meeting metadata, acquired using a computerized meeting organization and note-taking system, to improve automatic transcription of meetings. By applying a two-step language model adaptation process based on notes and agenda items, we were able to reduce perplexity by 9% and word error rate by 4% relative on a set of ten meetings recorded in-house. This approach can be used to leverage other types of metadata.
- Mohit Kumar, Nikesh Garera, Alexander I. Rudnicky. 2007. Learning from the Report-writing Behavior of Individuals. Abstract: We describe a briefing system that learns to predict the contents of reports generated by users who create periodic (weekly) reports as part of their normal activity. The system observes content-selection choices that users make and builds a predictive model that could, for example, be used to generate an initial draft report. Using a feature of the interface the system also collects information about potential user-specific features. The system was evaluated under realistic conditions, by collecting data in a project-based university course where student group leaders were tasked with preparing weekly reports for the benefit of the instructors, using the material from individual student reports. 
 
This paper addresses the question of whether data derived from the implicit supervision provided by end-users is robust enough to support not only model parameter tuning but also a form of feature discovery. Results indicate that this is the case: system performance improves based on the feedback from user activity. We find that individual learned models (and features) are user-specific, although not completely idiosyncratic. Thismay suggest that approaches which seek to optimizemodels globally (say over a large corpus of data) may not in fact produce results acceptable to all individuals.
- Giuseppe DiFabbrizio, Dilek Z. Hakkani-Tür, Oliver Lemon, M. Gilbert, Alexander I. Rudnicky. 2007. Panel on spoken dialog corpus composition and annotation for research. Abstract: The goal of this forum is to provide researchers from various institutes with the opportunity to comment on a proposed NSF-sponsored data collection plan for a spoken dialog corpus. The corpus is to be used for research in speech recognition, spoken language understanding, dialog management, machine learning, and language generation. Currently, there exists a corpus with over 600 dialog interactions, collected from users using the Discoh system (from the IEEE SLT 2006 workshop) and the Conquest system (from ICSLP 2006) to obtain general information about conference services. These systems were created as part of a joint collaboration between CMU, ATT, Edinburgh and ICSI.
- T. Harris, Alexander I. Rudnicky. 2007. TeamTalk: A Platform for Multi-Human-Robot Dialog Research in Coherent Real and Virtual Spaces. Abstract: Performing experiments with human-robot interfaces often requires the allocation of expensive and complex hardware and large physical spaces. Those costs constrain development and research to the currently affordable resources, and they retard the testing-and-redevelopment cycle. In order to explore research free from mundane allocation constraints and speed-up our platform development cycle, we have developed a platform for research of multi-human-robot spoken dialog in coherent real and virtual spaces. We describe the system, and speculate on how it will further research in this domain.
- Yi Wu, Rong Zhang, Alexander I. Rudnicky. 2007. Data selection for speech recognition. Abstract: This paper presents a strategy for efficiently selecting informative data from large corpora of transcribed speech. We propose to choose data uniformly according to the distribution of some target speech unit (phoneme, word, character, etc). In our experiment, in contrast to the common belief that "there is no data like more data", we found it possible to select a highly informative subset of data that produces recognition performance comparable to a system that makes use of a much larger amount of data. At the same time, our selection process is efficient and fast.
- S. Banerjee, Alexander I. Rudnicky. 2007. Segmenting meetings into agenda items by extracting implicit supervision from human note-taking. Abstract: Splitting a meeting into segments such that each segment contains discussions on exactly one agenda item is useful for tasks such as retrieval and summarization of agenda item discussions. However, accurate topic segmentation of meetings is a difficult task. In this paper, we investigate the idea of acquiring implicit supervision from human meeting participants to solve the segmentation problem. Specifically we have implemented and tested a note taking interface that gives value to users by helping them organize and retrieve their notes easily, but that also extracts a segmentation of the meeting based on note taking behavior. We show that the segmentation so obtained achieves a Pk value of 0.212 which improves upon an unsupervised baseline by 45% relative, and compares favorably with a current state-of-the-art algorithm. Most importantly, we achieve this performance without any features or algorithms in the classic sense.
- D. Bohus, Alexander I. Rudnicky. 2007. Implicitly-supervised Learning in Spoken Language Interfaces: an Application to the Confidence Annotation Problem. Abstract: In this paper we propose the use of a novel learning paradigm in spoken language interfaces – implicitly-supervised learning. The central idea is to extract a supervision signal online, directly from the user, from certain patterns that occur naturally in the conversation. The approach eliminates the need for developer supervision and facilitates online learning and adaptation. As a first step towards better understanding its properties, advantages and limitations, we have applied the proposed approach to the problem of confidence annotation. Experimental results indicate that we can attain performance similar to that of a fully supervised model, without any manual labeling. In effect, the system learns from its own experiences with the users. *
- J. Bongard, Derek P. Brock, S. Collins, R. Duraiswami, Timothy W. Finin, Ian Harrison, Vasant G Honavar, G. Hornby, A. Jónsson, Mike Kassoff, D. Kortenkamp, Sanjeev Kumar, Ken Murray, Alexander I. Rudnicky, G. Trajkovski. 2007. Reports on the 2006 AAAI Fall Symposia. Abstract: The American Association for Artificial Intelligence was pleased to present the AAAI 2006 Fall Symposium Series, held Friday through Sunday, October 13-15, at the Hyatt Regency Crystal City in Washington, DC. Seven symposia were held. The titles were (1) Aurally Informed Performance: Integrating Ma- chine Listening and Auditory Presentation in Robotic Systems; (2) Capturing and Using Patterns for Evidence Detection; (3) Developmental Systems; (4) Integrating Reasoning into Everyday Applications; (5) Interaction and Emergent Phenomena in Societies of Agents; (6) Semantic Web for Collaborative Knowledge Acquisition; and (7) Spacecraft Autonomy: Using AI to Expand Human Space Exploration.
- Alexander I. Rudnicky, Roni Rosenfeld, D. Bohus. 2007. Error awareness and recovery in conversational spoken language interfaces. Abstract: One of the most important and persistent problems in the development of conversational spoken language interfaces is their lack of robustness when confronted with understanding-errors. Most of these errors stem from limitations in current speech recognition technology, and, as a result, appear across all domains and interaction types. There are two approaches towards increased robustness: prevent the errors from happening, or recover from them through conversation, by interacting with the users. 
In this dissertation we have engaged in a research program centered on the second approach. We argue that three capabilities are needed in order to seamlessly and efficiently recover from errors: (1) systems must be able to detect the errors, preferably as soon as they happen, (2) systems must be equipped with a rich repertoire of error recovery strategies that can be used to set the conversation back on track, and (3) systems must know how to choose optimally between different recovery strategies at run-time, i.e. they must have good error recovery policies . This work makes a number of contributions in each of these areas. 
First, to provide a real-world experimental platform this error handling research program, we developed RavenClaw, a plan-based dialog management framework for task-oriented domains. The framework has a modular architecture that decouples the error handling mechanisms from the do main-specific dialog control logic; in the process, it lessens system authoring effort, promotes portability and reusability, and ensures consistency in error handling behaviors both within and across domains. To date, RavenClaw has been used to develop and successfully deploy a number of spoken dialog systems spanning different domains an interaction types. Together with these systems, RavenClaw provides the infrastructure for the error handling work described in this dissertation. 
To detect errors, spoken language interfaces typically rely on confidence scores. In this work we investigated in depth current supervised learning techniques for building error detection models. In addition, we proposed a novel, implicitly-supervised approach for this task. No developer supervision is required in this case; rather, the system obtains the supervision signal online, from naturally-occurring patterns in the interaction. We believe this learning paradigm represents an important step towards constructing autonomously self-improving systems. Furthermore, we developed a scalable, data-driven approach that allows a system to continuously monitor and update beliefs throughout the conversation; the proposed approach leads to significant improvements in both the overall effectiveness and efficiency of the interaction. 
We developed and empirically investigated a large set of recovery strategies, targeting two types of understanding-errors that commonly occur in these systems: misunderstandings and nonunderstandings. Our results add to an existing body of knowledge about the advantages and disadvantages of these strategies, and highlight the importance of good recovery policies. 
In the last part of this work, we proposed and evaluated a novel online-learning based approach for developing recovery policies. The system constructs runtime estimates for the likelihood of success of each recovery strategy, together with confidence bounds for those estimates. These estimates are then used to construct a policy online, while balancing the system's exploration and exploitation goals. Experiments with a deployed spoken dialog system showed that the system was able to learn a more effective recovery policy in a relatively short time period.
- D. Bohus, A. Raux, T. Harris, M. Eskénazi, Alexander I. Rudnicky. 2007. Olympus: an open-source framework for conversational spoken language interface research. Abstract: We introduce Olympus, a freely available framework for research in conversational interfaces. Olympus' open, transparent, flexible, modular and scalable nature facilitates the development of large-scale, real-world systems, and enables research leading to technological and scientific advances in conversational spoken language interfaces. In this paper, we describe the overall architecture, several systems spanning different domains, and a number of current research efforts supported by Olympus.
- Mohit Kumar, Dipanjan Das, Alexander I. Rudnicky. 2007. Summarizing non-textual events with 'Briefing' focus. Abstract: We describe a learning-based system for generating reports based on a mix of text and event data. The system incorporates several stages of processing, including aggregation, template-filling and importance ranking. Aggregators and templates were based on a corpus of reports evaluated by human judges. Importance and granularity were learned from this corpus as well. We find that high-scoring reports (with a recall of 0.89) can be reliably produced using this procedure given a set of oracle features. The report drafting system is part of a learning cognitive assistant RADAR, and is used to describe its performance.
- Mohit Kumar, Nikesh Garera, Alexander I. Rudnicky. 2006. A Briefing Tool that Learns Individual Report-Writing Behavior. Abstract: We describe a briefing system that learns to predict the contents of reports generated by users who create periodic (weekly) reports as part of their normal activity. We address the question whether data derived from the implicit supervision provided by end-users is robust enough to support not only model parameter tuning but also a form of feature discovery. The system was evaluated under realistic conditions, by collecting data in a project-based university course where student group leaders were tasked with preparing weekly reports for the benefit of the instructors, using the material from individual student reports
- S. Banerjee, Alexander I. Rudnicky. 2006. A texttiling based approach to topic boundary detection in meetings. Abstract: Our goal is to automatically detect boundaries between discussions of different topics in meetings. Towards this end we adapt the TextTiling algorithm [1] to the context of meetings. Our features include not only the overlapped words between adjacent windows, but also overlaps in the amount of speech contributed by each meeting participant. We evaluate our algorithm by comparing the automatically detected boundaries with the true ones, and computing precision, recall and f–measure. We report average precision of 0.85 and recall of 0.59 when segmenting unseen test meetings. Error analysis of our results shows that although the basic idea of our algorithm is sound, it breaks down when participants stray from typical behavior (such as when they monopolize the conversation for too long).
- Rong Zhang, Alexander I. Rudnicky. 2006. Investigations of issues for using multiple acoustic models to improve continuous speech recognition. Abstract: This paper investigates two important issues in constructing and combining ensembles of acoustic models for reducing recognition errors. First, we investigate the applicability of the AnyBoost algorithm for acoustic model training. AnyBoost is a generalized Boosting method that allows the use of an arbitrary loss function as the training criterion to construct ensemble of classifiers. We choose the MCE discriminative objective function for our experiments. Initial test results on a real-world meeting recognition corpus show that AnyBoost is a competitive alternate to the standard AdaBoost algorithm. Second, we investigate ROVER-based combination, focusing on the technique for selecting correct hypothesized words from aligned WTN. We propose a neural network based insertion detection and word scoring scheme for this. Our approach consistently outperforms the current voting technique used by ROVER in the experiments.
- A. Nanavati, Nitendra Rajput, Alexander I. Rudnicky, Roberto Sicconi. 2006. SiMPE: speech in mobile and pervasive environments. Abstract: Traditionally, voice-based applications have been accessed using unintelligent telephone devices through Voice Browsers that reside on the server. The proliferation of pervasive devices and the increase in their processing capabilities, client-side speech processing is emerging as a viable alternative. This workshop will explore the various possibilities and issues that arise while enabling speech processing on resource-constrained, possibly mobile devices. The workshop will highlight the many open areas that require research attention, identify key problems that need to be addressed, and also discuss a few approaches for solving some of them - to build the next generation of conversational systems.
- J. Bongard, Derek P. Brock, S. Collins, R. Duraiswami, Timothy W. Finin, Ian Harrison, Vasant G Honavar, G. Hornby, A. Jónsson, Mike Kassoff, D. Kortenkamp, Sanjeev Kumar, Ken Murray, Alexander I. Rudnicky, G. Trajkovski. 2006. Aurally Informed Performance: Integrating Machine Listening and Auditory Presentation in Robotic Systems, Papers from the 2006 AAAI Fall Symposium, Washington, DC, USA, October 13-15, 2006. Abstract: This symposium brought together a number of researchers who are concerned with performance issues that robots face that depend, in some way, on sound. Many commercially marketed robotic platforms, as well as others that are moving from the laboratory into specialized public settings, already have rudimentary speech communication interfaces, and some are even being engineered for specific types of auditory tasks. In general, though, the ability of robots to monitor the auditory scene before them and to execute interactive behaviors informed by the interpretation or production of sound information remains far behind the broad and mostly transparent skills of human beings. It is an easy thing, for instance, for people to discern on the basis of audition alone who a familiar voice is, where the voice is located in the environment, and to act on other aspects of the au■ The American Association for Artificial Intelligence was pleased to present the AAAI 2006 Fall Symposium Series, held Friday through Sunday, October 13–15, at the Hyatt Regency Crystal City in Washington, DC. Seven symposia were held. The titles were (1) Aurally Informed Performance: Integrating Machine Listening and Auditory Presentation in Robotic Systems; (2) Capturing and Using Patterns for Evidence Detection; (3) Developmental Systems; (4) Integrating Reasoning into Everyday Applications; (5) Interaction and Emergent Phenomena in Societies of Agents; (6) Semantic Web for Collaborative Knowledge Acquisition; and (7) Spacecraft Autonomy: Using AI to Expand Human Space Exploration. ditory scene before them, such as noise, informative sounds, or the need for proximity or loudness to facilitate verbal communication. When these auditory skills are integrated with people’s other perceptual and reasoning abilities, substantial capacities for performance and interaction arise. The design goals for robot audition and utterance of information by sound, though, are not just those that correspond to human skills. Machine auditory sensing can be designed, in certain ways, to be more capable and acute than human hearing, and going beyond speech, robotic auditory displays can be engineered to render nonspeech auditory information in and for a variety of manners and purposes. Thus, a substantial interaction design space arises when different modes of human-robot interaction are augmented by conventional and enhanced auditory functions. Since the idea of “aurally informed performance” can be thought of as a two-sided proposition, involving both listening and presentation behaviors, the symposium was organized to focus on these themes separately and then conclude with a session on integrated systems. Much of the contributed research fit easily into this division. On each of the two main days, we began with an outline of recent research trends in the day’s topic, “Listening” on the first day and “Presentation” on the second, and then heard contributed talks from participants. Afternoons were devoted to critical discussions of talks given in the morning and examination of a relevant philosophical question about the nature and role of sound as information in the design context of robotics. The outline of trends in machine listening covered developments in sound localization and techniques for recognizing and extracting information from sound. Papers given under this day’s theme made several contributions in these areas. Novel methods for classifying acoustical environments, localizing sounds with headrelated transfer functions, organizing sounds with similar meanings, and perceiving and synthesizing speech were presented, and a chip being engineered for real-time classification of Reports on the 2006 AAAI Fall Symposia
- Rong Zhang, Alexander I. Rudnicky, Tanja Schultz, R. Stern, Karthik Venkat Ramanan. 2006. Improving the Performance of LVCSR Using Ensemble of Acoustic Models. Abstract: Recent advances in Machine Learning have brought to attention new theories of learning as well as new approaches. Among these, the Ensemble method has received wide attention and has been shown to be a promising method for classification problems. Simply speaking, the ensemble method is a learning algorithm that constructs a set of “weak” classifiers and then combines their predictions to produce a more accurate classification. The underlying idea of the ensemble method is that the combination of diversified classifiers that have uncorrelated, and ideally complementary, error patterns can offer improved performance and a robust generalization capability. Given its successes for many classification problems, we began investigating the problem of adapting ensemble techniques to continuous speech recognition. Continuous Speech Recognition has been acknowledged as one of the most challenging tasks in classification. The performance of an ASR system is negatively impacted by a number of issues, such as corruption of noise, variability of speaker and speaking mode, change of environment conditions, transmission of channel, inaccuracy of model assumption, complexity of language, etc.. The primary goal of our research is to discover methods suitable for ensemble construction and combination that meet these special requirements of continuous speech recognition. We propose several novel ensemble-based acoustic model training and combination schemes, and test their effectiveness using real-world speech corpora. Preliminary results are described in this proposal, in particular • Utterance-level Boosting training algorithm for large scale acoustic modeling • Frame-level Boosting training algorithm using a Word Error Rate reduction criterion • N-Best list re-ranking and Rover combination to generate a better hypothesis Encouraging experimental results convince us that the ensemble technique is a promising method and that it has the potential to substantially improve the performance of a LVCSR system. However research on ensemble methods for speech recognition is still in its early stage and unsolved questions on ensemble generation and hypothesis combination remain to be addressed. This proposal sets out several key research topics that, if successfully addressed will have the potential to significantly increase the accuracy of ensemble-based speech recognition systems. These include the following: • Training criteria targeted at reducing Word Error Rate rather than Sentence Error Rate. • Integrating data manipulation and feature manipulation methods for continuous speech recognition. • Combination methods working on different objects, different levels and different decoding stages. • Ensemble-based semi-supervised acoustic model training algorithm using labeled and unlabeled data.
- David Huggins-Daines, Mohit Kumar, Arthur Chan, A. Black, M. Ravishankar, Alexander I. Rudnicky. 2006. Pocketsphinx: A Free, Real-Time Continuous Speech Recognition System for Hand-Held Devices. Abstract: The availability of real-time continuous speech recognition on mobile and embedded devices has opened up a wide range of research opportunities in human-computer interactive applications. Unfortunately, most of the work in this area to date has been confined to proprietary software, or has focused on limited domains with constrained grammars. In this paper, we present a preliminary case study on the porting and optimization of CMU Sphinx-11, a popular open source large vocabulary continuous speech recognition (LVCSR) system, to hand-held devices. The resulting system operates in an average 0.87 times real-time on a 206 MHz device, 8.03 times faster than the baseline system. To our knowledge, this is the first hand-held LVCSR system available under an open-source license
- S. Banerjee, Alexander I. Rudnicky. 2006. SmartNotes: Implicit Labeling of Meeting Data through User Note-Taking and Browsing. Abstract: We have implemented SmartNotes, a system that automatically acquires labeled meeting data as users take notes during meetings and browse the notes afterwards. Such data can enable meeting understanding components such as topic and action item detectors to automatically improve their performance over a sequence of meetings. The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system. We shall demonstrate the functionalities of this system, and will also demonstrate the labeled data obtained during typical meetings and browsing sessions.
- D. Bohus, B. Langner, A. Raux, A. Black, M. Eskénazi, Alexander I. Rudnicky. 2006. ONLINE SUPERVISED LEARNING OF NON-UNDERSTANDING RECOVERY POLICIES. Abstract: Spoken dialog systems typically use a limited number of non- understanding recovery strategies and simple heuristic policies to engage them (e.g. first ask user to repeat, then give help, then transfer to an operator). We propose a supervised, online method for learning a non-understanding recovery policy over a large set of recovery strategies. The approach consists of two steps: first, we construct runtime estimates for the likelihood of success of each recovery strategy, and then we use these estimates to construct a policy. An experiment with a publicly available spoken dialog system shows that the learned policy produced a 12.5% relative improvement in the non-understanding recovery rate.
- Rong Zhang, Alexander I. Rudnicky. 2006. A New Data Selection Approach for Semi-Supervised Acoustic Modeling. Abstract: Current approaches to semi-supervised incremental learning prefer to select unlabeled examples predicted with high confidence for model re-training. However, this strategy can degrade the classification performance rather than improve it. We present an analysis for the reasons of this phenomenon, showing that only relying on high confidence for data selection can lead to an erroneous estimate to the true distribution when the confidence annotator is highly correlated with the classifier in the information they use. We propose a new data selection approach to address this problem and apply it to a variety of applications, including machine learning and speech recognition. Encouraging improvements in recognition accuracy are observed in our experiments
- S. Banerjee, Alexander I. Rudnicky. 2006. You Are What You Say: Using Meeting Participants’ Speech to Detect their Roles and Expertise. Abstract: Our goal is to automatically detect the functional roles that meeting participants play, as well as the expertise they bring to meetings. To perform this task, we build decision tree classifiers that use a combination of simple speech features (speech lengths and spoken keywords) extracted from the participants' speech in meetings. We show that this algorithm results in a role detection accuracy of 83% on unseen test data, where the random baseline is 33.3%. We also introduce a simple aggregation mechanism that combines evidence of the participants' expertise from multiple meetings. We show that this aggregation mechanism improves the role detection accuracy from 66.7% (when aggregating over a single meeting) to 83% (when aggregating over 5 meetings).
- Rong Zhang, Alexander I. Rudnicky. 2006. A New Data Selection Principle for Semi-Supervised Incremental Learning. Abstract: Current semi-supervised incremental learning approaches select unlabeled examples with predicted high confidence for model re-training. We show that for many applications this data selection strategy is not correct. This is because the confidence score is primarily a metric to measure the classification correctness on a particular example, rather than one to measure the example's contribution to the training of an improved model, especially in the case that the information used in the confidence annotator is correlated with that generated by the classifier. To address this problem, we propose a performance-driven principle for unlabeled data selection in which only the unlabeled examples that help to improve classification accuracy are selected for semi-supervised learning. Encouraging results are presented for a variety of public benchmark datasets
- D. Bohus, Alexander I. Rudnicky. 2006. A “K Hypotheses + Other” Belief Updating Model. Abstract: Spoken dialog systems typically rely on recognition confidence scores to guard against potential misunderstandings. While confidence scores can provide an initial assessment for the reliability of the information obtained from the user, ideally systems should leverage information that is available in subsequent user responses to update and improve the accuracy of their beliefs. We present a machine-learning based solution for this problem. We use a compressed representation of beliefs that tracks up to k hypotheses for each concept at any given time. We train a generalized linear model to perform the updates. Experimental results show that the proposed approach significantly outperforms heuristic rules used for this task in current systems. Furthermore, a user study with a mixed-initiative spoken dialog system shows that the approach leads to significant gains in task success and in the efficiency of the interaction, across a wide range of recognition error-rates.
- Rong Zhang, Alexander I. Rudnicky. 2006. FOR SEMI-SUPERVISED ACOUSTIC MODELING. Abstract: Current approaches to semi-supervised incremental learning prefer to select unlabeled examples predicted with high confidence for model re-training. However, this strategy can degrade the classification performance rather than improve it. We present an analysis for the reasons of this phenomenon, showing that only relying on high confidence for data selection can lead to an erroneous estimate to the true distribution when the confidence annotator is highly correlated with the classifier in the information they use. We propose a new data selection approach to address this problem and apply it to a variety of applications, including machine learning and speech recognition. Encouraging improvements in recognition accuracy are observed in our experiments.
- M. Dias, T. Harris, Brett Browning, E. Jones, B. Argall, M. Veloso, A. Stentz, Alexander I. Rudnicky. 2006. Dynamically Formed Human-Robot Teams Performing Coordinated Tasks. Abstract: In this new era of space exploration where human-robot teams are envisioned maintaining a long-term presence on other planets, effective coordination of these teams is paramount. Three critical research challenges that must be solved to realize this vision are the human-robot team challenge, the pickup-team challenge, and the effective humanrobot communication challenge. In this paper, we address these challenges, propose a novel approach towards solving these challenges, and situate our approach in the newly introduced treasure hunt domain.
- Derek P. Brock, R. Duraiswami, Alexander I. Rudnicky. 2006. Aurally informed performance : integrating machine listening and auditory presentation in robotic systems : Papers from the AAAI Fall Symposium : Technical Report FS-06-01. Abstract: AAAI maintains compilation copyright for this technical report and retains the right of first refusal to any publication (including electronic distribution) arising from this AAAI event. Please do not make any inquiries or arrangements for hardcopy or electronic publication of all or part of the papers contained in these working notes without first exploring the options available through AAAI Press and AI Magazine (concurrent submission to AAAI and an another publisher is not acceptable). A signed release of this right by AAAI is required before publication by a third party.
- David Huggins-Daines, Alexander I. Rudnicky. 2006. A constrained baum-welch algorithm for improved phoneme segmentation and efficient training. Abstract: We describe an extension to the Baum-Welch algorithm for training Hidden Markov Models that uses explicit phoneme segmentation to constrain the forward and backward lattice. The HMMs trained with this algorithm can be shown to improve the accuracy of automatic phoneme segmentation. In addition, this algorithm is significantly more computationally efficient than the full BaumWelch algorithm, while producing models that achieve equivalent accuracy on a standard phoneme recognition task.
- T. Harris, S. Banerjee, Alexander I. Rudnicky. 2005. Heterogeneous Multi-Robot Dialogues for Search Tasks. Abstract: Dialogue agents are often designed with the tacit assumption that at any one time, there is but one agent and one human, and that their communication channel is exclusive. We are interested in examining situations in which multiple heterogeneous dialogue agents need to interact with a human interlocutor, and where the communication channel becomes necessarily shared. To this end we have constructed a multi-agent dialogue test-bed on which to study dialogue coordination issues in multi-
- Arthur Chan, M. Ravishankar, Alexander I. Rudnicky. 2005. On improvements to CI-based GMM selection. Abstract: Gaussian Mixture Model (GMM) computation is known to be one of the most computation-intensive components of speech recognition. In our previous work, context-independent model based GMM selection (CIGMMS) was found to be an effective way to reduce the cost of GMM computation without significant loss in recognition accuracy. In this work, we propose three methods to further improve the performance of CIGMMS. Each method brings an additional 5-10% relative speed improvement, with a cumulative improvement up to 37% on some tasks. Detailed analysis and experimental results on three corpora are presented.
- Nikesh Garera, Alexander I. Rudnicky. 2005. Briefing Assistant: Learning Human Summarization Behavior over Time. Abstract: We describe a system intended to help report writers produce summaries of important activities based on weekly interviews with members of a project. A key element of this system is to learn different user and audience preferences in order to produce tailored summaries. The system learns desired qualities of summaries based on observation of user selection behavior, and builds a regression-based model using item features as parameters. The system’s assistance consists of presenting the writer with a successively better ordered list of items from which to choose. Our evaluation study indicates a significant improvement in average precision (and other metrics) by the end of the learning period as compared to baseline of no learning. We also describe our ongoing work on automatic feature extraction to make this approach domain independent.
- D. Bohus, Alexander I. Rudnicky. 2005. Sorry and I Didn’t Catch That! - An Investigation of Non-understanding Errors and Recovery Strategies. Abstract: We present results from an extensive empirical analysis of non-understanding errors and ten non-understanding recovery strategies, based on a corpus of dialogs collected with a spoken dialog system that handles conference room reservations. More specifically, the issues we investigate are: what are the main sources of non-understanding errors? What is the impact of these errors on global performance? How do various strategies for recovery from non-understandings compare to each other? What are the relationships between these strategies and subsequent user response types, and which response types are more likely to lead to successful recovery? Can dialog performance be improved by using a smarter policy for engaging the non-understanding recovery strategies? If so, can we learn such a policy from data? Whenever available, we compare and contrast our results with other studies in the literature. Finally, we summarize the lessons learned and present our plans for future work inspired by this analysis.
- D. Bohus, Alexander I. Rudnicky. 2005. A principled approach for rejection threshold optimization in spoken dialog systems. Abstract: A common design pattern in spoken dialog systems is to reject an input when the recognition confidence score falls below a preset rejection threshold. However, this introduces a potentially non-optimal tradeoff between various types of errors such as misunderstandings and false rejections. In this paper, we propose a data-driven method for determining the relative costs of these errors, and then use these costs to optimize state-specific rejection thresholds. We illustrate the use of this approach with data from a spoken dialog system that handles conference room reservations. The results obtained confirm our intuitions about the costs of the errors, and are consistent with anecdotal evidence gathered throughout the use of the system.
- D. Bohus, Alexander I. Rudnicky. 2005. Error Handling in the RavenClaw Dialog Management Architecture. Abstract: We describe the error handling architect-ture underlying the RavenClaw dialog management framework. The architecture provides a robust basis for current and future research in error detection and recovery. Several objectives were pursued in its development: task-independence, ease-of-use, adaptability and scalability. We describe the key aspects of architectural de-sign which confer these properties, and discuss the deployment of this architect-ture in a number of spoken dialog systems spanning several domains and interaction types. Finally, we outline current research projects supported by this architecture.
- Stefanie Tomko, T. Harris, Arthur R. Toth, James Sanders, Alexander I. Rudnicky, R. Rosenfeld. 2005. Towards efficient human machine speech communication: The speech graffiti project. Abstract: This research investigates the design and performance of the Speech Graffiti interface for spoken interaction with simple machines. Speech Graffiti is a standardized interface designed to address issues inherent in the current state-of-the-art in spoken dialog systems such as high word-error rates and the difficulty of developing natural language systems. This article describes the general characteristics of Speech Graffiti, provides examples of its use, and describes other aspects of the system such as the development toolkit. We also present results from a user study comparing Speech Graffiti with a natural language dialog system. These results show that users rated Speech Graffiti significantly better in several assessment categories. Participants completed approximately the same number of tasks with both systems, and although Speech Graffiti users often took more turns to complete tasks than natural language interface users, they completed tasks in slightly less time.
- D. Bohus, Alexander I. Rudnicky. 2005. Error handling in the RavenClaw dialog management framework. Abstract: We describe the error handling architectture underlying the RavenClaw dialog management framework. The architecture provides a robust basis for current and future research in error detection and recovery. Several objectives were pursued in its development: task-independence, ease-of-use, adaptability and scalability. We describe the key aspects of architectural design which confer these properties, and discuss the deployment of this architectture in a number of spoken dialog systems spanning several domains and interaction types. Finally, we outline current research projects supported by this architecture.
- D. Bohus, Alexander I. Rudnicky. 2005. Constructing accurate beliefs in spoken dialog systems. Abstract: We propose a novel approach for constructing more accurate beliefs over concept values in spoken dialog systems by integrating information across multiple turns in the conversation. In particular, we focus our attention on updating the confidence score of the top hypothesis for a concept, in light of subsequent user responses to system confirmation actions. Our data-driven approach bridges previous work in confidence annotation and correction detection, providing a unified framework for belief updating. The approach significantly outperforms heuristic rules currently used in most spoken dialog systems
- Rong Zhang, Ziad Al Bawab, Arthur Chan, A. Chotimongkol, David Huggins-Daines, Alexander I. Rudnicky. 2005. Investigations on ensemble based semi-supervised acoustic model training. Abstract: Semi-supervised learning has been recognized as an effective way to improve acoustic model training in cases where sufficient transcribed data are not available. Different from most of existing approaches only using single acoustic model and focusing on how to refine it, this paper investigates the feasibility of using ensemble methods for semi-supervised acoustic modeling training. Two methods are investigated here, one is a generalized Boosting algorithm, a second one is based on data partitions. Both methods demonstrate substantial improvement over baseline. More than 15% relative reduction of word error rate was observed in our experiments using a large real-world meeting recognition dataset.
- Alexander I. Rudnicky, P. Rybski, S. Banerjee, Francisco Veloso. 2005. Intelligently Integrating Information from Speech and Vision Processing to Perform Light-weight Meeting Understanding. Abstract: Important information is often generated at meetings but identifying, and retrieving that information after the meeting is not always simple. Automatically capturing such information and making it available for later retrieval has therefore become a topic of some interest. Most approaches to this problem have involved constructing specialized instrumented meeting rooms that allow a meeting to be captured in great detail. We propose an alternate approach that focuses on people’s information retrieval needs and makes use of a light-weight data collection system that allows data acquisition on portable equipment, such as personal laptops. Issues that arise include the integration of information from different audio and video streams and optimum use of sparse computing resources. This paper describes our current development of a light-weight portable meeting recording infrastructure, as well as the use of streams of visual and audio information to derive structure from meetings. The goal is to make meeting contents easily accessible to people.
- Rong Zhang, Alexander I. Rudnicky. 2004. Apply n-best list re-ranking to acoustic model combinations of boosting training. Abstract: The object function for Boosting training method in acoustic modeling aims to reduce utterance level error rate. This is different from the most commonly used performance metric in speech recognition, word error rate. This paper proposes that the combination of N-best list re-ranking and ROVER can partly address this problem. In particular, model combination is applied to re-ranked hypotheses rather than to the original top-1 hypotheses and carried on word level. Improvement of system performance is observed in our experiments. In addition, we describe and evaluate a new confidence feature that measures the correctness of frame level decoding result.
- D. Bohus, Alexander I. Rudnicky. 2004. Users’ Performance and Preferences for Online Graphic, Text and Auditory Presentation of Instructions. Abstract: Traditional technical manuals consist primarily of text supplemented by tabular and graphic presentation of information. In the past decade technical information systems have increasingly been authored for presentation on computers instead of on paper; however a stable set of standards for such manuals has yet to evolve. There are strong beliefs but little empirical evidence to guide standards development within companies producing Interactive Electronic Technical Manuals (IETMs). The current study compares three different modes of instruction presentation for mechanical assembly tasks (graphic, text, and auditory), using a Wizard of Oz paradigm. Study participants preferred graphically-presented information and they completed the tasks fastest using this presentation mode. We found no significant difference in performance or preference between text and audio conditions. Nevertheless users indicated a clear desire that graphic presentation be supplemented by other modes. Study results will be useful for designers of multi-modal interfaces for online instruction systems.
- Arthur Chan, M. Ravishankar, Alexander I. Rudnicky, J. Sherwani. 2004. Four-layer categorization scheme of fast GMM computation techniques in large vocabulary continuous speech recognition systems. Abstract: Large vocabulary continuous speech recognition systems are known to be computationally intensive. A major bottleneck is the Gaussian mixture model (GMM) computation and various techniques have been proposed to address this problem. We present a systematic study of fast GMM computation techniques. As there are a large number of these and it is impractical to exhaustively evaluate all of them, we first categorized techniques into four layers and selected representative ones to evaluate in each layer. Based on this framework of study, we provide a detailed analysis and comparison of GMM computation techniques from the four-layer perspective and explore two subtle practical issues, 1) how different techniques can be combined effectively and 2) how beam pruning will affect the performance of GMM computation techniques. All techniques are evaluated in the CMU Communicator domain. We also compare their performance with others reported in the literature.
- P. Rybski, S. Banerjee, F. D. L. Torre, Carlos Vallespí, Alexander I. Rudnicky, M. Veloso. 2004. Segmentation and classification of meetings using multiple information streams. Abstract: We present a meeting recorder infrastructure used to record and annotate events that occur in meetings. Multiple data streams are recorded and analyzed in order to infer a higher-level state of the group's activities. We describe the hardware and software systems used to capture people's activities as well as the methods used to characterize them.
- Rong Zhang, Alexander I. Rudnicky. 2004. A frame level boosting training scheme for acoustic modeling. Abstract: Conventional Boosting algorithms for acoustic modeling have two notable weaknesses. (1) The objective function aims to minimize utterance error rate, though the goal for most speech recognition systems is to reduce word error rate. (2) During Boosting training, an utterance is treated as a unit for resampling and each frame within the same utterance is assigned equal weight. Intuitively, the frames associated with a misclassified word should be given more emphasis than others. We propose a frame level Boosting training scheme that addresses these shortcomings and allows each frame to have a different weight. We describe a technique and provide experimental results for this approach.
- Rong Zhang, Alexander I. Rudnicky. 2004. Optimizing boosting with discriminative criteria. Abstract: We describe the use of discriminative criteria to optimize Boosting based ensembles. Boosting algorithms may create hundreds of individual classifiers in order to fit the training data. However, this strategy isn’t feasible and necessary for complex classification problems, such as real-time continuous speech recognition, in which only the combination of a few of acoustic models is practical. How to improve the classification accuracy for small size of ensemble is the focus of this paper. Two discriminative criteria that attempt to minimize the true Bayes error rate are investigated. Improvements are observed over a variety of datasets including image and speech recognition, indicating the prospective utility of these two criteria.
- S. Banerjee, Jason Cohen, Thomas R Quisel, Arthur Chan, Yash Patodia, Ziad Al Bawab, Rong Zhang, A. Black, R. Stern, Alexander I. Rudnicky, P. Rybski, M. Veloso. 2004. Creating Multi-Modal, User-Centric Records of Meetings with the Carnegie Mellon Meeting Recorder Architecture. Abstract: Our goal is to build conversational agents that combine information from speech, gesture, hand-writing, text and presentations to create an understanding of the ongoing conversation (e.g. by identifying the action items agreed upon), and that can make useful contributions to the meeting based on such an understanding (e.g. by confirming the details of the action items). To create a corpus of relevant data, we have implemented the Carnegie Mellon Meeting Recorder to capture detailed multi-modal recordings of meetings. This software differs somewhat from other meeting room architectures in that it focuses on instrumenting the individual rather than the room and assumes that the meeting space is not fixed in advance. Thus, most of the sensors are user-centric (closetalking microphones connected to laptop computers, instrumented note-pads, instrumented presentation software, etc), although some are indeed ”room-centric” (instrumented whiteboard, distant cameras, table-top microphones, etc). This paper describes the details of our data collection environment. We report on the current status of our data collection, transcription and higher-level discourse annotation efforts. We also describe some of our initial research on conversational turn-taking based on this corpus.
- S. Banerjee, Alexander I. Rudnicky. 2004. Using simple speech-based features to detect the state of a meeting and the roles of the meeting participants. Abstract: We introduce a simple taxonomy of meeting states and participant roles. Our goal is to automatically detect the state of a meeting and the role of each meeting participant and to do so concurrent with a meeting. We trained a decision tree classifier that learns to detect these states and roles from simple speech–based features that are easy to compute automatically. This classifier detects meeting states 18% absolute more accurately than a random classifier, and detects participant roles 10% absolute more accurately than a majority classifier. The results imply that simple, easy to compute features can be used for this purpose.
- Alexander Hauptmann, Alexander I. Rudnicky. 2004. A Comparison of Speech vs Typed Input. Abstract: We conducted a series of empirical experiments in which users were asked to enter digit strings into the computer by voice or keyboard. Two different ways of verifying and correcting the spoken input were examined. Extensive timing analyses were performed to determine which aspects of the interface were critical to speedy completion of the task. The results show that speech is preferable for strings that require more than a few keystrokes. The results emphasize the need for fast and accurate speech recognition, but also demonstrate how error correction and input validation are crucial for an effective speech interface.
- D. Bohus, Alexander I. Rudnicky. 2003. Ravenclaw: dialog management using hierarchical task decomposition and an expectation agenda. Abstract: We describe RavenClaw, a new dialog management framework developed as a successor to the Agenda [1] architecture used in the CMU Communicator. RavenClaw introduces a clear separation between task and discourse behavior specification, and allows rapid development of dialog management components for spoken dialog systems operating in complex, goal-oriented domains. The system development effort is focused entirely on the specification of the dialog task, while a rich set of domain-independent conversational behaviors are transparently generated by the dialog engine. To date, RavenClaw has been applied to five different domains allowing us to draw some preliminary conclusions as to the generality of the approach. We briefly describe our experience in developing these systems.
- Rong Zhang, Alexander I. Rudnicky. 2003. Comparative study of boosting and non-boosting training for constructing ensembles of acoustic models. Abstract: This paper compares the performance of Boosting and nonBoosting training algorithms in large vocabulary continuous speech recognition (LVCSR) using ensembles of acoustic models. Both algorithms demonstrated significant word error rate reduction on the CMU Communicator corpus. However, both algorithms produced comparable improvements, even though one would expect that the Boosting algorithm, which has a solid theoretic foundation, should work much better than the non-Boosting algorithm. Several voting schemes for hypothesis combining were evaluated, including weighted voting, un-weighted voting and ROVER.
- Rong Zhang, Alexander I. Rudnicky. 2003. Improving the performance of an LVCSR system through ensembles of acoustic models. Abstract: This paper describes our work on applying ensembles of acoustic models to the problem of large vocabulary continuous speech recognition (LVCSR). We propose three algorithms for constructing ensembles. The first two have their roots in bagging algorithms; however, instead of randomly sampling examples our algorithms construct training sets based on the word error rate. The third one is a boosting style algorithm. Different from other boosting methods which demand large resources for computation and storage, our method present a more efficient solution suitable for acoustic model training. We also investigate a method that seeks optimal combination for models. We report experimental results on a large real world corpus collected from the Carnegie Mellon Communicator dialog system. Significant improvements on system performance are observed in that up to 15.56% relative reduction on word error rate is achieved.
- Christina L. Bennett, A. F. Llitjós, Stefanie Shriver, Alexander I. Rudnicky, A. Black. 2002. Building voiceXML-based applications. Abstract: Abstract : The Language Technologies Institute (LTI) at Carnegie Mellon University has, for the past several years, conducted a lab course in building spoken-language dialog systems. In the most recent versions of the course, we have used (commercial) web-based development environments to build systems. This paper describes our experiences and discusses the characteristics of applications that are developed within this framework.
- Christina L. Bennett, Alexander I. Rudnicky. 2002. The carnegie mellon communicator corpus. Abstract: As part of the DARPA Communicator program, Carnegie Mellon has, over the past three years, collected a large corpus of speech produced by callers to its Travel Planning system. To date, a total of 180,605 utterances (90.9 hours) have been collected. The data were used for a number of purposes, including acoustic and language modeling and the development of a spoken dialog system. The collection, transcription and annotation of these data prompted us to develop a number of procedures for managing the transcription process and for ensuring accuracy. We describe these, as well as some results based on these data. A portion of this corpus, covering the years 1999-2001, is being published for research purposes.
- M. Walker, Alexander I. Rudnicky, J. Aberdeen, Elizabeth Owen Bratt, J. Garofolo, H. Hastie, Audrey N. Le, B. Pellom, A. Potamianos, R. Passonneau, R. Prasad, S. Roukos, G. Sanders, S. Seneff, D. Stallard. 2002. DARPA communicator evaluation: progress from 2000 to 2001. Abstract: ABSTRACTThis paper describes the evaluation methodology and resultsof the DARPA Communicator spoken dialog system evaluationexperiments in 2000 and 2001. Nine spoken dialog systems in thetravel planning domain participated in the experiments resulting ina total corpus of 1904 dialogs. We describe and compare the ex-perimental design of the 2000 and 2001 DARPA evaluations. Wedescribe how we established a performance baseline in 2001 forcomplex tasks. We present our overall approach to data collection,the metrics collected, and the application of PARADISE to thesedata sets. We compare the results we achieved in 2000 for a num-ber of core metrics with those for 2001. These results demonstratelarge performance improvements from 2000 to 2001 and show thatthe Communicator program goal of conversational interaction forcomplex tasks has been achieved.1. INTRODUCTIONThe objective of the DARPA Communicator project is to supportrapid development of multi-modal speech-enabled dialog systemswith advanced conversational capabilities. Figure 1 illustrates theCommunicator challenge problem; asystem must support complexconversational interaction to complete this task within 10 minutes.You are in Denver, Friday night at 8pm on the road to the air-port after a great meeting. As a result of the meeting, youneed to attend a group meeting in San Diego on Point Lomaon Monday at 8:30, a meeting Tuesday morning at Miramar at7:30, then one from 3-5 pm in Monterey; you need reservations(car, hotel, air).You pull over to the side of the road and whip out your Com-municator. Through spoken dialog (augmented with a displayand pointing), you make the appropriate reservations, discovera conﬂict, and send an e-mail message (dictated) to inform thegroup of the changed schedule. Do this in 10 minutes.Fig. 1. Darpa Communicator Challenge ProblemDuring the course of the Communicator program, we havebeen involved in developing methods for measuring progress to-wards the program goals and assessing advances in the componenttechnologies required to achieve such goals. In previous work, wereport on an exploratory data collection experiment with nine par-ticipating Communicator systems in the travel planning domain
- Rong Zhang, Alexander I. Rudnicky. 2002. A large scale clustering scheme for kernel K-Means. Abstract: Kernel functions can be viewed as a non-linear transformation that increases the separability of the input data by mapping them to a new high dimensional space. The incorporation of kernel functions enables the K-Means algorithm to explore the inherent data pattern in the new space. However, the previous applications of the kernel K-Means algorithm are confined to small corpora due to its expensive computation and storage cost. To overcome these obstacles, we propose a new clustering scheme which changes the clustering order from the sequence of samples to the sequence of kernels, and employs a disk-based strategy to control data. The new clustering scheme has been demonstrated to be very efficient for a large corpus by our experiments on handwritten digits recognition, in which more than 90% of the running time was saved.
- R. Frederking, E. Steinbrecher, Ralf D. Brown, Alexander I. Rudnicky, J. Moody. 2002. Speech Translation on a Tight Budget without Enough Data. Abstract: The Tongues speech-to-speech translation system was developed for the US Army chaplains, with fairly stringent constraints on time, budget, and available data. The resulting prototype was required to undergo a quite realistic field test. We describe the development and architecture of the system, the field test, and our analysis of its results. The system performed quite well, especially given its development constraints.
- D. Bohus, Alexander I. Rudnicky. 2002. Integrating Multiple Knowledge Sources for Utterance-Level Confidence Annotation in the CMU Communicator Spoken Dialog System. Abstract: Abstract : In the recent years, automated speech recognition has been the main drive behind the advent of spoken language interfaces, but at the same a time a severe limiting factor in the development of these systems. We believe that increased robustness in the face of recognition errors can be achieved by making the systems aware of their own misunderstandings, and employing appropriate recovery techniques when breakdowns in interacted occur. In this paper we address the first problem: the development of an utterance-level confidence annotator for a spoken dialog system. After a brief introduction to the CMU Communicator spoken dialog system (which provided the target platform for the developed annotator), we cast the confidence annotation problem as a machine learning classification task, and focus on selecting relevant features and on empirically identifying the best classification techniques for this task. The results indicate that significant reductions in classification error rate can be obtained using several different classifiers. Furthermore, we propose a data driven approach to assessing the impact of the errors committed by the confidence annotator on dialog performance, with a view to optimally fine-tuning the annotator. Several models were constructed, and the resulting error costs were in accordance with our intuition. We found, surprisingly, that, at least for a mixed-initiative spoken dialog system as the CMU Communicator, these errors trade-all equally over a wide operating characteristic range.
- M. Walker, Alexander I. Rudnicky, R. Prasad, J. Aberdeen, Elizabeth Owen Bratt, J. Garofolo, H. Hastie, Audrey N. Le, B. Pellom, A. Potamianos, R. Passonneau, S. Roukos, G. Sanders, S. Seneff, D. Stallard. 2002. DARPA communicator: cross-system results for the 2001 evaluation. Abstract: This paper describes the evaluation methodology and results of the 2001 DARPA Communicator evaluation. The experiment spanned 6 months of 2001 and involved eight DARPA Communicator systems in the travel planning domain. It resulted in a corpus of 1242 dialogs which include many more dialogues for complex tasks than the 2000 evaluation. We describe the experimental design, the approach to data collection, and the results. We compare the results by the type of travel plan and by system. The results demonstrate some large differences across sites and show that the complex trips are clearly more difficult.
- A. Chotimongkol, Alexander I. Rudnicky. 2002. Automatic concept identification in goal-oriented conversations. Abstract: We address the problem of identifying key domain concepts automatically from an unannotated corpus of goal-oriented human-human conversations. We examine two clustering algorithms, one based on mutual information and another one based on Kullback-Liebler distance. In order to compare the results from both techniques quantitatively, we evaluate the outcome clusters against reference concept labels using precision and recall metrics adopted from the evaluation of topic identification task. However, since our system allows more than one cluster to associate with each concept an additional metric, a singularity score, is added to better capture cluster quality. Based on the proposed quality metrics, the results show that Kullback-Liebler-based clustering outperforms mutual information-based clustering for both the optimal quality and the quality achieved using an automatic stopping criterion.
- Rong Zhang, Alexander I. Rudnicky. 2002. Improve latent semantic analysis based language model by integrating multiple level knowledge. Abstract: We describe an extension to the use of Latent Semantic Analysis (LSA) for language modeling. This technique makes it easier to exploit long distance relationships in natural language for which the traditional n-gram is unsuited. However, with the growth of length, the semantic representation of the history may be contaminated by irrelevant information, increasing the uncertainty in predicting the next word. To address this problem, we propose a multilevel framework dividing the history into three levels corresponding to document, paragraph and sentence. To combine the three levels of information with the n-gram, a Softmax network is used. We further present a statistical scheme that dynamically determines the unit scope in the generalization stage. The combination of all the techniques leads to a 14% perplexity reduction on a subset of Wall Street Journal, compared with the trigram model.
- A. Black, Ralf D. Brown, R. Frederking, K. Lenzo, J. Moody, Alexander I. Rudnicky, Rita Singh, E. Steinbrecher. 2002. RAPID DEVLOPEMENT OF SPEECH-TO-SPEECH TRANSLATION SYSTEMS. Abstract: This paper describes building of the basic components, par-ticularly speech recognition and synthesis, of a speech-to-speech translation system. This work is described within the framework of the “Tongues: small footprint speech-to-speech translation device” developed at CMU and Lockheed Martin for use by US Army Chaplains.
- Christina L. Bennett, A. F. Llitjós, Stefanie Shriver, Alexander I. Rudnicky. 2002. BUILDING VOICEXML-BAS. Abstract: The Language Technologies Institute (LTI) at Carnegie Mellon University has, for the past several years, conducted a lab course in building spoken-language dialog systems. In the most recent versions of the course, we have used (commercial) webbased development environments to build systems. This paper describes our experiences and discusses the characteristics of applications that are developed within this framework.
- Alice H. Oh, Alexander I. Rudnicky. 2002. Submitted to Computer Speech and Language Stochastic Natural Language Generation for Spoken Dialog Systems. Abstract: We describe a corpus-based approach to natural language generation (NLG). The approach has been implemented as a component of a spoken dialog system and a series of evaluations were carried out. Our system uses n-gram language models, which have been found useful in other language technology applications, in a generative mode. It is not yet clear whether the simple n-grams can adequately model human language generation in general, but we show that we can successfully apply this ubiquitous modeling technique to the task of natural language generation for spoken dialog systems. In this paper, we discuss applying corpus-based stochastic language generation at two levels: content selection and sentence planning/realization. At the content selection level, output utterances are modeled by bigrams, and the appropriate attributes are chosen using bigram statistics. In sentence planning and realization, corpus utterances are modeled by n-grams of varying length, and new utterances are generated stochastically. Through this work, we show that a simple statistical model alone can generate appropriate language for a spoken dialog system. The results describe a promising avenue for using a statistical approach in future NLG systems. A. H. Oh: Stochastic Natural Language Generation 3 Natural Language Understanding Natural Language Generation Surface Realization Semantic (Syntactic) Representation Semantic (Syntactic) Representation Surface Realization Figure 1: NLU and NLG
- Alexander I. Rudnicky, Alice H. Oh. 2002. Dialog Annotation for Stochastic Generation. Abstract: Individuals who successfully make their livelihood by talking with others, for example travel agents, can be presumed to have optimized their language for the task at hand in terms of conciseness and intelligibility. It makes sense to exploit this effort for the purpose of building better generation components for a spoken dialog system. The Stochastic Generation technique, introduced by Oh and Rudnicky (2002), is one such approach. In this approach, utterances in a corpus of domain expert utterances are classified as to speech act and individual concepts tagged. Statistical n-gram models are built for each speech-act class then used generatively to create novel utterances. These have been shown to be comparable in quality to human productions. The class and tag scheme is concrete and closely tied to the domain at hand; we believe this produces a distinct advantage in speed of implementation and quality of results. The current paper describes the classification and tagging procedures used for Stochastic Generation, and discusses the advantages and limitations of the techniques.
- M. Walker, Alex Rudnicky, R. Prasad, J. Aberdeen, Elizabeth Owen Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom, Alex Potamianos, R. Passonneau, S. Roukos, Greg Sanders, S. Seneff, Dave Stallard, M. A. Walker, Alexander I. Rudnicky, R. Prasad, J. Aberdeen, E. Owen Bratt, J. Garofolo, H. Hastie, A. Le, B. Pellom, A. Potamianos, R. Passonneau, S. Roukos, G. Sanders, S. Seneff, D. Stallard, Alex Potami-Anos. 2002. DARPA Communicator Evaluation : Progress from 2000 to 2001. Abstract: This paper describes the evaluation methodology and results of the DARPA Communicator spoken dialog system evaluation experiments in 2000 and 2001. Nine spoken dialog systems in the travel planning domain participated in the experiments resulting in a total corpus of 1904 dialogs. We describe and compare the experimental design of the 2000 and 2001 DARPA evaluations. We describe how we established a performance baseline in 2001 for complex tasks. We present our overall approach to data collection, the metrics collected, and the application of PARADISE to these data sets. We compare the results we achieved in 2000 for a number of core metrics with those for 2001. These results demonstrate large performance improvements from 2000 to 2001 and show that the Communicator program goal of conversational interaction for complex tasks has been achieved.
- M. Walker, J. Aberdeen, Julie E. Boland, Elizabeth Owen Bratt, J. Garofolo, L. Hirschman, Audrey N. Le, Sungbok Lee, Shrikanth S. Narayanan, K. Papineni, B. Pellom, J. Polifroni, A. Potamianos, P. Prabhu, Alexander I. Rudnicky, G. Sanders, S. Seneff, D. Stallard, S. Whittaker. 2001. DARPA communicator dialog travel planning systems: the june 2000 data collection. Abstract: This paper describes results of an experiment with 9 different DARPA Communicator Systems who participated in the June 2000 data collection. All systems supported travel planning and utilized some form of mixed-initiative interaction. However they varied in several critical dimensions: (1) They targeted different back-end databases for travel information; (2) The used different modules for ASR, NLU, TTS and dialog management. We describe the experimental design, the approach to data collection, the metrics collected, and results comparing the systems.
- Stefanie Shriver, Arthur R. Toth, Xiaojin Zhu, Alexander I. Rudnicky, R. Rosenfeld. 2001. A unified design for human-machine voice interaction. Abstract: We describe a unified design for voice interaction with simple machines; discuss the motivation for and main features of the approach, include a short sample interaction, and report the results of two preliminary experiments.
- Stefanie Shriver, R. Rosenfeld, Xiaojin Zhu, Arthur R. Toth, Alexander I. Rudnicky, M. Flueckiger. 2001. Universalizing speech: notes from the USI project. Abstract: This paper discusses progress in designing a standardized interface for speech interaction with simple machines – the Universal Speech Interface (USI) project. We discuss the motivation for such a design and issues that must be addressed by such an interface. We present our current proposals for handling these issues, and comment on the usability of these approaches based on user interactions with the system. Finally, we discuss future work and plans for the USI project.
- R. Rosenfeld, D. Olsen, Alexander I. Rudnicky. 2001. Universal speech interfaces. Abstract: In recent years speech recognition has reached the point of commercial viability realizable on any off-the-shelf computer. This is a goal that has long been sought by both the research community and by prospective users. Anyone who has used these technologies understands that the recognition has many flaws and there is much still to be done. The recognition algorithms are not the whole story. There is still the question of how speech can and should actually be used. Related to this is the issue of tools for development of speech-based applications. Achieving reliable, accurate speech recognition is similar to building an inexpensive mouse and keyboard. The underlying input technology is available but the question of how to build the application interface still remains. We have been considering these problems for some time [Rosenfeld et. al., 2000a]. In this paper we present some of our thoughts about the future of speech-based interaction. This paper is not a report of results we have obtained, but rather a vision of a future to be explored.
- T. M. DuBois, Alexander I. Rudnicky. 2001. CONCEPT METRIC FOR ASSESSING DIALOG SYSTEM COMPLEXITY. Abstract: Techniques for assessing dialog system performance commonly focus on characteristics of the interaction, using metrics such as completion, satisfaction or time on task. However, such metrics are not always capable of differentiating systems that operate on fundamentally different principles, particularly when tested on tasks that focus on common-denominator capabilities. We introduce a new metric, the open concept count, and show how it can be used to capture useful system properties of a dialog system.
- T. M. DuBois, Alexander I. Rudnicky. 2001. An open concept metric for assessing dialog system complexity. Abstract: Techniques for assessing dialog system performance commonly focus on characteristics of the interaction, using metrics such as completion, satisfaction or time on task. However, such metrics are not always capable of differentiating systems that operate on fundamentally different principles, particularly when tested on tasks that focus on common-denominator capabilities. We introduce a new metric, the open concept count, and show how it can be used to capture useful system properties of a dialog system.
- A. Chotimongkol, Alexander I. Rudnicky. 2001. N-best speech hypotheses reordering using linear regression. Abstract: We propose a hypothesis reordering technique to improve speech recognition accuracy in a dialog system. For such systems, additional information external to the decoding process itself is available, in particular features derived from the parse and the dialog. Such features can be combined with recognizer features by means of a linear regression model to predict the most likely entry in the hypothesis list. We introduce the use of concept error rate as an alternative accuracy measurement and compare it withy the use of word error rate. The proposed model performs better than human subjects performing the same hypothesis reordering task.
- Rong Zhang, Alexander I. Rudnicky. 2001. Word level confidence annotation using combinations of features. Abstract: This paper describes the development of a word-level confidence metric suitable for use in a dialog system. Two aspects of the problems are investigated: the identification of useful features and the selection of an effective classifier. We find that two parse-level features, Parsing-Mode and SlotBackoff-Mode, provide annotation accuracy comparable to that observed for decoder-level features. However, both decoderlevel and parse-level features independently contribute to confidence annotation accuracy. In comparing different classification techniques, we found that Support Vector Machines (SVMs) appear to provide the best accuracy. Overall we achieve 39.7% reduction in annotation uncertainty for a binary confidence decision in a travel-planning domain.
- D. Bohus, Alexander I. Rudnicky. 2001. Modeling the cost of misunderstanding errors in the CMU Communicator dialog system. Abstract: We describe a data-driven approach that allows us to quantify the costs of various types of errors made by the utterance-level confidence annotator in the Carnegie Mellon Communicator system. Knowing these costs we can determine the optimal tradeoff point between these errors, and tune the confidence annotator accordingly. We describe several models, based on concept transmission efficiency. The models fit our data quite well and the relative costs of errors are in accordance with our intuition. We also find, surprisingly, that for a mixed-initiative system such as the CMU Communicator, false positive and false negative errors trade-off equally over a wide operating range.
- Paul Carpenter, Chunxiang Jin, Daniel Wilson, Rong Zhang, D. Bohus, Alexander I. Rudnicky. 2001. Is this conversation on track?. Abstract: Confidence annotation allows a spoken dialog system to accurately assess the likelihood of misunderstanding at the utterance level and to avoid breakdowns in interaction. We describe experiments that assess the utility of features from the decoder, parser and dialog levels of processing. We also investigate the effectiveness of various classifiers, including Bayesian Networks, Neural Networks, SVMs, Decision Trees, AdaBoost and Naive Bayes, to combine this information into an utterancelevel confidence metric. We found that a combination of a subset of the features considered produced promising results with several of the classification algorithms considered, e.g., our Bayesian Network classifier produced a 45.7% relative reduction in confidence assessment error and a 29.6% reduction relative to a handcrafted rule.
- Wei Xu, Alexander I. Rudnicky. 2000. Task-based dialog management using an agenda. Abstract: Dialog management addresses two specific problems: (1) providing a coherent overall structure to interaction that extends beyond the single turn, (2) correctly managing mixed-initiative interaction. We propose a dialog management architecture based on the following elements: handlers that manage interaction focussed on tightly coupled sets of information, a product that reflects mutually agreed-upon information and an agenda that orders the topics relevant to task completion.
- Alexander I. Rudnicky, Christina L. Bennett, A. Black, A. Chotimongkol, K. Lenzo, Alice H. Oh, Rita Singh. 2000. Task and domain specific modelling in the Carnegie Mellon communicator system. Abstract: The Carnegie Mellon Communicator is a telephone-based dialog system that supports planning in a travel domain. The implementation of such a system requires two complimentary components, an architecture capable of managing interaction and the task, as well as a knowledge base that captures the speech, language and task characteristics specific to the domain. Given a suitable architecture, the principal effort in development in taken up in the acquisition and processing of a domain knowledge base. This paper describes a variety of techniques we have applied to modeling in acoustic, language, task, generation and synthesis components of the system.
- Alice H. Oh, Alexander I. Rudnicky. 2000. Stochastic Language Generation for Spoken Dialogue Systems. Abstract: The two current approaches to language generation, template-based and rule-based (linguistic) NLG, have limitations when applied to spoken dialogue systems, in part because they were developed for text generation. In this paper, we propose a new corpus-based approach to natural language generation, specifically designed for spoken dialogue systems.
- P. Constantinides, Alexander I. Rudnicky. 1999. Dialog analysis in the carnegie mellon communicator. Abstract: In this paper, we present a formative evaluation procedure that we have applied to the Communicator dialog system. In the system improvement process, we have recognized the need to identify interaction failures through passive observation of system use. By systematizing the process of dialog evaluation, we hope to gain a mechanism for effectively communicating descriptions of interaction failures, specifically for use in system improvement. Additionally, we argue that this process can be taught to and executed by an evaluator external to the system development process, with the same proficiency as someone intimately familiar with the mechanics of the system components.
- Alexander I. Rudnicky, Eric H. Thayer, P. Constantinides, C. Tchou, R. Shern, K. Lenzo, W. Xu, Alice H. Oh. 1999. Creating natural dialogs in the carnegie mellon communicator system. Abstract: The Carnegie Mellon Communicator system helps users create complex travel itineraries through a conversational interface. Itineraries consist of (multi-leg) flights, hotel and car reser-vations and are built from actual travel information for North America, obtained from the Web. The system manages dialog using a schema-based approach. Schemas correspond to major units of task information (such as a flight leg) and define conversational topics, or foci of interaction, meaningful to the user.
- Alexander I. Rudnicky. 1999. AN AGENDA-BASED DIALOG MANAGEMENT ARCHITECTURE FOR SPOKEN LANGUAGE SYSTEMS. Abstract: Dialog management can be seen as a solution to two specific problems: (1) providing a coherent overall structure to interaction that extends beyond the single turn, (2) correctly manage mixed-initiative interaction, allowing users to guide interaction as per their (not necessarily explicitly shared) goals while allowing the system to guide interaction towards successful completion. We propose a dialog management architecture based on the following elements: handlers that manage interaction focussed on tightly coupled sets of information, a product that reflects mutually agreed-upon information and an agenda that orders the topics relevant to task completion.
- M. Eskénazi, Alexander I. Rudnicky, Karin Gregory, P. Constantinides, R. Brennan, Christina L. Bennett, Jwan Allen. 1999. Data collection and processing in the carnegie mellon communicator. Abstract: In order to create a useful, gracefully functioning system for travel arrangements, we have first observed the task as it is accomplished by a human. We then imitated the human while making the user believe he was dialoguing with an automatic system. As we gradually built our system, we devised ways to assess progress and to detect errors. The following described the manner in which the Carnegie Mellon Communicator was built, data collected, and assessment begun using these criteria.
- R. Frederking, C. Hogan, Alexander I. Rudnicky. 1999. A new approach to the translating telephone. Abstract: The Translating Telephone has been a major goal of speech translation for many years. Previous approaches have attempted to work from limited-domain, fully-automatic translation towards broad-coverage, fully-automatic translation. We are approaching the problem from a different direction: starting with a broad-coverage but not fully-automatic system, and working towards full automation. We believe that working in this direction will provide us with better feedback, by observing users and collecting language data under realistic conditions, and thus may allow more rapid progress towards the same ultimate goal. Our initial approach relies on the wide-spread availability of Internet connections and web browsers to provide a user interface. We describe our initial work, which is an extension of the Diplomat wearable speech translator.
- Bertrand A. Damiba, Alexander I. Rudnicky. 1998. Internationalizing Speech Technology through Language Independent Lexical Acquisition. Abstract: Software internationalization, the process of making software easier to localize for specific languages, has deep implications when applied to speech technology, where the goal of the task lies in the very essence of the particular language. A great deal of work and fine-tuning normally goes into the development of speech software for a single language, say English. This tuning complicates a port to different languages. The inherent identity of a language manifests itself in its lexicon, where its character set, phoneme set, pronunciation rules are revealed. We propose a decomposition of the lexicon building process, into four discrete and sequential steps: (a) Transliteration code points from Unicode. (b) Orthographic standardization rules. (c) Application of grapheme to phoneme rules. (d) Application of phonological rules. In following these steps one should gain accessibility to most of the existing speech/language processing tools, thereby internationalizing one's speech technology. In addition, adhering to this decomposition should allow for a reduction of rule conflicts that often plague the phoneticizing process. Our work makes two main contributions: it proposes a systematic procedure for the internationalization of automatic speech recognition (ASR) systems. It also proposes a particular decomposition of the phoneticization process that facilitates internationalization by non-expert informants.
- P. Constantinides, Scott Hansma, C. Tchou, Alexander I. Rudnicky. 1998. A schema based approach to dialog control. Abstract: Frame-based approaches to spoken language interaction work well for limited tasks such as information access, given that the goal of the interaction is to construct a correct query then execute it. More complex tasks, however, can benefit from more active system participation. We describe two mechanisms that provide this, a modified stack that allows the system to track multiple topics, and form-specific schema that allow the system to deal with tasks that involve completion of multiple forms. Domain-dependent schema specify system behavior and are executed by a domain-independent engine. We describe implementations for a personal calendar system and for an air travel planning system.
- Bertrand A. Damiba, Alexander I. Rudnicky. 1997. Language-Independent Lexical Acquisition. Abstract: Lexicon construction is at the core of internationalizing speech systems, as it is the locus at which the correspondence between the written and spoken forms of a language is specified. For the most part, speech systems for a given language benefit from the attention of native speakers and the opportunity to tune performance over time, allowing the cost of lexicon development to be amortized over time. On the other hand rapid deployment of recognition capability for new languages stresses the need for rapid availability of a usable lexicon. We propose a decomposition of the lexicon building process, into four discrete and sequential steps that simplify and speed up the creation of language knowledge bases for recognition and synthesis. Results from four languages are discussed.
- Alexander I. Rudnicky. 1996. Speech Interface Guidelines. Abstract: This document provides an overview of speech interface design principles as applied to the range of applications that have been developed at Carnegie Mellon. For the most part these are workstation-based applications based on spoken language understanding technology. Nevertheless the guidelines should be applicable to a wider range of applications. Speech interfaces have two properties not normally found in more mature interface technologies:
- Alexander I. Rudnicky, Stephen Reed, Eric H. Thayer. 1996. SPEECHWEAR: a mobile speech system. Abstract: We describe a system that allows ambulating users to perform data entry and retrieval using a speech interface to a wearable computer. The interface is a speech-enabled Web browser that allows the user to access both locally stored documents as well as remote ones through a wireless link.
- Alexander I. Rudnicky. 1995. Language Modeling with Limited Domain Data. Abstract: Generic recognition systems contain language models which arerepresentative of a broad corpus. In actual practice, however, recognitionis usually on a coherent text covering a single topic, suggestingthat knowledge of the topic at hand can be used to advantage. A basemodel can be augmented with information from a small sample ofdomain-specific language data to significantly improve recognitionperformance. Good performance may be obtained by merging inonly those n-grams that include words that are out of vocabularywith respect to the base model.
- Alexander Hauptmann, M. Witbrock, Alexander I. Rudnicky. 1995. Speech for multimedia information retrieval. Abstract: We describe the Informediatm News-on-Demand system. News-on-Demand is an innovative example of indexing and searching broadcast video and audio material by text content. The fully-automatic system monitors TV news and allows selective retrieval of news items based on spoken queries. The user then plays the appropriate video "paragraph". The system runs on a Pentium PC using MPEG-I video compression and the Sphinx-II continuous speech recognition system [6].
- D. Dahl, M. Bates, Michael Brown, W. Fisher, Kate Hunicke-Smith, D. S. Pallett, Christine Pao, Alexander I. Rudnicky, Elizabeth Shriberg. 1994. Expanding the Scope of the ATIS Task: The ATIS-3 Corpus. Abstract: The Air Travel Information System (ATIS) domain serves as the common evaluation task for ARPA spoken language system developers. To support this task, the Multi-Site ATIS Data COllection Working group (MADCOW) coordinates data collection activities. This paper describes recent MADCOW activities. In particular, this paper describes the migration of the ATIS task to a richer relational database and development corpus (ATIS-3) and describes the ATIS-3 corpus. The expanded database, which includes information on 46 US and Canadian cities and 23,457 flights, was released in the fall of 1992, and data collection for the ATIS-3 corpus began shortly thereafter. The ATIS-3 corpus now consists of a total of 8297 released training utterances and 3211 utterances reserved for testing, collected at BBN, CMU, MIT, NIST and SRI. 2906 of the training utterances have been annotated with the correct information from the database. This paper describes the ATIS-3 corpus in detail, including breakdowns of data by type (e.g. context-independent, context-dependent, and unevaluable)and variations in the data collected at different sites. This paper also includes a description of the ATIS-3 database. Finally, we discuss future data collection and evaluation plans.
- Alexander I. Rudnicky, Alexander Hauptmann, Kai-Fu Lee. 1994. Survey of current speech technology. Abstract: Speech recognition and speech synthesis are technologies of particular interest for their support of direct communication between humans and computers through a communications mode humans commonly use among themselves and at which they are highly skilled. Both manipulate speech in terms of its information content; recognition transforms human speech into text to be used literally (e.g., for dictation) or interpreted as commands to control applications, and synthesis allows the generation of spoken utterances from text
- L. Hirschman, M. Bates, D. Dahl, W. Fisher, J. Garofolo, D. S. Pallett, Kate Hunicke-Smith, P. Price, Alexander I. Rudnicky, E. Tzoukermann. 1993. Multi-Site Data Collection and Evaluation in Spoken Language Understanding. Abstract: The Air Travel Information System (ATIS) domain serves as the common task for DARPA spoken language system research and development. The approaches and results possible in this rapidly growing area are structured by available corpora, annotations of that data, and evaluation methods. Coordination of this crucial infrastructure is the charter of the Multi-Site ATIS Data COllection Working group (MADCOW). We focus here on selection of training and test data, evaluation of language understanding, and the continuing search for evaluation methods that will correlate well with expected performance of the technology in applications.
- Alexander I. Rudnicky. 1993. Session 1: Spoken Language Systems. Abstract: Without the ability to interpret natural language, speech recognition is suited only for a subset of tasks (though certainly not trivial ones), such as data entry, simple commands or dictation. Similarly, without speech recognition natural language is restricted to the interpretation of written language, a stylized form of human communication. Spoken language systems thus represent an attempt to automate speech communication. While limited in terms of the target behavior, they still represents an advance over the capabilities of the individual technologies.
- Alexander I. Rudnicky. 1993. Factors affecting choice of speech over keyboard and mouse in a simple data-retrieval task. Abstract: This paper describes some recent experiments that assess user mode selection behavior in amulti-modal environment inwhich actions can be performed with equivalent effect by speech, keyboard or scroller. Results indicate that users freely choose speech over other modalities, even when it is less efcient in objective terms, such as time-to-completion or input error. Additional evidence indicates that users appear to focus on simple input time in making their choice of mode, in effect minimizing the amount of personal effort expended.
- Alexander I. Rudnicky. 1993. Mode preference in a simple data-retrieval task. Abstract: This paper describes some recent experiments that assess user behavior in a multi-modal environment in which actions can be performed with equivalent effect in speech, keyboard or scroiler modes. Results indicate that users freely choose speech over other modalities, even when it is less efficient in objective terms, such as time-to-completion or input error.
- S. L. Teal, Alexander I. Rudnicky. 1992. A performance model of system delay and user strategy selection. Abstract: This study lays the ground work for a predictive, zero-parameter engineering model that characterizes the relationship between system delay and user performance. This study specifically investigates how system delays affects a user's selection of task strategy. Strategy selection is hypothesized to be based on a cost function combining two factors: (1) the effort required to synchronize input system availability and (2) the accuracy level afforded. Results indicate that users, seeking to minimize effort and maximize accuracy, choose among three strategies – automatic performance, pacing, and monitoring. These findings provide a systematic account of the influence of system delay on user performance, based on adaptive strategy choice drive by cost.
- S. L. Teal, Alexander I. Rudnicky. 1991. CHANGES IN USER TASK STRATEGY DUE TO SYSTEM RESPONSE DELAY. Abstract: Despite recent advances in computer technologies, system response time remains an important factor in determining system usability, especially for newer and yet unperfected technologies such as speech recognition. Previous investigations of response delay have produced contradictory results. This study attempts to systematically investigate the relationship between response delay and a user's choice of task strategy. We address two questions. First, does response delay have a significant effect on user performance? Second, if response delay does affect user performance, can we define a model that describes the relationship?
- Alexander I. Rudnicky, Alexander Hauptmann. 1991. Models for evaluating interaction protocols in speech recognition. Abstract: Recognition errors complicate the assessment of speech systems. This paper presents a new approach to modeling spoken language interaction protocols, based on finite Markov chains. An interaction protocol, prescribed by the interface design, defines a set of primitive transaction steps and the order of their execut ion. The efficiency of an interface depends on the interaction protocol as well as the cost of each different transaction step. Markov chains provide a simple and computationally eflicient method for modeling errorful systems. They allow for detailed comparisons between different interaction protocols and between different modalities. The method is illustrated by application to example protocols.
- Jean-Michel Lunati, Alexander I. Rudnicky. 1991. Spoken language interfaces: the OM system. Abstract: The intrinsic properties of speech communication (e.g., the presence-of malformed utterances) and the characteristics of current recognition technology (inaccurate recognition) pose special problems for the design of a speech interface. We are interested in understanding these problems and in identifying an interface structure that allows speech to be a useful form of computer input. Ultimately, our goal is to understand how to turn speech into a conventional input modality, well integrated into a multimodal interface that includes keyboard and mouse. To fully exploit the advantages of spoken communication, a spoken language system must afford the user the following forms of flexibility: natural production, natural language, and a natural flow of interaction. The Carnegie Mellon Spoken Language Shell (CMSLS) attempts to provide such flexibility through the use of speaker-independent continuous-speech recognition, natural language processing, as well as rudiment ary “conversational skill” heuristics.
- Alexander I. Rudnicky, Jean-Michel Lunati, A. Franz. 1991. Spoken language recognition in an office management domain. Abstract: The authors highlight needs related to a voice interface and describe the implementation of a general-purpose spoken language interface, the Carnegie Mellon Spoken Language Shell (CM-SLS). CM-SLS provides voice interface services to different applications running on the same computer. CM-SLS was used to build the Office Manager, a collection of applications that includes an appointment calendar, a personal database, voice mail, and a calculator. The performance of several system components is described.<<ETX>>
- Alexander I. Rudnicky. 1990. The design of spoken language interfaces. Abstract: This report describes how a speech application using a speaker-independent continuous speech system is designed and implemented. The topics covered include task analysis, language design and interface design. An example of such an application, a voice spreadsheet, is described. Evaluation techniques are discussed. This research was supported by the Defense Advanced Research Projects Agency (DOD) and monitored by the Space and Naval Warfare Systems Command under Contract N00039-85-C-0163, ARPA Order No. 5167. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of DARPA or the U.S. government.
- Jean-Michel Lunati, Alexander I. Rudnicky. 1990. The design of a spoken language interface. Abstract: Fast and accurate speech recognition systems systems bring with them the possibility of designing effective voice driven applications. Efforts to this date have involved the construction of monolithic systems, necessitating repetition of effort as each new system is implemented. In this paper, we describe an initial implementation of a general spoken language interface, the Carnegie Mellon Spoken Language Shell (CM-SLS) which provides voice interface services to a variable number of applications running on the same computer. We also present a system built using CM-SLS, the Office Manager, which provides the user with voice access to facilities such as an appointment calendar, a personal database, and voice mail.
- Alexander Hauptmann, Alexander I. Rudnicky. 1990. A Comparison of Speech and Typed Input. Abstract: Meaningful evaluation of spoken language interfaces must be based on detailed comparisons with an alternate, well-understood input modality, such as the keyboard. This paper presents an empirical study in which users were asked to enter digit strings into the computer by voice and by keyboard. Two different ways of verifying and correcting the spoken input were also examined using either voice or keyboard. Timing analyses were performed to determine which aspects of the interface were critical to speedy completion of the task. The results show that speech is preferable for strings that require more than a few keystrokes. The results emphasize the need for fast and accurate speech recognition, but also demonstrate how error correction and input validation are crucial components of a speech interface.
- Alexander I. Rudnicky, M. Sakamoto, J. Polifroni. 1990. Spoken language interaction in a goal-directed task. Abstract: To study the spoken language interface in the context of a complex problem-solving task, a group of users are asked to perform a spreadsheet task, alternating voice and keyboard input. A total of 40 tasks are performed by each participant, the first 30 in a group (over several days), the remaining ones a month later. The voice spreadsheet program is extensively instrumented to provide detailed information about the components of the interaction. These data, as well as analysis of the participant's utterances and recognizer output, provide a fairly detailed picture of spoken language interaction. Although task completion by voice takes longer than by keyboard, analysis shows that users would be able to perform the spreadsheet task faster by voice, if two key criteria could be met: recognition occurs in real-time, and the error rate is sufficiently low. This initial experience with a spoken language system also allows the identification of several metrics, beyond those traditionally associated with speech recognition, that can be used to characterize system performance.<<ETX>>
- J. Polifroni, Alexander I. Rudnicky. 1989. Modeling lexical stress in read and spontaneous speech. Abstract: Although prosodic information has long been thought important for speech recognition, few demonstrations exist of its effective use in recognition systems. Lexical stress information has been shown to improve recognition performance by allowing the differentiation of confusable words (e.g., Rudnicky and Li, DARPA Workshop on Speech Recogn., June 1988). In this study, lexical stress modeling for a spreadsheet system with significant number of confusable words (e.g., EIGHTY and EIGHTEEN) is examined. The models used here have been evaluated on both read and spontaneous speech. A database of over 400 spreadsheet and numeric utterances was available for training a (HMM‐based) speaker‐independent continuous‐speech system with a 273‐word vocabulary and language perplexity of about 51. Testing data used in this study were based on read utterances and data generated in a separate study examining the use of a spoken‐language spreadsheet. This latter set includes: (a) a “spontaneous” set, composed of parsable utter...
- Alexander I. Rudnicky. 1989. The design of voice-driven interfaces. Abstract: This paper presents some issues that arise in building voice-driven interfaces to complex applications and describes some of the approaches that we have developed for this purpose. To test these approaches, we have implemented a voice spreadsheet and have begun observation of users interacting with it.
- Alexander I. Rudnicky, M. Sakamoto. 1989. Transcription conventions and evaluation techniques for spoken language system research. Abstract: We describe the transcription conventions currently in use for spontaneous speech at Carnegie Mellon University. Two sets of conventions are described, a detail-rich system for wizard experi ments, and a more rigid evaluation system designed for purposes of SLS evaluation. The latter is suitable for automatic scoring using the existing NBS (now NIST) scoring software. A sample wizard transcription is included as well as a sample of live-system transcription together with system output Transcripts can be used to generate a number of diagnostic metrics useful for system evaluation. The research described in this paper was sponsored by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 5167, monitored by SPAWAR under contract N00039-85-C-0163. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or of the US Government. Abstract This note describes the transcription conventions currently in use for spontaneous speech at Carnegie Mellon University. Two sets of conventions arc described, a detail-rich system for wizard experiments, and a more rigid system designed for purposes of SLS evaluation. The latter is suitable for automatic scoring using the existing NBS (now NIST) scoring software. A sample wizard transcription is included as well as a sample of live-system transcription together with system output. Transcripts can be used to generate a number of diagnostic metrics useful for system evaluation.
- Alexander I. Rudnicky, M. Sakamoto, J. Polifroni. 1989. Evaluating spoken language interaction. Abstract: To study the spoken language interface in the context of a complex problem-solving task, a group of users were asked to perform a spreadsheet task, alternating voice and keyboard input. A total of 40 tasks were performed by each participant, the first thirty in a group (over several days), the remaining ones a month later. The voice spreadsheet program used in this study was extensively instrumented to provide detailed information about the components of the interaction. These data, as well as analysis of the participants's utterances and recognizer output, provide a fairly detailed picture of spoken language interaction.Although task completion by voice took longer than by keyboard, analysis shows that users would be able to perform the spreadsheet task faster by voice, if two key criteria could be met: recognition occurs in real-time, and the error rate is sufficiently low. This initial experience with a spoken language system also allows us to identify several metrics, beyond those traditionally associated with speech recognition, that can be used to characterize system performance.
- Alexander I. Rudnicky. 1989. Goal-directed Speech in a Spoken Language System. Abstract: The advent of reliable speaker‐independent continuous speech recognition systems has made it possible to design systems that use speech as a replacement for keyboard input. To understand the nature of a system that accepts spontaneous goal‐directed speech (as opposed to the current standard of read speech), a spoken‐language spreadsheet was implemented and users performing a series of tasks using this system were studied. The system was instrumented to allow the collection of detailed timing information about the components of the interaction cycle. The (HMM‐based) recognition system incorporates a lexicon of 273 words and a language of perplexity 51. Four users performed a series of 40 tasks (involving the entry of personal financial information) alternating voice and keyboard input. Users completed 30 tasks in one block of sessions, then returned a month later to complete the remainder. The utterances spoken into the system (over 7500) were stored for later analysis. The data collected provide a compreh...
- Alexander I. Rudnicky, Alexander Hauptmann. 1989. Conversational interaction with speech systems. Abstract: This paper discusses design principles for spoken language applications. We focus on those aspects of interface design that are separate from basic speech recognition and that are concerned with the global process of performing a task using a speech interface. Six basic speech user interface design principles are discussed: 1. User plasticity. This property describes how much users can adapt to speech interfaces. 2. Interaction protocol styles. We explain how different interaction protocols for speech interfaces impact on basic task throughput. 3. Error correction. Alternate ways to correct recognition errors are examined. 4. Response Time. The response time requirements of a speech user interface is presented based on experimental results. 5. Task structure. The use of task structure to reduce the complexity of the speech recognition problem is discussed and the resulting benefits are demonstrated. 6. Multi-Modality. The opportunity for integration of several modalities into the interface is evaluated. Since these design principles are different from others for standard applications with typing or pointing, we present experimental support for the importance of these principles as well as perspectives towards solutions and further research. The research described in this paper was sponsored by the Defense Advanced Research P m i * r t c views and conclusions contained in this document are those of the authors and should not be i n t L e t e d ^xzz^s^:~ ~ Table of
- Alexander I. Rudnicky, Zong-ge Li, J. Polifroni, Eric H. Thayer, J. L. Gale. 1988. An unanchored matching algorithm for lexical access. Abstract: Describes the lexical access component of the Carnegie-Mellon University (CMU) continuous speech recognition system. The word recognition algorithm operates in a left to right fashion, building words as it traverses an input network. Search is initiated at each node in the input network. The score assigned to a word is a function of both arc phone probabilities assigned by the acoustic phonetic module and knowledge of expected phone duration and frequency of occurrence of different word pronunciations. The algorithm also incorporates knowledge-based strategies to control the number of hypotheses generated by the matcher. These strategies use criteria external to the search. Performance characteristics are reported using a 1029 word lexicon built automatically from standard pronunciation base forms by context-dependent phonetic rules. Lexical rules are independent of specific lexicons and are derived by examination of transcribed speech data. The lexical representation now includes juncture rules that model specific inter-word phenomena. A junction validation module is also described, whose task is to evaluate the connectivity of words in the word hypotheses lattice.<<ETX>>
- Alexander I. Rudnicky, R. Brennan, J. Polifroni. 1988. Interactive problem solving with speech. Abstract: Until recently, systems offering high‐performance speaker‐independent continuous speech recognition were not available, making it difficult to understand how speech should be used in interactive systems. The advent of the SPHINX system developed al Carnegie‐Mellon University [K.‐F. Lee and H.‐W. Hon, Proc. IEEE ICASSP‐88, 123–126 (1988)] has made it practical to address the issue of designing systems that integrate speech into “real‐world” tasks. This paper describes experience with several tasks that use tightly coupled speech interaction: a programmable voice calculator, a personal scheduler, and a spreadsheet. The goal of this work is to create an environment that allows for the rapid prototyping of “spoken language” systems and allows the study of human factors issues that these entail. The environment that was developed includes the ability to rapidly configure and refine recognition systems using declarative specifications, and the ability to define control structures suitable to particular tasks. [...
- H. Murveit, M. Weintraub, Michael Cohen, J. Bernstein, Alexander I. Rudnicky. 1987. Lexical access with lattice input. Abstract: This paper describes an alternative approach to lexical access in the CMU ANGEL speech recognition system. Using this approach, the asynchronous phonetic hypotheses generated by an acoustic-phonetics module are converted to a directed graph. This graph is compared to a pronunciation dictionary. Performance results for this approach and the original CMU approach are similar. An error analysis indicates promising directions for further work.
- Alexander I. Rudnicky, Lynn K. Baumeister, Kevin H. DeGraaf, Eric Lehmann. 1987. The lexical access component of the CMU continuous speech recognition system. Abstract: The CMU Lexical Access system hypothesizes words from a phonetic lattice, supplemented by a coarse labelling of the speech signal. Word hypotheses are anchored on syllabic nuclei and are generated independently for different parts of the utterance. Junctures between words are resolved separately, on demand from the Parser module. The lexical representation is generated by rule from baseforms, in a completely automatic process. A description of the various components of the system is provided, as well as performance data.
- Alexander I. Rudnicky. 1987. Using features to empirically generate pronunciation networks. Abstract: The construction of high‐quality lexical models for speech recognition systems is a labor‐intensive process, requiring not only a great deal of time but also appreciable expertise on the part of the human model builder. Techniques that automate all or parts of this process are therefore highly desirable. Two approaches have been used: the generation of pronunciation networks from baseforms by application of phonological rules and the abstraction of such networks from transcription data. This paper describes a semiautomatic procedure for deriving lexical models from manual phonetic transcriptions. The algorithm uses a feature‐based distance metric to align different pronunciations and produces phonetic networks that specify alternate pronunciations for words. The paper describes the results of constructing lexical networks for a 1000‐word vocabulary, using a corpus of 10 000 work tokens. It should be noted that the technique described here is not restricted to manual transcriptions, but can be extended to ...
- Alexander I. Rudnicky, P. A. Kolers. 1984. Size and case of type as stimuli in reading.. Abstract: The role of size and case of print have provoked a number of experiments in the recent past. One strongly argued position is that the reader abstracts a canonical representation from a string of letters that renders its variations irrelevant and then carries out recognition procedures on that abstraction. An alternate view argues that the reader proceeds by analyzing the print, taking account of its manifold physical attributes such as length of words, their orientation, shape, and the like. In the present experiments size and case were varied in several ways, and the task was also varied to include both silent reading and reading aloud. Clear evidence for shape-sensing operations was brought forward, but they were shown to be optional rather than obligatory processes, used when it served the reader's purpose to do so. However, it was also shown that such skills, normally useful, could be tricked into operating even when their presence hindered the reader's performance. The conclusion is drawn that reading goes forward in many ways at once rather than through an orderly sequence of operations, consistent with the reader's skills and the requirements of the task. Overarching theories of performance seem premature in the absence of detailed analysis of task components.
- Alexander I. Rudnicky. 1984. Speaker‐independent recognition of vocalic segments. Abstract: Speaker variations produce substantial differences in vocalic spectra: Vowel templates generated from one speaker's voice will not accurately match another voice. It is possible, however, to impose transformations on the spectrum that factor out speaker differences. The current work presents two sets of experiments that examine such transformations. The first set of experiments used steady‐state vowels (/i e a o u/); for ten male and ten female talkers, the results indicate that a log‐ratio transformation that incorporates pitch and formants into a three‐dimensional L space [log(F1/F0),log(F2/F1),log(F3/F2)] allows 93% classification accuracy. By comparison, spectral matching gives 44% accuracy. Extended vocalic segments (e.g., as in “away,” /əʏ/) have dynamically varying formant patterns. In L space, these patterns appear as tracks. The seeood set of experiments investigates a recognition technique that uses the encoded track shape as part of the representation. Speaker‐independent performance on isolate...
- Alexander I. Rudnicky, A. Waibel, N. Krishnan. 1982. Adding a zero-crossing count to spectral information in template-based speech recognition. Abstract: Abstract : Zero-crossing data can provide important feature information about an utterance which is not available in a purely spectral representation. This report describes the incorporation of zero-crossing information into the spectral representation used in a template-matching system (CICADA). An analysis of zero-crossing data for an extensive (2880 utterance, 8 talker) alpha-digit data base is described. On the basis of this analysis, a zero-crossing algorithm is proposed. The algorithm was evaluated using a confusible subset of the alpha-digit vocabulary (the E-set ). Inclusion of zero-crossing information in the representation leads to a 10-13% reduction in error rate, depending on the spectral representation.
- Alexander I. Rudnicky, A. Waibel, N. Krishnan. 1981. Using zero crossing counts to provide discriminative information in isolated word recognition. Abstract: Template matching systems that use a purely spectral representation are potentially insensitive to some important phonetic dimensions of speech. For example, [PI] is often confused with both [BI] and [TI] because the necessary vocalic—aspirated—fricative discrimination cannot be reliably made on the basis of spectrum alone. The zero crossing count is known to contain sufficient information for this and for other discriminations. We evaluated the usefulness of the zero crossing count in a set of experiments that used a highly confusable subset of the alpha‐digit vocabulary (3, b, c, d, e, g, p, t, v, z). In these experiments, a zero crossing count was added to a 15 coefficient spectral representation and the resulting vector was used to calculate frame distances within a dynamic warping algorithm. Testing over a corpus of 800 utterances (eight talkers), we achieved a 10% reduction in error rate. In order to obtain optimal improvement, the count had to be clipped to a restricted range, quantized to a small ...
- Alexander I. Rudnicky. 1979. The perception of speech in an unfamiliar language. Abstract: Experiments in speech perception almost invariably use speech tokens and response categories drawn from the language native to the listeners participating in the experiments. This has served to confound the listeners' extensive knowledge of their own language with their ability to perceive speech stimuli as such. The present study attempts to separate these two components of speech perception. Short (4–5 s) excerpts of English and non‐English speech were altered in one of two ways: by the deletion of a closure silence or by the addition of a 100 ms silence to an existing closure silence. Native English listeners were highly adept at detecting and identifying alterations in English. However, their performance was at a chance level on non‐English excerpts. The original foreign talkers also listened to the excerpts: they performed well with excerpts of their own language and poorly in all other languages. These experiments underline the importance of experience in speech perception and suggest that the perce...
- Alexander I. Rudnicky, R. Cole. 1977. Vowel identification and subsequent context. Abstract: The disyllable [da ga] (with stress on the second syllable) was recorded on magnetic tape. When [ga] was excised from the tape, subjects heard [dag]. However, when a [ba], taken from a [da ba] utterance was substituted for the [ga], listeners heard [da i∧ ba]. Perception of the diphthong demonstrates that coarticulatory information contained in the syllable‐final formant transitions of [da] is interpreted differently, depending upon the subsequent phonetic context. An experiment was performed to investigate the temporal course of this effect, by introducing successively longer silent intervals between the [da] and the [ba]. Given a sufficient separation, listeners again reported hearing [dag ba], instead of [da i∧ ba]. The results demonstrate that the interpretation of information in a syllable can be influenced by acoustic events in another, separate syllable occurring later in time. The results also suggest that perception takes into account allowable articulatory movements.
- Alexander I. Rudnicky, R. Cole. 1976. Selective adaptation produced by ongoing speech. Abstract: In the selective adaptation paradigm, subjects typically hear repetitions of a single syllable (e.g., [pha]) and subsequently identify syllables from a test series spanning two phonetic categories (e.g., [ba] − [pha]). Experiments have shown that the adapting syllable produces a shift in the phoneme boundary of the test series, such that fewer syllables are assigned to the phonetic category of the adapting syllable. In the present experiments we determined that selective adaptation could be observed using ongoing speech. Subjects were first presented with syllables for identification from an acoustic continuum (e.g., [tha] − [da]) in order to determine their phoneme boundary in an unadapted state. Subjects then heard three sentences containing a predominance of a particular phoneme or phonetic feature. For example, a “voiceless stop” sentence might be: “Carl tickled Tillie's toes till Tillie told Carl to quit tickling her toes.” After the third such sentence, subjects were again presented with syllables f...
- A. Bregman, Alexander I. Rudnicky. 1975. Auditory segregation: stream or streams?. Abstract: When auditory material segregates into "streams," is the unattended stream actually organized as an entity? An affirmative answer is suggested by the observation that the organizational structure of the unattended material interacts with the structure of material to which the subject is trying to attend. Specificially, a to-be-rejected stream can, because of its structure, capture from a to-be-judged stream elements that would otherwiise be acceptable members of the to-be-judged stream.
