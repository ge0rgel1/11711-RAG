Alexander Waibel
Paper count: 719
- Tuan-Nam Nguyen, Ngoc-Quan Pham, A. Waibel. 2023. SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization. Abstract: Conventional multi-speaker text-to-speech synthesis (TTS) is known to be capable of synthesizing speech for multiple voices, yet it cannot generate speech in different accents. This limitation has motivated us to develop SYNTACC (Synthesizing speech with accents) which adapts conventional multi-speaker TTS to produce multi-accent speech. Our method uses the YourTTS model and involves a novel multi-accent training mechanism. The method works by decomposing each weight matrix into a shared component and an accent-dependent component, with the former being initialized by the pretrained multi-speaker TTS model and the latter being factorized into vectors using rank-1 matrices to reduce the number of training parameters per accent. This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition. Our SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents.
- Danni Liu, T. Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, A. Waibel, J. Niehues. 2023. KIT’s Multilingual Speech Translation System for IWSLT 2023. Abstract: Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.
- T. Nguyen, A. Waibel. 2023. Convoifilter: A case study of doing cocktail party speech recognition. Abstract: This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise (ConVoiFilter) and an ASR module. The model can decrease ASR's word error rate (WER) from 80% to 26.4% through this approach. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter.
- Ngoc-Quan Pham, J. Niehues, A. Waibel. 2023. Continually learning new languages. Abstract: Multilingual speech recognition with neural networks is often implemented with batch-learning, when all of the languages are available before training. An ability to add new languages after the prior training sessions can be economically bene-ﬁcial, but the main challenge is catastrophic forgetting. In this work, we combine the qualities of weight factorization, transfer learning and Elastic Weight Consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly. Such combination allowed us to eliminate catastrophic forgetting while still achieving performance for the new languages comparable with having all languages at once, in experiments of learning from an initial 10 languages to achieve 27 languages.
- Peter Polák, Brian Yan, Shinji Watanabe, A. Waibel, Ondrej Bojar. 2023. Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff. Abstract: Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \textit{incremental} translation to users. Further, this method lacks mechanisms for \textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.
- Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel. 2023. End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. Abstract: The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.
- Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Polák, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, M. Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos. 2023. FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN. Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.
- Christian Huber, Enes Yavuz Ugan, A. Waibel. 2022. Code-Switching without Switching: Language Agnostic End-to-End Speech Translation. Abstract: We propose a) a Language Agnostic end-to-end Speech Translation model (LAST), and b) a data augmentation strategy to increase code-switching (CS) performance. With increasing globalization, multiple languages are increasingly used interchangeably during fluent speech. Such CS complicates traditional speech recognition and translation, as we must recognize which language was spoken first and then apply a language-dependent recognizer and subsequent translation component to generate the desired target language output. Such a pipeline introduces latency and errors. In this paper, we eliminate the need for that, by treating speech recognition and translation as one unified end-to-end speech translation problem. By training LAST with both input languages, we decode speech into one target language, regardless of the input language. LAST delivers comparable recognition and speech translation accuracy in monolingual usage, while reducing latency and error rate considerably when CS is observed.
- Enes Yavuz Ugan, Christian Huber, Juan Hussain, A. Waibel. 2022. Language-agnostic Code-Switching in End-To-End Speech Recognition. Abstract: Code-Switching (CS) is referred to the phenomenon of al-ternately using words and phrases from different languages. While today’s neural end-to-end (E2E) models deliver state-of-the-art performances on the task of automatic speech recognition (ASR) it is commonly known that these systems are very data-intensive. However, there is only a few transcribed and aligned CS speech available. To overcome this problem and train multilingual systems which can transcribe CS speech, we propose a simple yet effective data augmentation in which audio and corresponding labels of different source languages are concatenated. By using this training data, our E2E model improves on transcribing CS speech and improves performance over the multilingual model, as well. The results show that this augmentation technique can even improve the model’s performance on inter-sentential language switches not seen during training by 5,03% WER.
- Ngoc-Quan Pham, J. Niehues, A. Waibel. 2022. Towards continually learning new languages. Abstract: Multilingual speech recognition with neural networks is often implemented with batch-learning, when all of the languages are available before training. An ability to add new languages after the prior training sessions can be economically beneficial, but the main challenge is catastrophic forgetting. In this work, we combine the qualities of weight factorization and elastic weight consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly. Such combination allowed us to eliminate catastrophic forgetting while still achieving performance for the new languages comparable with having all languages at once, in experiments of learning from an initial 10 languages to achieve 26 languages without catastrophic forgetting and a reasonable performance compared to training all languages from scratch.
- Ngoc-Quan Pham, A. Waibel, J. Niehues. 2022. Adaptive multilingual speech recognition with pretrained models. Abstract: Multilingual speech recognition with supervised learning has achieved great results as reflected in recent research. With the development of pretraining methods on audio and text data, it is imperative to transfer the knowledge from unsupervised multilingual models to facilitate recognition, especially in many languages with limited data. Our work investigated the effectiveness of using two pretrained models for two modalities: wav2vec 2.0 for audio and MBART50 for text, together with the adaptive weight techniques to massively improve the recognition quality on the public datasets containing CommonVoice and Europarl. Overall, we noticed an 44% improvement over purely supervised learning, and more importantly, each technique provides a different reinforcement in different languages. We also explore other possibilities to potentially obtain the best model by slightly adding either depth or relative attention to the architecture.
- Tuan-Nam Nguyen, Ngoc-Quan Pham, A. Waibel. 2022. Accent Conversion using Pre-trained Model and Synthesized Data from Voice Conversion. Abstract: Accent conversion (AC) aims to generate synthetic audios by changing the pronunciation pattern and prosody of source speakers (in source audios) while preserving voice quality and linguistic content. There has not been a parallel corpus that contains pairs of audios having the same contents yet coming from the same speakers in different accents, the authors hence work on a solution to synthesize one as training input. The training pipeline is conducted via two steps. First, a voice conversion (VC) model is constructed to synthesize a training data set, con-taining pairs of audios in the same voice but two different accents. Second, an AC model is trained with the synthesized data to convert a source accented speech to a target accented speech. Given the recognized success of self-supervised learning speech representation (wav2vec 2.0) on certain speech problems such as VC, speech recognition, speech translation, and speech-to-speech translation, we adopt this architecture with some cus-tomization to train the AC model in the second step. With just 9-hour synthesized training data, the encoder initialized by the weight of the pre-trained wav2vec 2.0 model outperforms the LSTM-based encoder.
- Louisa Lambrecht, Felix Schneider, A. Waibel. 2022. Machine Translation from Standard German to Alemannic Dialects. Abstract: Machine translation has been researched using deep neural networks in recent years. These networks require lots of data to learn abstract representations of the input stored in continuous vectors. Dialect translation has become more important since the advent of social media. In particular, when dialect speakers and standard language speakers no longer understand each other, machine translation is of rising concern. Usually, dialect translation is a typical low-resourced language setting facing data scarcity problems. Additionally, spelling inconsistencies due to varying pronunciations and the lack of spelling rules complicate translation. This paper presents the best-performing approaches to handle these problems for Alemannic dialects. The results show that back-translation and conditioning on dialectal manifestations achieve the most remarkable enhancement over the baseline. Using back-translation, a significant gain of +4.5 over the strong transformer baseline of 37.3 BLEU points is accomplished. Differentiating between several Alemannic dialects instead of treating Alemannic as one dialect leads to substantial improvements: Multi-dialectal translation surpasses the baseline on the dialectal test sets. However, training individual models outperforms the multi-dialectal approach. There, improvements range from 7.5 to 10.6 BLEU points over the baseline depending on the dialect.
- Enes Yavuz Ugan, Christian Huber, Juan Hussain, A. Waibel. 2022. Language-agnostic Code-Switching in Sequence-To-Sequence Speech Recognition. Abstract: Code-Switching (CS) is referred to the phenomenon of alternately using words and phrases from different languages. While today's neural end-to-end (E2E) models deliver state-of-the-art performances on the task of automatic speech recognition (ASR) it is commonly known that these systems are very data-intensive. However, there is only a few transcribed and aligned CS speech available. To overcome this problem and train multilingual systems which can transcribe CS speech, we propose a simple yet effective data augmentation in which audio and corresponding labels of different source languages are concatenated. By using this training data, our E2E model improves on transcribing CS speech. It also surpasses monolingual models on monolingual tests. The results show that this augmentation technique can even improve the model's performance on inter-sentential language switches not seen during training by 5,03% WER.
- Antonios Anastasopoulos, Loïc Barrault, L. Bentivogli, Marcely Zanon Boito, Ondrej Bojar, Roldano Cattoni, Anna Currey, Georgiana Dinu, Kevin Duh, Maha Elbayad, Clara Emmanuel, Y. Estève, Marcello Federico, C. Federmann, Souhir Gahbiche, Hongyu Gong, Roman Grundkiewicz, B. Haddow, B. Hsu, Dávid Javorský, Věra Kloudová, Surafel Melaku Lakew, Xutai Ma, Prashant Mathur, Paul McNamee, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, J. Niehues, Xing Niu, John E. Ortega, J. Pino, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, M. Turchi, Yogesh Virkar, A. Waibel, Changhan Wang, Shinji Watanabe. 2022. Findings of the IWSLT 2022 Evaluation Campaign. Abstract: The evaluation campaign of the 19th International Conference on Spoken Language Translation featured eight shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Speech to speech translation, (iv) Low-resource speech translation, (v) Multilingual speech translation, (vi) Dialect speech translation, (vii) Formality control for speech translation, (viii) Isometric speech translation. A total of 27 teams participated in at least one of the shared tasks. This paper details, for each shared task, the purpose of the task, the data that were released, the evaluation metrics that were applied, the submissions that were received and the results that were achieved.
- Leonard Bärmann, A. Waibel. 2022. Where did I leave my keys? — Episodic-Memory-Based Question Answering on Egocentric Videos. Abstract: Humans have a remarkable ability to organize, compress and retrieve episodic memories throughout their daily life. Current AI systems, however, lack comparable capabilities as they are mostly constrained to an analysis with access to the raw input sequence, assuming an unlimited amount of data storage which is not feasible in realistic deployment scenarios. For instance, existing Video Question Answering (VideoQA) models typically reason over the video while already being aware of the question, thus requiring to store the complete video in case the question is not known in advance.In this paper, we address this challenge with three main contributions: First, we propose the Episodic Memory Question Answering (EMQA) task as a specialization of VideoQA. Specifically, EMQA models are constrained to keep only a constant-sized representation of the video input, thus automatically limiting the computation requirements at query time. Second, we introduce a new egocentric VideoQA dataset called QaEgo4D, far larger than existing egocentric VideoQA datasets and featuring video length unprecedented in VideoQA datasets in general. Third, we present extensive experiments on the new dataset, comparing various baseline models in both the VideoQA and the EMQA setting. To facilitate future research on egocentric VideoQA as well as episodic memory representation and retrieval, we publish our code and dataset.
- Peter Pol'ak, Ngoc-Quan Ngoc, Tuan-Nam Nguyen, Danni Liu, Carlos Mullov, J. Niehues, Ondvrej Bojar, A. Waibel. 2022. CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022. Abstract: In this paper, we describe our submission to the Simultaneous Speech Translation at IWSLT 2022. We explore strategies to utilize an offline model in a simultaneous setting without the need to modify the original model. In our experiments, we show that our onlinization algorithm is almost on par with the offline setting while being 3x faster than offline in terms of latency on the test set. We also show that the onlinized offline model outperforms the best IWSLT2021 simultaneous system in medium and high latency regimes and is almost on par in the low latency regime. We make our system publicly available.
- Christian Huber, Rishu Kumar, Ondvrej Bojar, A. Waibel. 2022. Short-Term Word-Learning in a Dynamically Changing Environment. Abstract: Neural sequence-to-sequence automatic speech recognition (ASR) systems are in principle open vocabulary systems, when using appropriate modeling units. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, numbers or technical terms. To alleviate this problem, Huber et al. proposed to supplement an end-to-end ASR system with a word/phrase memory and a mechanism to access this memory to recognize the words and phrases correctly. In this paper we study, a) methods to acquire important words for this memory dynamically and, b) the trade-off between improvement in recognition accuracy of new words and the potential danger of false alarms for those added words. We demonstrate significant improvements in the detection rate of new words with only a minor increase in false alarms (F1 score 0.30 $\rightarrow$ 0.80), when using an appropriate number of new words. In addition, we show that important keywords can be extracted from supporting documents and used effectively.
- Niklas Bühler, A. Waibel, T. Asfour. 2021. Cross-lingual, Language-independent Phoneme Alignment. Abstract: When documenting an unknown language – and especially its pronunciation – for the first time, linguists are oftentimes missing the necessary technology to do so efficiently. The work described in this thesis might facilitate future advancements in this regard by further developing techniques that make the computer-assisted examination of languages more efficient. However, not only the process of manually examining an unseen language is very costly. Even in cases where automatic speech recognition technology is already utilized, the necessary process of data collection and preparation still remains costly. Cross-lingual approaches can alleviate this problem. The goal of this thesis is to apply cross-lingual, multilingual techniques on the task of phoneme alignment, i.e. the task of temporally aligning a phonetic transcript to its corresponding audio recording. Three different neural network architectures are trained on a multilingual data set and utilized as a source of emission probabilities in hybrid HMM/ANN systems. These HMM/ANN systems enable the computation of phoneme alignments via the Viterbi algorithm. By iterating this process, multilingual acoustic models are bootstrapped and the resulting systems are used to cross-lingually align data from a previously unseen target language. Finally, the results are scored and compared against each other.
- Antonios Anastasopoulos, Ondrej Bojar, Jacob Bremerman, R. Cattoni, Maha Elbayad, Marcello Federico, Xutai Ma, Satoshi Nakamura, Matteo Negri, J. Niehues, J. Pino, Elizabeth Salesky, Sebastian Stüker, Katsuhito Sudoh, M. Turchi, A. Waibel, Changhan Wang, Matthew Wiesner. 2021. FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN. Abstract: The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the tasks. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions.
- Leonard Bärmann, Fabian Peller-Konrad, Stefan Constantin, T. Asfour, A. Waibel. 2021. Deep Episodic Memory for Verbalization of Robot Experience. Abstract: The ability to verbalize robot experience in natural language is key for a symbiotic human-robot interaction. While first works approached this problem using template-based verbalization on symbolic episode data only, we explore a novel way in which deep learning methods are used for the creation of an episodic memory from experiences as well as the verbalization of such experience in natural language. To this end, we first collected a complex dataset consisting of more than a thousand multimodal robot episode recordings both from simulation as well as real robot executions, together with representative natural language questions and answers about the robot's past experience. Second, we propose and evaluate an episodic memory verbalization model consisting of a speech encoder and decoder based on the Transformer architecture, combined with an LSTM-based episodic memory auto-encoder, and evaluate the model on simulated and real data from robot execution examples. Our experimental results provide a proof-of-concept for episodic-memory-based verbalization of robot experience.
- Ngoc-Quan Pham, Tuan-Nam Nguyen, Thanh-Le Ha, Sebastian Stüker, A. Waibel, Dan He. 2021. Multilingual Speech Translation KIT @ IWSLT2021. Abstract: This paper contains the description for the submission of Karlsruhe Institute of Technology (KIT) for the multilingual TEDx translation task in the IWSLT 2021 evaluation campaign. Our main approach is to develop both cascade and end-to-end systems and eventually combine them together to achieve the best possible results for this extremely low-resource setting. The report also confirms certain consistent architectural improvement added to the Transformer architecture, for all tasks: translation, transcription and speech translation.
- Carlos Mullov, Ngoc-Quan Pham, A. Waibel. 2021. Unsupervised Transfer Learning in Multilingual Neural Machine Translation with Cross-Lingual Word Embeddings. Abstract: In this work we look into adding a new language to a multilingual NMT system in an unsupervised fashion. Under the utilization of pre-trained cross-lingual word embeddings we seek to exploit a language independent multilingual sentence representation to easily generalize to a new language. While using cross-lingual embeddings for word lookup we decode from a yet entirely unseen source language in a process we call blind decoding. Blindly decoding from Portuguese using a basesystem containing several Romance languages we achieve scores of 36.4 BLEU for Portuguese-English and 12.8 BLEU for Russian-English. In an attempt to train the mapping from the encoder sentence representation to a new target language we use our model as an autoencoder. Merely training to translate from Portuguese to Portuguese while freezing the encoder we achieve 26 BLEU on English-Portuguese, and up to 28 BLEU when adding artificial noise to the input. Lastly we explore a more practical adaptation approach through non-iterative backtranslation, exploiting our model's ability to produce high quality translations through blind decoding. This yields us up to 34.6 BLEU on English-Portuguese, attaining near parity with a model adapted on real bilingual data.
- Ondrej Bojar, Dominik Machácek, Sangeet Sagar, O. Smrz, J. Kratochvíl, Peter Polák, Ebrahim Ansari, Mohammad Mahmoudi, Rishu Kumar, Dario Franceschini, Chiara Canton, I. Simonini, T. Nguyen, Felix Schneider, Sebastian Stüker, A. Waibel, B. Haddow, Rico Sennrich, Philip Williams. 2021. ELITR Multilingual Live Subtitling: Demo and Strategy. Abstract: This paper presents an automatic speech translation system aimed at live subtitling of conference presentations. We describe the overall architecture and key processing components. More importantly, we explain our strategy for building a complex system for end-users from numerous individual components, each of which has been tested only in laboratory conditions. The system is a working prototype that is routinely tested in recognizing English, Czech, and German speech and presenting it translated simultaneously into 42 target languages.
- Zhong Zhou, A. Waibel. 2021. Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages. Abstract: We translate a closed text that is known in advance and available in many languages into a new and severely low resource language. Most human translation efforts adopt a portion-based approach to translate consecutive pages/chapters in order, which may not suit machine translation. We compare the portion-based approach that optimizes coherence of the text locally with the random sampling approach that increases coverage of the text globally. Our results show that the random sampling approach performs better. When training on a seed corpus of ~1,000 lines from the Bible and testing on the rest of the Bible (~30,000 lines), random sampling gives a performance gain of +11.0 BLEU using English as a simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a Mayan language. Furthermore, we compare three ways of updating machine translation models with increasing amount of human post-edited data through iterations. We find that adding newly post-edited data to training after vocabulary update without self-supervision performs the best. We propose an algorithm for human and machine to work together seamlessly to translate a closed text into a severely low resource language.
- A. Waibel. 2021. The Visualisation of Polyadic Sustained Shared Thinking Interactions: A Methodological Approach. Abstract: Sustained Shared Thinking (SST) wird als wichtiges Element einer qualitativ hochwertigen Fachperson-Kind-Interaktion (SIRAJ-BLATCHFORD, SYLVA, MUTTOCK, GILDEN & BELL 2002) angesehen. In fruhkindlichen Institutionen kommt SST jedoch selten vor und wird hauptsachlich in dyadischen Interaktionen beobachtet. Da Kommunikation im Kindergarten auch in Gruppensettings stattfindet, wurde der Fokus in dieser Studie auf polyadische SST-Dialoge gelegt. Videos, Literacy-Tests fur Kinder und Informationen uber die Familiensprache der Kinder (mono-/multilingual) aus dem internationalen Forschungsprojekt "SpriKiDS" (VOGT et al. 2019) wurden mittels linguistischer Gesprachsanalyse (BRINKER & SAGER 2010) und Grounded-Theory-Methodologie (STRAUSS & CORBIN 1996 [1990]) untersucht. Dabei wurden Mikroprozesse in geteilten Denkprozessen analysiert, um Strategien zu identifizieren, die SST in Kindergruppen fordern. Im Zuge der Analyse wurden Visualisierungen zur Erforschung von polyadischen SST-Interaktionen entwickelt. In diesem Artikel werden Moglichkeiten und Grenzen von Visualisierungen zu Analyse- und Prasentationszwecken anhand von zwei Spielsequenzen in unterschiedlichen Gruppengrosen beschrieben. Der Einsatz von Visualisierungen scheint die Analyse der Interaktionsstrategien von Fachpersonen zu unterstutzen und ermoglicht die Entdeckung von Mustern, indem eine Art analytische Linse auf Mikroprozesse rund um SST-Beitrage der Kinder gelegt wird. Ein Vorteil wird in der grafischen Darstellung komplexer Zusammenhange gesehen, die im Analyseprozess und in Prasentationen zum Verstandnis beitragen konnen.
- Ngoc-Quan Pham, Tuan-Nam Nguyen, S. Stueker, A. Waibel. 2021. Efficient Weight factorization for Multilingual Speech Recognition. Abstract: End-to-end multilingual speech recognition involves using a single model training on a compositional speech corpus including many languages, resulting in a single neural network to handle transcribing different languages. Due to the fact that each language in the training data has different characteristics, the shared network may struggle to optimize for all various languages simultaneously. In this paper we propose a novel multilingual architecture that targets the core operation in neural networks: linear transformation functions. The key idea of the method is to assign fast weight matrices for each language by decomposing each weight matrix into a shared component and a language dependent component. The latter is then factorized into vectors using rank-1 assumptions to reduce the number of parameters per language. This efficient factorization scheme is proved to be effective in two multilingual settings with $7$ and $27$ languages, reducing the word error rates by $26\%$ and $27\%$ rel. for two popular architectures LSTM and Transformer, respectively.
- Fabian Retkowski, A. Waibel. 2021. Value-Based Reinforcement Learning for Sequence-to-Sequence Models. Abstract: This paper demonstrates the theoretical possibility of applying advanced value-based reinforcement learning methods on sequence-to-sequence models for the first time. This approach avoids major issues that have emerged with supervised sequence-to-sequence models such as loss-evaluation mismatch, exposure bias and search error. At the same time, when compared to policy gradient methods, it does not rely on well-trained fully supervised models and is not restricted to fine-tuning. Specifically, a sequence-to-sequence model is introduced, which is trained in a Rainbow-like setup. While such a model is practically still limited by its scalability, the work contributes towards a more generally applicable approach to reinforcement learning in natural language processing which is beyond the scope of fine-tuning. For this, the paper provides a theoretical and practical framework, a first baseline, and valuable insights by studying ablated models and different approaches for utilizing demonstration data.
- Christian Huber, Juan Hussain, Sebastian Stüker, A. Waibel. 2021. Instant One-Shot Word-Learning for Context-Specific Neural Sequence-to-Sequence Speech Recognition. Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition (ASR). When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, numbers or technical terms. To alleviate this problem we supplement an end-to-end ASR system with a word/phrase memory and a mechanism to access this memory to recognize the words and phrases correctly. After the training of the ASR system, and when it has already been deployed, a relevant word can be added or subtracted instantly without the need for further training. In this paper we demonstrate that through this mechanism our system is able to recognize more than 85% of newly added words that it previously failed to recognize compared to a strong baseline.
- Ngoc-Quan Pham, Thanh-Le Ha, Tuan-Nam Nguyen, T. Nguyen, Elizabeth Salesky, S. Stueker, J. Niehues, A. Waibel. 2020. Relative Positional Encoding for Speech Recognition and Direct Translation. Abstract: Transformer models are powerful sequence-to-sequence architectures that are capable of directly mapping speech inputs to transcriptions or translations. However, the mechanism for modeling positions in this model was tailored for text modeling, and thus is less ideal for acoustic inputs. In this work, we adapt the relative position encoding scheme to the Speech Transformer, where the key addition is relative distance between input states in the self-attention network. As a result, the network can better adapt to the variable distributions present in speech data. Our experiments show that our resulting model achieves the best recognition result on the Switchboard benchmark in the non-augmentation condition, and the best published result in the MuST-C speech translation benchmark. We also show that this model is able to better utilize synthetic data than the Transformer, and adapts better to variable sentence segmentation quality for speech translation.
- T. Nguyen, S. Stueker, A. Waibel. 2020. Super-Human Performance in Online Low-latency Recognition of Conversational Speech. Abstract: Achieving super-human performance in recognizing human speech has been a goal for several decades, as researchers have worked on increasingly challenging tasks. In the 1990's it was discovered, that conversational speech between two humans turns out to be considerably more difficult than read speech as hesitations, disfluencies, false starts and sloppy articulation complicate acoustic processing and require robust handling of acoustic, lexical and language context, jointly. Early attempts with statistical models could only reach error rates over 50% and far from human performance (WER of around 5.5%). Neural hybrid models and recent attention-based encoder-decoder models have considerably improved performance as such contexts can now be learned in an integral fashion. However, processing such contexts requires an entire utterance presentation and thus introduces unwanted delays before a recognition result can be output. In this paper, we address performance as well as latency. We present results for a system that can achieve super-human performance (at a WER of 5.0%, over the Switchboard conversational benchmark) at a word based latency of only 1 second behind a speaker's speech. The system uses multiple attention-based encoder-decoder networks integrated within a novel low latency incremental inference approach.
- T. Nguyen, Ngoc-Quan Pham, S. Stueker, A. Waibel. 2020. High Performance Sequence-to-Sequence Model for Streaming Speech Recognition. Abstract: Recently sequence-to-sequence models have started to achieve state-of-the-art performance on standard speech recognition tasks when processing audio data in batch mode, i.e., the complete audio data is available when starting processing. However, when it comes to performing run-on recognition on an input stream of audio data while producing recognition results in real-time and with low word-based latency, these models face several challenges. For many techniques, the whole audio sequence to be decoded needs to be available at the start of the processing, e.g., for the attention mechanism or the bidirectional LSTM (BLSTM). In this paper, we propose several techniques to mitigate these problems. We introduce an additional loss function controlling the uncertainty of the attention mechanism, a modified beam search identifying partial, stable hypotheses, ways of working with BLSTM in the encoder, and the use of chunked BLSTM. Our experiments show that with the right combination of these techniques, it is possible to perform run-on speech recognition with low word-based latency without sacrificing in word error rate performance.
- Ebrahim Ansari, Amittai Axelrod, Nguyen Bach, Ondrej Bojar, R. Cattoni, Fahim Dalvi, Nadir Durrani, Marcello Federico, C. Federmann, Jiatao Gu, Fei Huang, Kevin Knight, Xutai Ma, Ajay Nagesh, Matteo Negri, J. Niehues, J. Pino, Elizabeth Salesky, Xing Shi, Sebastian Stüker, M. Turchi, A. Waibel, Changhan Wang. 2020. FINDINGS OF THE IWSLT 2020 EVALUATION CAMPAIGN. Abstract: The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2020) featured this year six challenge tracks: (i) Simultaneous speech translation, (ii) Video speech translation, (iii) Offline speech translation, (iv) Conversational speech translation, (v) Open domain translation, and (vi) Non-native speech translation. A total of teams participated in at least one of the tracks. This paper introduces each track’s goal, data and evaluation metrics, and reports the results of the received submissions.
- Stefan Constantin, A. Waibel. 2020. Error correction and extraction in request dialogs. Abstract: We propose a component that gets a request and a correction and outputs a corrected request. To get this corrected request, the entities in the correction phrase replace their corresponding entities in the request. In addition, the proposed component outputs these pairs of corresponding reparandum and repair entity. These entity pairs can be used, for example, for learning in a life-long learning component of a dialog system to reduce the need for correction in future dialogs. For the approach described in this work, we fine-tune BERT for sequence labeling. We created a dataset to evaluate our component; for which we got an accuracy of 93.28 %. An accuracy of 88.58 % has been achieved for out-of-domain data. This accuracy shows that the proposed component is learning the concept of corrections and can be developed to be used as an upstream component to avoid the need for collecting data for request corrections for every new domain.
- T. Nguyen, J. Niehues, Eunah Cho, Thanh-Le Ha, Kevin Kilgour, Markus Muller, Matthias Sperber, S. Stueker, A. Waibel. 2020. Low Latency ASR for Simultaneous Speech Translation. Abstract: User studies have shown that reducing the latency of our simultaneous lecture translation system should be the most important goal. We therefore have worked on several techniques for reducing the latency for both components, the automatic speech recognition and the speech translation module. Since the commonly used commitment latency is not appropriate in our case of continuous stream decoding, we focused on word latency. We used it to analyze the performance of our current system and to identify opportunities for improvements. In order to minimize the latency we combined run-on decoding with a technique for identifying stable partial hypotheses when stream decoding and a protocol for dynamic output update that allows to revise the most recent parts of the transcription. This combination reduces the latency at word level, where the words are final and will never be updated again in the future, from 18.1s to 1.1s without sacrificing performance in terms of word error rate.
- Dario Franceschini, Chiara Canton, I. Simonini, Armin Schweinfurth, Adelheid Glott, Sebastian Stüker, T. Nguyen, Felix Schneider, Thanh-Le Ha, A. Waibel, B. Haddow, P. Williams, Rico Sennrich, Ondrej Bojar, Sangeet Sagar, Dominik Machácek, O. Smrz. 2020. Removing European Language Barriers with Innovative Machine Translation Technology. Abstract: This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech translation for conferences and remote meetings live subtitling. The platform has been designed with a focus on very low latency and high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided on the most important components and we summarize the test deployment events we ran so far.
- Matthias Sperber, Ngoc-Quan Pham, T. Nguyen, J. Niehues, Markus Müller, Thanh-Le Ha, Sebastian Stüker, A. Waibel. 2020. KIT’s IWSLT 2020 SLT Translation System. Abstract: This paper describes KIT’s submissions to the IWSLT2020 Speech Translation evaluation campaign. We first participate in the simultaneous translation task, in which our simultaneous models are Transformer based and can be efficiently trained to obtain low latency with minimized compromise in quality. On the offline speech translation task, we applied our new Speech Transformer architecture to end-to-end speech translation. The obtained model can provide translation quality which is competitive to a complicated cascade. The latter still has the upper hand, thanks to the ability to transparently access to the transcription, and resegment the inputs to avoid fragmentation.
- Maciej Modrzejewski, M. Exel, Bianka Buschbeck, Thanh-Le Ha, A. Waibel. 2020. Incorporating External Annotation to improve Named Entity Translation in NMT. Abstract: The correct translation of named entities (NEs) still poses a challenge for conventional neural machine translation (NMT) systems. This study explores methods incorporating named entity recognition (NER) into NMT with the aim to improve named entity translation. It proposes an annotation method that integrates named entities and inside–outside–beginning (IOB) tagging into the neural network input with the use of source factors. Our experiments on English→German and English→ Chinese show that just by including different NE classes and IOB tagging, we can increase the BLEU score by around 1 point using the standard test set from WMT2019 and achieve up to 12% increase in NE translation rates over a strong baseline.
- Felix Schneider, A. Waibel. 2020. Towards Stream Translation: Adaptive Computation Time for Simultaneous Machine Translation. Abstract: Simultaneous machine translation systems rely on a policy to schedule read and write operations in order to begin translating a source sentence before it is complete. In this paper, we demonstrate the use of Adaptive Computation Time (ACT) as an adaptive, learned policy for simultaneous machine translation using the transformer model and as a more numerically stable alternative to Monotonic Infinite Lookback Attention (MILk). We achieve state-of-the-art results in terms of latency-quality tradeoffs. We also propose a method to use our model on unsegmented input, i.e. without sentence boundaries, simulating the condition of translating output from automatic speech recognition. We present first benchmark results on this task.
- A. Waibel. 2020. What is DARPA ? How to Design Successful Technology Disruption. Abstract: Introduction Throughout history, humanity’s successes and failures and the survival of societies and nations have derived in large parts from technical innovations and disruptive technologies that have replaced the status-quo with new and better ways of doing things. It is thus understandable, indeed necessary, that nations and governments ask what mechanisms and instruments they should put in place to encourage scientific discoveries and to create technical breakthroughs, particularly for technologies with a transformative, strategic dimension that a nation can ill-afford to miss or fail to understand, control and shape.
- Christian Huber, Juan Hussain, Tuan-Nam Nguyen, Kai Song, Sebastian Stüker, A. Waibel. 2020. Supervised Adaptation of Sequence-to-Sequence Speech Recognition Systems using Batch-Weighting. Abstract: When training speech recognition systems, one often faces the situation that sufficient amounts of training data for the language in question are available but only small amounts of data for the domain in question. This problem is even bigger for end-to-end speech recognition systems that only accept transcribed speech as training data, which is harder and more expensive to obtain than text data. In this paper we present experiments in adapting end-to-end speech recognition systems by a method which is called batch-weighting and which we contrast against regular fine-tuning, i.e., to continue to train existing neural speech recognition models on adaptation data. We perform experiments using theses techniques in adapting to topic, accent and vocabulary, showing that batch-weighting consistently outperforms fine-tuning. In order to show the generalization capabilities of batch-weighting we perform experiments in several languages, i.e., Arabic, English and German. Due to its relatively small computational requirements batch-weighting is a suitable technique for supervised life-long learning during the life-time of a speech recognition system, e.g., from user corrections.
- Juan Hussain, Mohammed Mediani, M. Behr, M. Amin Cheragui, Sebastian Stüker, A. Waibel. 2020. German-Arabic Speech-to-Speech Translation for Psychiatric Diagnosis. Abstract: In this paper we present the natural language processing components of our German-Arabic speech-to-speech translation system which is being deployed in the context of interpretation during psychiatric, diagnostic interviews. For this purpose we have built a pipe-lined speech-to-speech translation system consisting of automatic speech recognition, text post-processing/segmentation, machine translation and speech synthesis systems. We have implemented two pipe-lines, from German to Arabic and Arabic to German, in order to be able to conduct interpreted two-way dialogues between psychiatrists and potential patients. All systems in our pipeline have been realized as all-neural end-to-end systems, using different architectures suitable for the different components. The speech recognition systems use an encoder/decoder + attention architecture, the text segmentation component and the machine translation system are based on the Transformer architecture, and for the speech synthesis systems we use Tacotron 2 for generating spectrograms and WaveGlow as vocoder. The speech translation is deployed in a server-based speech translation application that implements a turn based translation between a German speaking psychiatrist administrating the Mini-International Neuropsychiatric Interview (M.I.N.I.) and an Arabic speaking person answering the interview. As this is a very specific domain, in addition to the linguistic challenges posed by translating between Arabic and German, we also focus in this paper on the methods we implemented for adapting our speech translation system to the domain of this psychiatric interview.
- Juan Hussain, Oussama Zenkri, Sebastian Stüker, A. Waibel. 2020. DaCToR: A Data Collection Tool for the RELATER Project. Abstract: Collecting domain-specific data for under-resourced languages, e.g., dialects of languages, can be very expensive, potentially financially prohibitive and taking long time. Moreover, in the case of rarely written languages, the normalization of non-canonical transcription might be another time consuming but necessary task. In order to collect domain-specific data in such circumstances in a time and cost-efficient way, collecting read data of pre-prepared texts is often a viable option. In order to collect data in the domain of psychiatric diagnosis in Arabic dialects for the project RELATER, we have prepared the data collection tool DaCToR for collecting read texts by speakers in the respective countries and districts in which the dialects are spoken. In this paper we describe our tool, its purpose within the project RELATER and the dialects which we have started to collect with the tool.
- T. Nguyen, Sebastian Stüker, A. Waibel. 2020. Toward Cross-Domain Speech Recognition with End-to-End Models. Abstract: In the area of multi-domain speech recognition, research in the past focused on hybrid acoustic models to build cross-domain and domain-invariant speech recognition systems. In this paper, we empirically examine the difference in behavior between hybrid acoustic models and neural end-to-end systems when mixing acoustic training data from several domains. For these experiments we composed a multi-domain dataset from public sources, with the different domains in the corpus covering a wide variety of topics and acoustic conditions such as telephone conversations, lectures, read speech and broadcast news. We show that for the hybrid models, supplying additional training data from other domains with mismatched acoustic conditions does not increase the performance on specific domains. However, our end-to-end models optimized with sequence-based criterion generalize better than the hybrid models on diverse domains. In term of word-error-rate performance, our experimental acoustic-to-word and attention-based models trained on multi-domain dataset reach the performance of domain-specific long short-term memory (LSTM) hybrid models, thus resulting in multi-domain speech recognition systems that do not suffer in performance over domain specific ones. Moreover, the use of neural end-to-end models eliminates the need of domain-adapted language models during recognition, which is a great advantage when the input domain is unknown.
- Ondrej Bojar, Dominik Machácek, Sangeet Sagar, O. Smrz, J. Kratochvíl, Ebrahim Ansari, Dario Franceschini, Chiara Canton, I. Simonini, T. Nguyen, Felix Schneider, Sebastian Stücker, A. Waibel, B. Haddow, Rico Sennrich, P. Williams. 2020. ELITR: European Live Translator. Abstract: ELITR (European Live Translator) project aims to create a speech translation system for simultaneous subtitling of conferences and online meetings targetting up to 43 languages. The technology is tested by the Supreme Audit Office of the Czech Republic and by alfaview®, a German online conferencing system. Other project goals are to advance document-level and multilingual machine translation, automatic speech recognition, and automatic minuting.
- F. Vogt, J. Quiring, C. Löffler, Andrea Haid, A. Zaugg, E. Frick, Mirja, Bohner-Kraus, Oscar Echkardt, Laura von Albedyhll, A. Waibel, Martina Zumtobel. 2020. Breaking through interdisciplinary barriers Abstract book. Abstract: book
- Zhong Zhou, Lori S. Levin, David R. Mortensen, A. Waibel. 2019. Using Interlinear Glosses as Pivot in Low-Resource Multilingual Machine Translation.. Abstract: We demonstrate a new approach to Neural Machine Translation (NMT) for low-resource languages using a ubiquitous linguistic resource, Interlinear Glossed Text (IGT). IGT represents a non-English sentence as a sequence of English lemmas and morpheme labels. As such, it can serve as a pivot or interlingua for NMT. Our contribution is four-fold. Firstly, we pool IGT for 1,497 languages in ODIN (54,545 glosses) and 70,918 glosses in Arapaho and train a gloss-to-target NMT system from IGT to English, with a BLEU score of 25.94. We introduce a multilingual NMT model that tags all glossed text with gloss-source language tags and train a universal system with shared attention across 1,497 languages. Secondly, we use the IGT gloss-to-target translation as a key step in an English-Turkish MT system trained on only 865 lines from ODIN. Thirdly, we we present five metrics for evaluating extremely low-resource translation when BLEU is no longer sufficient and evaluate the Turkish low-resource system using BLEU and also using accuracy of matching nouns, verbs, agreement, tense, and spurious repetition, showing large improvements.
- Ngoc-Quan Pham, T. Nguyen, Thanh-Le Ha, Juan Hussain, Felix Schneider, J. Niehues, Sebastian Stüker, A. Waibel. 2019. The IWSLT 2019 KIT Speech Translation System. Abstract: This paper describes KIT’s submission to the IWSLT 2019 Speech Translation task on two sub-tasks corresponding to two different datasets. We investigate different end-to-end architectures for the speech recognition module, including our new transformer-based architectures. Overall, our modules in the pipe-line are based on the transformer architecture which has recently achieved great results in various fields. In our systems, using transformer is also advantageous compared to traditional hybrid systems in term of simplicity while still having competent results.
- Ngoc-Quan Pham, J. Niehues, Thanh-Le Ha, A. Waibel. 2019. Improving Zero-shot Translation with Language-Independent Constraints. Abstract: An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots.
- Felix Schneider, A. Waibel. 2019. KIT’s Submission to the IWSLT 2019 Shared Task on Text Translation. Abstract: In this paper, we describe KIT’s submission for the IWSLT 2019 shared task on text translation. Our system is based on the transformer model [1] using our in-house implementation. We augment the available training data using back-translation and employ fine-tuning for the final model. For our best results, we used a 12-layer transformer-big config- uration, achieving state-of-the-art results on the WMT2018 test set. We also experiment with student-teacher models to improve performance of smaller models.
- T. Nguyen, Sebastian Stüker, A. Waibel. 2019. Using multi-task learning to improve the performance of acoustic-to-word and conventional hybrid models. Abstract: Acoustic-to-word (A2W) models that allow direct mapping from acoustic signals to word sequences are an appealing approach to end-to-end automatic speech recognition due to their simplicity. However, prior works have shown that modelling A2W typically encounters issues of data sparsity that prevent training such a model directly. So far, pre-training initialization is the only approach proposed to deal with this issue. In this work, we propose to build a shared neural network and optimize A2W and conventional hybrid models in a multi-task manner. Our results show that training an A2W model is much more stable with our multi-task model without pre-training initialization, and results in a significant improvement compared to a baseline model. Experiments also reveal that the performance of a hybrid acoustic model can be further improved when jointly training with a sequence-level optimization criterion such as acoustic-to-word.
- Tino Fuhrman, D. Schneider, Felix Altenberg, T. Nguyen, Simon Blasen, Stefan Constantin, A. Waibel. 2019. An Interactive Indoor Drone Assistant. Abstract: With the rapid advance of sophisticated control algorithms, the capabilities of drones to stabilise, fly and manoeuvre autonomously have dramatically improved, enabling us to pay greater attention to entire missions and the interaction of a drone with humans and with its environment during the course of such a mission. In this paper, we present an indoor office drone assistant that is tasked to run errands and carry out simple tasks at our laboratory, while given instructions from and interacting with humans in the space. To accomplish its mission, the system has to be able to understand verbal instructions from humans, and perform subject to constraints from control and hardware limitations, uncertain localisation information, unpredictable and uncertain obstacles and environmental factors. We combine and evaluate the dialogue, navigation, flight control, depth perception and collision avoidance components. We discuss performance and limitations of our assistant at the component as well as the mission level. A 78% mission success rate was obtained over the course of 27 missions.
- Matthias Sperber, Graham Neubig, Ngoc-Quan Pham, A. Waibel. 2019. Self-Attentional Models for Lattice Inputs. Abstract: Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. Previous work has extended recurrent neural networks to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. To extend such models to handle lattices, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also propose a method for adapting positional embeddings to lattice structures. We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both training and inference.
- Zhong Zhou, Lori S. Levin, David R. Mortensen, A. Waibel. 2019. Low-Resource Machine Translation using Interlinear Glosses. Abstract: Neural Machine Translation (NMT) does not handle low-resource translation well because NMT is data-hungry and low-resource languages, by their nature, have limited parallel data. Many low-resource languages are morphologically rich, which complicates matters further by increasing data sparsity. However, a good linguist is capable of building a morphological analyzer in far fewer hours than it would take to collect and translate the amount of parallel data needed for conventional NMT. We combine the benefits of both NMT and linguistic information in our work. We use morphological analyzer to automatically generate interlinear glosses with dictionary or parallel data, and translate the source text to interlinear gloss as an interlingua representation, and finally translate into the target text using NMT trained on the ODIN dataset that includes a large collection of interlinear glosses and their corresponding target translations. Our result for translating from the interlinear gloss to the target text using the entire ODIN dataset achieves a BLEU score of 35.07. And our qualitative results show positive findings in a low-resource scenario of Turkish-English translation using 865 lines of training data. Our translation system yield better results than training NMT directly from the source language to the target language in a constrained-data setting, and is helpful to produce translation with sufficiently good content and fluency when data is scarce.
- Verena Heusser, Niklas Freymuth, Stefan Constantin, A. Waibel. 2019. Bimodal Speech Emotion Recognition Using Pre-Trained Language Models. Abstract: Speech emotion recognition is a challenging task and an important step towards more natural human-machine interaction. We show that pre-trained language models can be fine-tuned for text emotion recognition, achieving an accuracy of 69.5% on Task 4A of SemEval 2017, improving upon the previous state of the art by over 3% absolute. We combine these language models with speech emotion recognition, achieving results of 73.5% accuracy when using provided transcriptions and speech data on a subset of four classes of the IEMOCAP dataset. The use of noise-induced transcriptions and speech data results in an accuracy of 71.4%. For our experiments, we created IEmoNet, a modular and adaptable bimodal framework for speech emotion recognition based on pre-trained language models. Lastly, we discuss the idea of using an emotional classifier as a reward for reinforcement learning as a step towards more successful and convenient human-machine interaction.
- Matthias Sperber, Graham Neubig, J. Niehues, A. Waibel. 2019. Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation. Abstract: Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts. Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech. However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora. In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage. As a remedy, we propose the use of end- to-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. We show that such models naturally decompose into multi-task–trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages. Our proposed model outperforms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models.
- T. Nguyen, S. Stueker, J. Niehues, A. Waibel. 2019. Improving Sequence-To-Sequence Speech Recognition Training with On-The-Fly Data Augmentation. Abstract: Sequence-to-Sequence (S2S) models recently started to show state-of-the-art performance for automatic speech recognition (ASR). With these large and deep models overfitting remains the largest problem, outweighing performance improvements that can be obtained from better architectures. One solution to the overfitting problem is increasing the amount of available training data and the variety exhibited by the training data with the help of data augmentation. In this paper we examine the influence of three data augmentation methods on the performance of two S2S model architectures. One of the data augmentation method comes from literature, while two other methods are our own development – a time perturbation in the frequency domain and sub-sequence sampling. Our experiments on Switchboard and Fisher data show state-of-the-art performance for S2S models that are trained solely on the speech training data and do not use additional text data.
- T. Nguyen, Sebastian Stüker, A. Waibel. 2019. Learning Shared Encoding Representation for End-to-End Speech Recognition Models. Abstract: In this work, we learn a shared encoding representation for a multi-task neural network model optimized with connectionist temporal classification (CTC) and conventional framewise cross-entropy training criteria. Our experiments show that the multi-task training not only tackles the complexity of optimizing CTC models such as acoustic-to-word but also results in significant improvement compared to the plain-task training with an optimal setup. Furthermore, we propose to use the encoding representation learned by the multi-task network to initialize the encoder of attention-based models. Thereby, we train a deep attention-based end-to-end model with 10 long short-term memory (LSTM) layers of encoder which produces 12.2\% and 22.6\% word-error-rate on Switchboard and CallHome subsets of the Hub5 2000 evaluation.
- A. Waibel. 2019. Connecting Humans with Humans: Multimodal, Multilingual, Multiparty Mediation. Abstract: Behind much of my research work over 4 decades has been the simple observation that people like people and love interacting with other people more than they like interacting with machines. Technologies that truly support such social desires are more likely to be adopted broadly. Consider email, texting, chat rooms, social media, video conferencing, the internet, speech translation, even videogames with a social element (e.g., Fortnite): we enjoy the technology whenever it brings us closer to our fellow humans, instead of imposing attention-grabbing clutter. If so, how then can we build better technologies that improve, encourage, support human-human interaction? In this talk, I will recount my own story along this journey. When I began, building technologies for the human-human experience, presented formidable challenges: Computer interfaces would need to anticipate and understand the way humans interact, but in 1976, a typical computer had only two instructions to interact with humans: character-in & character-out, and both only supported human-computer interaction. Over the decades that followed, we began to develop interfaces that can process the various modalities of human communication and we built systems that used several modalities in services to improve human-human interaction. These included: In my talk, I will discuss the challenges of interpreting multimodal signals of human-human interaction in the wild. I will show the resulting human-human systems, we developed and how to make them effective. Some went on to become services that affect the way we work and communicate today.
- Stefan Constantin, J. Niehues, A. Waibel. 2019. Incremental processing of noisy user utterances in the spoken language understanding task. Abstract: The state-of-the-art neural network architectures make it possible to create spoken language understanding systems with high quality and fast processing time. One major challenge for real-world applications is the high latency of these systems caused by triggered actions with high executions times. If an action can be separated into subactions, the reaction time of the systems can be improved through incremental processing of the user utterance and starting subactions while the utterance is still being uttered. In this work, we present a model-agnostic method to achieve high quality in processing incrementally produced partial utterances. Based on clean and noisy versions of the ATIS dataset, we show how to create datasets with our method to create low-latency natural language understanding components. We get improvements of up to 47.91 absolute percentage points in the metric F1-score.
- Markus Müller, Sebastian Stüker, A. Waibel. 2019. Neural Codes to Factor Language in Multilingual Speech Recognition. Abstract: In the past, we adapted neural network based multilingual acoustic models using language codes. In this work, we study the extracted language codes and the language properties they encode: We use the codes to generate language prototype vectors, which represent the features of a language. Computing distances between prototype vectors shows that languages from the same family have smaller distances. This structure found within the feature representation supports the assumption that language codes do encode language information and not other properties like, e.g. channel characteristics, and in addition providing a richer language representation than the language identity alone.The network architecture of our system is based on a factorized model, which consists of multiple language dependent subnets. While we recently demonstrated that this approach enables multilingual setups to outperform monolingual ones, we here propose further optimizations. We evaluated using a) more language dependent subnets and b) wider BiLSTM layers. Our results indicate that using a larger number of language dependent subnets increases the system performance and renders phonetic pretraining superfluous. In addition, increasing the size of the hidden layers further improved the performance, with the system now outperforming the monolingual baseline by 6.3% relative.
- Elizabeth Salesky, Matthias Sperber, A. Waibel. 2019. Fluent Translations from Disfluent Speech in End-to-End Speech Translation. Abstract: Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected ‘copy-edited’ references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, implicitly removing disfluencies in end-to-end translation of conversational speech.
- Ngoc-Quan Pham, T. Nguyen, J. Niehues, Markus Müller, A. Waibel. 2019. Very Deep Self-Attention Networks for End-to-End Speech Recognition. Abstract: Recently, end-to-end sequence-to-sequence models for speech recognition have gained significant interest in the research community. While previous architecture choices revolve around time-delay neural networks (TDNN) and long short-term memory (LSTM) recurrent neural networks, we propose to use self-attention via the Transformer architecture as an alternative. Our analysis shows that deep Transformer networks with high learning capacity are able to exceed performance from previous end-to-end approaches and even match the conventional hybrid systems. Moreover, we trained very deep models with up to 48 Transformer layers for both encoder and decoders combined with stochastic residual connections, which greatly improve generalizability and training efficiency. The resulting models outperform all previous end-to-end ASR approaches on the Switchboard benchmark. An ensemble of these models achieve 9.9% and 17.7% WER on Switchboard and CallHome test sets respectively. This finding brings our end-to-end models to competitive levels with previous hybrid systems. Further, with model ensembling the Transformers can outperform certain hybrid systems, which are more complicated in terms of both structure and training procedure.
- Andreas Sudmann, A. Waibel. 2019. “That is a 1984 Orwellian future at our doorstep, right?“. Abstract: Empfohlene Zitierung / Suggested Citation: Sudmann, Andreas; Waibel, Alexander: “That is a 1984 Orwellian future at our doorstep, right?“ Natural Language Processing, Artificial Neural Networks and the Politics of (Democratizing) AI. In: Andreas Sudmann (Hg.): The democratization of artificial intelligence. Net politics in the era of learning algorithms. Bielefeld: transcript 2019, S. 313– 323. DOI: https://doi.org/10.25969/mediarep/13547.
- Patrick Huber, J. Niehues, A. Waibel. 2018. A Hierarchical Approach to Neural Context-Aware Modeling. Abstract: We present a new recurrent neural network topology to enhance state-of-the-art machine learning systems by incorporating a broader context. Our approach overcomes recent limitations with extended narratives through a multi-layered computational approach to generate an abstract context representation. Therefore, the developed system captures the narrative on word-level, sentence-level, and context-level. Through the hierarchical set-up, our proposed model summarizes the most salient information on each level and creates an abstract representation of the extended context. We subsequently use this representation to enhance neural language processing systems on the task of semantic error detection. To show the potential of the newly introduced topology, we compare the approach against a context-agnostic set-up including a standard neural language model and a supervised binary classification network. The performance measures on the error detection task show the advantage of the hierarchical context-aware topologies, improving the baseline by 12.75% relative for unsupervised models and 20.37% relative for supervised models.
- T. Nguyen, Sebastian Stiiker, A. Waibel. 2018. Exploring Ctc-Network Derived Features with Conventional Hybrid System. Abstract: Recently in automatic speech recognition (ASR) a lot of attention has been given to decoding optimization to boost the performance of connectionist temporal classification criterion (CTC) systems in all neural setups. Different from that, we investigated the use of the output of CTC network as input features to traditional HMM/ANN hybrid systems. By doing so, we benefit from the strengths of the CTC network at label discrimination and the highly optimized decoding stack of conventional hybrid systems. In a Switchboard setup, a feedforward network system using our proposed CTC-network derived features with cross-entropy training outperforms a strong CTC baseline by a margin of 5% rel. in word error rate. With the same model, we achieved further improvements of 9% rel. when combining them with bottleneck features. Additionally, we revealed the possible elimination of the blank label during decoding and the alignment relationship between the CTC model and the traditional HMM system.
- Markus Müller, Sebastian Stüker, A. Waibel. 2018. E NHANCING M ULTILINGUAL G RAPHEMIC RNN B ASED ASR S YSTEMS U SING P HONE I NFORMATION. Abstract: : In the past, we proposed the use of Language Feature Vectors (LFVs) to better adapt multilingual speech recognition systems to languages. Recently, we applied this method to RNN/CTC based systems. The recognition accuracy could be improved by modulating the network using LFVs. In this work, we propose an improvement to this approach by reﬁning the network architecture as well as the training strategy. We ﬁrst evaluated multiple methods for applying the modulation. As we are using bi-directional layers, each unit outputs two values, one per direction. Optimizing the combination of the outputs for each direction did improve the performance. In addition, we propose a method for including phonetic information into the training process of a graphemic system. By pre-training layers using phones as targets, the network did learn features to discriminate phones. Adding more layers and a two stage ﬁne-tuning process using graphemes, we ﬁrst forced the network map phonetic features to graphemes. In the second stage, we allowed the network to update the phonetic feature detectors as well. Both methods improved the performance of our setup. We evaluated our setup using a combination of 4 languages (English, French, German, Turkish), with a joint set of acoustic units.
- Maren Kucza, J. Niehues, Thomas Zenkel, A. Waibel, Sebastian Stüker. 2018. Term Extraction via Neural Sequence Labeling a Comparative Evaluation of Strategies Using Recurrent Neural Networks. Abstract: Traditionally systems for term extraction use a two stage approach of ﬁrst identifying candiate terms, and the scoring them in a second process for identifying actual terms. Thus, research in this ﬁeld has often mainly focused on reﬁning and improv-ing the scoring process of term candidates, which commonly are identiﬁed using linguistic and statistical features. Machine learning techniques and especially neural networks are currently only used in the second stage, that is to score candidates and classify them. In contrast to that we have built a system that identiﬁes terms via directly performing sequence-labeling with a BILOU scheme on word sequences. To do so we have worked with different kinds of recurrent neural networks and word embeddings. In this paper we describe how one can built a state-of-the-art term extraction systems with this single-stage technique and compare different network types and topologies and also exam-ine the inﬂuence of the type of input embedding used for the task. We further investigated which network types and topologies are best suited when applying our term extraction systems to other domains than that of the training data of the networks.
- Zhong Zhou, Matthias Sperber, A. Waibel. 2018. Paraphrases as Foreign Languages in Multilingual Neural Machine Translation. Abstract: Paraphrases, rewordings of the same semantic meaning, are useful for improving generalization and translation. Unlike previous works that only explore paraphrases at the word or phrase level, we use different translations of the whole training data that are consistent in structure as paraphrases at the corpus level. We treat paraphrases as foreign languages, tag source sentences with paraphrase labels, and train on parallel paraphrases in the style of multilingual Neural Machine Translation (NMT). Our multi-paraphrase NMT that trains only on two languages outperforms the multilingual baselines. Adding paraphrases improves the rare word translation and increases entropy and diversity in lexical choice. Adding the source paraphrases boosts performance better than adding the target ones, while adding both lifts performance further. We achieve a BLEU score of 57.2 for French-to-English translation using 24 corpus-level paraphrases of the Bible, which outperforms the multilingual baselines and is +34.7 above the single-source single-target NMT baseline.
- Thanh-Le Ha, J. Niehues, Matthias Sperber, Ngoc-Quan Pham, A. Waibel. 2018. KIT-Multi: A Translation-Oriented Multilingual Embedding Corpus. Abstract: Cross-lingual word embeddings are the representations of words across languages in a shared continuous vector space. Cross-lingual word embeddings have been shown to be helpful in the development of cross-lingual natural language processing tools. In case of more than two languages involved, we call them multilingual word embeddings. In this work, we introduce a multilingual word embedding corpus which is acquired by using neural machine translation. Unlike other cross-lingual embedding corpora, the embeddings can be learned from significantly smaller portions of data and for multiple languages at once. An intrinsic evaluation on monolingual tasks shows that our method is fairly competitive to the prevalent methods but on the cross-lingual document classification task, it obtains the best figures. We are in the process to produce the embeddings for more languages, especially the languages which belong to the same family or sematically close to each others, such as Japanese-Korean, Chinese-Vietnamese, German-Dutch, or Latin-based languagues. Furthermore, the corpus is being analyzedd regarding its usage and usefulness in other cross-lingual tasks.
- Markus Müller, Sebastian Stüker, A. Waibel. 2018. Neural Language Codes for Multilingual Acoustic Models. Abstract: Multilingual Speech Recognition is one of the most costly AI problems, because each language (7,000+) and even different accents require their own acoustic models to obtain best recognition performance. Even though they all use the same phoneme symbols, each language and accent imposes its own coloring or "twang". Many adaptive approaches have been proposed, but they require further training, additional data and generally are inferior to monolingually trained models. In this paper, we propose a different approach that uses a large multilingual model that is \emph{modulated} by the codes generated by an ancillary network that learns to code useful differences between the "twangs" or human language. 
We use Meta-Pi networks to have one network (the language code net) gate the activity of neurons in another (the acoustic model nets). Our results show that during recognition multilingual Meta-Pi networks quickly adapt to the proper language coloring without retraining or new data, and perform better than monolingually trained networks. The model was evaluated by training acoustic modeling nets and modulating language code nets jointly and optimize them for best recognition performance.
- F. Hamlaoui, Emmanuel-Moselly Makasso, Markus Müller, Jonas Engelmann, G. Adda, A. Waibel, Sebastian Stüker. 2018. BULBasaa: A Bilingual Basaa-French Speech Corpus for the Evaluation of Language Documentation Tools. Abstract: B`as`a´a is one of the three Bantu languages of BULB ( Breaking the Unwritten Language Barrier ), a project whose aim is to provide NLP-based tools to support linguists in documenting under-resourced and unwritten languages. To develop technologies such as automatic phone transcription or machine translation, a massive amount of speech data is needed. Approximately 50 hours of B`as`a´a speech were thus collected and then carefully re-spoken and orally translated into French in a controlled environment by a few bilingual speakers. For a subset of ≈ 10 hours of the corpus, each utterance was additionally phonetically transcribed to establish a golden standard for the output of our NLP tools. The experiments described in this paper are meant to provide an automatic phonetic transcription using a set of derived phone-like units. As every language features a speciﬁc set of idiosyncrasies, automating the process of phonetic unit discovery in its entirety is a challenging task. Within BULB, we envision a workﬂow where linguists are able to reﬁne the set of automatically discovered units and the system is then able to re-iterate on the data, providing a better approximation of the actual phone set.
- J. Niehues, Ngoc-Quan Pham, Thanh-Le Ha, Matthias Sperber, A. Waibel. 2018. Low-Latency Neural Speech Translation. Abstract: Through the development of neural machine translation, the quality of machine translation systems has been improved significantly. By exploiting advancements in deep learning, systems are now able to better approximate the complex mapping from source sentences to target sentences. But with this ability, new challenges also arise. An example is the translation of partial sentences in low-latency speech translation. Since the model has only seen complete sentences in training, it will always try to generate a complete sentence, though the input may only be a partial sentence. We show that NMT systems can be adapted to scenarios where no task-specific training data is available. Furthermore, this is possible without losing performance on the original training data. We achieve this by creating artificial data and by using multi-task learning. After adaptation, we are able to reduce the number of corrections displayed during incremental output construction by 45%, without a decrease in translation quality.
- Patrick Huber, J. Niehues, A. Waibel. 2018. Automated Evaluation of Out-of-Context Errors. Abstract: We present a new approach to evaluate computational models for the task of text understanding by the means of out-of-context error detection. Through the novel design of our automated modification process, existing large-scale data sources can be adopted for a vast number of text understanding tasks. The data is thereby altered on a semantic level, allowing models to be tested against a challenging set of modified text passages that require to comprise a broader narrative discourse. Our newly introduced task targets actual real-world problems of transcription and translation systems by inserting authentic out-of-context errors. The automated modification process is applied to the 2016 TEDTalk corpus. Entirely automating the process allows the adoption of complete datasets at low cost, facilitating supervised learning procedures and deeper networks to be trained and tested. To evaluate the quality of the modification algorithm a language model and a supervised binary classification model are trained and tested on the altered dataset. A human baseline evaluation is examined to compare the results with human performance. The outcome of the evaluation task indicates the difficulty to detect semantic errors for machine-learning algorithms and humans, showing that the errors cannot be identified when limited to a single sentence.
- Ngoc-Quan Pham, J. Niehues, A. Waibel. 2018. The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2018. Abstract: We present our experiments in the scope of the news translation task in WMT 2018, in directions: English→German. The core of our systems is the encoder-decoder based neural machine translation models using the transformer architecture. We enhanced the model with a deeper architecture. By using techniques to limit the memory consumption, we were able to train models that are 4 times larger on one GPU and improve the performance by 1.2 BLEU points. Furthermore, we performed sentence selection for the newly available ParaCrawl corpus. Thereby, we could improve the effectiveness of the corpus by 0.5 BLEU points.
- Florian Dessloch, Thanh-Le Ha, Markus Müller, J. Niehues, T. Nguyen, Ngoc-Quan Pham, Elizabeth Salesky, Matthias Sperber, Sebastian Stüker, Thomas Zenkel, A. Waibel. 2018. KIT Lecture Translator: Multilingual Speech Translation with One-Shot Learning. Abstract: In today’s globalized world we have the ability to communicate with people across the world. However, in many situations the language barrier still presents a major issue. For example, many foreign students coming to KIT to study are initially unable to follow a lecture in German. Therefore, we offer an automatic simultaneous interpretation service for students. To fulfill this task, we have developed a low-latency translation system that is adapted to lectures and covers several language pairs. While the switch from traditional Statistical Machine Translation to Neural Machine Translation (NMT) significantly improved performance, to integrate NMT into the speech translation framework required several adjustments. We have addressed the run-time constraints and different types of input. Furthermore, we utilized one-shot learning to easily add new topic-specific terms to the system. Besides better performance, NMT also enabled us increase our covered languages through multilingual NMT. % Combining these techniques, we are able to provide an adapted speech translation system for several European languages.
- Elizabeth Salesky, Susanne Burger, J. Niehues, A. Waibel. 2018. Towards Fluent Translations From Disfluent Speech. Abstract: When translating from speech, special consideration for conversational speech phenomena such as disfluencies is necessary. Most machine translation training data consists of well-formed written texts, causing issues when translating spontaneous speech. Previous work has introduced an intermediate step between speech recognition (ASR) and machine translation (MT) to remove disfluencies, making the data better-matched to typical translation text and significantly improving performance. However, with the rise of end-to-end speech translation systems, this intermediate step must be incorporated into the sequence-to-sequence architecture. Further, though translated speech datasets exist, they are typically news or rehearsed speech without many disfluencies (e.g. TED), or the disfluencies are translated into the references (e.g. Fisher). To generate clean translations from disfluent speech, cleaned references are necessary for evaluation. We introduce a corpus of cleaned target data for the Fisher Spanish-English dataset for this task. We compare how different architectures handle disfluencies and provide a baseline for removing disfluencies in end-to-end translation.
- Thomas Zenkel, Matthias Sperber, J. Niehues, Markus Müller, Ngoc-Quan Pham, Sebastian Stüker, A. Waibel. 2018. Open Source Toolkit for Speech to Text Translation. Abstract: Abstract In this paper we introduce an open source toolkit for speech translation. While there already exists a wide variety of open source tools for the essential tasks of a speech translation system, our goal is to provide an easy to use recipe for the complete pipeline of translating speech. We provide a Docker container with a ready to use pipeline of the following components: a neural speech recognition system, a sentence segmentation system and an attention-based translation system. We provide recipes for training and evaluating models for the task of translating English lectures and TED talks to German. Additionally, we provide pre-trained models for this task. With this toolkit we hope to facilitate the development of speech translation systems and to encourage researchers to improve the overall performance of speech translation systems.
- Jörg K.H. Franke, J. Niehues, A. Waibel. 2018. Robust and Scalable Differentiable Neural Computer for Question Answering. Abstract: Deep learning models are often not easily adaptable to new tasks and require task-specific adjustments. The differentiable neural computer (DNC), a memory-augmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this model to new tasks. We analyze the DNC and identify possible improvements within the application of question answering. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this model intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new tasks with passable results on the CNN RC task without adaptions.
- Markus Müller, Sebastian Stüker, A. Waibel. 2018. Parameter Optimization for CTC Acoustic Models in a Less-resourced Scenario: An Empirical Study. Abstract: In this work, we performed a study evaluating parameter configurations for acoustic models trained using the connectionist temporal classification (CTC) loss function. We varied the sizes of the hidden layers and mini-batches. We further used different optimizer strategies, comparing using SGD with a static learning rate and newbob scheduling. We further evaluated multiple configurations for data augmentation: As the acoustic features were extracted using overlapping windows, neighboring frames contain redundant information. By systematically omitting frames, thus creating multiple versions of the same utterances, the network can be trained on "more" data. Applying this scheme, we generated multiple versions of each utterance using different shifts. We evaluated parameter configurations using two conditions: A multilingual setup with 4 languages and 45h of data per language which can be considered less-resourced in the regimen of RNN/CTC acoustic models. In addition, we used a very low-resource dataset to assess network configurations if less than 10h of data is available.
- Matthias Sperber, J. Niehues, Graham Neubig, Sebastian Stüker, A. Waibel. 2018. Self-Attentional Acoustic Models. Abstract: Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.
- Ngoc-Quan Pham, J. Niehues, A. Waibel. 2018. Towards one-shot learning for rare-word translation with external experts. Abstract: Neural machine translation (NMT) has significantly improved the quality of automatic translation models. One of the main challenges in current systems is the translation of rare words. We present a generic approach to address this weakness by having external models annotate the training data as Experts, and control the model-expert interaction with a pointer network and reinforcement learning. Our experiments using phrase-based models to simulate Experts to complement neural machine translation models show that the model can be trained to copy the annotations into the output consistently. We demonstrate the benefit of our proposed framework in outof domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English-Spanish and German-English.
- Matthias Sperber, Ngoc-Quan Pham, T. Nguyen, J. Niehues, Markus Müller, Thanh-Le Ha, Sebastian Stüker, A. Waibel. 2018. KIT’s IWSLT 2018 SLT Translation System. Abstract: This paper describes KIT’s submission to the IWSLT 2018 Translation task. We describe a system participating in the baseline condition and a system participating in the end-to-end condition. The baseline system is a cascade of an ASR system, a system to segment the ASR output and a neural machine translation system. We investigate the combination of different ASR systems. For the segmentation and machine translation components, we focused on transformer-based architectures.
- Sebastian Stüker, Stefan Constantin, J. Niehues, T. Nguyen, Markus Müller, Ngoc-Quan Pham, Robin Rüde, A. Waibel. 2018. Speech interaction strategies for a humanoid assistant. Abstract: The goal of SecondHands, a H2020 project, is to design a robot that can offer help to a maintenance technician in a proactive manner. The robot is to act as a second pair of hands that can assist the technician when he is in need of help. In order for the robot to be of real help to the technician, it needs to understand his needs and follow his commands. Interaction via speech is a crucial part of this. Due to the nature of the situation in which the interactions take place, often the technician needs to speak to the robot when under stress performing strenuous physical labor, the classical turn based interaction schemes need to be transformed into dialogue systems that perform stream processing, anticipating user intentions, correcting itself as more information become available, in order to be able to respond in a rapid manner. In order to meet these demands, we are developing low-latency streaming based automatic speech recognition systems in combination with recurrent neural network based Natural Language Understanding systems that perform slot filling and intent recognition in order for the robot to provide assistance in a rapid manner, that can be partly based on speculative classifications that are then being refined as more speech becomes available.
- Patrick Huber, J. Niehues, A. Waibel. 2018. C L ] 2 3 M ar 2 01 8 Automated Evaluation of Out-of-Context Errors. Abstract: We present a new approach to evaluate computational models for the task of text understanding by the means of out-of-context error detection. Through the novel design of our automated modification process, existing large-scale data sources can be adopted for a vast number of text understanding tasks. The data is thereby altered on a semantic level, allowing models to be tested against a challenging set of modified text passages that require to comprise a broader narrative discourse. Our newly introduced task targets actual real-world problems of transcription and translation systems by inserting authentic out-of-context errors. The automated modification process is applied to the 2016 TEDTalk corpus. Entirely automating the process allows the adoption of complete datasets at low cost, facilitating supervised learning procedures and deeper networks to be trained and tested. To evaluate the quality of the modification algorithm a language model and a supervised binary classification model are trained and tested on the altered dataset. A human baseline evaluation is examined to compare the results with human performance. The outcome of the evaluation task indicates the difficulty to detect semantic errors for machine-learning algorithms and humans, showing that the errors cannot be identified when limited to a single sentence.
- Stefan Constantin, J. Niehues, A. Waibel. 2018. Multi-task learning to improve natural language understanding. Abstract: Recently advancements in sequence-to-sequence neural network architectures have led to an improved natural language understanding. When building a neural network-based Natural Language Understanding component, one main challenge is to collect enough training data. The generation of a synthetic dataset is an inexpensive and quick way to collect data. Since this data often has less variety than real natural language, neural networks often have problems to generalize to unseen utterances during testing. In this work, we address this challenge by using multi-task learning. We train out-of-domain real data alongside in-domain synthetic data to improve natural language understanding. We evaluate this approach in the domain of airline travel information with two synthetic datasets. As out-of-domain real data, we test two datasets based on the subtitles of movies and series. By using an attention-based encoder-decoder model, we were able to improve the F1-score over strong baselines from 80.76 % to 84.98 % in the smaller synthetic dataset.
- Zhong Zhou, Matthias Sperber, A. Waibel. 2018. Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation. Abstract: We work on translation from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a translation system that addresses these challenges using eight European language families as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6% accuracy in qualitative evaluation where our translations are akin to human translations in a preliminary study.
- Markus Müller, Jörg K.H. Franke, A. Waibel, Sebastian Stüker. 2017. Towards phoneme inventory discovery for documentation of unwritten languages. Abstract: Documenting unwritten languages is a challenging task, even for trained specialists. To help linguists in better and faster documenting new languages is the goal of the French-German ANR-DFG project BULB. To discover the phonetic inventory of a language the project follows three steps: estimating phoneme boundaries, classifying articulatory features (AFs) for each individual segment and clustering the segments into a phoneme inventory. In this work, we focus on estimating the phoneme boundaries and the extraction of AFs, but also perform a first simple clustering based on the recognized AFs. We demonstrate that our Deep Bidirectional LSTM-based approach for identifying phoneme boundaries achieves state-of-the-art performance and evaluate AF extraction based on feed forward neural networks.
- T. Nguyen, Markus Mueller, Matthias Sperber, Thomas Zenkel, S. Stueker, A. Waibel. 2017. The 2017 KIT IWSLT Speech-to-Text Systems for English and German. Abstract: This paper describes our German and English Speech-to-Text (STT) systems for the 2017 IWSLT evaluation campaign. The campaign focuses on the transcription of unsegmented lecture talks. Our setup includes systems using both the Janus and Kaldi frameworks. We combined the outputs using both ROVER [1] and confusion network combination (CNC) [2] to achieve a good overall performance. The individual subsystems are built by using different speaker-adaptive feature combination (e.g., lMEL with i-vector or bottleneck speaker vector), acoustic models (GMM or DNN) and speaker adaptation (MLLR or fMLLR). Decoding is performed in two stages, where the GMM and DNN systems are adapted on the combination of the first stage outputs using MLLR, and fMLLR. The combination setup produces a final hypothesis that has a significantly lower WER than any of the individual sub-systems. For the English lecture task, our best combination system has a WER of 8.3% on the tst2015 development set while our other combinations gained 25.7% WER for German lecture tasks.
- A. Waibel. 2017. Keynote Talk. Abstract: "89:&#;<=$ Dr. Alexander Waibel is a Professor of Computer Science at Carnegie Mellon University, Pittsburgh and at the Karlsruhe Institute of Technology, Germany. He is the director of the International Center for Advanced Communication Technologies (interACT). The Center works in a network with eight of the world’s top research institutions. Its mission is to develop advanced machine learning algorithms to improve human-human and human-machine communication technologies. Prof. Waibel and his team pioneered many statistical and neural learning algorithms that made such communication breakthroughs possible. Most notably, the “Time-Delay Neural Network” (1987) (now also known as “convolutional” neural network) is at the heart of many of today’s AI technologies. System breakthroughs that followed suit included early multimodal dialog interfaces, the first speech translation system in Europe&USA (1990/1991), the first simultaneous lecture interpretation system (2005), and Jibbigo, the first commercial speech translator on a phone (2009).
- Thomas Zenkel, Ramon Sanabria, Florian Metze, A. Waibel. 2017. Subword and Crossword Units for CTC Acoustic Models. Abstract: This paper proposes a novel approach to create an unit set for CTC based speech recognition systems. By using Byte Pair Encoding we learn an unit set of an arbitrary size on a given training text. In contrast to using characters or words as units this allows us to find a good trade-off between the size of our unit set and the available training data. We evaluate both Crossword units, that may span multiple word, and Subword units. By combining this approach with decoding methods using a separate language model we are able to achieve state of the art results for grapheme based CTC systems.
- Markus Müller, Sebastian Stüker, A. Waibel. 2017. Phonemic and Graphemic Multilingual CTC Based Speech Recognition. Abstract: Training automatic speech recognition (ASR) systems requires large amounts of data in the target language in order to achieve good performance. Whereas large training corpora are readily available for languages like English, there exists a long tail of languages which do suffer from a lack of resources. One method to handle data sparsity is to use data from additional source languages and build a multilingual system. Recently, ASR systems based on recurrent neural networks (RNNs) trained with connectionist temporal classification (CTC) have gained substantial research interest. In this work, we extended our previous approach towards training CTC-based systems multilingually. Our systems feature a global phone set, based on the joint phone sets of each source language. We evaluated the use of different language combinations as well as the addition of Language Feature Vectors (LFVs). As contrastive experiment, we built systems based on graphemes as well. Systems having a multilingual phone set are known to suffer in performance compared to their monolingual counterparts. With our proposed approach, we could reduce the gap between these mono- and multilingual setups, using either graphemes or phonemes.
- Chiori Hori, S. Furui, Robert G. Malkin, Hua Yu, A. Waibel. 2017. Title Automatic Speech Summarization Applied to English Broadcast News Speech Author. Abstract: This paper reports an automatic speech summarization method and experimental results using English broadcast news speech. In our proposed method, a set of words maximizing a summarization score indicating an appropriateness of summarization is extracted from automatically transcribed speech. This extraction is performed using a Dynamic Programming (DP) technique according to a target compression ratio. We have previously tested the performance of our method using Japanese broadcast news speech. Since our method is based on a statistical approach, it could be applied to any language. In this paper, English broadcast news speech transcribed using a speech recognizer is automatically summarized. In order to apply our method to English, the model of estimating word concatenation probabilities based on a dependency structure in the original speech given by a Stochastic Dependency Context Free Grammar (SDCFG) is modified. A summarization method for multiple utterances using two-level DP technique is also proposed.
- Markus Müller, Sebastian Stüker, A. Waibel. 2017. Multilingual Adaptation of RNN Based ASR Systems. Abstract: In this work, we focus on multilingual systems based on recurrent neural networks (RNNs), trained using the Connectionist Temporal Classification (CTC) loss function. Using a multilingual set of acoustic units poses difficulties. To address this issue, we proposed Language Feature Vectors (LFV s) to train language adaptive multilingual systems. Language adaptation, in contrast to speaker adaptation, needs to be applied not only on the feature level, but also to deeper layers of the network. In this work, we therefore extended our previous approach by introducing a novel technique which we call “modulation”. Based on this method, we modulated the hidden layers of RNNs using LFVs. We evaluated this approach in both full and low resource conditions, as well as for grapheme and phone based systems. Lower error rates throughout the different conditions could be achieved by the use of the modulation.
- Markus Müller, Sebastian Stüker, A. Waibel. 2017. DBLSTM based multilingual articulatory feature extraction for language documentation. Abstract: With more than 7,000 living languages in the world and many of them facing extinction, the need for language documentation is now more pressing than ever. This process is time-consuming, requiring linguists as each language features peculiarities that need to be addressed. While automating the whole process is difficult, we aim at providing methods to support linguists during documentation. One important step in the workflow is the discovery of the phonetic inventory. In the past, we proposed a first approach of first automatically segmenting recordings into phone-line units and second clustering these segments based on acoustic similarity, determined by articulatory features (AFs). We now propose a refined method using Deep Bi-directional LSTMs (DBLSTMs) over DNNs. Additionally, we use Language Feature Vectors (LFVs) which encode language specific peculiarities in a low dimensional representation. In contrast to adding LFVs to the acoustic input features, we modulated the output of the last hidden LSTM layer, forcing groups of LSTM cells to adapt to language related features. We evaluated our approach multilingually, using data from multiple languages. Results show an improvement in recognition accuracy across AF types: While LFVs improved the performance of DNNs, the gain is even bigger when using DBLSTMs.
- Robin Ruede, Markus Müller, Sebastian Stüker, A. Waibel. 2017. Enhancing Backchannel Prediction Using Word Embeddings. Abstract: Backchannel responses like “uh-huh”, “yeah”, “right” are used by the listener in a social dialog as a way to provide feedback to the speaker. In the context of human-computer interaction, these responses can be used by an artiﬁcial agent to build rapport in conversations with users. In the past, multiple approaches have been proposed to detect backchannel cues and to predict the most natural timing to place those backchannel utterances. Most of these are based on manually optimized ﬁxed rules, which may fail to generalize. Many systems rely on the location and duration of pauses and pitch slopes of speciﬁc lengths. In the past, we proposed an approach by training artiﬁcial neural networks on acoustic features such as pitch and power and also attempted to add word embeddings via word2vec. In this work, we reﬁned this approach by evaluating different methods to add timed word embeddings via word2vec. Comparing the performance using various feature combinations, we could show that adding linguistic features improves the performance over a prediction system that only uses acoustic features.
- Matthias Sperber, Graham Neubig, J. Niehues, A. Waibel. 2017. Neural Lattice-to-Sequence Models for Uncertain Inputs. Abstract: The input to a neural sequence-to-sequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM’s child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.
- Ngoc-Quan Pham, Matthias Sperber, Elizabeth Salesky, Thanh-Le Ha, J. Niehues, A. Waibel. 2017. KIT’s Multilingual Neural Machine Translation systems for IWSLT 2017. Abstract: In this paper, we present KIT’s multilingual neural machine translation (NMT) systems for the IWSLT 2017 evaluation campaign machine translation (MT) and spoken language translation (SLT) tasks. For our MT task submissions, we used our multi-task system, modified from a standard attentional neural machine translation framework, instead of building 20 individual NMT systems. We investigated different architectures as well as different data corpora in training such a multilingual system. We also suggested an effective adaptation scheme for multilingual systems which brings great improvements compared to monolingual systems. For the SLT track, in addition to a monolingual neural translation system used to generate correct punctuations and true cases of the data prior to training our multilingual system, we introduced a noise model in order to make our system more robust. Results show that our novel modifications improved our systems considerably on all tasks.
- Thomas Zenkel, Ramon Sanabria, Florian Metze, J. Niehues, Matthias Sperber, Sebastian Stüker, A. Waibel. 2017. Comparison of Decoding Strategies for CTC Acoustic Models. Abstract: Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR. For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string. We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area.
- J. Niehues, Eunah Cho, Thanh-Le Ha, A. Waibel. 2017. Analyzing Neural MT Search and Model Performance. Abstract: In this paper, we offer an in-depth analysis about the modeling and search performance. We address the question if a more complex search algorithm is necessary. Furthermore, we investigate the question if more complex models which might only be applicable during rescoring are promising. By separating the search space and the modeling using n-best list reranking, we analyze the influence of both parts of an NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations.
- Matthias Sperber, J. Niehues, A. Waibel. 2017. Toward Robust Neural Machine Translation for Noisy Input Sequences. Abstract: Translating noisy inputs, such as the output of a speech recognizer, is a difficult but important challenge for neural machine translation. One way to increase robustness of neural models is by introducing artificial noise to the training data. In this paper, we experiment with appropriate forms of such noise, exploring a middle ground between general-purpose regularizers and highly task-specific forms of noise induction. We show that with a simple generative noise model, moderate gains can be achieved in translating erroneous speech transcripts, provided that type and amount of noise are properly calibrated. The optimal amount of noise at training time is much smaller than the amount of noise in our test data, indicating limitations due to trainability issues. We note that unlike our baseline model, models trained on noisy data are able to generate outputs of proper length even for noisy inputs, while gradually reducing output length for higher amount of noise, as might also be expected from a human translator. We discuss these findings in details and give suggestions for future work.
- Markus Müller, Jörg K.H. Franke, Sebastian Stüker, A. Waibel. 2017. I MPROVING P HONEME S ET D ISCOVERY FOR D OCUMENTING U NWRITTEN L ANGUAGES ∗. Abstract: : Many of the 7,000 living languages in the world are currently threatened by extinction. In order to preserve these languages and the cultural heritage linked with them, they need to be documented. This is a challenging and time consuming task, even for trained specialists. Helping linguists in language documentation is the goal of the French-German ANR-DFG project BULB. The ﬁrst step in documenting a language is the discovery of the phonetic inventory. We aim at assisting linguists during this step by proposing a segmentation of audio data into phoneme-like units and by clustering these units using articulatory features. In this work, we reﬁne our existing approach by the use of Deep Bidirectional LSTM networks (DBLSTM), by which we could increase the recognition accuracy for articulatory features.
- Eunah Cho, J. Niehues, A. Waibel. 2017. NMT-Based Segmentation and Punctuation Insertion for Real-Time Spoken Language Translation. Abstract: Insertion of proper segmentation and punctuation into an ASR transcript is crucial not only for the performance of subsequent applications but also for the readability of the text. In a simultaneous spoken language translation system, the segmentation model has to fulfill real-time constraints and minimize latency as well. In this paper, we show the successful integration of an attentional encoder-decoder-based segmentation and punctuation insertion model into a real-time spoken language translation system. The proposed technique can be easily integrated into the real-time framework and improve the punctuation performance on reference transcripts as well as on ASR outputs. Compared to the conventional language model and prosody-based model, our experiments on end-to-end spoken language translation show that translation performance is improved by 1.3 BLEU points by adopting the NMT-based punctuation model, maintaining low-latency.
- Jan-Thorsten Peter, H. Ney, Ondrej Bojar, Ngoc-Quan Pham, J. Niehues, A. Waibel, Franck Burlot, François Yvon, Marcis Pinnis, Valters Sics, Jasmijn Bastings, Miguel Rios, Wilker Aziz, Philip Williams, F. Blain, Lucia Specia. 2017. The QT21 Combined Machine Translation System for English to Latvian. Abstract: This paper describes the joint submission of the QT21 projects for the English → Latvian translation task of the EMNLP 2017 Second Conference on Machine Translation (WMT 2017). The submission is a system combination which combines seven different statistical machine translation systems provided by the different groups. The systems are combined using either RWTH’s system combination approach, or USFD’s consensus-based system-selection approach. The ﬁnal submission shows an improvement of 0.5 B LEU compared to the best single system on newstest2017.
- Thanh-Le Ha, J. Niehues, A. Waibel. 2017. Effective Strategies in Zero-Shot Neural Machine Translation. Abstract: In this paper, we proposed two strategies which can be applied to a multilingual neural machine translation system in order to better tackle zero-shot scenarios despite not having any parallel corpus. The experiments show that they are effective in terms of both performance and computing resources, especially in multilingual translation of unbalanced data in real zero-resourced condition when they alleviate the language bias problem.
- Eunah Cho, J. Niehues, A. Waibel. 2017. Domain-independent Punctuation and Segmentation Insertion. Abstract: Punctuation and segmentation is crucial in spoken language translation, as it has a strong impact to translation performance. However, the impact of rare or unknown words in the performance of punctuation and segmentation insertion has not been thoroughly studied. In this work, we simulate various degrees of domain-match in testing scenario and investigate their impact to the punctuation insertion task. We explore three rare word generalizing schemes using part-of-speech (POS) tokens. Experiments show that generalizing rare and unknown words greatly improves the punctuation insertion performance, reaching up to 8.8 points of improvement in F-score when applied to the out-of-domain test scenario. We show that this improvement in punctuation quality has a positive impact on a following machine translation (MT) performance, improving it by 2 BLEU points.
- Linchen Zhu, A. Waibel, T. Asfour, Sebastian Stüker. 2017. Neural Network-based Small-Footprint Flexible Keyword Spotting. Abstract: Real-time keyword spotting (KWS) on mobile devices requires a small memory footprint, low latency, and low computational cost. Conventional approaches such as HMM-based KWS do not fulfill these requirements, as Viterbi decoding tends to lead to high computational cost. Therefore, this work proposes a neural network-based small-footprint flexible KWS system appropriate for mobile devices. Our novel KWS system is composed of three modules: the feature extraction module, the posterior probability estimation module, and the posterior handling module. The feature extraction module produces acoustic features from the input audio stream, while the posterior probability estimation module generates posterior probabilities for subword units such as phonemes and senones using various types of neural networks including feedforward neural networks and recurrent neural networks such as LSTM, TDNN-LSTM, and so on. Finally, the posterior handling module calculates final confidence scores for the predefined keywords based on the posteriors from networks. Experiments have been performed to prove the effectiveness of this KWS approach. Experimental results on neural network training show that RNNs such as LSTM, TDNN-LSTM, etc., outperform FFNNs significantly on framewise phoneme/senone classification. Then, the KWS system is built based on the trained networks, and the performance is evaluated by detecting the predefined key-phrase ”okay cosa”. Evaluation results demonstrate that this KWS system achieves lower computational cost and a smaller memory footprint compared with conventional KWS approaches, and therefore makes a step towards real-time KWS on mobile devices.
- Jörg K.H. Franke, Markus Müller, F. Hamlaoui, Sebastian Stüker, A. Waibel. 2016. Phoneme Boundary Detection using Deep Bidirectional LSTMs. Abstract: In this paper we investigate the automatic detection of phoneme boundaries in audio recordings with the help of deep bidirectional LSTMs. This work is motivated by the needs of the project BULB which aims to support linguists in documenting unwritten languages. The automatic detection of phoneme boundaries in audio recordings of a new language is part of the technical requirements of the BULB project. For our first experiments with LSTMs for this task, we worked on TIMIT and BUCKEYE and measured the performance of our LSTMs using accuracy, precision, recall and F-measure. We then applied the trained networks crosslingually to Basaa, one of the Bantu languages addressed in BULB. With the LSTMs trained for this paper we achieve a phoneme segmentation performance on TIMIT that, to the best of our knowledge, outperforms the systems reported in literature so far.
- J. Niehues, T. Nguyen, Eunah Cho, Thanh-Le Ha, Kevin Kilgour, Markus Müller, Matthias Sperber, Sebastian Stüker, A. Waibel. 2016. Dynamic Transcription for Low-Latency Speech Translation. Abstract: Latency is one of the main challenges in the task of simultaneous spoken language translation. While significant improvements in recent years have led to high quality automatic translations, their usefulness in real-time settings is still severely limited due to the large delay between the input speech and the delivered translation. In this paper, we present a novel scheme which reduces the latency of a large scale speech translation system drastically. Within this scheme, the transcribed text and its translation can be updated when more context is available, even after they are presented to the user. Thereby, this scheme allows us to display an initial transcript and its translation to the user with a very low latency. If necessary, both transcript and translation can later be updated to better, more accurate versions until eventually the final versions are displayed. Using this framework, we are able to reduce the latency of the source language transcript into half. For the translation, an average delay of 3.3s was achieved, which is more than twice as fast as our initial system.
- Markus Müller, Sebastian Stüker, A. Waibel. 2016. Language Feature Vectors for Resource Constraint Speech Recognition. Abstract: Deep Neural Networks (DNNs) are a key element of stateof-the-art speech recognition systems. Being a data-driven method, they require a significant amount of training data. There exist scenarios in which such an amount of data is not available for a particular language. Building systems for such resource constrained tasks requires special techniques. One common method is to use data from multiple languages to train the acoustic model. But there are limitations on knowledge transfer between different languages. By the use of Language Feature Vectors (LFVs), we try to mitigate these limitations by providing language information to DNNs. Similar to i-Vectors for speaker adaptation, LFVs enable DNNs to better capture and adapt to inter language characteristics. Previous experiments have shown that providing LFVs to DNNs improved system performance. In this paper, we show that by adding LFVs the performance gap between monoand multilingual systems decreases.
- Michael Wetzel, Matthias Sperber, A. Waibel. 2016. Audio Segmentation for Robust Real-Time Speech Recognition Based on Neural Networks. Abstract: Speech that contains multimedia content can pose a serious challenge for real-time automatic speech recognition (ASR) for two reasons: (1) The ASR produces meaningless output, hurting the readability of the transcript. (2) The search space of the ASR is blown up when multimedia content is encountered, resulting in large delays that compromise real-time requirements. This paper introduces a segmenter that aims to remove these problems by detecting music and noise segments in real-time and replacing them with silence. We propose a two step approach, consisting of frame classification and smoothing. First, a classifier detects speech and multimedia on the frame level. In the second step the smoothing algorithm considers the temporal context to prevent rapid class fluctuations. We investigate in frame classification and smoothing settings to obtain an appealing accuracy-latency-tradeoff. The proposed segmenter yields increases the transcript quality of an ASR system by removing on average 39 % of the errors caused by non-speech in the audio stream, while maintaining a real-time applicable delay of 270 milliseconds.
- Markus Müller, Sebastian Stüker, A. Waibel. 2016. Towards Improving Low-Resource Speech Recognition Using Articulatory and Language Features. Abstract: In an increasingly globalized world, there is a rising demand for speech recognition systems. Systems for languages like English, German or French do achieve a decent performance, but there exists a long tail of languages for which such systems do not yet exist. State-of-the-art speech recognition systems feature Deep Neural Networks (DNNs). Being a data driven method and therefore highly dependent on sufficient training data, the lack of resources directly affects the recognition performance. There exist multiple techniques to deal with such resource constraint conditions, one approach is the use of additional data from other languages. In the past, is was demonstrated that multilingually trained systems benefit from adding language feature vectors (LFVs) to the input features, similar to i-Vectors. In this work, we extend this approach by the addition of articulatory features (AFs). We show that AFs also benefit from LFVs and that multilingual system setups benefit from adding both AFs and LFVs. Pretending English to be a low-resource language, we restricted ourselves to use only 10h of English acoustic training data. For system training, we use additional data from French, German and Turkish. By using a combination of AFs and LFVs, we were able to decrease the WER from 18.1% to 17.3% after system combination in our setup using a multilingual phone set.
- Markus Müller, Sebastian Stüker, A. Waibel. 2016. Language Adaptive DNNs for Improved Low Resource Speech Recognition. Abstract: Deep Neural Network (DNN) acoustic models are commonly used in today’s state-of-the-art speech recognition systems. As neural networks are a data driven method, the amount of available training data directly impacts the performance. In the past, several studies have shown that multilingual training of DNNs leads to improvements, especially in resource constrained tasks in which only limited training data in the target language is available. Previous studies have shown speaker adaptation to be successfully performed on DNNs. This is achieved by adding speaker information (e.g. i-Vectors) as additional input features. Based on the idea of adding additional features, we here present a method for adding language information to the input features of the network. Preliminary experiments have shown improvements by providing supervised information about language identity to the network. In this work, we extended this approach by training a neural network to encode language specific features. We extracted those features unsupervised and used them to provide additional cues to the DNN acoustic model during training. Our results show that augmenting acoustic input features with this language code enabled the network to better capture language specific peculiarities. This improved the performance of systems trained using data from multiple languages.
- Thanh-Le Ha, J. Niehues, A. Waibel. 2016. Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder. Abstract: In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach in which the information shared among languages can be helpful in the translation of individual language pairs. We are then able to employ attention-based Neural Machine Translation for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, we point out a novel way to make use of monolingual data with Neural Machine Translation using the same approach with a 3.15-BLEU-score gain in IWSLT’16 English→German translation task.
- Marco Vetter, Markus Müller, F. Hamlaoui, Graham Neubig, Satoshi Nakamura, Sebastian Stüker, A. Waibel. 2016. Unsupervised Phoneme Segmentation of Previously Unseen Languages. Abstract: In this paper we investigate the automatic detection of phoneme boundaries in audio recordings of an unknown language. This work is motivated by the needs of the project BULB which aims to support linguists in documenting unwritten languages. The automatic phonemic transcription of recordings of the unwritten language is part of this. We cannot use multilingual phoneme recognizers as their phoneme inventory might not completely cover that of the new language. Thus we opted for pursuing a two step approach which is inspired by work from speech synthesis for previously unknown languages. First, we detect boundaries for phonemes, and then we classify the detected segments into phoneme units. In this paper we address the ﬁrst step, i.e. the detection of the phoneme boundaries. For this we again used multilingual and crosslingual phoneme recognizers but were only interested in the phoneme boundaries detected by them and not the phoneme identities. We measured the quality of the segmentations obtained this way using precision, re-call and F-measure. We compared the performance of different conﬁgurations of mono-and multilingual phoneme recognizers among each other and against a monolingual gold standard. Finally we applied the technique to Basaa, a Bantu language.
- Thanh-Le Ha, Eunah Cho, J. Niehues, Mohammed Mediani, Matthias Sperber, A. Allauzen, A. Waibel. 2016. The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2016. Abstract: We present our experiments in the scope of the news translation task in WMT 2017, in three directions: German → English, English → German and English → Latvian. The core of our systems is the encoder-decoder based neural machine translation models , enhanced with various modeling features, additional source side augmentation and output rescoring. We also ex-periment various methods in data selection and adaptation.
- Marvin Ritter, Markus Müller, Sebastian Stüker, Florian Metze, A. Waibel. 2016. Training Deep Neural Networks for Reverberation Robust Speech Recognition. Abstract: Recently hybrid systems of deep neural networks (DNNs) and hidden Markov models (HMMs) have shown state of the art results on various speech recognition tasks. Best results were achieved by training large neural networks (NNs) on huge data sets (≥ 2000h [11, 16, 20]). The required training data is often generated using different methods of data augmentation. We show that a simple approach using room impulse response (RIR) can be used to train systems more robust to reverberation. The method does not require multiple microphones or complex signal processing techniques. On a test set simulating large rooms we show improvements from 59.7% word error rate (WER) down to 41.9%. In case of known large lectures rooms with varying microphone positions the approach can be used to adopt the system to the environment. We compare systems trained with RIRs from one room, multiple rooms and simulated rooms.
- Matthias Sperber, Graham Neubig, Satoshi Nakamura, A. Waibel. 2016. Optimizing Computer-Assisted Transcription Quality with Iterative User Interfaces. Abstract: Computer-assisted transcription promises high-quality speech transcription at reduced costs. This is achieved by limiting human effort to transcribing parts for which automatic transcription quality is insufficient. Our goal is to improve the human transcription quality via appropriate user interface design. We focus on iterative interfaces that allow humans to solve tasks based on an initially given suggestion, in this case an automatic transcription. We conduct a user study that reveals considerable quality gains for three variations of iterative interfaces over a non-iterative from-scratch transcription interface. Our iterative interfaces included post-editing, confidence-enhanced post-editing, and a novel retyping interface. All three yielded similar quality on average, but we found that the proposed retyping interface was less sensitive to the difficulty of the segment, and superior when the automatic transcription of the segment contained relatively many errors. An analysis using mixed-effects models allows us to quantify these and other factors and draw conclusions over which interface design should be chosen in which circumstance.
- Markus Müller, T. Nguyen, J. Niehues, Eunah Cho, Bastian Krüger, Thanh-Le Ha, Kevin Kilgour, Matthias Sperber, Mohammed Mediani, Sebastian Stüker, A. Waibel. 2016. Lecture Translator - Speech translation framework for simultaneous lecture translation. Abstract: Foreign students at German universities often have difﬁculties following lectures as they are often held in German. Since human in-terpreters are too expensive for universities we are addressing this problem via speech translation technology deployed in KIT’s lecture halls. Our simultaneous lecture translation system automatically translates lectures from German to English in real-time. Other supported language directions are English to Spanish, English to French, English to German and German to French. Automatic simultaneous translation is more than just the concatenation of automatic speech recognition and machine translation technology, as the input is an unsegmented, practically inﬁ-nite stream of spontaneous speech. The lack of segmentation and the spontaneous nature of the speech makes it especially difﬁcult to recognize and translate it with sufﬁcient quality. In addition to quality, speed and latency are of the utmost importance in order for the system to enable students to follow lectures. In this paper we present our system that performs the task of simultaneous speech translation of university lectures by performing speech translation on a stream of audio in real-time and with low latency. The system features several techniques beyond the basic speech translation task, that make it ﬁt for real-world use. Examples of these features are a continuous stream speech recognition without any prior segmentation of the input audio, punctuation prediction, run-on decoding and run-on translation with continuously updating displays in order to keep the latency as low as possible.
- Eunah Cho, J. Niehues, Thanh-Le Ha, A. Waibel. 2016. Multilingual Disfluency Removal using NMT. Abstract: In this paper, we investigate a multilingual approach for speech disfluency removal. A major challenge of this task comes from the costly nature of disfluency annotation. Motivated by the fact that speech disfluencies are commonly observed throughout different languages, we investigate the potential of multilingual disfluency modeling. We suggest that learning a joint representation of the disfluencies in multiple languages can be a promising solution to the data sparsity issue. In this work, we utilize a multilingual neural machine translation system, where a disfluent speech transcript is directly transformed into a cleaned up text. Disfluency removal experiments on English and German speech transcripts show that multilingual disfluency modeling outperforms the single language systems. In a following experiment, we show that the improvements are also observed in a downstream application using the disfluency-removed transcripts as input.
- Lori S. Levin, A. Lavie, M. Woszczyna, D. Gates, Marsal Gavaldà, D. Koll, A. Waibel. 2016. The Janus-III Translation System: Speech-toSpeech Translation in Multiple. Abstract: The Janus-III system translates spoken languages in limited domains. The current re- search focus is on expanding beyond tasks involving a single limited semantic domain to significantly broader and richer domains. To achieve this goal, The MT components of our system have been engineered to build and manipulate multi-domain parse lattices that are based on modular grammars for multiple semantic domains. This approach yields solutions to several problems including multi- domain disambiguation, segmentation of spoken utterances into sentence units, modularity of system design, and re-use of earlier systems with incompatible output.
- Markus Müller, Sarah Fünfer, Sebastian Stüker, A. Waibel. 2016. Evaluation of the KIT Lecture Translation System. Abstract: To attract foreign students is among the goals of the Karlsruhe Institute of Technology (KIT). One obstacle to achieving this goal is that lectures at KIT are usually held in German which many foreign students are not sufficiently proficient in, as, e.g., opposed to English. While the students from abroad are learning German during their stay at KIT, it is challenging to become proficient enough in it in order to follow a lecture. As a solution to this problem we offer our automatic simultaneous lecture translation. It translates German lectures into English in real time. While not as good as human interpreters, the system is available at a price that KIT can afford in order to offer it in potentially all lectures. In order to assess whether the quality of the system we have conducted a user study. In this paper we present this study, the way it was conducted and its results. The results indicate that the quality of the system has passed a threshold as to be able to support students in their studies. The study has helped to identify the most crucial weaknesses of the systems and has guided us which steps to take next.
- Yang Zhang, J. Niehues, A. Waibel. 2016. Integrating Encyclopedic Knowledge into Neural Language Models. Abstract: Neural models have recently shown big improvements in the performance of phrase-based machine translation. Recurrent language models, in particular, have been a great success due to their ability to model arbitrary long context. In this work, we integrate global semantic information extracted from large encyclopedic sources into neural network language models. We integrate semantic word classes extracted from Wikipedia and sentence level topic information into a recurrent neural network-based language model. The new resulting models exhibit great potential in alleviating data sparsity problems with the additional knowledge provided. This approach of integrating global information is not restricted to language modeling but can also be easily applied to any model that profits from context or further data resources, e.g. neural machine translation. Using this model has improved rescoring quality of a state-of-the-art phrase-based translation system by 0.84 BLEU points. We performed experiments on two language pairs.
- A. Andonov, Maria Schmidt, J. Niehues, A. Waibel. 2016. Using Tweets as "Ice-Breaking" Sentences in a Social Dialog System. Abstract: Many goal-oriented spoken dialog systems lack a social component like small talk which often features in human-human communication. In this work we aim to alleviate part of this problem by generating sentences which have the goal to appeal to the user and increase the probability of a response. Such sentences are suitable to break the ice in the beginning of a conversation and are therefore referred to as "ice-breaking" throughout this paper. Furthermore, we use data from the Twitter account of the user in order to infer the users interests. By generating sentences about these interests we utilize the existence of homophily in social networks. A user study shows that the described sys-tem outperforms one which chooses interests at random. Furthermore, we note that 70% of the study participants would answer the system and continue talking on the same topic which was introduced by the generated sentence.
- Yajie Miao, M. Gowayyed, Xingyu Na, Tom Ko, Florian Metze, A. Waibel. 2016. An empirical exploration of CTC acoustic models. Abstract: The connectionist temporal classification (CTC) loss function has several interesting properties relevant for automatic speech recognition (ASR): applied on top of deep recurrent neural networks (RNNs), CTC learns the alignments between speech frames and label sequences automatically, which removes the need for pre-generated frame-level labels. CTC systems also do not require context decision trees for good performance, using context-independent (CI) phonemes or characters as targets. This paper presents an extensive exploration of CTC-based acoustic models applied to a variety of ASR tasks, including an empirical study of the optimal configuration and architectural variants for CTC. We observe that on large amounts of training data, CTC models tend to outperform state-of-the-art hybrid approach. Further experiments reveal that CTC can be readily ported to syllable-based languages, and can be enhanced by employing improved feature front-ends.
- J. Niehues, Eunah Cho, Thanh-Le Ha, A. Waibel. 2016. Pre-Translation for Neural Machine Translation. Abstract: Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur. When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach. In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation (PBMT) system or a combination of the PBMT output and the source sentence. We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result.
- Qin Jin, A. Waibel. 2016. Gaussian Mixture Model. Abstract: Modelling data drawn from an unknown statistical distribution with a weighted sum of distributions defines a finite mixture model, also known as a latent class method. The most common example incorporates a given number, say k, of Gaussian (i.e., Normal) distributions to model data. Mixture models can be applied to a wide range of applications to grouped data, such as density estimation and clustering. The routine G03GAF is new to Mark 24 of the NAG Fortran Library and fits a Gaussian k-mixture model with the following (co)variance structures: a. Group-wise covariances: the data are assumed to be drawn from k Normal distributions with different means and covariances. b. Pooled covariances: the data are assumed to be drawn from k Normal distributions with different means but equal covariances. c. Group-wise variances: the data are assumed to be drawn from k Normal distributions with different means and variances. d. Pooled variances: the data are assumed to be drawn from k Normal distributions with different means and equal variances. e. Overall variance: the data are assumed to be drawn from k Normal distributions with different means and equal (single) variance. Where option (a) gives the most flexible model as it requires the most parameters, and the flexibility of models reduces progressing towards (e). Given a data set assumed to arise from a known number of mixtures of Gaussians, G03GAF estimates the free parameters of a mixture model: the means and (co)variances. Examples include modelling buyer behaviour by identifying different customer groups or the distribution of power loads over a network. For the purpose of this example, we simulate data from a population consisting of two bivariate Normal distributions and compare G03GAF's parameter estimates with the known values. If the parameter estimates are " close " to their true values, the model reflects the true nature of the population. These distributions are defined by: Group 1. Mean (Variable 1 1 Variable 2 2) and covariance (Variable 1 Variable 2 Variable 1 1.1 0.2 Variable 2 0.2
- T. Nguyen, Markus Müller, Matthias Sperber, Thomas Zenkel, Kevin Kilgour, Sebastian Stüker, A. Waibel. 2016. The 2016 KIT IWSLT Speech-to-Text Systems for English and German. Abstract: This paper describes our German and English Speech-to-Text (STT) systems for the 2016 IWSLT evaluation campaign. The campaign focuses on the transcription of unsegmented TED talks. Our setup includes systems using both the Janus and Kaldi frameworks. We combined the outputs using both ROVER [1] and confusion network combination (CNC) [2] to archieve a good overall performance. The individual subsystems are built by using different speaker-adaptive feature combination (e.g., lMEL with i-vector or bottleneck speaker vector), acoustic models (GMM or DNN) and speaker adaption (MLLR or fMLLR). Decoding is performed in two stages, where the GMM and DNN systems are adapted on the combination of the first stage outputs using MLLR, and fMLLR. The combination setup produces a final hypothesis that has a significantly lower WER than any of the individual subsystems. For the English TED task, our best combination system has a WER of 7.8% on the development set while our other combinations gained 21.8% and 28.7% WERs for the English and German MSLT tasks.
- Matthias Sperber, Graham Neubig, J. Niehues, Sebastian Stüker, A. Waibel. 2016. Lightly Supervised Quality Estimation. Abstract: Evaluating the quality of output from language processing systems such as machine translation or speech recognition is an essential step in ensuring that they are sufficient for practical use. However, depending on the practical requirements, evaluation approaches can differ strongly. Often, reference-based evaluation measures (such as BLEU or WER) are appealing because they are cheap and allow rapid quantitative comparison. On the other hand, practitioners often focus on manual evaluation because they must deal with frequently changing domains and quality standards requested by customers, for which reference-based evaluation is insufficient or not possible due to missing in-domain reference data (Harris et al., 2016). In this paper, we attempt to bridge this gap by proposing a framework for lightly supervised quality estimation. We collect manually annotated scores for a small number of segments in a test corpus or document, and combine them with automatically predicted quality scores for the remaining segments to predict an overall quality estimate. An evaluation shows that our framework estimates quality more reliably than using fully automatic quality estimation approaches, while keeping annotation effort low by not requiring full references to be available for the particular domain.
- J. Niehues, Thanh-Le Ha, Eunah Cho, A. Waibel. 2016. Using Factored Word Representation in Neural Network Language Models. Abstract: Neural network language and translation models have recently shown their great potentials in improving the performance of phrase-based machine translation. At the same time, word representations using different word factors have been translation quality and are part of many state-of-theart machine translation systems. used in many state-of-the-art machine translation systems, in order to support better translation quality. In this work, we combined these two ideas by investigating the combination of both techniques. By representing words in neural network language models using different factors, we were able to improve the models themselves as well as their impact on the overall machine translation performance. This is especially helpful for morphologically rich languages due to their large vocabulary size. Furthermore, it is easy to add additional knowledge, such as source side information, to the model. Using this model we improved the translation quality of a state-of-the-art phrasebased machine translation system by 0.7 BLEU points. We performed experiments on three language pairs for the news translation task of the WMT 2016 evaluation.
- Eunah Cho, J. Niehues, Thanh-Le Ha, Matthias Sperber, Mohammed Mediani, A. Waibel. 2016. Adaptation and Combination of NMT Systems: The KIT Translation Systems for IWSLT 2016. Abstract: In this paper, we present the KIT systems of the IWSLT 2016 machine translation evaluation. We participated in the machine translation (MT) task as well as the spoken language language translation (SLT) track for English→German and German→English translation. We use attentional neural machine translation (NMT) for all our submissions. We investigated different methods to adapt the system using small in-domain data as well as methods to train the system on these small corpora. In addition, we investigated methods to combine NMT systems that encode the input as well as the output differently. We combine systems using different vocabularies, reverse translation systems, multi-source translation system. In addition, we used pre-translation systems that facilitate phrase-based machine translation systems. Results show that applying domain adaptation and ensemble technique brings a crucial improvement of 3-4 BLEU points over the baseline system. In addition, system combination using n-best lists yields further 1-2 BLEU points.
- Lucas Bechberger, Maria Schmidt, A. Waibel, Marcello Federico. 2016. Personalized News Event Retrieval for Small Talk in Social Dialog Systems. Abstract: This thesis explores the area of personalized news event retrieval in the context of social dialog systems. We developed the NewsTeller system which retrieves a relevant news event based on a user query and the user’s general interests (both represented as list of keywords). The retrieved news event can then be used by a social dialog system to initiate news-related small talk. The NewsTeller system is implemented as a pipeline with four stages: In a rst step, a (large) set of potentially relevant news events is retrieved. As about 84% of the found events do not ful ll our syntactic and semantic well-formedness criteria, the second step in the pipeline is concerned with ltering the found events. This ltering is done by a classi er which was trained on a data set of about 6,000 events. The results obtained in a ten-fold cross-validation indicate that a global random forest classi er is superior to an ensemble of specialized classi ers that were trained on speci c subproblems. The global classi er reaches a precision of 63.04% and a recall of 59.55%. The third step in the pipeline is concerned with ranking the remaining events according to their expected relevance and selecting the event with the highest expected relevance. Four ordered classes are used to describe an event’s relevance: Irrelevant, Partially Relevant, Relevant, and Very Relevant. Following the “learning to rank” approach from information retrieval, this task is framed as a regression problem on the relevance values of the events which is solved by training a random forest regressor on a data set of about 3,200 events. Two di erent approaches were compared: using only features de ned on the events and the user query and using also features de ned on the user’s general interests. The results in a user-level leave-one-out evaluation indicate that both regressors have comparable performance in avoiding Irrelevant events and that taking into account the user’s interests helps to improve the detection of Relevant and Very Relevant events. In the fourth step of the pipeline, a summary of the selected news event is created. This is done by extracting the sentence in which the event was mentioned. For evaluating the system, a user study with 48 participants was conducted. The results of this evaluation show that the two regression-based approaches are signi cantly better than a random baseline with respect to avoiding Irrelevant events. We could however not con rm our hypothesis that using information about the users’ general interests helps to improve the detection of Relevant and Very Relevant events.
- Jan-Thorsten Peter, Tamer Alkhouli, H. Ney, Matthias Huck, Fabienne Braune, Alexander M. Fraser, A. Tamchyna, Ondrej Bojar, B. Haddow, Rico Sennrich, F. Blain, Lucia Specia, J. Niehues, A. Waibel, A. Allauzen, Lauriane Aufrant, Franck Burlot, Elena Knyazeva, T. Lavergne, François Yvon, Marcis Pinnis, Stella Frank. 2016. The QT21/HimL Combined Machine Translation System. Abstract: This paper describes the joint submission of the QT21 and HimL projects for the English→Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of Sheffield, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016.
- T. Nguyen, Matthias Sperber, Kevin Kilgour, A. Waibel. 2015. The 2015 KIT IWSLT speech-to-text systems for English and German. Abstract: This paper describes our German and English Speechto-Text (STT) systems for the 2015 IWSLT evaluation campaign. This campaign focuses on the transcription of unsegmented TED talks. Our setup includes systems from both Janus and Kaldi. We combined the outputs using both ROVER [1] and confusion network combination (CNC) [2] to archieve a good overall performance. The individual subsystems are built by using different front-ends, (e.g., MVDRMFCC or lMel), acoustic models (GMM or modular DNN) and phone sets and by training on different sets of permissible training data. Decoding is performed in two stages, where the GMM systems are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cMLLR. The combination setup produces a final hypothesis that has a significantly lower WER than any of the individual subsystems. For English, our single best system based on Kaldi has a WER of 13.8% on the development set while in combination with Janus we lowered the WER to 12.8%.
- Eunah Cho, Thanh-Le Ha, J. Niehues, T. Herrmann, Mohammed Mediani, Yuqi Zhang, A. Waibel. 2015. The Karlsruhe Institute of Technology Translation Systems for the WMT 2015. Abstract: In this paper, the KIT systems submitted to the Shared Translation Task are presented. We participated in two translation directions: from German to English and from English to German. Both translations are generated using phrase-based translation systems. The performance of the systems was boosted by using language models built based on different tokens such as word, part-of-speech, and automacally generated word clusters. The difference in word order between German and English is addressed by part-of-speech and syntactic tree-based reordering models. In addition to a discriminative word lexicon, we used hypothesis rescoring using the ListNet algorithm after generating the translation with the phrase-based system. We evaluated the rescoring using only the baseline features as well as using additional computational complex features.
- Eunah Cho, Kevin Kilgour, J. Niehues, A. Waibel. 2015. Combination of NN and CRF models for joint detection of punctuation and disfluencies. Abstract: Inserting proper punctuation marks and deleting speech disfluencies are two of the most essential tasks in spoken language processing. This challenging task has prompted extensive research using various techniques, such as conditional random fields. Neural networks, however, are relatively under-explored for this task. Combining different modeling techniques with different advantages has the potential to lead to improvements. In this work, we first establish the performance of joint modeling of punctuation prediction and disfluency detection using neural networks. We then combine a conditional random fields based model and a neural networks based model log-linearly, and show that the combined approach outperforms both individual models, by 2.7% and 3.5% in F-score for speech disfluency and punctuation detection, respectively. When used as a preprocessing step to machine translation this also results in an improved translation quality of 2.5 BLEU points compared to the baseline and of 0.6 BLEU points compared to the non-combined model. Index Terms: speech disfluency detection, punctuation insertion, speech translation
- Maria Schmidt, Markus Müller, M. Wagner, Sebastian Stüker, A. Waibel, H. Hofmann, S. Werner. 2015. Evaluation of Crowdsourced User Input Data for Spoken Dialog Systems. Abstract: Using the Internet for the collection of data is quite common these days. This process is called crowdsourcing and enables the collection of large amounts of data at reasonable costs. While being an inexpensive method, this data typically is of lower quality. Filtering data sets is therefore required. The occurring errors can be classified into different groups. There are technical issues and human errors. For speech recording, technical issues could be a noisy background. Human errors arise when the task is misunderstood. We employ several techniques for recognizing errors and eliminating faulty data sets in user input data for a Spoken Dialog System (SDS). Furthermore, we compare three different kinds of questionnaires (QNRs) for a given set of seven tasks. We analyze the characteristics of the resulting data sets and give a recommendation which type of QNR might be the most suitable one for a given purpose.
- Mohammed Mediani, Yuqi Zhang, Thanh-Le Ha, J. Niehues, Eunah Cho, T. Herrmann, Rainer Kärgel, A. Waibel. 2015. The KIT translation systems for IWSLT 2015. Abstract: In this paper, we present the KIT systems participating in the TED translation tasks of the IWSLT 2015 machine translation evaluation. We submitted phrase-based translation systems for three directions, namely English!German, German!English, and English!Vietnamese. For the official directions (English!German and German!English), we built systems both for the machine translation (MT) as well as the spoken language translation (SLT) tracks. This year we improved our systems’ performance over last year through n-best list rescoring using neural networkbased translation and language models and novel discriminative models based on different source-side features and classification methods. For the SLT tracks, we used a monolingual translation system to translate the lowercased ASR hypotheses with all punctuation stripped to truecased, punctuated output as a preprocessing step to our usual translation system. In addition to punctuation insertion, we also trained that system for sentence boundary insertion since the SLT’s data this year come with no sentence boundary.
- G. Trovato, M. Zecca, Martin Do, Ömer Terlemez, Masuko Kuramochi, A. Waibel, T. Asfour, A. Takanishi. 2015. A Novel Greeting Selection System for a Culture-Adaptive Humanoid Robot. Abstract: Robots, especially humanoids, are expected to perform human-like actions and adapt to our ways of communication in order to facilitate their acceptance in human society. Among humans, rules of communication change depending on background culture: greetings are a part of communication in which cultural differences are strong. Robots should adapt to these specific differences in order to communicate effectively, being able to select the appropriate manner of greeting for different cultures depending on the social context. In this paper, we present the modelling of social factors that influence greeting choice, and the resulting novel culture-dependent greeting gesture and words selection system. An experiment with German participants was run using the humanoid robot ARMAR-IIIb. Thanks to this system, the robot, after interacting with Germans, can perform greeting gestures appropriate to German culture in addition to a repertoire of greetings appropriate to Japanese culture.
- Isabel Slawik, J. Niehues, A. Waibel. 2015. Stripping Adjectives: Integration Techniques for Selective Stemming in SMT Systems. Abstract: In this paper we present an approach to reduce data sparsity problems when translating from morphologically rich languages into less inflected languages by selectively stemming certain word types. We develop and compare three different integration strategies: replacing words with their stemmed form, combined input using alternative lattice paths for the stemmed and surface forms and a novel hidden combination strategy, where we replace the stems in the stemmed phrase table by the observed surface forms in the test data. This allows us to apply advanced models trained on the surface forms of the words. We evaluate our approach by stemming German adjectives in two German→English translation scenarios: a low-resource condition as well as a large-scale state-of-the-art translation system. We are able to improve between 0.2 and 0.4 BLEU points over our baseline and reduce the number of out-of-vocabulary words by up to 16.5%.
- Linchen Zhu, Kevin Kilgour, Sebastian Stüker, A. Waibel. 2015. Gaussian free cluster tree construction using deep neural network. Abstract: This paper presents a Gaussian free approach to constructing the cluster tree (CT) that context dependent acoustic models (CDAM) depend on. Over the last few years deep neural networks (DNN) have supplanted Gaussian mixture models (GMM) as the default method for acoustic modeling (AM). DNN AMs have also been successfully used to flat start context independent (CI) AMs and generate alignments on which CTs can be trained. Those approaches however still required Gaussians to build their CTs. Our proposed Gaussian free CT algorithm eliminates this requirements and allows, for the first time, the flat start training of state of the art DNN AMs without the use of Gaussian. An evaluation on the IWSLT transcription task demonstrates the effectiveness of this approach.
- Linchen Zhu, A. Waibel. 2015. GMM free ASR using DNN based Cluster Trees. Abstract: Deep neural networks (DNN), as a kind of statistical models, have shown their superiority over Gaussian Mixture Models (GMM) in acoustic modeling when they are used to estimate the emission distributions of HMM states. However a standard DNN-HMM hybrid system is still reliant on GMMs in two aspects initial training alignments and cluster tree building. Recent work has shown that the training of a context independent DNN can be flat started without the initial alignments generated by a trained GMM-HMM system. In this work we propose a novel GMM-free approach for phonetic cluster tree building. To do this a context independent DNN is trained first. From this the average CI-DNN output for each polyphone state is calculated by passing the training samples through the CI-DNN, the results from which can then be used to measure the entropy distance between two sets of polyphone states in the clustering process. Experiments are performed to realize our novel approach, and in addition, cluster trees of different sizes are built to investigate the best performance of our novel approach. All implemented systems are tested on a test database, the test results show that DNN based cluster trees outperform GMM based trees when the number of leaves is greater than 12k, and that both approaches achieve their best performance at 18k leaves.
- T. Herrmann, J. Niehues, A. Waibel. 2015. Source discriminative word lexicon for translation disambiguation. Abstract: This paper presents a source discriminative word lexicon that performs translation disambiguation for individual source words using structural features, such as context and dependency relations in the sentence. The individual translation predictions are combined into sentence scores that are used in N -best list re-ranking to improve the translation output of a state of the art phrase-based machine translation system. The approach is used to improve explicitly the translation of word categories that require grammatical agreement to hold in the target language after translation, e.g. pronouns, as well as subjects and verbs. The results show that the translation predictions provided by the source discriminative word lexicon increase the prediction accuracy by up to 10%. The translation quality can be improved by up to 0.6 BLEU points on English-German translation.
- Eunah Cho, J. Niehues, Kevin Kilgour, A. Waibel. 2015. Insertion for Real-time Spoken Language Translation. Abstract: Sentence segmentation and punctuation insertion in the output of automatic recognition systems is essential for its readability as well as for the performance of subsequent applications, such as machine translation systems. While a longer context can boost the accuracy of inserted punctuation marks, it drastically increases the delay in the spoken language translation system. In this work, we investigate the impact of shorter context in punctuation insertion task on simultaneous speech translation system. We suggest a new scheme within stream decoding where the time delay consumed on punctuation prediction is avoided. Our evaluations on the English TED talks show that our suggested scheme can be used as an efficient method to punctuate recognized streams in real-time scenarios. While outperforming a conventional language model and prosody based punctuation prediction system, our model maintains a comparable performance compared to systems that require longer contexts.
- Isabel Slawik, J. Niehues, A. Waibel. 2015. Stripping Adjectives: Integration Techniques for Selective Stemming in SMT Systems. Abstract: In this paper we present an approach to reduce data sparsity problems when translating from morphologically rich languages into less in ﬂ ected languages by selectively stemming certain word types. We develop and compare three different integration strategies: replacing words with their stemmed form, combined input using alternative lattice paths for the stemmed and surface forms and a novel hidden combination strategy, where we replace the stems in the stemmed phrase table by the observed surface forms in the test data. This allows us to apply advanced models trained on the surface forms of the words. We evaluate our approach by stemming German adjectives in two German → English translation scenarios: a low-resource condition as well as a large-scale state-of-the-art translation sys-tem. We are able to improve between 0.2 and 0.4 BLEU points over our baseline and reduce the number of out-of-vocabulary words by up to 16.5%.
- Adnene Zairi, A. Waibel, Kevin Kilgour. 2015. Sub-word Language Models for German LVCSR. Abstract: This work is devoted to build sub-word language models for German Large Vocabulary Continuous Speech Recognition (LVCSR) Systems. The motivation of using a sub-words based system comes from its ability to model unseen compound words in the training corpus by compounding sequences of sub-units forming new words. The two techniques we apply for the generation of sub-words, are based on syllables and letter n-grams. In three scenarios, we investigate different techniques acting on the input split vocabulary and the vocabulary for the language model and the pronunciation dictionary. Using a combination of full-words and syllables from the training corpus presents the most efficient split method, which also leads to the best word error rate (WER) results during the decoding task. This split method consists on keeping the most frequent words in the training corpus and splitting the infrequent ones into syllables or characters.
- B. Suhm, B. Myers, A. Waibel. 2015. DESIGNING INTERACTIVE ERROR RECOVERY METHODS FOR SPEECH INTERFACES. Abstract: We present an approach to interactive recovery from speech recognition errors in the context of spoken language applications, which is motivated from research in the field of linguistics on repair dialogues in human to human conversations. We propose a framework to compare error recovery methods, arguing that a rational user will prefer error recovery methods which provide an optimal trade off between accuracy, speed and naturalness. We conjecture to beat skilled typing in speed, we need real-time speech recognition technology which performs on a 90% word accuracy level. Finally, we describe how we augmented our JANUS speech to speech translation system with prototypical error recovery methods, and presents results from a preliminary evaluation.
- Markus Müller, A. Waibel. 2015. Using language adaptive deep neural networks for improved multilingual speech recognition. Abstract: Building Large Vocabulary Continuous Speech Recognition (LVCSR) systems for under-resourced languages is a challenging task. While plenty of data is available for English, many other languages suffer from a lack of data. There are different methods for tackling this challenge. One possibility is to use data from different languages to boost the performance of a system for a particular target language. With the emerging of LVCSR systems using neural networks (NNs), many research groups have demonstrated the benefits from using additional data in order to improve the system performance. In this work, we propose a method for providing the language information directly to the network, thus enabling it to become language adaptive. We demonstrate the effectiveness of our approach in a series of experiments.
- Yury Titov, Kevin Kilgour, Sebastian Stüker, A. Waibel. 2015. The 2011 KIT QUAERO Speech-to-Text System for Russian. Abstract: This paper describes our current speech-to-text system for Russian that we are developing within the Quaero program. The system uses two different front-ends for obtaining different acoustic models that are used for system combination and cross-adaptation within a multi-stage decoding set-up. The acoustic model have been trained on manually transcribed data as well as untranscribed data, the language model on large amounts of data obtained from different sources. An error analysis shows the influence of pecularities of the Russian language on the word error rate, such as its morphological structure and specific orthorgaphic properties. Our system achieves a word error rate of 19.3% on the official Quaero 2010 evaluation set.
- Markus Freitag, Joern Wuebker, Stephan Peitz, H. Ney, Matthias Huck, Alexandra Birch, Nadir Durrani, Philipp Koehn, Mohammed Mediani, Isabel Slawik, J. Niehues, Eunah Cho, A. Waibel, N. Bertoldi, M. Cettolo, Marcello Federico. 2015. Explorer Combined Spoken Language Translation. Abstract: EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. One of the collaborative efforts within EU-BRIDGE is to produce joint submissions of up to four different partners to the evaluation campaign at the 2014 International Workshop on Spoken Language Translation (IWSLT). We submitted combined translations to the German→English spoken language translation (SLT) track as well as to the German→English, English→German and English→French machine translation (MT) tracks. In this paper, we present the techniques which were applied by the different individual translation systems of RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show the combination approach developed at RWTH Aachen University which combined the individual systems. The consensus translations yield empirical gains of up to 2.3 points in BLEU and 1.2 points in TER compared to the best individual system.
- Maite Taboada, Lori S. Levin, A. Waibel, N. Green, A. Lavie. 2015. Discourse Information for Disambiguation The Phoenix Approach in JANUS M S Project Report. Abstract: For any given utterance out of what we can loosely call context there is usually more than one possible interpretation A speaker s utterance of an elliptical expression like the gure twelve fteen might have a di erent meaning depending on the discourse context the way the conver sation has evolved until that point and the previous speaker s utterance If this is a problem for any human listener the problem grows considerably when it is a parser doing the disambiguation In this project I intend to help a parser disambiguate among di erent possible parses for an input sentence with the nal goal of improving the translation in an end to end speech translation system
- Q. Do, T. Herrmann, J. Niehues, A. Allauzen, François Yvon, A. Waibel. 2015. The KIT-LIMSI Translation System for WMT 2015. Abstract: This paper presented the joined submission of KIT and LIMSI to the English to German translation task of WMT 2015. In this year submission, we integrated a neural network-based translation model into a phrase-based translation model by rescoring the n-best lists. Since the computation complexity is one of the main issues for continuous space models, we compared two techniques to reduce the computation cost. We investigated models using a structured output layer as well as models trained with noise contrastive estimation. Furthermore, we evaluated a new method to obtain the best log-linear combination in the rescoring phase. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points.
- C. Fügen, Tanja Schultz, Jiacheng Hu, A. Waibel. 2015. RECENT ADVANCES IN LINGWEAR : A WEARABLE LINGUISTIC ASSISTANT FOR TOURISTS. Abstract: In this paper we describe our recent advances in LingWear, a wearable linguistic assistant for tourists. LingWear allows uninformed users to find their way in foreign cities or to ask for information about sightseeing, accommodations, and other places of interest. Moreover, the system allows the user to communicate with local residents through integrated speech-to-speech translation. Furthermore, the graphical user interface (GUI) of LingWear runs also on small hand-held devices (e.g. Compaq’s iPAQ). In this client-server solution the main components of the system are running on a wireless connected server. The user can query LingWear either by means of spontaneous speech or via touch screen and receive the system’s responds either by the integrated speech synthesis or by display messages.
- Thanh-Le Ha, J. Niehues, A. Waibel. 2015. Lexical translation model using a deep neural network architecture. Abstract: In this paper we combine the advantages of a model using global source sentence contexts, the Discriminative Word Lexicon, and neural networks. By using deep neural networks instead of the linear maximum entropy model in the Discriminative Word Lexicon models, we are able to leverage dependencies between different source words due to the non-linearity. Furthermore, the models for different target words can share parameters and therefore data sparsity problems are effectively reduced. 
By using this approach in a state-of-the-art translation system, we can improve the performance by up to 0.5 BLEU points for three different language pairs on the TED translation task.
- J. Niehues, Q. Do, A. Allauzen, A. Waibel. 2015. ListNet-based MT Rescoring. Abstract: The log-linear combination of different features is an important component of SMT systems. It allows for the easy integartion of models into the system and is used during decoding as well as for nbest list rescoring. With the recent success of more complex models like neural network-based translation models, n-best list rescoring attracts again more attention. In this work, we present a new technique to train the log-linear model based on the ListNet algorithm. This technique scales to many features, considers the whole list and not single entries during learning and can also be applied to more complex models than a log-linear combination. Using the new learning approach, we improve the translation quality of a largescale system by 0.8 BLEU points during rescoring and generate translations which are up to 0.3 BLEU points better than other learning techniques such as MERT or MIRA.
- Markus Müller, M. Wagner, Juan Hussain, Sebastian Stüker, A. Waibel. 2015. Effectiveness of Histogram Equalization and SyDOCC Features on Speech Recognition Performance on a Real-World Noisy Speech Task. Abstract: When building systems for automatic speech recognition, one often faces the challenge of dealing with speech signals containing noise. This additional noise leads to a drop in recognition performance, especially, when the acoustic environment varies during training and testing of a system. There exist several approaches to deal with noisy data or mismatched conditions. We evaluate two different approaches: Histogram Equalization (HEQ) and Synchronized Damped Oscillator Cepstral Coefficients (SyDOCC). While HEQ tries to normalize the statistical properties of the input features in an unsupervised manner without requiring a noise estimate, SyDOCCs model the acoustic properties of the human ear more accurately than Mel-Frequency Cepstral Coefficients (MFCCs). We evaluate both approaches using data with artificially added noise as well as data that contains genuine noise due to the recording conditions.
- Kevin Kilgour, A. Waibel. 2015. Multifeature modular deep neural network acoustic models. Abstract: This paper presents and examines multifeature modular deep neural network acoustic models. The proposed setup uses well trained bottleneck networks to extract features from multiple combinations of input features and combines them using a classification deep neural network (DNN). The effectiveness of each feature combination is evaluated empirically on multiple test sets for both a classical DNN as well as a for modular DNNs using only a single module. Modular DNNs using two or more modules are shown to reduce the WER by up to 11.5% relatively compared to a baseline DNN and give the best overall performance on both test sets.
- Eunah Cho, Thanh-Le Ha, Mohammed Mediani, J. Niehues, T. Herrmann, Isabel Slawik, A. Waibel. 2014. The Karlsruhe Institute of Technology Translation Systems for the WMT 2014. Abstract: In this paper, we present the KIT systems participating in the Shared Translation Task translating between English$German and English$French. All translations are generated using phrase-based translation systems, using different kinds of word-based, part-ofspeech-based and cluster-based language models trained on the provided data. Additional models include bilingual language models, reordering models based on part-of-speech tags and syntactic parse trees, as well as a lexicalized reordering model. In order to make use of noisy web-crawled data, we apply filtering and data selection methods for language modeling. A discriminative word lexicon using source context information proved beneficial for all translation directions.
- Markus Freitag, Joern Wuebker, Stephan Peitz, H. Ney, Matthias Huck, Alexandra Birch, Nadir Durrani, Philipp Koehn, Mohammed Mediani, Isabel Slawik, J. Niehues, Eunah Cho, A. Waibel, N. Bertoldi, M. Cettolo, Marcello Federico. 2014. Combined spoken language translation. Abstract: EU-BRIDGE 1 is a European research project which is aimed at developing innovative speech translation technology. One of the collaborative efforts within EU-BRIDGE is to produce joint submissions of up to four different partners to the evaluation campaign at the 2014 International Workshop on Spoken Language Translation (IWSLT). We submitted combined translations to the German!English spoken language translation (SLT) track as well as to the German!English, English!German and English!French machine translation (MT) tracks. In this paper, we present the techniques which were applied by the different individual translation systems of RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show the combination approach developed at RWTH Aachen University which combined the individual systems. The consensus translations yield empirical gains of up to 2.3 points in BLEU and 1.2 points in TER compared to the best individual system.
- Matthias Sperber, Mirjam Simantzik, Graham Neubig, Satoshi Nakamura, A. Waibel. 2014. Segmentation for Efficient Supervised Language Annotation with an Explicit Cost-Utility Tradeoff. Abstract: In this paper, we study the problem of manually correcting automatic annotations of natural language in as efficient a manner as possible. We introduce a method for automatically segmenting a corpus into chunks such that many uncertain labels are grouped into the same chunk, while human supervision can be omitted altogether for other segments. A tradeoff must be found for segment sizes. Choosing short segments allows us to reduce the number of highly confident labels that are supervised by the annotator, which is useful because these labels are often already correct and supervising correct labels is a waste of effort. In contrast, long segments reduce the cognitive effort due to context switches. Our method helps find the segmentation that optimizes supervision efficiency by defining user models to predict the cost and utility of supervising each segment and solving a constrained optimization problem balancing these contradictory objectives. A user study demonstrates noticeable gains over pre-segmented, confidence-ordered baselines on two natural language processing tasks: speech transcription and word segmentation.
- A. Waibel. 2014. A World without Barriers: Connecting the World across Languages, Distances and Media. Abstract: As our world becomes increasingly interdependent and globalization brings people together more than ever, we quickly discover that it is no longer the absence of connectivity (the "digital divide") that separates us, but that new and different forms of alienation still keep us apart, including language, culture, distance and interfaces. Can technology provide solutions to bring us closer to our fellow humans? In this talk, I will present multilingual and multimodal interface technology solutions that offer the best of both worlds: maintaining our cultural diversity and locale while providing for better communication, greater integration and collaboration. We explore: Smart phone based speech translators for everyday travelers and humanitarian missions Simultaneous translation systems and services to translate academic lectures and political speeches in real time (at Universities, the European Parliament and broadcasting services) Multimodal language-transparent interfaces and smartrooms to improve joint and distributed communication and interaction. We will first discuss the difficulties of language processing; review how the technology works today and what levels of performance are now possible. Key to today's systems is effective machine learning, without which scaling multilingual and multimodal systems to unlimited domains, modalities, accents, and more than 6,000 languages would be hopeless. Equally important are effective human-computer interfaces, so that language differences fade naturally into the background and communication and interaction become natural and engaging. I will present recent research results as well as examples from our field trials and deployments in educational, commercial, humanitarian and government settings.
- Eunah Cho, J. Niehues, A. Waibel. 2014. Machine translation of multi-party meetings: segmentation and disfluency removal strategies. Abstract: Translating meetings presents a challenge since multispeaker speech shows a variety of disfluencies. In this paper we investigate the importance of transforming speech into well-written input prior to translating multi-party meetings. We first analyze the characteristics of this data and establish oracle scores. Sentence segmentation and punctuation are performed using a language model, turn information, or a monolingual translation system. Disfluencies are removed by a CRF model trained on in-domain and out-of-domain data. For comparison, we build a combined CRF model for punctuation insertion and disfluency removal. By applying these models, multi-party meetings are transformed into fluent input for machine translation. We evaluate the models with regard to translation performance and are able to achieve an improvement of 2.1 to 4.9 BLEU points depending on the availability of turn information.
- Timo Abele, A. Waibel. 2014. Source Sentence Reordering for English to Japanese Machine Translation. Abstract: Maschinelle Übersetzung, d.h das Übersetzen von Text von einer Sprache in eine andere, ist in den letzten Jahren zu einer wichtigen Bereicherung unseres Alltags geworden. Trotz gewisser Unzulänglichkeiten oder manchmal unnatürlicher Übersetzungen, erfreut sich maschinelle Übersetzung Beliebtheit, wenn menschliche Übersetzer zu teuer sind. Während das Übersetzen von Wörtern einfach mit einem Lexikon bewerkstelligt werden kann, bleibt reordering (der Platzwechsel der einzelnen Wörter) eine große Herausforderung in der maschinellen Übersetzung. Besonders phrasenbasierte Ansätze haben hier Probleme, da sie nicht von sich aus mit syntaktischer Information arbeiten. Ein neuerlicher Ansatz, dieses Problem in der maschinellen Übersetzung anzugehen, ist es, solche reordering-Regeln automatisch zu erlernen und vor der eigentlichen Übersetzung auf die Quellsätze anzuwenden. Dieser Ansatz ist kürzlich durch ein verfeinertes Reorderingmodell erweitert worden, das die Informationen aus Syntaxbäumen nutzt, um mögliche Umordnungen zu bestimmen. Dieses Modell ist erfolgreich auf zwei europäischen Sprachpaaren getestet worden. In dieser Thesis wenden wir diesen Ansatz auf ein sehr gegensätzliches Sprachpaar an: Englisch und Japanisch. Japanisch ist eine Subjekt-Objekt-Verb (SOV) Sprache, d.h. das Subjekt steht an vorderster Stelle, gefolgt vom Objekt und am Ende steht das Verb. OSV Sätze sind auch möglich, es ist aber zwingend, dass das Verb an letzter Stelle steht. Dies ist ein Leichtes für ein regelbasiertes System, aber in der statistischen maschinellen Übersetzung müssen wir ein Modell entwickeln. Wir vergleichen wortklassenbasierte Regeln mit Regeln die auf Syntaxbäumen aufbauen. Außerdem experimentieren wir zum einen mit Variationen, z.B. rekursive Regelanwendung, lattice phrase extraction, bei der wir unsere phrase table aus der umgeordneten Quellseite des Trainingscorpus aufbauen, zum anderen mit verschiedenen Ansätzen zum Parsen. Wir testen unsere Systeme auf einem Satz von Wikipedia-Artikeln zum Thema Kyoto. Zur Evaluation unserer Experimente nutzen wir BLEU und RIBES für eine umfassende Einschätzung unserer Konfigurationen, so wie Kendall’s τ und chunk fragmentation, eine Metrik, die die Anzahl der Satzfragmente misst, die nicht korrekt angeordnet sind, um eine Einschätzung der reordering-Qualität alleine zu erhalten. Weiterhin führen wir eine manuelle Analyse auf 100 Sätzen durch, um zu sehen, ob unsere Systeme in der Lage sind, das Verb ans Ende des Satzes zu schieben. Wir melden eine maximale Verbesserung von 1.95 BLEU-Punkten über ein System ohne Regeln. Wir zeigen außerdem, dass der Einsatz von discontinuous reordering rules
- Kevin Kilgour, Michael Heck, Markus Müller, Matthias Sperber, Sebastian Stüker, A. Waibel. 2014. The 2014 KIT IWSLT speech-to-text systems for English, German and Italian. Abstract: This paper describes our German, Italian and English Speech-to-Text (STT) systems for the 2014 IWSLT TED ASR track. Our setup uses ROVER and confusion network combination from various subsystems to achieve a good overall performance. The individual subsystems are built by using different front-ends, (e.g., MVDR-MFCC or lMel), acoustic models (GMM or modular DNN) and phone sets and by training on various subsets of the training data. Decoding is performed in two stages, where the GMM systems are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cMLLR. The combination setup produces a final hypothesis that has a significantly lower WER than any of the individual subsystems.
- Matthias Eck, Yury Zemlyanskiy, Joy Zhang, A. Waibel. 2014. Extracting translation pairs from social network content. Abstract: We introduce two methods to collect additional training data for statistical machine translation systems from public social network content. The first method identifies multilingual content where the author self-translated their own post to reach additional friends, fans or customers. Once identified, we can split the post in the language segments and extract translation pairs from this content. The second methods considers web links (URLs) that users add as part of their post to point the reader to a video, article or website. If the same URL is shared from different language users, there is a chance they might give the same comment in their respective language. We use a support vector machine (SVM) as a classifier to identify true translations from all candidate pairs. We collected additional translation pairs using both methods for the language pairs Spanish-English and Portuguese-English. Testing the collected data as additional training data for statistical machine translations on in-domain test sets resulted in very significant improvements of up to 5 BLEU.
- Matthias Sperber, Graham Neubig, Satoshi Nakamura, A. Waibel. 2014. On-the-fly user modeling for cost-sensitive correction of speech transcripts. Abstract: We propose an on-the-fly updating framework for cost-sensitive manual correction of automatically recognized speech transcripts. This framework trains cost-models during the transcription process, and does not require the transcriber enrollment necessary in previous work. We use a baseline method that optimizes a segmentation into segments to supervise or not to supervise in a cost-sensitive fashion that minimizes human effort, and introduce a much faster algorithm for computing such a segmentation that can be used for on-the-fly updates. Besides removing the need to carry out enrollments, experiments show that our updating framework results in 28% higher human supervision efficiency than previous cost-sensitive approaches.
- Markus Freitag, Stephan Peitz, Joern Wuebker, H. Ney, Matthias Huck, Rico Sennrich, Nadir Durrani, Maria Nadejde, Philip Williams, Philipp Koehn, T. Herrmann, Eunah Cho, A. Waibel. 2014. EU-BRIDGE MT: Combined Machine Translation. Abstract: This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two European language pairs, German→English and English→German. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems.
- Quoc Bao Nguyen, Jonas Gehring, Markus Müller, Sebastian Stüker, A. Waibel. 2014. Multilingual shifting deep bottleneck features for low-resource ASR. Abstract: In this work, we propose a deep bottleneck feature architecture that is able to leverage data from multiple languages. We also show that tonal features are helpful for non-tonal languages. Evaluations are performed on a low-resource conversational telephone speech transcription task in Bengali, while additional data for DBNF training is provided in Assamese, Pashto, Tagalog, Turkish, and Vietnamese. We obtain relative reductions of up to 17.3% and 9.4% WER over mono-lingual GMMs and DBNFs, respectively.
- Eunah Cho, Sarah Fünfer, Sebastian Stüker, A. Waibel. 2014. A Corpus of Spontaneous Speech in Lectures: The KIT Lecture Corpus for Spoken Language Processing and Translation. Abstract: With the increasing number of applications handling spontaneous speech, the needs to process spoken languages become stronger. Speech disfluency is one of the most challenging tasks to deal with in automatic speech processing. As most applications are trained with well-formed, written texts, many issues arise when processing spontaneous speech due to its distinctive characteristics. Therefore, more data with annotated speech disfluencies will help the adaptation of natural language processing applications, such as machine translation systems. In order to support this, we have annotated speech disfluencies in German lectures at KIT. In this paper we describe how we annotated the disfluencies in the data and provide detailed statistics on the size of the corpus and the speakers. Moreover, machine translation performance on a source text including disfluencies is compared to the results of the translation of a source text without different sorts of disfluencies or no disfluencies at all.
- T. Herrmann, J. Niehues, A. Waibel. 2014. Manual Analysis of Structurally Informed Reordering in German-English Machine Translation. Abstract: Word reordering is a difficult task for translation. Common automatic metrics such as BLEU have problems reflecting improvements in target language word order. However, it is a crucial aspect for humans when deciding on translation quality. This paper presents a detailed analysis of a structure-aware reordering approach applied in a German-to-English phrase-based machine translation system. We compare the translation outputs of two translation systems applying reordering rules based on parts-of-speech and syntax trees on a sentence-by-sentence basis. For each sentence-pair we examine the global translation performance and classify local changes in the translated sentences. This analysis is applied to three data sets representing different genres. While the improvement in BLEU differed substantially between the data sets, the manual evaluation showed that both global translation performance as well as individual types of improvements and degradations exhibit a similar behavior throughout the three data sets. We have observed that for 55-64% of the sentences with different translations, the translation produced using the tree-based reordering was considered to be the better translation. As intended by the investigated reordering model, most improvements are achieved by improving the position of the verb or being able to translate a verb that could not be translated before.
- Q. Do, T. Herrmann, J. Niehues, Alexander Allauzen, François Yvon, A. Waibel. 2014. The KIT-LIMSI Translation System for WMT 2014. Abstract: This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score.
- J. Niehues, Alexander Allauzen, François Yvon, A. Waibel. 2014. Combining techniques from different NN-based language models for machine translation. Abstract: This paper presents two improvements of language models based on Restricted Boltzmann Machine (RBM) for large machine translation tasks. In contrast to other continuous space approach, RBM based models can easily be integrated into the decoder and are able to directly learn a hidden representation of the n-gram. Previous work on RBM-based language models do not use a shared word representation and therefore, they might suffer of a lack of generalization for larger contexts. Moreover, since the training step is very time consuming, they are only used for quite small copora. In this work we add a shared word representation for the RBM-based language model by factorizing the weight matrix. In addition, we propose an efficient and tailored sampling algorithm that allows us to drastically speed up the training process. Experiments are carried out on two German to English translation tasks and the results show that the training time could be reduced by a factor of 10 without any drop in performance. Furthermore, the RBM-based model can also be trained on large size corpora.
- Eunah Cho, J. Niehues, A. Waibel. 2014. Tight Integration of Speech Disfluency Removal into SMT. Abstract: Speech disfluencies are one of the main challenges of spoken language processing. Conventional disfluency detection systems deploy a hard decision, which can have a negative influence on subsequent applications such as machine translation. In this paper we suggest a novel approach in which disfluency detection is integrated into the translation process. We train a CRF model to obtain a disfluency probability for each word. The SMT decoder will then skip the potentially disfluent word based on its disfluency probability. Using the suggested scheme, the translation score of both the manual transcript and ASR output is improved by around 0.35 BLEU points compared to the CRF hard decision system.
- Ge Wu, Yuqi Zhang, A. Waibel. 2014. Rule-based preordering on multiple syntactic levels in statistical machine translation. Abstract: We propose a novel data-driven rule-based preordering approach, which uses the tree information of multiple syntactic levels. This approach extend the tree-based reordering from one level into multiple levels, which has the capability to process more complicated reordering cases. We have conducted experiments in English-to-Chinese and Chinese-to-English translation directions. Our results show that the approach has led to improved translation quality both when it was applied separately or when it was combined with some other reordering approaches. As our reordering approach was used alone, it showed an improvement of 1.61 in BLEU score in the English-to-Chinese translation direction and an improvement of 2.16 in BLEU score in the Chinese-to-English translation direction, in comparison with the baseline, which used no word reordering. As our preordering approach were combined with the short rule [1], long rule [2] and tree rule [3] based preordering approaches, it showed further improvements of up to 0.43 in BLEU score in the English-to-Chinese translation direction and further improvements of up to 0.3 in BLEU score in the Chinese-to-English translation direction. Through the translations that used our preordering approach, we have also found many translation examples with improved syntactic structures.
- Sebastian Stüker, Markus Müller, Quoc Bao Nguyen, A. Waibel. 2014. Training time reduction and performance improvements from multilingual techniques on the BABEL ASR task. Abstract: In the IARPA sponsored program BABEL we are faced with the challenge of training automatic speech recognition systems in sparse data conditions in very little time. In this paper we show that by using multilingual bootstrapping techniques in combination with multilingual deep belief bottle neck features that are only fine tuned on the target language the training time of an LVCSR system can be essentially halved while the word error rate stays the same. We show this for recognition systems on Tagalog, making use of multilingual systems trained on the other four languages of the Babel base period: Cantonese, Pashto, Turkish, and Vietnamese.
- Markus Müller, Sebastian Stüker, Zaid A. W. Sheikh, Florian Metze, A. Waibel. 2014. Multilingual deep bottle neck features: a study on language selection and training techniques. Abstract: Previous work has shown that training the neural networks for bottle neck feature extraction in a multilingual way can lead to improvements in word error rate and average term weighted value in a telephone key word search task. In this work we conduct a systematic study on a) which multilingual training strategy to employ, b) the effect of language selection and amount of multilingual training data used and c) how to find a suitable combination for languages. We conducted our experiment on the key word search task and the languages of the IARPA BABEL program. In a first step, we assessed the performance of a single language out of all available languages in combination with the target language. Based on these results, we then combined a multitude of languages. We also examined the influence of the amount of training data per language, as well as different techniques for combining the languages during network training. Our experiments show that data from arbitrary additional languages does not necessarily increase the performance of a system. But when combining a suitable set of languages, a significant gain in performance can be achieved.
- Matthias Sperber, Graham Neubig, Satoshi Nakamura, A. Waibel. 2014. SESLA TRANSCRIBER: A SPEECH TRANSCRIPTION TOOL THAT ADAPTS TO YOUR SKILL AND TIME BUDGET. Abstract: We present a speech transcription tool targeted at situations in which cost is a critical or limiting factor. This tool actively guides the transcription process by taking an automatically created transcript as a starting point, and asking for correction of only the parts likely to contain errors. The transcriber speciﬁes a time budget, and the software uses models of transcription accuracy and cost to choose which segments should be transcribed to achieve the highest error reduction. This approach has been found to be 25% more efﬁcient than cost-insensitive approaches in previous work. The cost model is adapted to the transcriber on-the-ﬂy during the transcription process, so no user enrollment is necessary. The segmentation is updated regularly to reﬂect improved cost models, and to recover from potential time prediction errors. The user interface was designed to be easy to learn and efﬁcient to use. It allows either transcribing each segment from scratch or post-editing, and has logging features that allow detailed user studies.
- Ankur Gandhe, Florian Metze, A. Waibel, Ian Lane. 2014. Optimization of Neural Network Language Models for keyword search. Abstract: Recent works have shown Neural Network based Language Models (NNLMs) to be an effective modeling technique for Automatic Speech Recognition. Prior works have shown that these models obtain lower perplexity and word error rate (WER) compared to both standard n-gram language models (LMs) and more advanced language models including maximum entropy and random forest LMs. While these results are compelling, prior works were limited to evaluating NNLMs on perplexity and word error rate. Our initial results showed that while NNLMs improved speech recognition accuracy, the improvement in keyword search was negligible. In this paper we propose alternate optimizations of NNLMs for the task of keyword search. We evaluate the performance of the proposed methods for keyword search on the Vietnamese dataset provided in phase one of the BABEL1 project and demonstrate that by penalizing low frequency words during NNLM training, keyword search metrics such as actual term weighted value (ATWV) can be improved by up to 9.3% compared to the standard training methods.
- Markus Freitag, Stephan Peitz, Joern Wuebker, H. Ney, Nadir Durrani, Matthias Huck, Philipp Koehn, Thanh-Le Ha, J. Niehues, Mohammed Mediani, T. Herrmann, A. Waibel, N. Bertoldi, M. Cettolo, Marcello Federico. 2013. EU-BRIDGE MT: text translation of talks in the EU-BRIDGE project. Abstract: EU-BRIDGE 1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes one of the collaborative efforts within EUBRIDGE to further advance the state of the art in machine translation between two European language pairs, English!French and German!English. Four research institutions involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the machine translation track of the evaluation campaign at the 2013 International Workshop on Spoken Language Translation (IWSLT). We present the methods and techniques to achieve high translation quality for text translation of talks which are applied at RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show how we have been able to considerably boost translation performance (as measured in terms of the metrics BLEU and TER) by means of system combination. The joint setups yield empirical gains of up to 1.4 points in BLEU and 2.8 points in TER on the IWSLT test sets compared to the best single systems.
- Yajie Miao, Florian Metze, A. Waibel. 2013. Learning discriminative basis coefficients for eigenspace MLLR unsupervised adaptation. Abstract: Eigenspace MLLR is effective for fast adaptation when the amount of adaptation data is limited, e.g., less than 5s. The general motivation is to represent the MLLR transform as a linear combination of basis matrices. In this paper, we present a framework to estimate a speaker-independent discriminative transform over the combination coefficients. This discriminative basis coefficients transform (DBCT) is learned by optimizing discriminative criteria over all the training speakers. During recognition, the ML basis coefficients for each testing speaker are firstly found, on which DBCT is applied to give the final MLLR transform discrimination ability. Experiments show that DBCT results in consistent WER reduction in unsupervised adaptation, compared with both standard ML and discriminatively trained transforms.
- Eunah Cho, C. Fügen, T. Herrmann, Kevin Kilgour, Mohammed Mediani, Christian Mohr, J. Niehues, Kay Rottmann, Christian Saam, Sebastian Stüker, A. Waibel. 2013. A real-world system for simultaneous translation of German lectures. Abstract: We present a real-time automatic speech translation system for university lectures that can interpret several lectures in parallel. University lectures are characterized by a multitude of diverse topics and a large amount of technical terms. This poses specific challenges, e.g., a very specific vocabulary and language model are needed. In addition, in order to be able to translate simultaneously, i.e., to interpret the lectures, the components of the systems need special modifications. The output of the system is delivered in the form or realtime subtitles via a web site that can be accessed by the students attending the lecture through mobile phones, tablet computers or laptops. We evaluated the system on our German to English lecture translation task at the Karlsruhe Institute of Technology. The system is now being installed in several lecture halls at KIT and is able to provide the translation to the students in several parallel sessions.
- Jonas Gehring, Yajie Miao, Florian Metze, A. Waibel. 2013. Extracting deep bottleneck features using stacked auto-encoders. Abstract: In this work, a novel training scheme for generating bottleneck features from deep neural networks is proposed. A stack of denoising auto-encoders is first trained in a layer-wise, unsupervised manner. Afterwards, the bottleneck layer and an additional layer are added and the whole network is fine-tuned to predict target phoneme states. We perform experiments on a Cantonese conversational telephone speech corpus and find that increasing the number of auto-encoders in the network produces more useful features, but requires pre-training, especially when little training data is available. Using more unlabeled data for pre-training only yields additional gains. Evaluations on larger datasets and on different system setups demonstrate the general applicability of our approach. In terms of word error rate, relative improvements of 9.2% (Cantonese, ML training), 9.3% (Tagalog, BMMI-SAT training), 12% (Tagalog, confusion network combinations with MFCCs), and 8.7% (Switchboard) are achieved.
- Henning Sperr, J. Niehues, A. Waibel. 2013. Letter N-Gram-based Input Encoding for Continuous Space Language Models. Abstract: We present a letter-based encoding for words in continuous space language models. We represent the words completely by letter n-grams instead of using the word index. This way, similar words will automatically have a similar representation. With this we hope to better generalize to unknown or rare words and to also capture morphological information. We show their influence in the task of machine translation using continuous space language models based on restricted Boltzmann machines. We evaluate the translation quality as well as the training time on a German-to-English translation task of TED and university lectures as well as on the news translation task translating from English to German. Using our new approach a gain in BLEU score by up to 0.4 points can be achieved.
- E. Shin, Sebastian Stüker, Kevin Kilgour, C. Fügen, A. Waibel. 2013. Maximum entropy language modeling for Russian ASR. Abstract: Russian is a challenging language for automatic speech recognition systems due to its rich morphology. This rich morphology stems from Russian’s highly inflectional nature and the frequent use of preand suffixes. Also, Russian has a very free word order, changes in which are used to reflect connotations of the sentences. Dealing with these phenomena is rather difficult for traditional n-gram models. We therefore investigate in this paper the use of a maximum entropy language model for Russian whose features are specifically designed to deal with the inflections in Russian, as well as the loose word order. We combine this with a subword based language model in order to alleviate the problem of large vocabulary sizes necessary for dealing with highly inflecting languages. Applying the maximum entropy language model during re-scoring improves the word error rate of our recognition system by 1.2% absolute, while the use of the sub-word based language model reduces the vocabulary size from 120k to 40k and the OOV rate from 4.8% to 2.1%.
- Florian Metze, Zaid A. W. Sheikh, A. Waibel, Jonas Gehring, Kevin Kilgour, Quoc Bao Nguyen, V. Nguyen. 2013. Models of tone for tonal and non-tonal languages. Abstract: Conventional wisdom in automatic speech recognition asserts that pitch information is not helpful in building speech recognizers for non-tonal languages and contributes only modestly to performance in speech recognizers for tonal languages. To maintain consistency between different systems, pitch is therefore often ignored, trading the slight performance benefits for greater system uniformity/ simplicity. In this paper, we report results that challenge this conventional approach. We present new models of tone that deliver consistent performance improvements for tonal languages (Cantonese, Vietnamese) and even modest improvements for non-tonal languages. Using neural networks for feature integration and fusion, these models achieve significant gains throughout, and provide us with system uniformity and standardization across all languages, tonal and non-tonal.
- Joshua Winebarger, Quoc Bao Nguyen, Jonas Gehring, Sebastian Stüker, A. Waibel. 2013. The 2013 KIT Quaero speech-to-text system for French. Abstract: This paper describes our Speech-to-Text (STT) system for French, which was developed as part of our efforts in the Quaero program for the 2013 evaluation. Our STT system consists of six subsystems which were created by combining multiple complementary sources of pronunciation modeling including graphemes with various feature front-ends based on deep neural networks and tonal features. Both speaker-independent and speaker adaptively trained versions of the systems were built. The resulting systems were then combined via confusion network combination and crossadaptation. Through progressive advances and system combination we reach a word error rate (WER) of 16.5% on the 2012 Quaero evaluation data.
- T. Herrmann, J. Niehues, A. Waibel. 2013. Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation. Abstract: We describe a novel approach to combining lexicalized, POS-based and syntactic treebased word reordering in a phrase-based machine translation system. Our results show that each of the presented reordering methods leads to improved translation quality on its own. The strengths however can be combined to achieve further improvements. We present experiments on German-English and GermanFrench translation. We report improvements of 0.7 BLEU points by adding tree-based and lexicalized reordering. Up to 1.1 BLEU points can be gained by POS and tree-based reordering over a baseline with lexicalized reordering. A human analysis, comparing subjective translation quality as well as a detailed error analysis show the impact of our presented tree-based rules in terms of improved sentence quality and reduction of errors related to missing verbs and verb positions.
- Kevin Kilgour, Igor Tseyzer, Quoc Bao Nguyen, A. Waibel. 2013. Warped Minimum Variance Distortionless Response based bottle neck features for LVCSR. Abstract: This paper presents the results of our experiments on bottleneck feature applied to a wMVDR (Warped Minimum Variance Distortionless Response) frontend. We examine how to best optimize wMVDR-BNF features and wMVDR combined with MFCC bottleneck features (wMVDR+MFCC-BNF). Our wMVDR+MFCC-BNF frontend improves a single pass system from 18.7% (20.7%) to 18.1% compared to a MFCC-BNF (MFCC) system tested on the Quaero 2010 German evaluation set. When used in a system combination our wMVDR-BNF and wMVDR+MFCC-BNF systems reduced the overall WER from 14.3% to 13.3% on the IWSLT 2010 test set while at the same time reducing the number of systems needed from 9 to 5. Our result of 11.9% on the 2012 IWSLT testset is better than the best result submitted during the evaluation campaign.
- J. Niehues, A. Waibel. 2013. An MT Error-Driven Discriminative Word Lexicon using Sentence Structure Features. Abstract: The Discriminative Word Lexicon (DWL) is a maximum-entropy model that predicts the target word probability given the source sentence words. We present two ways to extend a DWL to improve its ability to model the word translation probability in a phrase-based machine translation (PBMT) system. While DWLs are able to model the global source information, they ignore the structure of the source and target sentence. We propose to include this structure by modeling the source sentence as a bag-of-n-grams and features depending on the surrounding target words. Furthermore, as the standard DWL does not get any feedback from the MT system, we change the DWL training process to explicitly focus on addressing MT errors.
- Narine Kokhlikyan, A. Waibel, Yuqi Zhang, J. Zhang. 2013. Measuring the Structural Importance through Rhetorical Structure Index. Abstract: In this paper, we propose a novel Rhetorical Structure Index (RSI) to measure the structural importance of a word or a phrase. Unlike TF-IDF and other content-driven measurements, RSI identifies words or phrases that are structural cues in an unstructured document. We show structurally motivated features with high RSI values are more useful than content-driven features for applications such as segmenting unstructured lecture transcripts into meaningful segments. Experiments show that using RSI significantly improves the segmentation accuracy compared to TF-IDF, a traditional content-based feature weighting scheme.
- Jonas Gehring, Wonkyum Lee, Kevin Kilgour, Ian Lane, Yajie Miao, A. Waibel. 2013. Modular combination of deep neural networks for acoustic modeling. Abstract: In this work, we propose a modular combination of two popular applications of neural networks to large-vocabulary continuous speech recognition. First, a deep neural network is trained to extract bottleneck features from frames of mel scale filterbank coefficients. In a similar way as is usually done for GMM/HMM systems, this network is then applied as a nonlinear discriminative feature-space transformation for a hybrid setup where acoustic modeling is performed by a deep belief network. This effectively results in a very large network, where the layers of the bottleneck network are fixed and applied to successive windows of feature frames in a time-delay fashion. We show that bottleneck features improve the recognition performance of DBN/HMM hybrids, and that the modular combination enables the acoustic model to benefit from a larger temporal context. Our architecture is evaluated on a recently released and challenging Tagalog corpus containing conversational telephone speech.
- Yajie Miao, Florian Metze, A. Waibel. 2013. Subspace mixture model for low-resource speech recognition in cross-lingual settings. Abstract: The subspace Gaussian mixture model (SGMM) has been exploited for cross-lingual speech recognition. The general motivation is that the subspace parameters can be estimated on multiple source languages and then transferred to the target language. In this work, we investigate an extension to SGMM, referred to as subspace mixture model (SMM), in which subspace parameters on the target language are casted as a linear mixture of the subspaces derived from source languages. This approach reduces the number of SGMM model parameters, while retaining the flexibility of subspace learning on the target language. Experiments show that the proposed SMM method outperforms SGMM significantly when the target language has limited training data.
- Michael Heck, Sebastian Stüker, S. Sakti, A. Waibel, Satoshi Nakamura. 2013. Incremental unsupervised training for university lecture recognition. Abstract: In this paper we describe our work on unsupervised adaptation of the acoustic model of our simultaneous lecture translation system. We trained a speaker independent acoustic model, with which we produce automatic transcriptions of new lectures in order to improve the system for a specific lecturer. We compare our results against a model that was trained in a supervised way on an exact manual transcription. We examine four different ways of processing the decoder outputs of the automatic transcription with respect to the treatment of pronunciation variants and noise words. We will show that, instead of fixating the latter informations in the transcriptions, it is of advantage to let the Viterbi algorithm during training decide which pronunciations to use and where to insert which noise words. Further, we utilize word level posterior probabilities obtained during decoding by weighting and thresholding the words of a transcription.
- Quoc Bao Nguyen, Jonas Gehring, Kevin Kilgour, A. Waibel. 2013. Optimizing deep bottleneck feature extraction. Abstract: We investigate several optimizations to a recently published architecture for extracting bottleneck features for large-vocabulary speech recognition with deep neural networks. We are able to improve recognition performance of first-pass systems from a 12% relative word error rate reduction reported previously to 21%, compared to MFCC baselines on a Tagalog conversational telephone speech corpus. This is achieved by using different input features, training the network to predict context-dependent targets, employing an efficient learning rate schedule and varying several architectural details. Evaluations on two larger German and French speech transcription tasks show that the optimizations proposed are universally applicable and yield comparable gains on other corpora (19.9% and 22.8%, respectively).
- T. Herrmann, Jochen Weiner, J. Niehues, A. Waibel. 2013. Analyzing the potential of source sentence reordering in statistical machine translation. Abstract: We analyze the performance of source sentence reordering, a common reordering approach, using oracle experiments on German-English and English-German translation. First, we show that the potential of this approach is very promising. Compared to a monotone translation, the optimally reordered source sentence leads to improvements of up to 4.6 and 6.2 BLEU points, depending on the language. Furthermore, we perform a detailed evaluation of the different aspects of the approach. We analyze the impact of the restriction of the search space by reordering lattices and we can show that using more complex rule types for reordering results in better approximation of the optimally reordered source. However, a gap of about 3 to 3.8 BLEU points remains, presenting a promising perspective for research on extending the search space through better reordering rules. When evaluating the ranking of different reordering variants, the results reveal that the search for the best path in the lattice performs very well for German-English translation. For English-German translation there is potential for an improvement of up to 1.4 BLEU points through a better ranking of the different reordering possibilities in the reordering lattice.
- Matthias Sperber, Graham Neubig, C. Fügen, Satoshi Nakamura, A. Waibel. 2013. Efficient speech transcription through respeaking. Abstract: We propose a method for efficient off-line speech transcription through respeaking. Speech is segmented into smaller utterances using an initial automatic transcript. Respeaking is performed segment by segment, while confidence filtering helps save supervision effort. We conduct detailed experiments comparing speaking vs. typing, sequential vs. confidence-ordered supervision, and examine the effect of the respeaking word error rate on correction efficiency. Our results demonstrate that the proposed method can not only outperform typing in terms of correction efficiency, but is also much less demanding for the respeakers than traditional respeaking methods, consequently helping to keep costs down.
- Christian Mohr, Christian Saam, Kevin Kilgour, Jonas Gehring, Sebastian Stüker, A. Waibel. 2013. Slightly Supervised Adaptation of Acoustic Models on Captioned BBC Weather Forecasts. Abstract: In this paper we investigate the exploitation of loosely transcribed audio data, in the form of captions for weather forecast recordings, in order to adapt acoustic models for automatically transcribing these kinds of forecasts. We focus on dealing with inaccurate time stamps in the captions and the fact that they often deviate from the exact spoken word sequence in the forecasts. Furthermore, different adaptation algorithms are compared when incrementally increasing the amount of adaptation material, for example, by recording new forecasts on a daily basis. Index Terms: speech recognition, acoustic model adaptation, slightly supervised training, loose transcripts, adaptation methods
- A. Waibel, A. Fischer. 2013. Simultaneous Translation by Machine. Abstract: Introducing the First Simultaneous Translation Service by Computer - Simultaneous Speech Translation | Panel Discussion: Dr. Susanne Altenberg, European Parliament | Prof. Tamim Asfour, KIT | Prof. Jaime Carbonell, Carnegie Mellon Univ. | Dr. Roberto Cencioni, European Commission | Prof. Ru?diger Dillmann, KIT | Karl Karnadi, KIT
- Eunah Cho, Thanh-Le Ha, A. Waibel. 2013. CRF-based disfluency detection using semantic features for German to English spoken language translation. Abstract: Disfluencies in speech pose severe difficulties in machine translation of spontaneous speech. This paper presents our conditional random field (CRF)-based speech disfluency detection system developed on German to improve spoken language translation performance. In order to detect speech disfluencies considering syntactics and semantics of speech utterances, we carried out a CRF-based approach using information learned from the word representation and the phrase table used for machine translation. The word representation is gained using recurrent neural networks and projected words are clustered using the k-means algorithm. Using the output from the model trained with the word representations and phrase table information, we achieve an improvement of 1.96 BLEU points on the lecture test set. By keeping or removing humanannotated disfluencies, we show an upper bound and lower bound of translation quality. In an oracle experiment we gain 3.16 BLEU points of improvement on the lecture test set, compared to the same set with all disfluencies.
- Jonas Gehring, Quoc Bao Nguyen, Florian Metze, A. Waibel. 2013. DNN acoustic modeling with modular multi-lingual feature extraction networks. Abstract: In this work, we propose several deep neural network architectures that are able to leverage data from multiple languages. Modularity is achieved by training networks for extracting high-level features and for estimating phoneme state posteriors separately, and then combining them for decoding in a hybrid DNN/HMM setup. This approach has been shown to achieve superior performance for single-language systems, and here we demonstrate that feature extractors benefit significantly from being trained as multi-lingual networks with shared hidden representations. We also show that existing mono-lingual networks can be re-used in a modular fashion to achieve a similar level of performance without having to train new networks on multi-lingual data. Furthermore, we investigate in extending these architectures to make use of language-specific acoustic features. Evaluations are performed on a low-resource conversational telephone speech transcription task in Vietnamese, while additional data for acoustic model training is provided in Pashto, Tagalog, Turkish, and Cantonese. Improvements of up to 17.4% and 13.8% over mono-lingual GMMs and DNNs, respectively, are obtained.
- Kevin Kilgour, Christian Mohr, Michael Heck, Quoc Bao Nguyen, V. Nguyen, E. Shin, Igor Tseyzer, Jonas Gehring, Markus Müller, Matthias Sperber, Sebastian Stüker, A. Waibel. 2013. The 2013 KIT IWSLT speech-to-text systems for German and English. Abstract: This paper describes our English Speech-to-Text (STT) systems for the 2013 IWSLT TED ASR track. The systems consist of multiple subsystems that are combinations of different front-ends, e.g. MVDR-MFCC based and lMel based ones, GMM and NN acoustic models and different phone sets. The outputs of the subsystems are combined via confusion network combination. Decoding is done in two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cMLLR.
- Stephan Peitz, Saab Mansour, Matthias Huck, Markus Freitag, H. Ney, Eunah Cho, T. Herrmann, Mohammed Mediani, J. Niehues, A. Waibel, Alexander Allauzen, Q. Do, Bianka Buschbeck-Wolf, Tonio Wandmacher. 2013. Joint WMT 2013 Submission of the QUAERO Project. Abstract: This paper describes the joint submission of the QUAERO project for the German!English translation task of the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT 2013). The submission was a system combination of the output of four different translation systems provided by RWTH Aachen University, Karlsruhe Institute of Technology (KIT), LIMSI-CNRS and SYSTRAN Software, Inc. The translations were joined using the RWTH’s system combination approach. Experimental results show improvements of up to 1.2 points in BLEU and 1.2 points in TER compared to the best single translation.
- Paul Maergner, A. Waibel, Ian Lane. 2012. Unsupervised vocabulary selection for real-time speech recognition of lectures. Abstract: In this work, we propose a novel method for vocabulary selection to automatically adapt automatic speech recognition systems to the diverse topics that occur in educational and scientific lectures. Utilizing materials that are available before the lecture begins, such as lecture slides, our proposed framework iteratively searches for related documents on the web and generates a lecture-specific vocabulary based on the resulting documents. In this paper, we propose a novel method for vocabulary selection where we first collect documents similar to an initial seed document and then rank the resulting vocabulary based on a score which is calculated using a combination of word features. This is a critical component for adaptation that has typically been overlooked in prior works. On the inter ACT German-English simultaneous lecture translation system our proposed approach significantly improved vocabulary coverage, reducing the out-of-vocabulary rate, on average by 57.0% and up to 84.9%, compared to a lecture-independent baseline. Furthermore, our approach reduced the word error rate, by 12.5% on average and up to 25.3%, compared to a lecture-independent baseline.
- Markus Freitag, Stephan Peitz, Matthias Huck, H. Ney, J. Niehues, T. Herrmann, A. Waibel, H. Le, T. Lavergne, A. Allauzen, Bianka Buschbeck-Wolf, J. Crego, Jean Senellart. 2012. Joint WMT 2012 Submission of the QUAERO Project. Abstract: This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT German→English task. Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission. Experimental results show improvements of up to 1.7 points in Bleu and 3.4 points in Ter compared to the best single system.
- Sebastian Stüker, Florian Kraft, Christian Mohr, T. Herrmann, Eunah Cho, A. Waibel. 2012. The KIT Lecture Corpus for Speech Translation. Abstract: Academic lectures offer valuable content, but often do not reach their full potential audience due to the language barrier. Human translations of lectures are too expensive to be widely used. Speech translation technology can be an affordable alternative in this case. State-of-the-art speech translation systems utilize statistical models that need to be trained on large amounts of in-domain data. In order to support the KIT lecture translation project in its effort to introduce speech translation technology in KIT's lecture halls, we have collected a corpus of German lectures at KIT. In this paper we describe how we recorded the lectures and how we annotated them. We further give detailed statistics on the types of lectures in the corpus and its size. We collected the corpus with the purpose in mind that it should not just be suited for training a spoken language translation system the traditional way, but should also enable us to research techniques that enable the translation system to automatically and autonomously adapt itself to the varying topics and speakers of lectures
- A. Waibel, Nguyen Bach. 2012. Dependency structures for statistical machine translation. Abstract: Dependency structures represent a sentence as a set of dependency relations. Normally the dependency structures from a tree connect all the words in a sentence. One of the most defining characters of dependency structures is the ability to bring long distance dependency between words to local dependency structures. Another the main attraction of dependency structures has been its close correspondence to meaning. This thesis focuses on integrating dependency structures into machine translation components including decoder algorithm, reordering models, confidence measure, and sentence simplification. 
First, we develop four novel cohesive soft constraints for a phrase-based decoder namely exhaustive interruption check, interruption count, exhaustive interruption count, and rich interruption constraints. To ensure the robustness and effectiveness of the proposed constraints, we conduct experiments on four different language pairs, including English-{Iraqi, Spanish} and {Arabic, Chinese}-English. The improvements are in between 0.4 and 1.8 BLEU points. These experiments also cover a wide range of training corpus sizes, ranging from 500K sentence pairs up to 10 million sentence pairs. Furthermore, to show the effectiveness of our proposed methods we apply them to systems using a 2.7 billion words 5-gram LM, different reordering models and dependency parsers. 
Second, to go beyond cohesive soft constraints, we investigate efficient algorithms for learning and decoding with source-side dependency tree reordering models. We propose a novel source-tree reordering model that exploits dependency subtree inside / outside movements and cohesive soft constraints. These movements and constraints enable us to efficiently capture the subtree-to-subtree transitions observed both in the source of word-aligned training data and in the decoding time. Representing subtree movements as features allows MERT to train the corresponding weights for these features relative to others in the model. Moreover, experimental results on English-{Iraqi, Spanish} show that we obtain improvements +0.8 BLEU and −1.4 TER on English-Spanish and +0.8 BLEU and −2.3 TER on English-Iraqi. 
Third, we develop Goodness, a novel framework to predict word and sentence level of machine translation confidence with dependency structures. The framework allows MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity. 
Finally, inspired by study in summarization we propose TriS, a novel framework to simplify source sentences before translating them. We build a statistical sentence simplification system with log-linear models. In contrast to state-of-the-art methods that drive sentence simplification process by hand-written linguistic rules, our method used a margin-based discriminative learning algorithm operates on a feature set. The feature set is defined on statistics of dependency structures as well as surface form and syntactic structures of sentences. A stack decoding algorithm is developed in order to efficiently generate and search simplification hypotheses. Experimental results show that the simplified text produced by the proposed system reduces 1.7 Flesch-Kincaid grade level when compared with the original text. We show that a comparison of a state-of-the-art rule-based system to the proposed system demonstrates an improvement of 0.2, 0.6, and 4.5 points in ROUGE-2, ROUGE-4, and AveF 10, respectively. We present subjective evaluations of the simplified translation quality for an English-German MT system.
- Mohammed Mediani, J. Niehues, A. Waibel. 2012. Parallel Phrase Scoring for Extra-large Corpora. Abstract: Parallel Phrase Scoring for Extra-large Corpora This paper presents a C++ implementation of the phrase scoring step in phrase-based systems that helps to exploit the available computing resources more efficiently and trains very large systems in reasonable time without sacrificing the system's performance in terms of Bleu score. Three parallelizing tools are made freely available. The first exploits shared memory parallelism and multiple disks for parallel IOs while the two others run in a distributed environment. We demonstrate the efficiency and consistency of our tools, in the framework of the Fr-En systems we developed for the WMT and IWSLT evaluation campaigns, in which we were able to generate the phrase table in one third up to one seventh of the time taken by Moses in the same tasks.
- Christian Saam, Christian Mohr, Kevin Kilgour, Michael Heck, Matthias Sperber, Keigo Kubo, Sebastian Stüker, S. Sakti, Graham Neubig, T. Toda, Satoshi Nakamura, A. Waibel. 2012. The 2012 KIT and KIT-NAIST English ASR systems for the IWSLT evaluation. Abstract: This paper describes our English Speech-to-Text (STT) systems for the 2012 IWSLT TED ASR track evaluation. The systems consist of 10 subsystems that are combinations of different front-ends, e.g. MVDR based and MFCC based ones, and two different phone sets. The outputs of the subsystems are combined via confusion network combination. Decoding is done in two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cMLLR. Index Terms: speech recognition, IWSLT, TED talks, evaluation system, system development
- Michael Heck, Keigo Kubo, Matthias Sperber, S. Sakti, Sebastian Stüker, Christian Saam, Kevin Kilgour, Christian Mohr, Graham Neubig, T. Toda, Satoshi Nakamura, A. Waibel. 2012. The KIT-NAIST (contrastive) English ASR system for IWSLT 2012. Abstract: This paper describes the KIT-NAIST (Contrastive) English speech recognition system for the IWSLT 2012 Evaluation Campaign. In particular, we participated in the ASR track of the IWSLT TED task. The system was developed by Karlsruhe Institute of Technology (KIT) and Nara Institute of Science and Technology (NAIST) teams in collaboration within the interACT project. We employ single system decoding with fully continuous and semi-continuous models, as well as a three-stage, multipass system combination framework built with the Janus Recognition Toolkit. On the IWSLT 2010 test set our single system introduced in this work achieves a WER of 17.6%, and our final combination achieves a WER of 14.4%.
- J. Niehues, A. Waibel. 2012. Continuous space language models using restricted Boltzmann machines. Abstract: We present a novel approach for continuous space language models in statistical machine translation by using Restricted Boltzmann Machines (RBMs). The probability of an n-gram is calculated by the free energy of the RBM instead of a feedforward neural net. Therefore, the calculation is much faster and can be integrated into the translation process instead of using the language model only in a re-ranking step. Furthermore, it is straightforward to introduce additional word factors into the language model. We observed a faster convergence in training if we include automatically generated word classes as an additional word factor. We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures. Instead of replacing the conventional n-grambased language model, we trained the RBM-based language model on the more important but smaller in-domain data and combined them in a log-linear way. With this approach we could show improvements of about half a BLEU point on the translation task.
- Ralf Huber, Florian Kraft, A. Waibel. 2012. Blind dereverberation of sinusoid signals using PLL-based combined phase and amplitude analysis. Abstract: In this paper a new `blind' single-microphone method for the dereverberation of sinusoid signals is presented. A phase-locked-loop is utilized to precisely track the amplitude, frequency and phase offset of a reverberated recording. This information can then be combined to calculate the amplitude and phase offset of single reverberated wavefronts, which allows to subtract them from the original recording. Experimental results have shown that the direct-to-reverberant ratio of recordings can be improved to an extent equal to a delay-and-sum beamformer with 5 microphones. At the end, extensions are outlined which might make the method suitable for dereverberation of real speech.
- Michael Heck, Sebastian Stüker, A. Waibel. 2012. A hybrid phonotactic language identification system with an SVM back-end for simultaneous lecture translation. Abstract: In this paper we describe our work in constructing a language identification system for use in our simultaneous lecture translation system. We first built PPR and PPRLM baseline systems that produce score-fusing language cue feature vectors for language discrimination and utilize an SVM back-end classifier for the actual language identification. On our bi-lingual lecture tasks the PPRLM system clearly outperforms the PPR system in various segment length conditions, however at the cost of slower run-time. By using lexical information in the form of keyword spotting, and additional language models we show ways to improve the performance of both baseline systems. In order to combine the faster run-time of the PPR system with the better performance of the PPRLM system we finally built a hybrid of both approaches that clearly outperforms the PPR system while not adding any additional computing time. This hybrid system is therefore our choice for the use in the lecture translation system due to its faster run-time and good performance.
- J. Niehues, A. Waibel. 2012. Detailed Analysis of Different Strategies for Phrase Table Adaptation in SMT. Abstract: This paper gives a detailed analysis of different approaches to adapt a statistical machine translation system towards a target domain using small amounts of parallel in-domain data. Therefore, we investigate the differences between the approaches addressing adaptation on the two main steps of building a translation model: The candidate selection and the phrase scoring. For the latter step we characterized the differences by four key aspects. We performed experiments on two different tasks of speech translation and analyzed the influence of the different aspects on the overall translation quality. On both tasks we could show significant improvements by using the presented adaptation techniques.
- Eunah Cho, J. Niehues, A. Waibel. 2012. Segmentation and punctuation prediction in speech language translation using a monolingual translation system. Abstract: In spoken language translation (SLT), finding proper segmentation and reconstructing punctuation marks are not only significant but also challenging tasks. In this paper we present our recent work on speech translation quality analysis for German-English by improving sentence segmentation and punctuation. From oracle experiments, we show an upper bound of translation quality if we had human-generated segmentation and punctuation on the output stream of speech recognition systems. In our oracle experiments we gain 1.78 BLEU points of improvements on the lecture test set. We build a monolingual translation system from German to German implementing segmentation and punctuation prediction as a machine translation task. Using the monolingual translation system we get an improvement of 1.53 BLEU points on the lecture test set, which is a comparable performance against the upper bound drawn by the oracle experiments.
- Henrich Kolkhorst, Kevin Kilgour, Sebastian Stüker, A. Waibel. 2012. Evaluation of interactive user corrections for lecture transcription. Abstract: In this work, we present and evaluate the usage of an interactive web interface for browsing and correcting lecture transcripts. An experiment performed with potential users without transcription experience provides us with a set of example corrections. On German lecture data, user corrections greatly improve the comprehensibility of the transcripts, yet only reduce the WER to 22%. The precision of user edits is relatively low at 77% and errors in inflection, case and compounds were rarely corrected. Nevertheless, characteristic lecture data errors, such as highly specific terms, were typically corrected, providing valuable additional information.
- Mohammed Mediani, Eunah Cho, J. Niehues, T. Herrmann, A. Waibel. 2011. The KIT English-French translation systems for IWSLT 2011. Abstract: This paper presents the KIT system participating in the English!French TALK Translation tasks in the framework of the IWSLT 2011 machine translation evaluation. Our system is a phrase-based translation system using POS-based reordering extended with many additional features. First of all, a special preprocessing is devoted to the Giga corpus in order to minimize the effect of the great amount of noise it contains. In addition, the system gives more importance to the in-domain data by adapting the translation and the language models as well as by using a wordcluster language model. Furthermore, the system is extended by a bilingual language model and a discriminative word lexicon. The automatic speech transcription input usually has no or wrong punctuation marks, therefore these marks were especially removed from the source training data for the SLT system training.
- L. Lamel, Sandrine Courcinous, Julien Despres, J. Gauvain, Yvan Josse, Kevin Kilgour, Florian Kraft, V. Le, H. Ney, M. Nußbaum-Thom, I. Oparin, Tim Schlippe, R. Schlüter, Tanja Schultz, Thiago Fraga-Silva, Sebastian Stüker, M. Sundermeyer, Bianca Vieru-Dimulescu, Ngoc Thang Vu, A. Waibel, Cécile Woehrling. 2011. Speech recognition for machine translation in Quaero. Abstract: This paper describes the speech-to-text systems used to provide automatic transcriptions used in the Quaero 2010 evaluation of Machine Translation from speech. Quaero (www.quaero.org) is a large research and industrial innovation program focusing on technologies for automatic analysis and classification of multimedia and multilingual documents. The ASR transcript is the result of a Rover combination of systems from three teams ( KIT, RWTH, LIMSI+VR) for the French and German languages. The casesensitive word error rates (WER) of the combined systems were respectively 20.8% and 18.1% on the 2010 evaluation data, relative WER reductions of 14.6% and 17.4% respectively over the best component system.
- J. Niehues, T. Herrmann, S. Vogel, A. Waibel. 2011. Wider Context by Using Bilingual Language Models in Machine Translation. Abstract: In past Evaluations for Machine Translation of European Languages, it could be shown that the translation performance of SMT systems can be increased by integrating a bilingual language model into a phrase-based SMT system. In the bilingual language model, target words with their aligned source words build the tokens of an n-gram based language model. We analyzed the effect of bilingual language models and show where they could help to better model the translation process. We could show improvements of translation quality on German-to-English and Arabic-to-English. In addition, for the Arabic-to-English task, training an extra bilingual language model on the POS tags instead of the surface word forms led to further improvements.
- Karim Boudahmane, Bianka Buschbeck-Wolf, Eunah Cho, J. Crego, Markus Freitag, T. Lavergne, H. Ney, J. Niehues, Stephan Peitz, Jean Senellart, Artem Sokolov, A. Waibel, Tonio Wandmacher, Joern Wuebker, François Yvon. 2011. Advances on spoken language translation in the Quaero program. Abstract: The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality.
- Sebastian Stüker, Kevin Kilgour, Christian Saam, A. Waibel. 2011. The 2011 KIT English ASR system for the IWSLT evaluation. Abstract: This paper describes our English Speech-to-Text (STT) system for the 2011 IWSLT ASR track. The system consists of 2 subsystems with different front-ends—one MVDR based, one MFCC based—which are combined using confusion network combination to provide a base for a second pass speaker adapted MVDR system. We demonstrate that this set-up produces competitive results on the IWSLT 2010 dev and test sets.
- Kevin Kilgour, Christian Saam, Christian Mohr, Sebastian Stüker, A. Waibel. 2011. The 2011 KIT QUAERO speech-to-text system for Spanish. Abstract: This paper describes our current Spanish speech-to-text (STT) system with which we participated in the 2011 Quaero STT evaluation that is being developed within the Quaero program. The system consists of 4 separate subsystems, as well as the standard MFCC and MVDR phoneme based subsystems we included a both a phoneme and grapheme based bottleneck subsystem. We carefully evaluate the performance of each subsystem. After including several new techniques we were able to reduce the WER by over 30% from 20.79% to 14.53%.
- J. Niehues, A. Waibel. 2011. Using Wikipedia to translate domain-specific terms in SMT. Abstract: When building a university lecture translation system, one important step is to adapt it to the target domain. One problem in this adaptation task is to acquire translations for domain specific terms. In this approach we tried to get these translations from Wikipedia, which provides articles on very specific topics in many different languages. To extract translations for the domain specific terms, we used the interlanguage links of Wikipedia . We analyzed different methods to integrate this corpus into our system and explored methods to disambiguate between different translations by using the text of the articles. In addition, we developed methods to handle different morphological forms of the specific terms in morphologically rich input languages like German. The results show that the number of out-of-vocabulary (OOV) words could be reduced by 50% on computer science lectures and the translation quality could be improved by more than 1 BLEU point.
- Nguyen Bach, Qin Gao, S. Vogel, A. Waibel. 2011. TriS: A Statistical Sentence Simplifier with Log-linear Models and Margin-based Discriminative Training. Abstract: We propose a statistical sentence simplification system with log-linear models. In contrast to state-of-the-art methods that drive sentence simplification process by hand-written linguistic rules, our method used a margin-based discriminative learning algorithm operates on a feature set. The feature set is defined on statistics of surface form as well as syntactic and dependency structures of the sentences. A stack decoding algorithm is used which allows us to efficiently generate and search simplification hypotheses. Experimental results show that the simplified text produced by the proposed system reduces 1.7 Flesch-Kincaid grade level when compared with the original text. We will show that a comparison of a state-ofthe-art rule-based system (Heilman and Smith, 2010) to the proposed system demonstrates an improvement of 0.2, 0.6, and 4.5 points in ROUGE-2, ROUGE-4, andAveF 10 , respectively.
- Paul Maergner, Kevin Kilgour, Ian Lane, A. Waibel. 2011. Unsupervised vocabulary selection for simultaneous lecture translation. Abstract: In this work, we propose a novel method for vocabulary selection which enables simultaneous lecture translation systems to automatically adapt to the diverse topics that occur in educational and scientific lectures. Utilizing materials that are available before the lecture begins, such as lecture slides, our proposed framework iteratively searches for related documents on the World Wide Web and generates a lecturespecific vocabulary and language model based on the resulting documents. In this paper, we introduce a novel method for vocabulary selection where we rank vocabulary that occurs in the collected documents based on a relevance score which is calculated using a combination of word features. Vocabulary selection is a critical component for topic adaptation that has typically been overlooked in prior works. On the interACT German-English simultaneous lecture translation system our proposed approach significantly improved vocabulary coverage, reducing the out-of-vocabulary rate on average by 60% and up to 84%, compared to a lectureindependent baseline. Furthermore, our approach reduced the word error rate by up to 25.3% (on average 13.2% across all lectures), compared to a lecture-independent baseline.
- Markus Freitag, Gregor Leusch, Joern Wuebker, Stephan Peitz, H. Ney, T. Herrmann, J. Niehues, A. Waibel, A. Allauzen, G. Adda, J. Crego, Bianka Buschbeck-Wolf, Tonio Wandmacher, Jean Senellart. 2011. Joint WMT Submission of the QUAERO Project. Abstract: This paper describes the joint QUAERO submission to the WMT 2011 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT German→English task. Each group translated the data sets with their own systems. Then RWTH system combination combines these translations to a better one. In this paper, we describe the single systems of each group. Before we present the results of the system combination, we give a short description of the RWTH Aachen system combination approach.
- J. Niehues, Mohammed Mediani, T. Herrmann, Michael Heck, Christian Herff, A. Waibel. 2010. The KIT translation system for IWSLT 2010. Abstract: This paper presents the KIT systems participating in the French to English BTEC and in the English to French TALK Translation tasks in the framework of the IWSLT 2010 machine translation evaluation. Starting with a state-of-the art phrase-based translation system we tested different modifications and extensions to improve the translation quality of the system. First, we improved the word reordering by learning POSbased reordering rules from an automatically word-aligned parallel corpus. Furthermore, different experiments to adapt the machine translation system towards the target domain were carried out. In addition, for the BTEC task we tried to avoid data-sparseness problems by using word stems instead of the full word forms.
- Ian Lane, Matthias Eck, Kay Rottmann, A. Waibel. 2010. Tools for Collecting Speech Corpora via Mechanical-Turk. Abstract: To rapidly port speech applications to new languages one of the most difficult tasks is the initial collection of sufficient speech corpora. State-of-the-art automatic speech recognition systems are typical trained on hundreds of hours of speech data. While pre-existing corpora do exist for major languages, a sufficient amount of quality speech data is not available for most world languages. While previous works have focused on the collection of translations and the transcription of audio via Mechanical-Turk mechanisms, in this paper we introduce two tools which enable the collection of speech data remotely. We then compare the quality of audio collected from paid part-time staff and unsupervised volunteers, and determine that basic user training is critical to obtain usable data.
- Narine Kokhlikyan, A. Waibel. 2010. Structure-dependent Summarization of Spoken Lectures. Abstract: Why summarize lectures? Improvement of summarization quality Missed the lecture ? I can summarize all you need to know, in 5 minutes • Retrieval of required information becomes a challenging task • A large number of recorded lectures Detecting hierarchical structure What makes speech summarization difficult?-" Welcome to today's lecture. "-" My name is Alex Waibel. "-" today we want to get busy with the second lecture of machine learning …. ….-" we want to get familiar with one of the most popular approaches , the so-called back-propagation algorithm. " ….-" Today we learned about neuronal networks, back-propagation algo… "-" we come to the end today. "-" I hope you will have a beautiful day. "
- D. Lim, Ian Lane, A. Waibel. 2010. Real-time spoken language identification and recognition for speech-to-speech translation. Abstract: For spoken language systems to effectively operate across multiple languages it is critical to rapidly apply the correct language-specific speech recognition models. Prior approaches consist of either, first identifying the language being spoken and selecting the appropriate languagespecific speech recognition engine; or alternatively, performing speech recognition in parallel and selecting the language and recognition hypothesis with maximum likelihood. Both these approaches, however, introduce a significant delay before back-end natural language processing can proceed. In this work, we propose a novel method for joint language identification and speech recognition that can operate in near real-time. The proposed approach compares partial hypotheses generated on-the-fly during decoding and generates a classification decision soon after the first full hypothesis has been generated. When applied within our English-Iraqi speech-to-speech translation system the proposed approach correctly identified the input language with 99.6% accuracy while introducing minimal delay to the end-to-end system.
- Sebastian Stüker, Michael Heck, Katja Renner, A. Waibel. 2010. Spoken news queries over the world wide web. Abstract: In this paper we present our work in expanding the View4You system developed at the Interactive Systems Laboratories (ISL). The View4You system allows the user the retrieval of automatically found news clips from recorded German broadcast news by natural spoken queries. While modular in design, so far, the architecture has required the components to at least run in a common file space. By utilizing Flash technology we turned this single machine setup into a distributed set-up that gives us access to our news database over the World Wide Web. The client side of our architecture only requires a web browser with Flash extension in order to record and send the speech of the queries to the servers and in order to display the retrieved news clips. Our future work will focus on turning the monolingual German system into a multilingual system that provides cross-lingual access and retrieval in multiple languages.
- Florian Kraft, Kevin Kilgour, Rainer Saam, Sebastian Stüker, Matthias Wölfel, T. Asfour, A. Waibel. 2010. Towards social integration of humanoid robots by conversational concept learning. Abstract: Several real world applications of humanoids in general will require continuous service over a long time period. A humanoid robot operating in different environments over a long period of time means that A) there will be a lot of variation in the speech it has to ground semantically and B) it has to know when a conversation is of interest in order to respond.
- Matthias Eck, Ian Lane, Y. Zhang, A. Waibel. 2010. Jibbigo: Speech-to-speech translation on mobile devices. Abstract: Jibbigo is a speech-to-speech translation application for iPhone, iPod touch, and iPad devices. Jibbigo allows the user to simply speak a sentence, and it speaks the sentence aloud in the other language, much like a personal human interpreter would. The speech-to-speech translation is bi-directional for a two way dialog between participants.
- M. Paulik, A. Waibel. 2010. Rapid development of speech translation using consecutive interpretation. Abstract: The development of a speech translation (ST) system is costly, largely because it is expensive to collect parallel data. A new language pair is typically only considered in the aftermath of an international crisis that incurs a major need of crosslingual communication. Urgency justifies the deployment of interpreters while data is being collected. In recent work, we have shown that audio recordings of interpreter-mediated communication can present a low-cost data resource for the rapid development of automatic text and speech translation. However, our previous experiments remain limited to English/Spanish simultaneous interpretation. In this work, we examine our approaches for exploiting interpretation audio as translation model training data in the context of English/Pashto consecutive interpretation. We show that our previously made findings remain valid, despite the more complex language pair and the additional challenges introduced by the strong resource-limitations of Pashto.
- M. Paulik, A. Waibel. 2010. Spoken language translation from parallel speech audio: Simultaneous interpretation as SLT training data. Abstract: In recent work, we proposed an alternative to parallel text as translation model (TM) training data: audio recordings of parallel speech (pSp), as it occurs in any communication scenario where interpreters are involved. Although interpretation compares poorly to translation, we reported surprisingly strong translation results for systems based on pSp trained TMs. This work extends the use of pSp as a data source for unsupervised training of all major models involved in statistical spoken language translation. We consider the scenario of speech translation between a resource rich and a resource-deficient language. Our seed models are based on 10h of transcribed audio and parallel text comprised of 100k translated words. With the help of 92h of untranscribed pSp audio, and by taking advantage of the redundancy inherent to pSp (the same information is given twice, in two languages), we report significant improvements for the resource-deficient acoustic, language and translation models.
- J. Niehues, A. Waibel. 2010. Domain Adaptation in Statistical Machine Translation using Factored Translation Models. Abstract: In recent years the performance of SMT increased in domains with enough training data. But under real-world conditions, it is often not possible to collect enough parallel data. We propose an approach to adapt an SMT system using small amounts of parallel in-domain data by introducing the corpus identifier (corpus id) as an additional target factor. Then we added features to model the generation of the tags and features to judge a sequence of tags. Using this approach we could improve the translation performance in two domains by up to 1 BLEU point when translating from German to English.
- J. Niehues, T. Herrmann, Mohammed Mediani, A. Waibel. 2010. The Karlsruhe Institute for Technology Translation System for the ACL-WMT 2010. Abstract: This paper describes our phrase-based Statistical Machine Translation (SMT) system for the WMT10 Translation Task. We submitted translations for the German to English and English to German translation tasks. Compared to state-of-the-art phrase-based systems we preformed additional preprocessing and used a discriminative word alignment approach. The word reordering was modeled using POS information and we extended the translation model with additional features.
- Ian Lane, A. Waibel. 2010. Named-entity projection and data-driven morphological decomposition for field maintainable speech-to-speech translation systems. Abstract: In this paper, we investigate methods to improve the handling of named-entities in speech-to-speech translation systems, specifically focusing on techniques applicable to under-resourced, morphologically complex languages. First, we introduce a method to efficiently bootstrap a named-entity recognizer for a new language by projecting tags from a well resourced language across a bilingual corpus; and second, we propose a novel approach to automatically induce decomposition rules for morphologically complex languages. In our English-Iraqi speech-to-speech translation system combining these two approaches significantly improved speech recognition and translation performance on military dialogs focused on the collection of information in the field.
- Chiori Hori, B. Zhao, S. Vogel, A. Waibel, H. Kashioka, Satoshi Nakamura. 2009. Consolidation-Based Speech Translation and Evaluation Approach. Abstract: The performance of speech translation systems combining automatic speech recognition (ASR) and machine translation (MT) systems is degraded by redundant and irrelevant information caused by speaker disfluency and recognition errors. This paper proposes a new approach to translating speech recognition results through speech consolidation, which removes ASR errors and disfluencies and extracts meaningful phrases. A consolidation approach is spun off from speech summarization by word extraction from ASR 1-best. We extended the consolidation approach for confusion network (CN) and tested the performance using TED speech and confirmed the consolidation results preserved more meaningful phrases in comparison with the original ASR results. We applied the consolidation technique to speech translation. To test the performance of consolidation-based speech translation, Chinese broadcast news (BN) speech in RT04 were recognized, consolidated and then translated. The speech translation results via consolidation cannot be directly compared with gold standards in which all words in speech are translated because consolidation-based translations are partial translations. We would like to propose a new evaluation framework for partial translation by comparing them with the most similar set of words extracted from a word network created by merging gradual summarizations of the gold standard translation. The performance of consolidation-based MT results was evaluated using BLEU. We also propose Information Preservation Accuracy (IPAccy) and Meaning Preservation Accuracy (MPAccy) to evaluate consolidation and consolidation-based MT. We confirmed that consolidation contributed to the performance of speech translation.
- M. Paulik, A. Waibel. 2009. Automatic translation from parallel speech: Simultaneous interpretation as MT training data. Abstract: State-of-the art statistical machine translation depends heavily on the availability of domain-specific bilingual parallel text. However, acquiring large amounts of bilingual parallel text is costly and, depending on the language pair, sometimes impossible. We propose an alternative to parallel text as machine translation (MT) training data; audio recordings of parallel speech (pSp) as it occurs in any scenario where interpreters are involved. Although interpretation (pSp) differs significantly from translation (parallel text), we achieve surprisingly strong translation results with our pSp-trained MT and speech translation systems.We argue that the presented approach is of special interest for developing speech translation in the context of resource-deficient languages where even monolingual resources are scarce.
- Sebastian Stüker, A. Waibel. 2009. Porting Speech Recognition Systems to New Languages Supported by Articulatory Feature Models. Abstract: Linguists estimate the number of currently existing languages to be between 5,000 and 7,000. In order to be able to cover as many languages as possible, techniques have to be developed in order to rapidly port speech recognition systems to new languages in a cost efficient way. In the past, phoneme based, language independent acoustic models have been studied for bootstrapping an acoustic model in a new language. These language independent models usually have seen multiple languages during training, and work under the assumption that phonemes are pronounced the same across languages. Similarly, models for acoustic features, describing the articulator targets of the different phonemes, can also be accurately recognized across languages and can be trained to become language independent in the same way as phonemes can. In the past we combined them with phoneme based models and their behavior on the training languages of the multilingual models was examined. In this paper we present experiments examining the suitability of monolingual and multilingual acoustic features for porting speech recognition systems to new languages. We combined them with monolingual and multilingual, phoneme based models in a stream based frame work in order to bootstrap a model in a new language. The results show that the incorporation of models for articulatory features into the porting framework significantly improves the performance when porting ASR systems to new languages, reducing the word error rate by up to 4.5% relative.
- Hassan Al-Haj, Roger Hsiao, Ian Lane, A. Black, A. Waibel. 2009. Pronunciation modeling for dialectal arabic speech recognition. Abstract: Short vowels in Arabic are normally omitted in written text which leads to ambiguity in the pronunciation. This is even more pronounced for dialectal Arabic where a single word can be pronounced quite differently based on the speaker's nationality, level of education, social class and religion. In this paper we focus on pronunciation modeling for Iraqi-Arabic speech. We introduce multiple pronunciations into the Iraqi speech recognition lexicon, and compare the performance, when weights computed via forced alignment are assigned to the different pronunciations of a word. Incorporating multiple pronunciations improved recognition accuracy compared to a single pronunciation baseline and introducing pronunciation weights further improved performance. Using these techniques an absolute reduction in word-error-rate of 2.4% was obtained compared to the baseline system.
- O. Hamon, C. Fügen, D. Mostefa, V. Arranz, M. Kolss, A. Waibel, K. Choukri. 2009. End-to-End Evaluation in Simultaneous Translation. Abstract: This paper presents the end-to-end evaluation of an automatic simultaneous translation system, built with state-of-the-art components. It shows whether, and for which situations, such a system might be advantageous when compared to a human interpreter. Using speeches in English translated into Spanish, we present the evaluation procedure and we discuss the results both for the recognition and translation components as well as for the overall system. Even if the translation process remains the Achilles' heel of the system, the results show that the system can keep at least half of the information, becoming potentially useful for final users.
- Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, S. Vogel, Tanja Schultz, Ian Lane, A. Waibel, A. Black. 2009. Incremental Adaptation of Speech-to-Speech Translation. Abstract: In building practical two-way speech-to-speech translation systems the end user will always wish to use the system in an environment different from the original training data. As with all speech systems, it is important to allow the system to adapt to the actual usage situations. This paper investigates how a speech-to-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two. The platform is the CMU Iraqi-English portable two-way speech-to-speech system as developed under the DARPA TransTac program. We show how machine translation, speech recognition and overall system performance can be improved on day 2 after adapting from day 1 in both a supervised and unsupervised way.
- J. Niehues, T. Herrmann, M. Kolss, A. Waibel. 2009. The Universität Karlsruhe Translation System for the EACL-WMT 2009. Abstract: In this paper we describe the statistical machine translation system of the Universitat Karlsruhe developed for the translation task of the Fourth Workshop on Statistical Machine Translation. The state-of-the-art phrase-based SMT system is augmented with alternative word reordering and alignment mechanisms as well as optional phrase table modifications. We participate in the constrained condition of German-English and English-German as well as in the constrained condition of French-English and English-French.
- Kevin Kilgour, Florian Kraft, Sebastian Stüker, A. Waibel. 2009. Multi Domain Language Model Adaptation using Explicit Semantic Analysis. Abstract: This paper presents an adaptive multi domain language model built from large sources of pre existing human created structured data. The sources’ structure is exploited to create a large array of ngram language models which are dynamically interpolated at decoding time to produce a context dependent language model that continuously adapts itself to the current domain. Because the use of human annotators is expensive and impractical we explore existing sources of human created structured data and how to extract our desired data from them. The language model is evaluated on its performance with a speech recognition system used to decode the Quaero 2009 evaluation data set. Compared to the baseline language model of our Quaero 2009 evaluation system our proposed adaptive language model reduces the WER of the speech recognition system by 0.5% absolute with some shows showing reductions of up to 14.4%. Index Terms: Speech Recognition, Language Model Adaptation, Explicit Semantic Analysis
- M. Paulik, A. Waibel. 2008. Extracting clues from human interpreter speech for spoken language translation. Abstract: In previous work, we reported dramatic improvements in automatic speech recognition (ASR) and spoken language translation (SLT) gained by applying information extracted from spoken human interpretations. These interpretations were artificially created by collecting read sentences from a clean parallel text corpus. Real human interpretations are significantly different. They suffer from frequent synopses, omissions and self-corrections. Expressing these differences in BLEU score by evaluating human interpretations with carefully created human translations, we found that human interpretations perform two to three times worse than state-of-the art SLT. Facing these stark differences, we address the question if and how ASR and SLT can profit from human interpretations. In the following we describe initial experiments that apply knowledge derived from real human interpretations for improving English and Spanish ASR and SLT. Our experiments are conducted on a small European Parliamentary Plenary Sessions development set.
- A. Waibel, Hua Gao. 2008. Face Registration with Active Appearance Models for Local Appearance-based Face Recognition. Abstract: Face recognition has received increasing attention from diverse research communities and the market over the past years. Various techniques have been intensively investigated aiming at high recognition accuracy and robustness against numerous facial appearance variations. Application areas of face recognition have also been expanded and more robust systems are required as the application scenarios become more unconstrained. In this work, variation in facial appearance caused by 3D head pose was considered. The problem is also known as the face registration problem, which is an important factor for face recognition as demonstrated in many previous studies. The registration approach studied in this thesis is able to normalize the head pose in some degree of rotation in depth and align the face into a common coordinate framework. Moreover, the quality of face registration is assessed so that only successfully registered face images are used for recognition. The developed face registration approach is based on active appearance model (AAM) fitting. A generic model was built in which both shape and appearance variations were modeled. After fitting the model on an input image, the pose of the input face was normalized and a frontal view of the input face was synthesized. To mitigate the influence of poor illumination, a modified histogram fitting approach was employed. Progressive model fitting was also investigated for a more robust estimate of model initialization. Face recognition was based on the fitted and pose normalized face images using our local appearance-based approach. Three experiments were conducted to evaluate the AAM-based face registration approach. The first experiment was designed to evaluate the pose correction based on AAM fitting in still images. The results showed a significant improvement in face recognition performance compared to the previous affinebased registration approach, which again demonstrated the importance of pose correction for face recognition. We also compared our local appearance-based face recognition approach with two well known holistic approaches. The local appearance-based approach significantly outperformed the holistic approaches and it was more robust against the error introduced by AAM fitting and face synthesis. The second experiment evaluated the eye localization with AAM fitting. Face tracking with AAM fitting was also evaluated on a video database for open set face recognition. A modified distance from feature space metric was employed to assess the quality of fitting on a single frame. Open set face recognition was performed on the successfully registered frames. The experimental results showed that both pose correction and registration quality assessment improved the recognition performance.
- M. Kolss, Matthias Wölfel, Florian Kraft, J. Niehues, M. Paulik, A. Waibel. 2008. Simultaneous German-English lecture translation.. Abstract: In an increasingly globalized world, situations in which people of different native tongues have to communicate with each other become more and more frequent. In many such situations, human interpreters are prohibitively expensive or simply not available. Automatic spoken language translation (SLT), as a cost-effective solution to this dilemma, has received increased attention in recent years. For a broad number of applications, including live SLT of lectures and oral presentations, these automatic systems should ideally operate in real time and with low latency. Large and highly specialized vocabularies as well as strong variations in speaking style ‐ ranging from read speech to free presentations suffering from spontaneous events ‐ make simultaneous SLT of lectures a challenging task. This paper presents our progress in building a simultaneous German-English lecture translation system. We emphasize some of the challenges which are particular to this language pair and propose solutions to tackle some of the problems encountered.
- A. Waibel. 2008. Speech Processing in Support of Human-Human Communication (Invited Paper). Abstract: Summary form only given. Computers have become an essential part of modern life, providing services in a multiplicity of ways. Access to these services, however, comes at a price: human attention is bound and directed toward a technical artifact in a human machine interaction setting at the expense of time and attention for other humans. This paper explores a new class of computer services that support human-human interaction and communication implicitly and transparently. Computers in the human interaction loop (CHIL), require consideration of all communication modalities, multimodal integration and more robust performance. We review the technologies and several CHIL services providing human-human support. Among them, we specifically highlight advanced computer services for cross-lingual communication.
- Philipp Große, H. Holzapfel, A. Waibel. 2008. Confidence based multimodal fusion for person identification. Abstract: Person identification is of great interest for various kinds of applications and interactive systems. In our system we use face recognition and voice recognition from data recorded in an interactive dialogue system. In such a system, sequential images and sequential utterances can be used to improve recognition accuracy over single hypotheses. The presented approach uses confidence-based fusion for sequence hypotheses, for multimodal fusion, and to provide a reliability measure of the classification quality that can be used to decide when to trust and when to ignore classification results.
- Keni Bernardin, R. Stiefelhagen, A. Waibel. 2008. Probabilistic integration of sparse audio-visual cues for identity tracking. Abstract: In the context of smart environments, the ability to track and identify persons is a key factor, determining the scope and flexibility of analytical components or intelligent services that can be provided. While some amount of work has been done concerning the camera-based tracking of multiple users in a variety of scenarios, technologies for acoustic and visual identification, such as face or voice ID, are unfortunately still subjected to severe limitations when distantly placed sensors have to be used. Because of this, reliable cues for identification can be hard to obtain without user cooperation, especially when multiple users are involved.
 In this paper, we present a novel technique for the tracking and identification of multiple persons in a smart environment using distantly placed audio-visual sensors. The technique builds on the opportunistic integration of tracking as well as face and voice identification cues, gained from several cameras and microphones, whenever these cues can be captured with a sufficient degree of confidence. A probabilistic model is used to keep track of identified persons and update the belief in their identities whenever new observations can be made. The technique has been systematically evaluated on the CLEAR Interactive Seminar database, a large audio-visual corpus of realistic meeting scenarios captured in a variety of smart rooms.
- H. Holzapfel, A. Waibel. 2008. Modelling multimodal user ID in dialogue. Abstract: This paper presents an approach to model user ID in dialogue. A belief network is used to integrate ID classifiers, such as face ID and voice ID, and person related information, such as the first name and last name of a person from speech recognition or spelling. Different network structures are analyzed and compared with each other and are compared with a rule-based user model. The approach is evaluated on dialogue data collected in a person identification scenario, which includes both, identification of known persons and interactive learning of names and ID of unknown persons.
- Sebastian Stüker, A. Waibel. 2008. Towards human translations guided language discovery for ASR systems. Abstract: ABSTRACTNatural language processing systems, e.g for AutomaticSpeech Recognition (ASR) or Machine Translation (MT),havebeenstudiedonlyforafractionoftheapprox.7000lan-guagesthatexistintoday’sworld,themajorityofwhichhaveonlycomparativelyfewspeakersandfewresources.Thetra-ditionalapproachofcollectingandannotatingthenecessarytrainingdataisduetoeconomicconstraintsnotfeasibleformostofthem. AtthesametimeitisofvitalinteresttohaveNLPsystemsaddresspracticallyalllanguagesintheworld.New,efﬁcientwaysofgatheringtheneededtrainingmaterialhavetobefound.InthispaperweproposeanewtechniqueofcollectingsuchdatabyexploitingtheknowledgegainedfromHumansimultaneoustranslationsthathappenfrequentlyintherealworld. Toshowthefeasibilityofourapproachwepresentﬁrstexperimentstowardsconstructingapronuncia-tiondictionaryfromthedatagained.Index Terms — Automatic Speech Recognition, Lan-guage Discovery, Machine Translation, Under-ResourcedLanguages1. INTRODUCTION1.1. The Traditional Way to Acquire Training DataTraining large vocabulary continuous speech recognition(LVCSR) systems requires a number resources in the tar-getedlanguage. Fortrainingtheacousticmodelofarecog-nitionsystemlargeamountsoftranscribedaudiorecordingsofspeechareneeded.Thetrainingofthelanguagemodelre-quireslargeamountsofwrittentextinthetargetedlanguage.Whenusingphonemebasedacousticmodels,apronunciationdictionaryisneededthatmapsthewrittenrepresentationofawordtothesequenceofitsphonemeswhenbeingspoken.Approximately7,000languagesexisttoday,thecurrenteditionofEthnologue[1]lists7,299.Sofar,automaticspeechrecognition (ASR) systems and machine translation (MT)
- A. Waibel, C. Fügen. 2008. Spoken language translation. Abstract: In this article we have reviewed state-of-the-art speech translation systems. We have discussed issues of performance as well as deployment, and we reviewed the history and technical underpinnings of this growing and challenging research area. The field provides a plethora of fascinating research challenges for scientists as well as opportunities for true impact in the society of tomorrow.
- Ian Lane, A. Waibel. 2008. Class-based statistical machine translation for field maintainable speech-to-speech translation. Abstract: Current speech-to-speech translation systems lack any mechanism to handle out-of-vocabulary words that did not appear in the training data. To improve the usability of these systems we have developed a field maintainable speech-to-speech translation framework that enables users to add new words to the system while it is being used in the field. To realize such a framework, a novel class-based statistical machine translation framework is proposed, that applies class-based translation models and class n-gram language models during translation. To obtain consistent labelling of the parallel training corpora, on which these models are trained, we introduce a bilingual tagger that jointly labels both sides of the parallel corpora. On a Japanese-English evaluation system, the proposed framework significantly improved translation quality, obtaining a relative improvement in BLEU-score of 15% for both translation directions.
- H. Holzapfel, A. Waibel. 2008. Learning and Verification of Names with Multimodal User ID in Dialog. Abstract: Acquiring new knowledge is a key functionality for humanoid robots. By envisioning a robot that can provide personalized services the system needs to detect, recognize and memorize information about specific persons. Recent work al- ready shows promising results in the area of speech recognition, voice identification and face identification that enable a system to reliably detect and recognize persons, as well as approaches to interactively learn to know new persons in dialog acquiring their names and ID information. One problem in this area is verification, namely to detect which person is known versus which person is unknown; a second problem is the learning phase, namely to learn the name of a person and store it in a database with associated face and voice classifier information. This paper presents work to interactively acquire ID information, combining both of the above problems into one learning dialog. In dialog we combine multimodal input including spoken name recognition, name pronunciation (phoneme recognition), name spelling (grapheme representation), face identification and voice identification and seek to build dialogs optimized to verify or learn a person's name and ID. For designing and training of optimized dialogs we use a reinforcement learning approach and propose a mul- timodal simulation modeling the user's actions and multimodal ID recognition components including stochastic error models. I. INTRODUCTION In this paper we present work on learning names and person ID information in a multimodal dialog system for a humanoid robot. One part of the dialogs that can be con- ducted with the robot are dialogs to identify and especially to learn to know new persons. We have conducted experiments with a receptionist scenario, where one task of the robot receptionist was to identify the visiting person or learn the name of the person if unknown. In the following we present efforts on especially this task namely isolated identification dialogs within the receptionist scenario. These dialogs fulfill two purposes: In case the person is known, confirm the name of the person. In case the person is unknown, classify the person as unknown and conduct a learning dialog to obtain the person's name. The presented experiments make use of standard per- ceptual components available on a humanoid robot. These components are visual perception with a stereo camera and acoustic perception with distant and close-talk microphones. Visual perception provides face detection and identification. Acoustic perception provides voice identification and speech recognition including name recognition, spelling and pho- netic understanding. These components provide recognition hypotheses which are interpreted by the dialog manager. The challenge of this task is to define a dialog strategy, including when to confirm ID information, when to ask for name pronunciation or spelling. With the goal of optimizing dialogs regarding success, length, and subjective measures, we have implemented a reinforcement learning approach which combines both verification and learning into one dia- log integrating the multiple input modalities presented above. For achieving this goal, we implemented a first rule based dialog strategy, and later a reinforcement learning strategy, which was trained in a multimodal user simulation. In the following we present the setup for multimodal integration in dialog, definition of the handcrafted strategy and learning of dialog strategies in the multimodal user simulation. Both dialog strategies are evaluated within the simulation and are compared against each other. First results from a real user experiment are reported.
- Matthias Eck, S. Vogel, A. Waibel. 2008. Communicating Unknown Words in Machine Translation. Abstract: A new approach to handle unknown words in machine translation is presented. The basic idea is to find definitions for the unknown words on the source language side and translate those definitions instead. Only monolingual resources are required, which generally offer a broader coverage than bilingual resources and are available for a large number of languages. In order to use this in a machine translation system definitions are extracted automatically from online dictionaries and encyclopedias. The translated definition is then inserted and clearly marked in the original hypothesis. This is shown to lead to significant improvements in (subjective) translation quality.
- Matthias Wölfel, M. Kolss, Florian Kraft, J. Niehues, M. Paulik, A. Waibel. 2008. Simultaneous machine translation of german lectures into english: Investigating research challenges for the future. Abstract: An increasingly globalized world fosters the exchange of students, researchers or employees. As a result, situations in which people of different native tongues are listening to the same lecture become more and more frequent. In many such situations, human interpreters are prohibitively expensive or simply not available. For this reason, and because first prototypes have already demonstrated the feasibility of such systems, automatic translation of lectures receives increasing attention. A large vocabulary and strong variations in speaking style make lecture translation a challenging, however not hopeless, task. The scope of this paper is to investigate a variety of challenges and to highlight possible solutions in building a system for simultaneous translation of lectures from German to English. While some of the investigated challenges are more general, e.g. environment robustness, other challenges are more specific for this particular task, e.g. pronunciation of foreign words or sentence segmentation. We also report our progress in building an end-to-end system and analyze its performance in terms of objective and subjective measures.
- M. Paulik, A. Waibel. 2008. Lightly supervised acoustic model training on EPPS recordings. Abstract: Debates in the European Parliament are simultaneously translated into the official languages of the Union. These interpretations are broadcast live via satellite on separate audio channels. After several months, the parliamentary proceedings are published as final text editions (FTE). FTEs are formatted for an easy readability and can differ significantly from the original speeches and the live broadcast interpretations. We examine the impact on German word error rate (WER) when introducing supervision based on German FTEs and supervision based on German automatic translations extracted from the English and Spanish audio. We show that FTE based supervision and additional interpretation based supervision provide significant reductions in WER. We successfully apply FTE supervised acoustic model (AM) training using 143h of recordings. Combining the new AM with the mentioned supervision techniques, we achieve a significant WER reduction of 13.3% relative.
- M. Kolss, S. Vogel, A. Waibel. 2008. Stream decoding for simultaneous spoken language translation. Abstract: In the typical speech translation system, the first-best speech recognizer hypothesis is segmented into sentence-like units which are then fed to the downstream machine translation component. The need for a sufficiently large context in this intermediate step and for the MT introduces delays which are undesirable in many application scenarios, such as real-time subtitling of foreign language broadcasts or simultaneous translation of speeches and lectures. In this paper, we propose a statistical machine translation decoder which processes a continuous input stream, such as that produced by a run-on speech recognizer. By decoupling decisions about the timing of translation output generation from any fixed input segmentation, this design can guarantee a maximum output lag for each input word while allowing for full word reordering within this time window. Experimental results show that this system achieves competitive translation performance with a minimum of translationinduced latency.
- R. Stiefelhagen, H. K. Ekenel, C. Fügen, Petra Gieselmann, H. Holzapfel, Florian Kraft, Kai Nickel, M. Voit, A. Waibel. 2007. Enabling Multimodal Human–Robot Interaction for the Karlsruhe Humanoid Robot. Abstract: In this paper, we present our work in building technologies for natural multimodal human-robot interaction. We present our systems for spontaneous speech recognition, multimodal dialogue processing, and visual perception of a user, which includes localization, tracking, and identification of the user, recognition of pointing gestures, as well as the recognition of a person's head orientation. Each of the components is described in the paper and experimental results are presented. We also present several experiments on multimodal human-robot interaction, such as interaction using speech and gestures, the automatic determination of the addressee during human-human-robot interaction, as well on interactive learning of dialogue strategies. The work and the components presented here constitute the core building blocks for audiovisual perception of humans and multimodal human-robot interaction used for the humanoid robot developed within the German research project (Sonderforschungsbereich) on humanoid cooperative robots.
- Stephan Konn, H. Holzapfel, H. K. Ekenel, A. Waibel. 2007. Integrating Face-ID into an Interactive Person-ID Learning System. Abstract: Acquiring knowledge about persons is a key functionality for humanoid robots. By envisioning a robot that can provide personalized services the system needs to detect, recognize and memorize information about specific persons. To reach this goal we present an approach for extensible person identification based on visual processing, as one com- ponent of an interactive system able to interactively acquire information about persons. This paper describes an approach for face-ID recognition and identifica- tion over image sequences and its integration into the interactive system. We compare the approach of sequence hypotheses against results from single image hypotheses, and a standard approach and show improve- ments in both cases. We furthermore explore the usage of confidence scores to allow other system components to estimate the accuracy of face-ID hypotheses.
- Nguyen Bach, Matthias Eck, Paisarn Charoenpornsawat, Thilo Köhler, Sebastian Stüker, ThuyLinh Nguyen, Roger Hsiao, A. Waibel, S. Vogel, Tanja Schultz, A. Black. 2007. The CMU TransTac 2007 Eyes-free and Hands-free Two-way Speech-to-Speech Translation System. Abstract: The paper describes our portable two-way speech-tospeech translation system using a completely eyesfree/hands-free user interface. This system translates between the language pair English and Iraqi Arabic as well as between English and Farsi, and was built within the framework of the DARPA TransTac program. The Farsi language support was developed within a 90-day period, testing our ability to rapidly support new languages. The paper gives an overview of the system’s components along with the individual component objective measures and a discussion of issues relevant for the overall usage of the system. We found that usability, flexibility, and robustness serve as severe constraints on system architecture and design.
- Tanja Schultz, A. Waibel, Qin Jin. 2007. Robust speaker recognition. Abstract: The automatic speaker recognition technologies have developed into more and more important modern technologies required by many speech-aided applications. The main challenge for automatic speaker recognition is to deal with the variability of the environments and channels from where the speech was obtained. In previous work, good results have been achieved for clean high-quality speech with matched training and test acoustic conditions, such as high accuracy of speaker identification and verification using clean wideband speech and Gaussian Mixture Models (GMM). However, under mismatched conditions and noisy environments, often expected in real-world conditions, the performance of GMM-based systems degrades significantly, far away from the satisfactory level. Therefore, robustness becomes a crucial research issue in speaker recognition field. 
In this thesis, our main focus is to-improve the robustness of speaker recognition systems on far-field distant microphones. We investigate approaches to improve robustness from two directions. First, we investigate approaches to improve robustness for traditional speaker recognition system which is based on low-level spectral information. We introduce a new reverberation compensation approach which, along with feature warping in the feature processing procedure, improves the system performance significantly. We propose four multiple channel combination approaches, which utilize information from multiple far-field microphones, to improve robustness under mismatched training-testing conditions. Secondly, we investigate approaches to use high-level speaker information to improve robustness. We propose new techniques to model speaker pronunciation idiosyncrasy from two dimensions: the cross-stream dimension and the time dimension. Such high-level information is expected to be robust under different mismatched conditions. We also built systems that support robust speaker recognition. We implemented a speaker segmentation and clustering system aiming at improving the robustness of speaker recognition as well as automatic speech recognition performance in the multiple-speaker scenarios such as telephony conversations and meetings. We also integrate speaker identification modality with face recognition modality to build a robust person identification system.
- Matthias Eck, S. Vogel, A. Waibel. 2007. Translation Model Pruning via Usage Statistics for Statistical Machine Translation. Abstract: We describe a new pruning approach to remove phrase pairs from translation models of statistical machine translation systems. The approach applies the original translation system to a large amount of text and calculates usage statistics for the phrase pairs. Using these statistics the relevance of each phrase pair can be estimated. The approach is tested against a strong baseline based on previous work and shows significant improvements.
- H. Holzapfel, A. Waibel. 2007. Behavior models for learning and receptionist dialogs. Abstract: We present a dialog model for identifying persons, learning person names, and associated face IDs in a receptionist dialog. The proposed model allows a decomposition of the main dialog task into separate dialog behaviors which can be implemented separately and allow a mixture of handcrafted models and dialog strategies trained with reinforcement learning. The dialog model was implemented on our robot and tested in a number of experiments in a receptionist task. A Wizard-of-Oz experiment is used to evaluate the dialog structure, delivers information for the deﬁnition of metrics, and delivers a data corpus which is used to train a user simulation and component error model. Using these models we train a dialog module for learning a person’s name with reinforcement learning.
- Sebastian Stüker, M. Paulik, M. Kolss, C. Fügen, A. Waibel. 2007. Speech Translation Enhanced ASR for European Parliament Speeches - On the Influence of ASR Performance on Speech Translation. Abstract: In this paper we describe our work in coupling automatic speech recognition (ASR) and machine translation (MT) in a speech translation enhanced automatic speech recognition (STE-ASR) framework for transcribing and translating European parliament speeches. We demonstrate the influence of the quality of the ASR component on the MT performance, by comparing a series of WERs with the corresponding automatic translation scores. By porting an STE-ASR framework to the task at hand, we show how the word errors for transcribing English and Spanish speeches can be lowered by 3.0% and 4.8% relative, respectively.
- A. Waibel, Tanja Schultz. 2007. The CMU TransTac 2007 eyes-free two-way speech-to-speech translation system. Abstract: The paper describes our portable two-way speech-tospeech translation system using a completely eyesfree/hands-free user interface. This system translates between the language pair English and Iraqi Arabic as well as between English and Farsi, and was built within the framework of the DARPA TransTac program. The Farsi language support was developed within a 90-day period, testing our ability to rapidly support new languages. The paper gives an overview of the system’s components along with the individual component objective measures and a discussion of issues relevant for the overall usage of the system. We found that usability, flexibility, and robustness serve as severe constraints on system architecture and design.
- S. Jou, Tanja Schultz, A. Waibel. 2007. Continuous Electromyographic Speech Recognition with a Multi-Stream Decoding Architecture. Abstract: In our previous work, we reported a surface electromyographic (EMG) continuous speech recognition system with a novel EMG feature extraction method, E4, which is more robust to EMG noise than traditional spectral features. In this paper, we show that articulatory feature (AF) classifiers can also benefit from the E4 feature, which improve the F-score of the AF classifiers from 0.492 to 0.686. We also show that the E4 feature is less correlated across EMG channels and thus channel combination gains larger improvement in F-score. With a stream architecture, the AF classifiers are then integrated into the decoding framework and improve the word error rate by 11.8% relative from 33.9% to 29.9%.
- DomainAlon Lavie, Lori S. Levin, A. Waibel, D. Gates, Marsal Gavaldà. 2007. JANUS: a Multi-lingual Speech-to-speech Translation System for Spontaneously Spoken Language in a Limited Domain. Abstract: .Ta.mu; is a. rnulLi-lingual r,;peech tra.nr,;la.tion :;yr,;tem currently opera.ting in the domain of meeting r,;cheduling. Tra.nslating spontaneom; speech require:, a high degree of rolmstness to overcome the disfluenries of spoken l;wguage as well as errors in speerh rerognition. In t his system desuiption , we focus on the robust speech translation cornµonents in Janus-the skiµping GLR* par:;er, the segmentation of full utterances into semantic dialogue units (SDU s), and the late-stage disambigua tion of utterances. \Ve will also describe how the end-to-end translation performance of the system i:; evaluated and pre:,cnL our la.test Spanish-Lo-English cvaluaLion rc:,ulLs.
- Chiori Hori, B. Zhao, S. Vogel, A. Waibel. 2007. Consolidation based speech translation. Abstract: To alleviate the degradation of the performance of speech translation, this paper proposes a new approach to translate ASR results through consolidation which extracts meaningful phrases and remove redundant and irrelevant information caused by speaker's disfluency and recognition errors. The speech translation results via consolidation are partial translation and can not be directly compared with gold standards in which all words are translated. We would like to propose a new evaluation framework for partial translation by comparing with the most similar set of words extracted from a word network created by merging gradual summarizations of the gold standard translation. Chinese broadcast news speech in RT04 were recognized, consolidated and then translated. The performance of MT results was evaluated using BLEU. We propose information preservation accuracy (IPAccy) and meaning preservation accuracy (MPAccy) for consolidation and consolidation-based MT.
- M. Paulik, S. Stuker, C. Fugen, Tanja Schultz, A. Waibel. 2007. Translating language with technology's help. Abstract: In this article, we introduced an iterative system for improving speech recognition in the context of human mediated translation scenarios. In contrast to related work conducted in this field, we included scenarios in which only spoken language representations are available. One key feature of our iterative system is that all involved system components, ASR as well as MT, are improved. Particularly in the context of a spoken source language representation, not only is the target language ASR automatically improved but so is the source language ASR. Using Spanish as the source language and English as the target language, we were able to reduce the WER of the English ASR by 35.8% when given a written-source language representation. Given a spoken-source language representation, we achieved a relative WER reduction of 29.9% for English and 20.9% for Spanish
- A. Waibel, M. Bett. 2007. MEETING BROWSER: TRACKING AND SUMMARIZING MEETINGS. Abstract: To provide rapid access to meetings between human beings, transcription, tracking, retrieval and summarization of on-going human-to-human conversation has to be achieved. In DARPA and DoD sponsored work (projects GENOA and CLARITY) we aim to develop strategies to transcribe human discourse and provide rapid access to the structure and content of this human exchange. The system consists of four major components: 1.) the speech transcription engine, based on the JANUS recognition toolkit, 2.) the summarizer, a statistical tool that attempts to find salient and novel turns in the exchange, 3.) the discourse component that attempts to identify the speech acts, and 4.) the non-verbal structure, including speaker types and non-verbal visual cues. The meeting browser also attempts to identify the speech acts found in the turns of the meeting, and track topics. The browser is implemented in Java and also includes video capture of the individuals in the meeting. It attempts to identify the speakers, and their focus of attention from acoustic and visual cues. 1. THE MEETING RECOGNITION ENGINE The speech recognition component of the meeting browser is based on the JANUS Switchboard recognizer trained for the 1997 NIST Hub-5E evaluation [3]. The gender independent, vocal tract length normalized, large vocabulary recognizer features dynamic, speaking mode adaptive acoustic and pronunciation models [2] which allow for robust recognition of conversational speech as observed in human to human dialogs. 1.1 Speaking Mode Dependent Pronunciation Modeling In spontaneous conversational human-to-human speech as observed in meetings there is a large amount of variability due to accents, speaking styles and speaking rates (also known as the speaking mode [6]. Because current recognition systems usually use only a relatively small number of pronunciation variants for the words in their dictionaries, the amount of variability that can be modeled is limited. Increasing the number of variants per dictionary entry may seem to be the obvious solution, but doing so actually results in a increase in error rate. This is explained by the greater confusion between the dictionary entries, particularly, for short reduced words. We developed a probabilistic model based on context dependent phonetic rewrite rules to derive a list of possible pronunciations for all words or sequences of words [2][4]. In order to reduce the confusion of this expanded dictionary, each variant of a word is annotated with an observation probability. To this aim we automatically retranscribe the corpus based on all allowable variants using flexible utterance transcription graphs (Flexible Transcription Alignment (FTA) [5]) and speaker adapted models. The alignments are then used to train a model of how likely which form of variation (i.e. rule) is and how likely a variant is, to be observed in a certain context (acoustic, word, speaking mode or dialogue) is. For decoding, the probability of encountering pronunciation variants is then defined to be a function of the speaking style (phonetic context, linguistic context, speaking rate and duration). The probability function is learned through decision trees from rule based generated pronunciation variants as observed on the Switchboard corpus [2]. 1.2 Experimental Setup To date, we have experimented with three different meeting environments and tasks to assess the performance in terms of word accuracy and summarization quality: i.) Switchboard human to human telephone conversations, ii.) Research group meetings recorded in the Interactive Systems labs and iii.) Simulated crisis management meetings (3 participants) which also include video capture of the individuals. We report results from speech recognition experiments in the first two conditions. 1) Human to Human Telephone The test set to evaluate the use of the flexible transcription alignment approach consisted of the Switchboard and CallHome partitions of the 1996 NIST Hub-5e evaluation set. All test runs were carried out using a Switchboard recognizer trained with the JANUS Recognition Toolkit (JRTk) [4]. The preprocessing of the system begins by extracting MFCC based feature vectors every 10 ms. A truncated LDA transformation is performed over a concatenation of MFCCs and their first and second order derivatives are determined. Vocal tract length normalization and cepstral mean subtraction are computed to reduce speaker and channel differences. The rule-based expanded dictionary that was used in these tests included 1.78 pronunciation variants/word, compared to 1.13 found in the baseform dictionary (PronLex). The first list of results in Table 1 is based on a recognizer whose polyphonic decision trees were still trained on Viterbi alignments based on the unexpanded dictionary. We compare a baseline system trained on the base dictionary with an expanded dictionary FTA trained system tested in two different ways: with the base dictionary and with the expanded one. It turns out, that FTA training reduces the word error rate significantly, which means, that we improved the quality of the transcriptions through FTA and pronunciation modeling. Due to the added confusion of the expanded dictionary the test with the large dictionary without any weighting of the variants yields slightly worse results than testing with the baseline dictionary. Condition SWB WER CH WER Baseline 32.2% 43.7% FTA traing test w.basedict 30.7% 41.9% FTA traing test w.expanded dict 31.1% 42.5% Table 1 Recognition results using flexible transcription alignment training and label boosting. The test using the expanded dictionary was done without weighting the variants Adding vowel stress related questions to the phonetic clustering procedure and regrowing the polyphonic decision tree based on FTA labels improved the performance by 2.6% absolute on SWB and 2.2% absolute on CallHome. Table 2 shows results for mode dependent pronunciation weighting. We gain an additional ~2% absolute by weighting the pronunciation based on mode related features. Condition SWB WER CH WER unweighted 28.7% 38.6% Weighted p(r|w) 27.1% 36.7% Weighted p(r|w,m) 26.7% 36.1% Table 2 Results using different pronunciation variant weighting schemes. 2) Research Group Meetings In a second experiment we used recorded during internal group meetings at our lab. We placed lapel microphones on three out of ten participants, and recorded the signals on those three channels. Each meeting was approximately one hour in length, for a total of three hours of speech on which to adapt and test. Since we have no additional training data collected in this particular environment, the following unsupervised adaptation techniques was used to adapt a read speech, clean environment Wall Street Journal dictation recognizer to the meeting conditions: 1. MLLR based adaptation: In our system, we employed a regression tree, constructed using an acoustic similarity criterion for the defnition of regression classes. The tree is pruned as necessary to ensure sufficient adaptation data on each leaf. For each leaf node we calculate a linear transformation that maximizes the likelihood of the adaptation data. The number of transformations is determined automatically. 2. Iterative batch-mode unsupervised adaptation: The quality of adaptation depends directly on the quality of the hypotheses on which the alignments are based. We iterate the adaptation procedure, improving both the acoustic models and the hypotheses they produce. Significant gains were observed during the two iterations, after which performance converges. 3. Adaptation wth confidence measures: Confidence measures were used to automatically select the best candidates for adaptation. We used the stability of a hypothesis in a lattice as indicator of confidence. If, in rescoring the lattice with a variety of language model weights and insertion penalties, a word appears in every possible top-1 hypothesis, acoustic stability is indicated. Such acoustic stability often identifies a good candidate for adaptation. Using only these words in the adaptation procedure produces 1-2% gains in word accuracy over blind adaptation [9]. The baseline performance of the JRTk based WSJ Recognizer over the Hub4-Nov94 test set is about 7% WER. These preliminary experiments suggest that due to the effects of spontaneous human-to-human speech, significant differences in recording conditions, significant crosstalk on the recorded channels, significantly different microphone characteristics, and inappropriate language models the error rate on meetings is in a range of 40-50\% WER. Adaptation Iterations Speaker 0 1 2 Adaptation Gain maxl 51.7 45.3 45.2 12% fdmg 48.4 43.8 44.9 9% flsl 63.8 59.5 59.6 7% Total 54.8 49.6 49.9 Table 3 Error rates for three different speakers in a research group meeting using JRTk trained over WSJ dictation data.
- S. Jou, L. Maier-Hein, Tanja Schultz, A. Waibel. 2006. Articulatory Feature Classification using Surface Electromyography. Abstract: In this paper, we present an approach for articulatory feature classification based on surface electromyographic signals generated by the facial muscles. With parallel recorded audible speech and electromyographic signals, experiments are conducted to show the anticipatory behavior of electromyographic signals with respect to speech signals. On average, we found that the signals to be time delayed by 0.02 to 0.12 second. Furthermore, it is shown that different articulators have different anticipatory behavior. With offset-aligned signals, we improved the average F-score of the articulatory feature classifiers in our baseline system from 0.467 to 0.502
- Andreas Zollmann, Ashish Venugopal, S. Vogel, A. Waibel. 2006. The CMU-UKA syntax augmented machine translation system for IWSLT-06. Abstract: We present the CMU-UKA Syntax Augmented Machine Translation System that was used in the IWSLT-06 evaluation campaign. We participated in the C-Star data track using only the Full BTEC corpus, for Chinese-English translation, focusing on transcript translation. We applied techniques that produce true-cased, punctuated translations from non-punctuated Chinese transcripts, generating translations which score higher against the Official metric than against the lower-cased, punctuation removed metric. Our results demonstrate the impact of syntax and hierarchy based models for speech transcript translation.
- C. Fügen, M. Kolss, Dietmar Bernreuther, M. Paulik, Sebastian Stüker, S. Vogel, A. Waibel. 2006. Open Domain Speech Recognition &amp; Translation:Lectures and Speeches. Abstract: For years speech translation has focused on the recognition and translation of discourses in limited domains, such as hotel reservations or scheduling tasks. Only recently research projects have been started to tackle the problem of open domain speech recognition and translation of complex tasks such as lectures and speeches. In this paper we present the on-going work at our laboratory in open domain speech translation of lectures and parliamentary speeches. Starting from a translation system for European parliamentary plenary sessions and a lecture speech recognition system we show how both components perform in unison on speech translation of lectures
- H. Holzapfel, A. Waibel. 2006. Providing Cognitive Functions for Interactive Learning with Speech and Multimodal Processing. Abstract: Cognitive systems can roughly be categorized into systems that try to imitate human cognition and systems that try to imitate cognition on a functional basis, to compare which functionality human cognition can achieve. Both approaches are driven forward with usually different research approaches. In this paper we address influences to our work by both approaches and especially look for hints that can help us further in designing a cognitive system, while focussing on the development of an interactive learning component that uses speech and multimodal information. An important part of cognition is the ability to acquire new knowledge though learning mechanisms, which enhances an artificial system with the ability to develop or adapt to a new environment. In contrast to most learning algorithms applied in machine learning today, which mainly work with offline learning on training samples, learning in a cognitive system needs to be performed autonomously and through interaction with the environment or with other agents/humans. We present ongoing work in this area of interactive learning with first experiments on having the robot learn words, objects and their semantic interpretation, and argue for a modality independent approach to knowledge acquisition through dialog.
- A. Waibel. 2006. Local Appearance-based 3D Face Recognition. Abstract: iv
- H. Holzapfel, A. Waibel. 2006. A multilingual expectations model for contextual utterances in mixed-initiative spoken dialogue. Abstract: This paper describes a model of generating expectations that are used to improve speech recognition and to resolve elliptical expressions in dialogue context. The algorithm is domain and language independent and part of the dialogue manager. We use the expectation model to weight a speech recognizer’s grammar rules in dialogue context which improves recognition rates significantly as shown in the evaluation. We explain what types of expectations the system can generate and give a classification of system actions based on speech act theory, explain the resolution of elliptical expressions and their interpretation in context, and evaluate the presented algorithm in a multilingual system with English and German speech recognition.
- Robert G. Malkin, Datong Chen, Jie Yang, A. Waibel. 2006. Directing Attention in Online Aggregate Sensor Streams via Auditory Blind Value Assignment. Abstract: Multiparty collaborative applications in which groups of people act in concert to achieve some real-world goal abound. In these situations, it is useful for a central planning agent to receive online audio-visual information from all participants. However, as the size of the group grows, it becomes difficult to process all the sensory streams; cognitive overload prevents direct analysis of sensory streams for situational awareness. To avoid this situation, an automatic method is needed to assign value to each stream and direct the attention of the planning agent to those streams which are most valuable. We present an audio-based blind value assignment (BVA) method to address this problem, and experiments demonstrating the method's efficacy. We demonstrate that use of audio BVA techniques results in automatic value judgments which are broadly similar to human value judgments and superior to automatic judgments based on video information
- Robert G. Malkin, Datong Chen, Jie Yang, A. Waibel. 2006. Multimodal estimation of user interruptibility for smart mobile telephones. Abstract: Context-aware computer systems are characterized by the ability to consider user state information in their decision logic. One example application of context-aware computing is the smart mobile telephone. Ideally, a smart mobile telephone should be able to consider both social factors (i.e., known relationships between contactor and contactee) and environmental factors (i.e., the contactee's current locale and activity) when deciding how to handle an incoming request for communication.Toward providing this kind of user state information and improving the ability of the mobile phone to handle calls intelligently, we present work on inferring environmental factors from sensory data and using this information to predict user interruptibility. Specifically, we learn the structure and parameters of a user state model from continuous ambient audio and visual information from periodic still images, and attempt to associate the learned states with user-reported interruptibility levels. We report experimental results using this technique on real data, and show how such an approach can allow for adaptation to specific user preferences.
- Matthias Eck, S. Vogel, A. Waibel. 2006. A Flexible Online Server for Machine Translation Evaluation. Abstract: We present an Online Server for Machine Translation Evaluation that offers improvements over the standard usage of the typical scoring scripts. Users are able to interactively define their own test sets, experiments and pre-processing steps. Several scores are automatically calculated for submitted translations and the hypotheses and scores are organized and archived for later review. The server offers a nice web based user interface.
- Roger Hsiao, Ashish Venugopal, Thilo Köhler, Y. Zhang, Paisarn Charoenpornsawat, Andreas Zollmann, S. Vogel, A. Black, Tanja Schultz, A. Waibel. 2006. Optimizing components for handheld two-way speech translation for an English-iraqi Arabic system. Abstract: This paper described our handheld two-way speech translation system for English and Iraqi. The focus is on developing a field usable handheld device for speech-to-speech translation. The computation and memory limitations on the handheld impose critical constraints on the ASR, SMT, and TTS components. In this paper we discuss our approaches to optimize these components for the handheld device and present performance numbers from the evaluations that were an integral part of the project. Since one major aspect of the TransTac program is to build fieldable systems, we spent significant effort on developing an intuitive interface that minimizesthetrainingtimeforusersbutalsoprovidesusefulinfor
- Matthias Walliczek, Florian Kraft, S. Jou, Tanja Schultz, A. Waibel. 2006. Sub-word unit based non-audible speech recognition using surface electromyography. Abstract: In this paper we present a novel approach for a surface electromyographic speech recognition system based on sub-word units. Rather than using full word models as integrated in our previous work we propose here smaller sub-word units as prerequisites for large vocabulary speech recognition. This allows the recognition of words not seen in the training set based on seen sub-word units. Therefore we report on experiments with syllables and phonemes as sub-word units. We also developed a new feature extraction method that gains significant improvement for words and sub-word units. Index Terms: silent speech, non-audible speech recognition, electromyography, sub-word unit comparison
- A. Waibel, Fei Huang. 2006. Multilingual named entity extraction and translation from text and speech. Abstract: Named entities (NE), the noun or noun phrases referring to persons, locations and organizations, are among the most information-bearing linguistic structures. Extracting and translating named entities benefits many natural language processing problems such as cross-lingual information retrieval, cross-lingual question answering and machine translation. 
In this theisis we propose an efficient and effective framework to extract and translate NEs from text and speech. We adopt the hidden Markov model (HMM) as a baseline NE extraction system, and investigate its performance in multiple language pairs with varying amounts of training data. We expand the baseline text NE tagger with a context-based NE extraction model, which aims to detect and correct NE recognition errors from automatic speech recognition hypotheses. We also adapt the broadcast stews trained NE tagger for meeting transcripts. 
We develop several language-independent features to capture phonetic and semantic similarity measures between source and target NE pairs. We incorporate these features to solve various NE translation problems presented in different language pairs (Chinese to English, Arabic to English and Hindi to English), with varying resources (parallel and non-parallel corpora as well as the World Wide Web) and different input data streams (text and speech). 
We also propose a cluster-specific name transliteration framework. By grouping names from similar origins into one cluster and training cluster-specific transliteration and language models, we manage to dramatically reduce the name transliteration error rates.
- Thomas Prommer, H. Holzapfel, A. Waibel. 2006. Rapid simulation-driven reinforcement learning of multimodal dialog strategies in human-robot interaction. Abstract: In this work we propose a procedure model for rapid automatic strategy learning in multimodal dialogs. Our approach is tailored for typical task-oriented human-robot dialog interactions, with no prior knowledge about the expected user and system dynamics being present. For such scenarios, we propose the use of stochastic dialog simulation for strategy learning, where the user and sys-tem error models are solely trained through the initial execution of an inexpensive Wizard-of-Oz experiment. We argue that for the addressed dialogs, already a small data corpus combined with a low-conditioned simulation model facilitates learning of strong and complex dialog strategies. To validate our overall approach, we empirically show the supremacy of the learned strategy over a hand-crafted strategy for a concrete human-robot dialog scenario. To the authors’ knowledge, this work is the ﬁrst to perform strategy learning from multimodal dialog simulation.
- Petra Gieselmann, A. Waibel. 2006. Dynamic extension of a grammar-based dialogue system: constructing an all-recipes knowing robot. Abstract: In the upcoming field of humanoid and human-friendly robots, the ability of the robot for simple, unconstrained and natural communication with its users is of central importance. The basis for appropriate actions of the robot is the correct understanding of the user utterances. To be able to cover all the entities a user might talk about, we enhanced our dialogue manager with an ability for dynamic vocabulary generation out of information found across the internet. As a test case, we chose an internet recipe database integrated in the dialogue manager of our household robot so that it can understand several thousand recipes and ingredients now. Index Terms: dialogue management, human-robot interaction, vocabulary extension
- A. Waibel, Robert G. Malkin. 2006. Machine listening for context-aware computing. Abstract: Machine listening is an area of study which is rapidly increasing in importance. The proliferation of massive sensory corpora, together with the perceptual needs of smart computational devices and smart spaces has lead to this increase. Machine listening provides both a computationally cheap alternative to machine vision, and a source of information that is complementary to visual information; hence, perceptual systems which lack the ability to process auditory information will in general perform less well than those which can process auditory information. Machine listening is also interesting in its own right, as research into computational auditory processing can help to shed light on general principles of perception, and on how our own perceptual systems work. This thesis describes machine listening research designed to solve real-world problems in perceptual and context-aware computing. 
This thesis makes two claims. First, it claims that machine listening technologies are well-suited to the task of providing context awareness in real-world computational systems, whether these systems are intended to provide operational cues to smart devices or spaces, or to segment, summarize, or select segments of interest in multimedia corpora to make them more useful to human users. Second, it claims that the use of the core principle of perception, redundancy reduction, can guide the design of practical systems to provide context awareness in this way. The validity of these claims is supported by evidence from three application areas: multimedia gisting, acoustic environment recognition, and estimation of user interruptibility for the CHIL Connector service, a smart mobile telephone.
- S. Jou, Tanja Schultz, Matthias Walliczek, Florian Kraft, A. Waibel. 2006. Towards continuous speech recognition using surface electromyography. Abstract: We present our research on continuous speech recognition of the surface electromyographic signals that are generated by the human articulatory muscles. Previous research on electromyographic speech recognition was limited to isolated word recognition because it was very difficult to train phoneme-based acoustic models for the electromyographic speech recognizer. In this paper, we demonstrate how to train the phoneme-based acoustic models with carefully designed electromyographic feature extraction methods. By decomposing the signal into different feature space, we successfully keep the useful information while reducing the noise. Additionally, we also model the anticipatory effect of the electromyographic signals compared to the speech signal. With a 108-word decoding vocabulary, the experimental results show that the word error rate improves from 86.8% to 32.0% by using our novel feature extraction methods. Index Terms: speech recognition, electromyography, articulatory muscles, feature extraction.
- Matthias Eck, Ian Lane, Nguyen Bach, Sanjika Hewavitharana, M. Kolss, B. Zhao, Almut Silja Hildebrand, S. Vogel, A. Waibel. 2006. The UKA/CMU statistical machine translation system for IWSLT 2006. Abstract: In this paper we describe the CMU statistical machine translation system used in the IWSLT 2005 evaluation campaign. This system is based on phrase-to-phrase translations extracted from a bilingual corpus. We experimented with two different phrase extraction methods; PESA on-the-fly phrase extraction and alignment free extraction method. The translation model, language model and other features were combined in a log-linear model during decoding. We present our experiments on model adaptation for new data in a different domain, as well as combining different translation hypotheses to obtain better translations. We participated in the supplied data track for manual transcriptions in the translation directions: ArabicEnglish, Chinese-English, Japanese-English and KoreanEnglish. For Chinese-English direction we also worked on ASR output of the supplied data, and with additional data in unrestricted and C-STAR tracks.
- C. Fügen, M. Kolss, M. Paulik, Sebastian Stüker, Tanja Schultz, A. Waibel. 2006. Open Domain Speech Translation: From Seminars and Speeches to Lectures. Abstract: This paper describes our ongoing work in domain unlimited speech translation. We describe how we developed a lecture translation system by moving from speech translation of European Parliament Plenary Sessions and seminar talks to the open domain of lectures. We started with our speech recognition (ASR) and statistical machine translation (SMT) 2006 evaluation systems developed within the framework of TC-Star (Technology and Corpora for Speech to Speech Translation) and CHIL (Computers in the Human Interaction Loop). The paper presents the speech translation performance of these systems on lectures and gives an overview of our final real-time lecture translation system.
- Florian Kraft, Robert G. Malkin, Thomas Schaaf, A. Waibel. 2005. Temporal ICA for classification of acoustic events i a kitchen environment. Abstract: We describe a feature extraction method for general audio modeling using a temporal extension of Independent Component Analysis (ICA) and demonstrate its utility in the context of a sound classification task in a kitchen environment. Our approach accounts for temporal dependencies over multiple analysis frames much like the standard audio modeling technique of adding first and second temporal derivatives to the feature set. Using a real-world dataset of kitchen sounds, we show that our approach outperforms a canonical version of this standard front end, the mel-frequency cepstral coefficients (MFCCs), which has found successful application in automatic speech recognition tasks.
- Fei Huang, S. Vogel, A. Waibel. 2005. Clustering and Classifying Person Names by Origin. Abstract: In natural language processing, information about a person's geographical origin is an important feature for name entity transliteration and question answering. We propose a language-independent name origin clustering and classification framework. Provided with a small amount of bilingual name translation pairs with labeled origins, we measure origin similarities based on the perplexities of name character language and translation models. We group similar origins into clusters, then train a Bayesian classifier with different features. It achieves 84% classification accuracy with source names only, and 91% with both source and target name pairs. We apply the origin clustering and classification technique to a name transliteration task. The cluster-specific transliteration model dramatically improves the transliteration accuracy from 3.8% to 55%, reducing the transliteration character error rate from 50.3 to 13.5. Adding more unlabeled name pairs to the cluster-specific name transliteration model further improves the transliteration accuracy.
- Florian Metze, C. Fügen, Yue Pan, A. Waibel. 2005. Automatically transcribing meetings using distant microphones. Abstract: In this paper, we describe our efforts to develop acoustic models suitable for distant microphone automatic speech recognition. Our goal is to investigate how the performance of a system trained on a combination of close-talking and distant microphone data can be optimized, while assuming as little information about the configuration of (multiple) distant microphones as possible, to avoid guesstimates and lengthy calibration runs. We evaluated our system in NIST's RT-04S "Meeting" speech-to-text evaluation, where speech data was recorded at several sites with a varying number of different table-top microphones, but not with microphone arrays. Body-mounted microphones provide baseline numbers for distant ASR performance and allow for comparisons of meeting speech with other spontaneous speech data.
- Florian Kraft, A. Waibel, K. Kroschel, Thomas Schaaf, Robert G. Malkin. 2005. Continuous Audio Object Recognition Diploma Thesis. Abstract: The detection of sound events is a key technology for a various set of audio applications. Sounds are able to transport information through vision borders. Therefore, a humanoid robot assigned with kitchen tasks improves its interactive behavior with the environment a lot when using acoustics. While audio scene analysis employs a lot of subjects, this thesis deals with the recognition of presegmented as well as continuous audio objects using single channel microphone input. Further prior knowledge on scenes with single and multiple sources was not used. This means that recognition is performed without information on the audio context like source positions and statistical information on typical event sequences. The three explored feature sets consisted of MFCCs with first and second order temporal derivatives, PCA-ICA features without using temporal context and PCA-ICA features on several context window sizes. In a first batch of experiments those features were evaluated for GMMs, forward and ergodic HMMs on predefined segments for single source data, which was recorded in different kitchens. The results show that for single source data MFCC features perform worse than ICA features, independent of the classifier. Further, ICA features covering temporal context gave even better results. The comparison of forward and ergodic models for different number of states revealed that the kitchen task class set generally favors ergodic HMMs instead of left-right models. Another experiment confirmed the superiority of ICA to MFCCs with respect to the number of gaussian parameters. While the ICA features for an architecture, which cover shared global interclass properties, appeared to be superior on single source data, this benefit could not be shown under continuous real world cooking conditions with background noise. Scarce class occurrences in realworld conditioned data in combination with low recognition performance showed that source separation, confidence measures and multi track hypothesis output need to be considered in future research directions. Furthermore, the mapping of acoustic entities to semantics during labeling and training has to be performed carefully.
- Ashish Venugopal, Andreas Zollmann, A. Waibel. 2005. Training and Evaluating Error Minimization Decision Rules for Statistical Machine Translation. Abstract: Decision rules that explicitly account for non-probabilistic evaluation metrics in machine translation typically require special training, often to estimate parameters in exponential models that govern the search space and the selection of candidate translations. While the traditional Maximum A Posteriori (MAP) decision rule can be optimized as a piecewise linear function in a greedy search of the parameter space, the Minimum Bayes Risk (MBR) decision rule is not well suited to this technique, a condition that makes past results difficult to compare. We present a novel training approach for non-tractable decision rules, allowing us to compare and evaluate these and other decision rules on a large scale translation task, taking advantage of the high dimensional parameter space available to the phrase based Pharaoh decoder. This comparison is timely, and important, as decoders evolve to represent more complex search space decisions and are evaluated against innovative evaluation metrics of translation quality.
- Sanjika Hewavitharana, S. Vogel, A. Waibel. 2005. Augmenting a statistical translation system with a translation memory. Abstract: In this paper, we present a translation memory (TM) based system to augment a statistical translation (SMT) system. It is used for translating sentences which have close matches in the training corpus. Given a test sentence, we first extract sentence pairs from the training corpus, whose source side is similar to the test sentence. Then, the TM system modifies the translation of the sentences by a sequence of substitution, deletion and inser- tion operations, to obtain the desired result. Statistical phrase alignment model of the SMT system is used for this purpose. The system was evaluated using a corpus of Chinese- English conversational data. For close matching sentences, the translations produced by the translation memory approach were compared with the translations of the statistical decoder.
- M. Paulik, C. Fügen, Sebastian Stüker, Tanja Schultz, Thomas Schaaf, A. Waibel. 2005. Document driven machine translation enhanced ASR. Abstract: In human-mediated translation scenarios a human interpreter translates between a source and a target language using either a spoken or a written representation of the source language. In this paper we improve the recognition performance on the speech of the human translator spoken in the target language by taking advantage of the source language representations. We use machine translation techniques to translate between the source and target language resources and then bias the target language speech recognizer towards the gained knowledge, hence the name Machine Translation Enhanced Automatic Speech Recognition. We investigate several different techniques among which are restricting the search vocabulary, selecting hypotheses from n-best lists, applying cache and interpolation schemes to language modeling, and combining the most successful techniques into our final, iterative system. Overall we outperform the baseline system by a relative word error rate reduction of 37.6%.
- Chiori Hori, A. Waibel. 2005. Spontaneous speech consolidation for spoken language applications. Abstract: This paper describes the work done as a part of the International Workshop on Speech Summarization for Information Extraction and Machine Translation (IWSpS) , on spoken language processing including summarization, machine translation and question answering on lecture speech in the Translanguage English Database (TED) corpus . The hypotheses of lecture speech obtained by automatic speech recognition (ASR) system are ill-formed due to the spontaneity of speakers and recognition errors. The overall performance of spoken language processing components is affected by the errors introduced by the ASR system. In order to get more reliable phrases which maintain the original meaning and contribute positively to the total performance of the spoken language system, this paper proposes a consolidation fram ework. The consolidation approach extracts words by excluding redundant and irrelevant information and concatenating words so as to maintain the original meaning. Automatic consolidation performance is evaluated by comparing with manual consolidation by humans using a word accuracy metric . Our approach gives 58% accuracy on ASR output with 70% word accuracy.
- Maria Danninger, G. Flaherty, Keni Bernardin, H. K. Ekenel, Thilo Köhler, Robert G. Malkin, R. Stiefelhagen, A. Waibel. 2005. The connector: facilitating context-aware communication. Abstract: We present the Connector, a context-aware service that intelligently connects people. It maintains an awareness of its users' activities, preoccupations and social relationships to mediate a proper connection at the right time between them. In addition to providing users with important contextual cues about the availability of potential callees, the Connector adapts the behavior of the contactee's device automatically in order to avoid inappropriate interruptions.To acquire relevant context information, perceptual components analyze sensor input obtained from a smart mobile phone and --- if available --- from a variety of audio-visual sensors built into a smart meeting room environment. The Connector also uses any available multimodal interface (e.g. a speech interface to the smart phone, steerable camera-projector, targeted loudspeakers) in the smart meeting room, to deliver information to users in the most unobtrusive way possible.
- Sanjika Hewavitharana, B. Zhao, Almut Silja Hildebrand, Matthias Eck, Chiori Hori, S. Vogel, A. Waibel. 2005. The CMU Statistical Machine Translation System for IWSLT2005. Abstract: In this paper we describe the CMU statistical machine translation system used in the IWSLT 2005 evaluation campaign. This system is based on phrase-to-phrase translations extracted from a bilingual corpus. We experimented with two different phrase extraction methods; PESA on-the-fly phrase extraction and alignment free extraction method. The translation model, language model and other features were combined in a log-linear model during decoding. We present our experiments on model adaptation for new data in a different domain, as well as combining different translation hypotheses to obtain better translations. We participated in the supplied data track for manual transcriptions in the translation directions: ArabicEnglish, Chinese-English, Japanese-English and KoreanEnglish. For Chinese-English direction we also worked on ASR output of the supplied data, and with additional data in unrestricted and C-STAR tracks.
- Thilo Köhler, C. Fügen, Sebastian Stüker, A. Waibel. 2005. Rapid porting of ASR-systems to mobile devices. Abstract: Portable devices for the consumer market are becoming available in large quantities. Because of their design and use, human speech often is the input modality of choice, for example for car navigation systems or portable speech-to-speech translation devices. In this paper we describe our work in porting our existing desktop PC based speech recognition system to an off-the-shelf PDA running WindowsCE3.0. We do this in a way that our already well performing language and acoustic models can be taken over without the need of retraining them for the PDA. In order to achieve an acceptable run-time behavior we apply several optimization techniques to the preprocessing and decoding process. Among other things we introduce the newly developed early feature vector reduction. In that way the execution time of our recognition system can be reduced from initially 28x realtime to 2.6x real-time with a tolerable increase in word error rate. The size of the acoustic models is reduced to 25% of its original size.
- B. Zhao, E. Xing, A. Waibel. 2005. Bilingual Word Spectral Clustering for Statistical Machine Translation. Abstract: In this paper, a variant of a spectral clustering algorithm is proposed for bilingual word clustering. The proposed algorithm generates the two sets of clusters for both languages efficiently with high semantic correlation within monolingual clusters, and high translation quality across the clusters between two languages. Each cluster level translation is considered as a bilingual concept, which generalizes words in bilingual clusters. This scheme improves the robustness for statistical machine translation models. Two HMM-based translation models are tested to use these bilingual clusters. Improved perplexity, word alignment accuracy, and translation quality are observed in our experiments.
- S. Jou, Tanja Schultz, A. Waibel. 2005. Whispery speech recognition using adapted articulatory features. Abstract: This paper describes our research on adaptation methods applied to articulatory feature detection on soft whispery speech recorded with a throat microphone. Since the amount of adaptation data is small and the testing data is very different from the training data, a series of adaptation methods is necessary. The adaptation methods include: maximum likelihood linear regression, feature-space adaptation, and re-training with downsampling, sigmoidal low-pass filter, and linear multivariate regression. Adapted articulatory feature detectors are used in parallel to standard senone-based HMM models in a stream architecture for decoding. With these adaptation methods, articulatory feature detection accuracy improves from 87.82% to 90.52% with corresponding F-measure from 0.504 to 0.617, while the final word error rate improves from 33.8% to 31.2%.
- Matthias Eck, S. Vogel, A. Waibel. 2005. Low Cost Portability for Statistical Machine Translation based on N-gram Frequency and TF-IDF. Abstract: Statistical machine translation relies heavily on the available training data. In some cases it is necessary to limit the amount of training data that can be created for or actually used by the systems. We introduce weighting schemes which allow us to sort sentences based on the frequency of unseen n-grams. A second approach uses TF-IDF to rank the sentences. After sorting we can select smaller training corpora and we are able to show that systems trained on much less training data achieve a very competitive performance compared to baseline systems using all available training data.
- A. Waibel. 2005. CHIL - Computers in the Human Interaction Loop. Abstract: CHIL ("Computers in the Human Interaction Loop") is an Integrated Project under the European Commission's Sixth Framework Programme. The CHIL consortium is jointly coordinated by Universitat Karlsruhe (TH) and the Fraunhofer Institute IITB. CHIL was launched on January 1st, 2004. The objective of this project is to explore and create environments in which computers serve humans who focus on interacting with other humans as opposed to having to attend to and being preoccupied by the machines themselves. Instead of computers operating in an isolated manner, and humans [thrust] in the loop [of computers]. CHIL puts Computers in the Human Interaction Loop (CHIL). Fifteen partners from nine countries in Europe and the US collaborate in the CHIL Consortium to design Technologies and Computer Services that model humans and the state of their activities and intentions. A complete perceptual context enables a family of CHIL computing services that provide helpful assistance implicitly, requiring a minimum of human attention or interruptions. 1. PROJECT DESCRIPTION The objective of the CHIL project is to create environments in which computers serve humans who focus on interacting with other humans as opposed to having to attend to and being preoccupied with the machines themselves. Instead of computers operating in an isolated manner, and Humans [thrust] in the loop [of computers], we will put Computers in the Human Interaction Loop (CHIL). We design Computer Services that model humans and the state of their activities and intentions. Based on the understanding of the human perceptual context, CHIL computers are enabled to provide helpful assistance implicitly, requiring a minimum of human attention or interruptions (see also the CHIL – Scenarios section). To achieve this overall vision, a broad set of key scientific issues is proposed: Multimodal Perceptual User Interfaces that observe, recognize, fuse, and interpret all available cues and clues to explain human-human activities and intentions. Fundamental new algorithms are needed to achieve these capabilities (see the CHIL – Technologies section). A suite of Services that instantiate CHIL Computing based on perceptual context awareness and understanding of human activity. These services must balance implicit and explicit computer interaction, and must deliver information in an appropriate manner. Services include better ways of connecting people (without phone-tag), supporting human memory, & providing meeting support (see CHIL – Services) and more. A supportive infrastructure that supports CHIL Services including Automomic Computing, selfhealing and self-maintaining software, flexible architecture, and a networked infrastructure integrating numerous devices intermittently and dynamically. The resulting shift from HumanComputer Interaction only (requiring full human attention) to increased reliance on human-human interaction is expected to lead to human productivity gains and reduced computer frustration (see CHIL – Software Architecture).
- Ulf Krum, H. Holzapfel, A. Waibel. 2005. Questions to Improve Dialogue Flow and Speech Recognition in Spoken Dialogue Systems. Abstract: Within human-machine conversation, clarification is vital and may consist of various forms, as it is may by due to many different effects on different levels of communication. In this paper, we present a strategy for detecting situations where a need for clarification exists in a natural spoken dialogue system. We define rule sets which enable us, via an anomaly analysis, to detect these critical situations. Through the use of such rule sets, we show that ist is possible to enhance the strategy in such a manner that more different situations are detected. In a user test, we evaluate the success of the strategy and show that strategies with explicit clarification improve the naturalness of human-machine interaction.
- M. Paulik, S. Stuker, C. Fugen, Tanja Schultz, Thomas Schaaf, A. Waibel. 2005. Speech translation enhanced automatic speech recognition. Abstract: Nowadays official documents have to be made available in many languages, like for example in the EU with its 20 official languages. Therefore, the need for effective tools to aid the multitude of human translators in their work becomes easily apparent. An ASR system, enabling the human translator to speak his translation in an unrestricted manner, instead of typing it, constitutes such a tool. In this work we improve the recognition performance of such an ASR system on the target language of the human translator by taking advantage of an either written or spoken source language representation. To do so, machine translation techniques are used to translate between the different languages and then the involved ASR systems are biased towards the gained knowledge. We present an iterative approach for ASR improvement and outperform our baseline system by a relative word error rate reduction of 35.8%/29.9% in the case of a written/spoken source language representation. Further, we show how multiple target languages, as for example provided by different simultaneous translators during European Parliament debates, can be incorporated into our system design for an improvement of all involved ASR systems
- A. Waibel. 2005. CHIL computing to overcome techno-clutter. Abstract: After building computers that paid no intention to communicating with humans, we have in recent years developed ever more sophisticated interfaces that put the "human in the loop" of computers. These interfaces have improved usability by providing more appealing output (graphics, animations), more easy to use input methods (mouse, pointing, clicking, dragging) and more natural interaction modes (speech, vision, gesture, etc.). Yet the productivity gains that have been promised have largely not been seen and human-machine interaction still remains a partially frustrating and tedious experience, full of techno-clutter and excessive attention required by the technical artifact.In this talk, I will argue, that we must transition to a third paradigm of computer use, in which we let people interact with people, and move the machine into the background to observe the humans' activities and to provide services implicitly, that is, -to the extent possible- without explicit request. Putting the "Computer in the Human Interaction Loop" (CHIL), instead of the other way round, however, brings formidable technical challenges. The machine must now always observe and understand humans, model their activities, their interaction with other humans, the human state as well as the state of the space they are in, and finally, infer intentions and needs. From a perceptual user interface point of view, we must process signals from sensors that are always on, frequently inappropriately positioned, and subject to much greater variablity. We must also not only recognize WHAT was seen or said in a given space, but also a broad range of additional information, such as the WHO, WHERE, HOW, TO WHOM, WHY, WHEN of human interaction and engagement.In this talk, I will describe a variety of multimodal interface technologies that we have developed to answer these questions and some preliminary CHIL type services that take advantage of such perceptual interfaces.
- Matthias Eck, S. Vogel, A. Waibel. 2005. Low Cost Portability for Statistical Machine Translation based on N-gram Coverage. Abstract: Statistical machine translation relies heavily on the available training data. However, in some cases, it is necessary to limit the amount of training data that can be created for or actually used by the systems. To solve that problem, we introduce a weighting scheme that tries to select more informative sentences first. This selection is based on the previously unseen n-grams the sentences contain, and it allows us to sort the sentences according to their estimated importance. After sorting, we can construct smaller training corpora, and we are able to demonstrate that systems trained on much less training data show a very competitive performance compared to baseline systems using all available training data.
- M. Paulik, S. Sẗuker, C. Fügen, Tanja Schultz, Thomas Schaaf, A. Waibel. 2005. ENHANCED AUTOMATIC SPEECH RECOGNITION. Abstract: Nowadays official documents have to be made available in many languages, like for example in the EU with its 20 official languages. Therefore, the need for effective tools to aid the multitude of human translators in their work becomes easily apparent. An ASR system, enabling the human translator to speak his translation in an unrestricted manner, instead of typing it, constitutes such a tool. In this work we improve the recognition performance of such an ASR system on the target language of the human translator by taking advantage of an either written or spoken source language representation. To do so, machine translation techniques are used to translate between the different languages and then the invovled ASR systems are biased towards the gained knowledge. We present an iterative approach for ASR improvement and outperform our baseline system by a relative word error rate reduction of 35.8% / 29.9% in the case of a written / spoken source language representation. Further, we show how multiple target languages, as for example provided by different simultaneous translators during European Parliament debates, can be incorporated into our system design for an improvement of all involved ASR systems.
- Almut Silja Hildebrand, Matthias Eck, S. Vogel, A. Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. Abstract: In this paper we present experiments concerning translation model adaptation for statistical machine translation. We develop a method to adapt translation models using in- formation retrieval. The approach selects sentences similar to the test set to form an adapted training corpus. The method allows a better use of additionally available out-of-domain training data or finds in-domain data in a mixed corpus. The adapted translation models significantly improve the translation performance compared to competitive baseline sys- tems.
- Robert G. Malkin, A. Waibel. 2005. Classifying user environment for mobile applications using linear autoencoding of ambient audio. Abstract: Many mobile devices and applications can act in context-sensitive ways, but rely on explicit human action for context awareness. It would be preferable if our devices were able to attain context awareness without human intervention. One important aspect of user context is environment. We present a novel method for classifying environment types based on acoustic signals. This method makes use of linear autoencoding neural networks, and is motivated by the observation that biological coding systems seem to be heavily influenced by the statistics of their environments. We show that the autoencoder method achieved a lower error rate than a standard Gaussian mixture model on a representative sample task, and that a linear combination of autoencoders and GMMs yielded better performance than either alone.
- Ulf Krum, H. Holzapfel, A. Waibel. 2005. Clarification questions to improve dialogue flow and speech recognition in spoken dialogue systems. Abstract: Within human-machine conversation, clariﬁcation is vital and may consist of various forms, as it is may by due to many different effects on different levels of communication. In this paper, we present a strategy for detecting situations where a need for clariﬁcation exists in a natural spoken dialogue system. We de-ﬁne rule sets which enable us, via an anomaly analysis, to detect these critical situations. Through the use of such rule sets, we show that ist is possible to enhance the strategy in such a manner that more different situations are detected. In a user test, we evaluate the success of the strategy and show that strategies with explicit clariﬁcation improve the naturalness of human-machine interaction.
- L. Maier-Hein, Florian Metze, Tanja Schultz, A. Waibel. 2005. Session independent non-audible speech recognition using surface electromyography. Abstract: In this paper we introduce a speech recognition system based on myoelectric signals. The system handles audible and non-audible speech. Major challenges in surface electromyography based speech recognition ensue from repositioning electrodes between recording sessions, environmental temperature changes, and skin tissue properties of the speaker. In order to reduce the impact of these factors, we investigate a variety of signal normalization and model adaptation methods. An average word accuracy of 97.3% is achieved using seven EMG channels and the same electrode positions. The performance drops to 76.2% after repositioning the electrodes if no normalization or adaptation is performed. By applying our adaptation methods we manage to restore the recognition rates to 87.1%. Furthermore, we compare audibly to non-audibly spoken speech. The results suggest that large differences exist between the corresponding muscle movements. Still, our recognition system recognizes both speech manners accurately when trained on pooled data
- S. Vogel, Sanjika Hewavitharana, M. Kolss, A. Waibel. 2004. The ISL statistical translation system for spoken language translation. Abstract: In this paper we describe the components of our statistical machine translation system used for the spoken language translation evaluation campaign. This system is based on phrase-to-phrase translations extracted from a bilingual corpus. A new phrase alignment approaches will be introduced, which finds the target phrase by optimizing the overall word-to-word alignment for the sentence pair under the constraint that words within the source phrase are only aligned to words within the target phrase. The system will be used for Chinese-to-English translations under the small, additional and unlimited data conditions, and for the small Japanese-to-English translation track.
- Xilin Chen, Jie Yang, Jing Zhang, A. Waibel. 2004. Automatic detection and recognition of signs from natural scenes. Abstract: In this paper, we present an approach to automatic detection and recognition of signs from natural scenes, and its application to a sign translation task. The proposed approach embeds multiresolution and multiscale edge detection, adaptive searching, color analysis, and affine rectification in a hierarchical framework for sign detection, with different emphases at each phase to handle the text in different sizes, orientations, color distributions and backgrounds. We use affine rectification to recover deformation of the text regions caused by an inappropriate camera view angle. The procedure can significantly improve text detection rate and optical character recognition (OCR) accuracy. Instead of using binary information for OCR, we extract features from an intensity image directly. We propose a local intensity normalization method to effectively handle lighting variations, followed by a Gabor transform to obtain local features, and finally a linear discriminant analysis (LDA) method for feature selection. We have applied the approach in developing a Chinese sign translation system, which can automatically detect and recognize Chinese signs as input from a camera, and translate the recognized text into English.
- Hua Yu, A. Waibel. 2004. Integrating thumbnail features for speech recognition using conditional exponential models. Abstract: We describe a novel approach for modeling segmental information in speech recognition, through the use of thumbnail features. By taking into account dependencies at the segmental level, thumbnail features are more resistant to changes in speaking rates and other factors. While the traditional acoustic features are fixed for every utterance, one set of thumbnail features is computed for each hypothesis, which may violate the traditional scoring paradigm. To this end, we introduce a conditional exponential modeling framework. It allows better integration of various knowledge sources in a discriminative fashion. We present preliminary experiments on the Switchboard task.
- S. Jou, Tanja Schultz, A. Waibel. 2004. Adaptation for soft whisper recognition using a throat microphone. Abstract: This paper describes various adaptation methods applied to recognizing soft whisper recorded with a throat microphone. Since the amount of adaptation data is small and the testing data is very different from the training data, a series of adaptation methods is necessary. The adaptation methods include: maximum likelihood linear regression, feature-space adaptation, and re-training with downsampling, sigmoidal low-pass filter, or linear multivariate regression. With these adaptation methods, the word error rate improves from 99.3% to 32.9%.
- A. Waibel. 2004. Speech translation: past, present and future. Abstract: A decade after its first beginnings, the grand challenge of building automatic systems that translate speech has grown into an active research area, a focus for speech and language researchers worldwide. Workshops, conferences, sessions, journal issues are devoted to it, and research is supported by governments in Asia, Europe and the US. The problem has attracted much attention, as practical needs in an increasingly globalized world converge with scientific advances that bring possible solutions within reach. While great progress has been made, the problem is certainly not solved and much remains to be done. At a midway point along the way, we present this paper as a review of the past, of the successes so far, and as an attempt to chart a course for the future.
- Matthias Eck, S. Vogel, A. Waibel. 2004. Improving Statistical Machine Translation in the Medical Domain using the Unified Medical Language system. Abstract: Texts from the medical domain are an important task for natural language processing. This paper investigates the usefulness of a large medical database (the Unified Medical Language System) for the translation of dialogues between doctors and patients using a statistical machine translation system. We are able to show that the extraction of a large dictionary and the usage of semantic type information to generalize the training data significantly improves the translation performance.
- C. Fügen, H. Holzapfel, A. Waibel. 2004. Tight coupling of speech recognition and dialog management - dialog-context dependent grammar weighting for speech recognition. Abstract: In this paper we present our current work on a tight coupling of a speech recognizer with a dialog manager and our results by restricting the search space of our grammar based speech recognizer through the information given by the dialog manager. As a result of the tight coupling the same lingustic knowledge sources can be used in both, speech recognizer and dialog manager. Furthermore, the flexible context-free grammar implementation of our speech decoder Ibis allows weighting of specific rules at run-time to restrict the search space of the recognizer for the next decoding step. These rules are given by the dialog manager depending on the current dialog context. With this approach we were able to reduce the word error rate of user responses to system questions by 3.3% relative for close talking and 16.0% relative, when using distant speech input. The sentence error rates were reduced by 2.2%, 9.2% respectively.
- Yue Pan, A. Waibel. 2004. Minimum Kullback-Leibler distance based multivariate Gaussian feature adaptation for distant-talking speech recognition. Abstract: Multivariate Gaussian based speech compensation or mapping has been developed to reduce the mismatch between training and deployment conditions for robust speech recognition. The acoustic mapping procedure can be formulated as a feature space adaptation where a noisy input signal is transformed by a multivariate Gaussian network. We propose a novel algorithm to update the network parameters based on minimizing the Kullback-Leibler distance between the core recognizer's acoustic model and transformed features. It is designed to achieve optimal overall system performance rather than MMSE on a specific feature domain. An online stochastic gradient descent learning rule is derived. We evaluate the performance of the new algorithm using a JRTk broadcast news system on a distance-talking speech corpus and compare its performance with that of previous MMSE based approaches. The experiments show the KL based approach is more effective for a large vocabulary continuous speech recognition (LVCSR) system.
- B. Zhao, S. Vogel, Matthias Eck, A. Waibel. 2004. Phrase Pair Rescoring with Term Weighting for Statistical Machine Translation. Abstract: We propose to score phrase translation pairs for statistical machine translation using term weight based models. These models employ tf.idf to encode the weights of content and non-content words in phrase translation pairs. The translation probability is then modeled by similarity functions defined in a vector space. Two similarity functions are compared. Using these models in a statistical machine translation task shows significant improvements.
- A. Waibel, Tanja Schultz, S. Vogel, C. Fügen, M. Honal, M. Kolss, Jürgen Reichert, Sebastian Stüker. 2004. Towards language portability in statistical speech translation. Abstract: Speech translation has made significant advances over the last years. We believe that we can overcome today's limits of language and domain portable conversational speech translation systems by relying more radically on learning approaches and by the use of multiple layers of reduction and transformation to extract the desired content in another language. Therefore, we cascade stochastic source-channel models that extract an underlying message from a corrupt observed output. The three models effectively translate: (1) speech to word lattices (automatic speech recognition, ASR); (2) ill-formed fragments of word strings into a compact well-formed sentence (Clean); (3) sentences in one language to sentences in another (machine translation, MT). We present results of our research efforts towards rapid language portability of all these components. The results on translation suggest that MT systems can be successfully constructed for any language pair by cascading multiple MT systems via English. Moreover, end-to-end performance can be improved, if the interlingua language is enriched with additional linguistic information that can be derived automatically and monolingually in a data-driven fashion.
- Jürgen Reichert, A. Waibel. 2004. The ISL EDTRL system. Abstract: For the translation of text and speech, statistical methods on one side and interlingua based methods on the other have been used successfully. However, the former requires programming grammars for each language, plus the design of an interlingua, while the latter requires the collection of a large parallel corpus for every language pair. To alleviate these problems, we propose an approach that combines the advantages from both worlds. The proposed approach makes use of English or enriched English as an interlingua and can cascade data-driven translation systems into and from this interlingua. We show that enriching English with linguistic information that is automatically derived i only on English data performs better than pure cascaded systems.
- Y. Zhang, S. Vogel, A. Waibel. 2004. Interpreting BLEU/NIST Scores: How Much Improvement do We Need to Have a Better System?. Abstract: Automatic evaluation metrics for Machine Translation (MT) systems, such as BLEU and the related NIST metric, are becoming increasingly important in MT. Yet, their behaviors are not fully understood. In this paper, we analyze some flaws in the BLEU/NIST metrics. With a better understanding of these problems, we can better interpret the reported BLEU/NIST scores. In addition, this paper reports a novel method of calculating the confidence intervals for BLEU/NIST scores using bootstrapping. With this method, we can determine whether two MT systems are significantly different from each other.
- Matthias Eck, S. Vogel, A. Waibel. 2004. Language Model Adaptation for Statistical Machine Translation Based on Information Retrieval. Abstract: Language modeling is an important part for both speech recognition and machine translation systems. Adaptation has been successfully applied to language models for speech recognition. In this paper we present experiments concerning language model adaptation for statistical machine translation. We develop a method to adapt language models using information retrieval methods. The adapted language models drastically reduce perplexity over a general language model and we can show that it is possible to improve the translation quality of a statistical machine translation using those adapted language models instead of a general language model.
- S. Jaeger, S. Manke, A. Waibel. 2004. NPEN++ : AN ON-LINE HANDWRITING RECOGNITION SYSTEM. Abstract: This paper presents the on-line handwriting recognition system NPen++ developed at the University of Karlsruhe and the Carnegie Mellon University. The NPen++ recognition engine is based on a Multi-State Time Delay Neural Network and yields recognition rates from 96% for a 5000 word dictionary to 93.4% on a 20,000 word dictionary and 91.2% for a 50,000 word dictionary. The proposed tree search and pruning technique reduces the search space considerably without loosing too much recognition performance compared to an exhaustive search. This allows running the NPen++ recognizer in real-time with large dictionaries.
- G. Lazzari, A. Waibel, Chengqing Zong. 2004. Worldwide ongoing activities on multilingual speech to speech translation. Abstract: This paper presents an overview of worldwide going on activities on Speech-to-Speech Translation. After a short introduction of the field, including the major projects and milestones, activities and projects going on in Asia, Europe and US are presented and described.
- Ye-Yi Wang, A. Waibel. 2004. A CONNECTIONIST MODEL FOR DIALOG PROCESS 1. Abstract: This paper describes a novel connectionist system for dialog processing. Based on a script-like formalism, the system consists of several modular neural networks which can track the semantic flow of a dialog. The system can be extended to understand and translate dialogs in a certain domain.
- R. Stiefelhagen, C. Fügen, Petra Gieselmann, H. Holzapfel, Kai Nickel, A. Waibel. 2004. Natural human-robot interaction using speech, head pose and gestures. Abstract: In this paper we present our ongoing work in building technologies for natural multimodal human-robot interaction. We present our systems for spontaneous speech recognition, multimodal dialogue processing and visual perception of a user, which includes the recognition of pointing gestures as well as the recognition of a person's head orientation. Each of the components is described in the paper and experimental results are presented. In order to demonstrate and measure the usefulness of such technologies for human-robot interaction, all components have been integrated on a mobile robot platform and have been used for real-time human-robot interaction in a kitchen scenario.
- Tanja Schultz, D. Alexander, A. Black, Kay Peterson, Sinaporn Suebvisai, A. Waibel. 2004. A Thai Speech Translation System for Medical Dialogs. Abstract: In this paper we present our activities towards a Thai Speech-to-Speech translation system. We investigated in the design and implementation of a prototype system. For this purpose we carried out research on bootstrapping a Thai speech recognition system, developing a translation component, and building an initial Thai synthesis system using our existing tools.
- J. McDonough, A. Waibel. 2004. Performance comparisons of all-pass transform adaptation with maximum likelihood linear regression. Abstract: All-pass transform (APT) adaptation transforms the cepstral means of a hidden Markov model so as to mimic the effect of warping the short-time frequency axis of a segment of speech, much like vocal tract length normalization (VTLN). However, APT adaptation can be implemented as a linear transformation in the cepstral domain, much like the better known maximum likelihood linear regression (MLLR). Recent work demonstrated the superior performance of APT adaptation to MLLR for a large vocabulary conversational speech recognition task. This work presents similar comparisons on the switchboard corpus. We found that without VTLN, the best MLLR and APT systems achieved word error rates (WERs) of 43.0% and 40.2% respectively. Similarly, with VTLN the respective error rates were 40.3%, and 39.2%, so that APT adaptation is significantly better in both cases. We also undertook a set of experiments to determine whether APT adaptation can be combined with a linear semi-tied covariance (STC) transform. With a single APT per speaker, the application of STC reduced the WER from 42.9% to 39.4%.
- Matthias Wölfel, J. McDonough, A. Waibel. 2003. Minimum variance distortionless response on a warped frequency scale. Abstract: In this work we propose a time domain technique to estimate an all-pole model based on the minimum variance distortionless response (MVDR) using a warped short time frequency axis such as the Mel scale. The use of the MVDR eliminates the overemphasis of harmonic peaks typically seen in medium and high pitched voiced speech when spectral estimation is based on linear prediction (LP). Moreover, warping the frequency axis prior to MVDR spectral estimation ensures more parameters in the spectral model are allocated to the low, as opposed to high, frequency regions of the spectrum, thereby mimicking the human auditory system. In a series of speech recognition experiments on the Switchboard Corpus (spontaneous English telephone speech), the proposed approach achieved a word error rate (WER) of 32.1% for female speakers, which is clearly superior to the 33.2% WER obtained by the usual combination of Mel warping and linear prediction.
- A. Waibel, A. Badran, A. Black, R. Frederking, D. Gates, A. Lavie, Lori S. Levin, K. Lenzo, L. Tomokiyo, Jürgen Reichert, Tanja Schultz, D. Wallace, M. Woszczyna, Jing Zhang. 2003. Speechalator: two-way speech-to-speech translation on a consumer PDA. Abstract: This paper describes a working two-way speech-to-speech translation system that runs in near real-time on a consumer handheld computer. It can translate from English to Arabic and Arabic to English in the domain of medical interviews. We describe the general architecture and frameworks within which we developed each of the components: HMM-based recognition, interlingua translation (both rule and statistically based), and unit selection synthesis.
- Xilin Chen, Jie Yang, A. Waibel. 2003. Calibration of a hybrid camera network. Abstract: Visual surveillance using a camera network has imposed new challenges to camera calibration. An essential problem is that a large number of cameras may not have a common field of view or even be synchronized well. We propose to use a hybrid camera network that consists of catadioptric and perspective cameras for a visual surveillance task. The relations between multiple views of a scene captured from different cameras can be then calibrated under the catadioptric camera's coordinate system. We address the important issue of how to calibrate the hybrid camera network. We calibrate the hybrid camera network in three steps. First, we calibrate the catadioptric camera using only the vanishing points. In order to reduce computational complexity, we calibrate the camera without the mirror first and then calibrate the catadioptric camera system. Second, we determine 3D positions of some points using as few as two spatial parallel lines and some equidistance points. Finally, we calibrate other perspective cameras based on these known spatial points.
- A. Waibel, Tanja Schultz, M. Bett, Matthias Denecke, Robert G. Malkin, I. Rogina, R. Stiefelhagen, Jie Yang. 2003. SMaRT: the Smart Meeting Room Task at ISL. Abstract: As computational and communications systems become increasingly smaller, faster, more powerful, and more integrated, the goal of interactive, integrated meeting support rooms is slowly becoming reality. It is already possible, for instance, to rapidly locate task-related information during a meeting, filter it, and share it with remote users. Unfortunately, the technologies that provide such capabilities are as obstructive as they are useful - they force humans to focus on the tool rather than the task. Thus the veneer of utility often hides the true costs of use, which are longer, less focused human interactions. To address this issue, we present our current research efforts towards SMaRT: the Smart Meeting Room Task. The goal of SMaRT is to provide meeting support services that do not require explicit human-computer interaction. Instead, by monitoring the activities in the meeting room using both video and audio analysis, the room is able to react appropriately to users' needs and allow the users to focus on their own goals.
- A. Tribble, S. Vogel, A. Waibel. 2003. Overlapping phrase-level translation rules in an SMT engine. Abstract: We explore a technique for adding longer phrase translation pairs to a statistical machine translation (SMT) system. Merging existing phrase-level alignments that have overlapping words on both the source and target sides generates new phrases. The effect on translation quality is reported for an Arabic-English system in the news domain.
- A. Waibel, A. Badran, A. Black, R. Frederking, D. Gates, A. Lavie, Lori S. Levin, K. Lenzo, L. Tomokiyo, Jürgen Reichert, Tanja Schultz, D. Wallace, M. Woszczyna, Jing Zhang. 2003. Speechalator: Two-Way Speech-to-Speech Translation in Your Hand. Abstract: This demonstration involves two-way automatic speech-to-speech translation on a consumer off-the-shelf PDA. This work was done as part of the DARPA-funded Babylon project, investigating better speech-to-speech translation systems for communication in the field. The development of the Speechalator software-based translation system required addressing a number of hard issues, including a new language for the team (Egyptian Arabic), close integration on a small device, computational efficiency on a limited platform, and scalable coverage for the domain.
- Zhirong Wang, Tanja Schultz, A. Waibel. 2003. Comparison of acoustic model adaptation techniques on non-native speech. Abstract: The performance of speech recognition systems is consistently poor on non-native speech. The challenge for non-native speech recognition is to maximize the recognition performance with a small amount of available non-native data. We report on acoustic modeling adaptation for the recognition of non-native speech. Using non-native data from German speakers, we investigate how bilingual models, speaker adaptation, acoustic model interpolation and polyphone decision tree specialization methods can help to improve the recognizer performance. Results obtained from the experiments demonstrate the feasibility of these methods.
- J. McDonough, A. Waibel. 2003. Maximum mutual information speaker adapted training with semi-tied covariance matrices. Abstract: We present re-estimation formulae for semi-tied covariance (STC) transformation matrices based on a maximum mutual information (MMI) criterion. These re-estimation formulae are different from those that have appeared previously in the literature. Moreover, we present a positive definiteness criterion with which the regularization constant present in all NMI re-estimation formulae can be reliably set to provide both consistent improvements in the total mutual information of the training set, as well as fast convergence. We combine the STC re-estimation formulae with their like for speaker-independent means and variances, and update all parameters during NMI speaker adapted training (MMI-SAT). We present the results of two sets of speech recognition experiments conducted on the the 1998 Broadcast News evaluation set, as well as a corpus of meeting room data collected at the Interactive Systems Laboratories of the Carnegie Mellon University.
- Florian Metze, A. Waibel. 2003. Using articulatory information for speaker adaptation. Abstract: Articulatory features (AF) have proven beneficial for automatic speech recognition (ASR) in noisy environments, for hyper-articulated speech or in multilingual settings. A stream setup can combine standard sub-phone Gaussian mixture models with feature GMM; the weights assigned to each feature stream such as VOICED or BILABIAL could intuitively be used for adaptation to speaker or text. In this paper, we investigate this stream setup, which allows us to add articulatory information to a baseline CD-HMM recognizer, on a database containing several speakers in a number of recordings of spontaneous speech. Our findings indicate that articulatory features as we use them are not entirely a speaker-dependent property, but when using them for speaker adaptation, we find their performance to be comparable to that of constrained MLLR.
- S. Vogel, Y. Zhang, Fei Huang, Fei Huang, A. Tribble, Ashish Venugopal, Bing Zhao, A. Waibel. 2003. The CMU statistical machine translation system. Abstract: In this paper we describe the components of our statistical machine translation system. This system combines phrase-to-phrase translations extracted from a bilingual corpus using different alignment approaches. Special methods to extract and align named entities are used. We show how a manual lexicon can be incorporated into the statistical system in an optimized way. Experiments on Chinese-to-English and Arabic-to-English translation tasks are presented.
- H. Ye, A. Waibel. 2003. FLEXIBLE PARAMETER TYING FOR CONVERSATIONAL SPEECH RECOGNITION. Abstract: Modeling pronunciation variation is key for recognizing conversational speech. Previous efforts on pronunciation modeling by modifying dictionaries only yielded marginal improvement. Due to complex interaction between dictionaries and acoustic models, we believe a pronunciation modeling scheme is plausible only when closely coupled with the underlying acoustic model. This paper explores the use of flexible parameter tying for pronunciation modeling. In particular, two new techniques are investigated: Gaussian tying and flexible tree clustering. We report a 1.3% absolute WER improvement over the traditional modeling framework on the Switchboard task.
- B. Zhao, K. Zechner, S. Vogel, A. Waibel. 2003. Efficient Optimization for Bilingual Sentence Alignment Based on Linear Regression. Abstract: This paper presents a study on optimizing sentence pair alignment scores of a bilingual sentence alignment module. Five candidate scores based on perplexity and sentence length are introduced and tested. Then a linear regression model based on those candidates is proposed and trained to predict sentence pairs' alignment quality scores solicited from human subjects. Experiments are carried out on data automatically collected from Internet. The correlation between the scores generated by the linear regression model and the scores from human subjects is in the range of the inter-subject agreement score correlations. Pearson's correlation ranges from 0.53 up to 0.72 in our experiments.
- Fei Huang, S. Vogel, A. Waibel. 2003. Extracting named entity translingual equivalence with limited resources. Abstract: In this article we present an automatic approach to extracting Hindi-English (H-E) Named Entity (NE) translingual equivalences from bilingual parallel corpora. In the absence of a Hindi NE tagger or H-E translation dictionary, this approach adapts a Chinese-English (C-E) surface string transliteration model for H-E NE extraction. The model is initially trained using automatically extracted C-E NE pairs, then iteratively updated based on newly extracted H-E NE pairs. For each English person and location NE in each sentence pair, this approach searches for its Hindi correspondence with minimum transliteration cost and constructs an H-E NE list from the bilingual corpus. Experiments show that this approach extracted 1000 H-E NE pairs with a precision of 91.8%.
- Ashish Venugopal, S. Vogel, A. Waibel. 2003. Effective Phrase Translation Extraction from Alignment Models. Abstract: Phrase level translation models are effective in improving translation quality by addressing the problem of local re-ordering across language boundaries. Methods that attempt to fundamentally modify the traditional IBM translation model to incorporate phrases typically do so at a prohibitive computational cost. We present a technique that begins with improved IBM models to create phrase level knowledge sources that effectively represent local as well as global phrasal context. Our method is robust to noisy alignments at both the sentence and corpus level, delivering high quality phrase level translation pairs that contribute to significant improvements in translation quality (as measured by the BLEU metric) over word based lexica as well as a competing alignment based method.
- Sebastian Stüker, Florian Metze, Tanja Schultz, A. Waibel. 2003. Integrating multilingual articulatory features into speech recognition. Abstract: The use of articulatory features, such as place and manner of articulation, has been shown to reduce the word error rate of speech recognition systems under different conditions and in different settings. For example recognition systems based on features are more robust to noise and reverberation. In earlier work we showed that articulatory features can compensate for inter language variability and can be recognized across languages. In this paper we show that using cross- and multilingual detectors to support an HMM based speech recognition system significantly reduces the word error rate. By selecting and weighting the features in a discriminative way, we achieve an error rate reduction that lies in the same range as that seen when using language specific feature detectors. By combining feature detectors from many languages and training the weights discriminatively, we even outperform the case where only monolingual detectors are being used.
- Fei Huang, S. Vogel, A. Waibel. 2003. Automatic Extraction of Named Entity Translingual Equivalence Based on Multi-Feature Cost Minimization. Abstract: Translingual equivalence refers to the relationship between expressions of the same meaning from different languages. Identifying translingual equivalence of named entities (NE) can significantly contribute to multilingual natural language processing, such as crosslingual information retrieval, crosslingual information extraction and statistical machine translation. In this paper we present an integrated approach to extract NE translingual equivalence from a parallel Chinese-English corpus.Starting from a bilingual corpus where NEs are automatically tagged for each language, NE pairs are aligned in order to minimize the overall multi-feature alignment cost. An NE transliteration model is presented and iteratively trained using named entity pairs extracted from a bilingual dictionary. The transliteration cost, combined with the named entity tagging cost and word-based translation cost, constitute the multi-feature alignment cost. These features are derived from several information sources using unsupervised and partly supervised methods. A greedy search algorithm is applied to minimize the alignment cost. Experiments show that the proposed approach extracts NE translingual equivalence with 81% F-score and improves the translation score from 7.68 to 7.74.
- M. Wolfel, J. McDonough, A. Waibel. 2003. Warping and scaling of the minimum variance distortionless response. Abstract: Spectral estimation based on the minimum variance distortionless response (MVDR) is well-known in the signal processing literature and has been shown to be superior to linear prediction for robust speech recognition. In this work we propose two techniques to improve the resolution and the robustness of the MVDR spectral estimate: The first is a time-domain technique to estimate an all-pole model based on the warped short time frequency axis such as the Mel-frequency. The second is a method for scaling the height of the spectral envelope in order to extract robust features for large vocabulary continuous speech recognition systems which must operate in noisy conditions. Moreover, we show that these two techniques can be combined to good effect. In a series of speech recognition experiments on the Switchboard corpus, the combination of our proposed approaches achieved a word error rate (WER) of 35.9%, which is clearly superior to the 37.0% WER obtained by the common MVDR and the 37.2% WER obtained by the widely used Fourier transform.
- Xilin Chen, Jie Yang, Jing Zhang, A. Waibel. 2002. Automatic detection of signs with affine transformation. Abstract: In this paper, we propose an approach for detecting signs from natural scenes. The approach efficiently embeds multiresolution, adaptive search, and affine rectification algorithms in a hierarchical framework, with different emphases at each layer. We combine in multi-resolution and multi-scale edge detection techniques to effectively detect text in different sizes. By using the cites from text inside the image, we introduce affine rectification transformation to recover deformation of the text region caused by air inappropriate camera view angle. This procedure can significantly improve text detection rate and OCR (Optical Character Recognition) accuracy. Experimental results have demonstrated feasibility of the proposed algorithms. We have applied the proposed approach to a Chinese sign translation system, which can automatically detect Chinese text input from a camera, recognize the text, and translate the recognized text into English or voice stream.
- M. Finke, Maria Lapata, A. Lavie, Lori S. Levin, L. Tomokiyo, T. Polzin, K. Ries, A. Waibel, K. Zechner, finkem. 2002. CLARITY: INFERRING DISCOURSE STRUCTURE FROM SPEECH. Abstract: The goal of the CLARITY project is to explore the use of discourse structure in the understanding of conversational speech. Within project CLARITY we aim to develop automatic classifiers for three levels of discourse structure in Spanish telephone conversations: speech acts, dialogue games, and discourse segments. This paper presents our first results and research plans in three areas: definition of discourse structure units and manual annotation of CALLHOME SPANISH, speech recognition, and automated segmentation and labeling of speech acts.
- Qin Jin, Tanja Schultz, A. Waibel. 2002. Phonetic speaker identification. Abstract: This paper describes the exploration of text-independent speaker identification using novel approaches based on speakers’ phonetic features instead of traditional acoustic features. Different phonetic speaker identification approaches are discussed in this paper and evaluated using two speaker identification systems: one multilingual system and one single language multiple-engine system. Furthermore, text-independent speaker identification experiments are carried out on a distant-microphone database as well as gender identification experiments are investigated on the NIST 1999 Speaker Recognition Evaluation dataset. The results show that phonetic features are powerful for speaker identification and gender identification.
- Akira Ushioda, David A. Evans, Ted Gibson, A. Waibel. 2002. The Automatic Acquisition of Frequencies of Verb Subcategorization Frames from Tagged Corpora. Abstract: We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a finear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.
- Florian Metze, A. Waibel. 2002. A flexible stream architecture for ASR using articulatory features. Abstract: Recently, speech recognition systems based on articulatory features such as “voicing” or the position of lips and tongue have gained interest, because they promise advantages with respect to robustness and permit new adaptation methods to compensate for channel, noise, and speaker variability. These approaches are also interesting from a general point of view, because their models use phonological and phonetic concepts, which allow for a richer description of a speech act than the sequence of HMM-states, which is the prevalent ASR architecture today. In this work, we present a multi-stream architecture, in which CD-HMMS are supported by detectors for articulatory features, using a linear combination of log-likelihood scores. This multi-stream approach results in a 15% reduction of WER on a read Broadcast-News (BN) task and im-proves performance on a spontaneous scheduling task (ESST) by 7%. The proposed architecture potentially allows for new speaker and channel adaptation schemes, including stream asynchronicity.
- Jing Zhang, Xilin Chen, Jie Yang, A. Waibel. 2002. A PDA-based sign translator. Abstract: We propose an effective approach for a PDA-based sign system and present the sign translator. Its main functions include three parts: detection, recognition and translation. Automatic detection and recognition of text in natural scenes is a prerequisite for the automatic sign translator. In order to make the system robust for text detection in various natural scenes, the detection approach efficiently embeds multi-resolution, adaptive search in a hierarchical framework with different emphases at each layer. We also introduce an intensity-based OCR method to recognize characters in various fonts and lighting conditions, where we employ the Gabor transform to obtain local features, and LDA for selection and classification of features. The recognition rate is 92.4% for the testing set obtained from the natural sign. A sign is different from the normal used sentence. It is brief with a lot of abbreviations and place nouns. We only briefly introduce a rule-based place name translation. We have integrated all these functions in a PDA, which can capture sign images, auto segment and recognize the Chinese sign, and translate it into English.
- Jie Yang, Xilin Chen, Jing Zhang, Y. Zhang, A. Waibel. 2002. Automatic detection and translation of text from natural scenes. Abstract: Large amounts of information are embedded in natural scenes. Signs are good examples of natural objects with high information content. In this paper, we discuss problems in automatic detection and translation of text from natural scenes. We describe the chal1enges of automatic text detection and propose methods to address these chal1enges. We extend example based machine translation technology for sign translation and present a prototype system for Chinese sign translation. This system is capable of capturing images, automatically detecting and recognizing text, and translating the text into English. The translation can be displayed on a palm size PDA, or synthesized as a voice output message over the earphones.
- Yue Pan, A. Waibel. 2002. Experiments on distant-talking speech recognition in meeting room using extended MAM. Abstract: MAM has been successfully used to improve the noise robustness of speech recognizers. We apply this method in the exploration of distant-talking speech recognition for meeting room system, and propose a new extended MAM algorithm combined with LDA and MLLR techniques to compensate for additive noise and channel variation. By sharing the LDA matrix with the core recognition system, the MAM secondary model is coupled better with the intended system and can take the advantage of context speech frames. Using MAM in conjunction with model adaptation method MLLR can result in further improved recognition accuracy. Our database includes simulated 10dB additive noisy speech and real distant-talking speech by 8-channel simultaneous recording. We started from a baseline LVCSR meeting transcription system, with word error rate range from 25 to 55%. By applying extended MAM, the improved system achieves an average relative error rate reduction of 27% for 10dB additive noisy speech and 15% for real distant-talking speech.
- Fei Huang, A. Waibel. 2002. An adaptive approach to named entity extraction for meeting applications. Abstract: Named entity extraction has been intensively investigated in the past several years. Both statistical approaches and rule-based approaches have achieved satisfactory performance for regular written/spoken language. However when applied to highly informal or ungrammatical languages, e.g., meeting languages, because of the many mismatches in language genre, the performance of existing methods decreases significantly. 
 
In this paper we propose an adaptive method of named entity extraction for meeting understanding. This method combines a statistical model trained from broadcast news data with a cache model built online for ambiguous words, computes their global context name class probability from local context name class probabilities, and integrates name lists information from meeting profiles. Such a fusion of supervised and unsupervised learning has shown improved performance of named entity extraction for meeting applications. When evaluated using manual meeting transcripts, the proposed method demonstrates a 26.07% improvement over the baseline model. Its performance is also comparable to that of the statistical model trained from a small annotated meeting corpus. We are currently applying the proposed method to automatic meeting transcripts.
- Sebastian Stüker, Tanja Schultz, A. Waibel. 2002. Automatic Generation of Pronunciation Dictionaries. Abstract: In this report we will describe a data driven approach for creating pronunciation dictionaries for a new unseen target language by voting among phoneme recognizers in nine different languages other than the target language. In this process recordings of the new language that are transcribed on word level are decoded by the phoneme recognizers. This results in a hypothesis of nine phonemes per time frame, one from every language. Then two algorithms are described that can map the decoded hypotheses to a pronunciation dictionary entry. These algorithms make use of a confusion matrix based distance measure between the phonemes of the phoneme recognizers and the phonemes of the target language which dictionary is to be created. The confusion matrix is calculated with the help of 500 phonetically transcribed training utterances in the target language. The phoneme recognizers used in this work were derived from the context independent speech recognizers of the GlobalPhone project. In order to improve the mapping of the hypotheses of the phoneme recognizers to the dictionary entry we incorporated confidence measures that were derived from word lattices into our algorithms. Using the proposed algorithms we produced new pronunciation dictionaries for the target languages Swedish and Haitian Creole. The newly created dictionaries were evaluated by comparing the performance of large vocabulary continuous speech recognition systems trained with these dictionaries to reference systems trained with rule based pronunciation dictionaries. The results of the evaluation show that the process in its current form does not produce pronunciation dictionaries that are accurate enough to train large vocabulary continuous speech recognizers with them. We therefor make suggestions for future work in order to fix the error sources of the process.
- H. Soltau, Florian Metze, A. Waibel. 2002. Compensating for hyperarticulation by modeling articulatory properties. Abstract: In spoken dialogue systems, hyperarticulation occur as an eﬀect to recover previous recognition errors. It is commonly observed that users of automatic speech recognition systems apply similar recovery strategies as in human-human interactions. Previous studies have shown that current speech recognizers don’t cover hyperarticu-lated speech well. As an eﬀect of higher word error rates at hyperarticulated speech, humans try to reinforce this speaking style which results in even more recognition errors. In this study, we investigate the use of articulatory features to compensate hyperarticulated eﬀects. The underlying idea is, that acoustic models for articulatory features are more robust against variations in the speaking style compared to pure phone models. We present a streaming architecture which integrates articulatory features in a standard HMM based system. Using this approach, we achieved an error reduction of 25 . 1% for hyper-articulated speech and even 8 . 9% for normal speech without any use of hyperarticulated training data.
- Chiori Hori, S. Furui, Robert G. Malkin, Hua Yu, A. Waibel. 2002. Automatic summarization of English broadcast news speech. Abstract: This paper proposes an automatic speech summarization technique for English. In our proposed method, a set of words maximizing a summarization score indicating appropriateness of summarization is extracted from automatically transcribed speech and concatenated to create a summary. The extraction process is performed using a Dynamic Programming (DP) technique according to a target compression ratio. In this paper, English broadcast news speech transcribed using a speech recognizer is automatically summarized. In order to apply our method, originally proposed for Japanese, to English, the model of estimating word concatenation probabilities based on a dependency structure in the original speech given by a Stochastic Dependency Context Free Grammar (SDCFG) is modified. A summarization method for multiple utterances using two-level DP technique is also proposed. The automatically summarized sentences are evaluated by a summarization accuracy based on the comparison with the manual summarization of correctly transcribed speech by human subjects. Experimental results show that our proposed method effectively extracts relatively important information and remove redundant and irrelevant information from English news speech.
- Tanja Schultz, Qin Jin, K. Laskowski, A. Tribble, A. Waibel. 2002. Speaker, accent, and language identification using multilingual phone strings. Abstract: In this paper we investigated the identification of non-verbal cues from spoken speech, namely speaker, accent, and language. For these tasks, a joint framework is developed which uses phone strings, derived from different language phone recognizers, as intermediate features and which performs classification decisions based on their perplexities. Our evaluation on variable distance data proved the robustness of the approach, achieving a 96.7% speaker identification rate. Furthermore, we achieved 93.7% accent discrimination accuracy between native and non-native speakers. For language identification, we obtained 95.5% classification accuracy for utterances 5 seconds in length and up to 99.89% on longer utterances. The experiments were carried out in a language independent nature, on languages not presented to the phone recognizers for training, suggesting that they could be successfully ported to non-verbal cue classification in other languages.
- R. Cattoni, G. Lazzari, N. Mana, F. Pianesi, E. Pianta, Florian Metze, J. McDonough, H. Soltau, E. Costantini, D. Gates, A. Lavie, L. Levin, C. Langley, K. Peterson, T. Schultz, A. Waibel, D. Wallace, L. Besacier, H. Blanchon, D. Vaufreydaz. 2002. Not only Translation Quality : Evaluating the NESPOLE ! Speech-to-Speech Translation System along other Viewpoints. Abstract: Performance and usability of real-world speech-to-speech translation systems, like the one developed within the Nespole! project, are affected by several aspects that go beyond the pure translation quality provided by the Human Language Technology components of the system. In this paper we describe these aspects as viewpoints along which we have evaluated the Nespole! system. Four main issues are investigated: (1) assessing system performance under various network traffic conditions; (2) a study on the usage and utility of multi-modality in the context of multi-lingual communication; (3) the features of the single speech recognition engines, and (4) an end-to-end evaluation of the whole system. Not only Translation Quality: Evaluating the NESPOLE! Speech-to-Speech Translation System along other Viewpoints
- Jing Zhang, Xilin Chen, Andreas Hanneman, Jie Yang, A. Waibel. 2002. A robust approach for recognition of text embedded in natural scenes. Abstract: In this paper, we propose a robust approach for recognition of text embedded in natural scenes. Instead of using binary information as most other OCR systems do, we extract features from intensity of an image directly. We utilize a local intensity normalization method to effectively handle lighting variations. We then employ Gabor transform to obtain local features, and use the linear discriminant analysis (LDA) for selection and classification of features. The proposed method has been applied to a Chinese sign recognition task. The system can recognize a vocabulary of 3755 level I Chinese characters in the Chinese national standard character set GB2312-80 with various print fonts. We tested the system on 1630 test characters in sign images captured from the natural scenes, and the recognition accuracy was 92.46%. We have integrated the system into our automatic Chinese sign translation system.
- H. Soltau, Florian Metze, C. Fügen, A. Waibel. 2002. Efficient language model lookahead through polymorphic linguistic context assignment. Abstract: In this study, we examine how fast decoding of conversational speech with large vocabularies profits from efficient use of linguistic information, i.e. language models and grammars. Based on a re-entrant single pronunciation prefix tree, we use the concept of linguistic context polymorphism to achieve an early incorporation of language model information. This approach allows us to use all available language model information in a one-pass decoder, using the same engine to decode with statistical n-gram language models as well as context free grammars or re-scoring of lattices in an efficient way. We compare this approach to our previous decoder, which needed three passes to incorporate all available information. The results on a very large vocabulary task show that the search can be speeded up by almost a factor of three, without introducing additional search errors. On all examined tasks, we observed significant improvements by using an exact language model lookahead over usual bigram lookahead strategies, even for very hard tasks with unmatched conditions, without introducing extra memory overhead.
- Y. Zhang, B. Zhao, Jie Yang, A. Waibel. 2002. Automatic sign translation. Abstract: Large amounts of information is embedded in the natural scenes. Signs are good examples of objects in natural environments which have rich information content. In this paper, we present our efforts in the automatic sign translation. We describe the challenges in the automatic sign translation and introduce the architecture of our current system for automatic detection and translation of Chinese signs. Two data-driven machine translation methods: Example Based Machine Translation (EBMT) and Statistical Machine Translation (SMT) are compared for the task of translating Chinese signs into English. We report the experimental results of both methods that are trained from a small bilingual sign corpus combined with a bilingual glossary. The experiment results indicate that EBMT generates more correct translations while SMT is better at inferring unseen patterns. We are currently working on developing a multi-engine machine translation system that can incrementally learn from the data and combine the results from EBMT and SMT.
- R. Stiefelhagen, Jie Yang, A. Waibel. 2002. Modeling focus of attention for meeting indexing based on multiple cues. Abstract: A user's focus of attention plays an important role in human-computer interaction applications, such as a ubiquitous computing environment and intelligent space, where the user's goal and intent have to be continuously monitored. We are interested in modeling people's focus of attention in a meeting situation. We propose to model participants' focus of attention from multiple cues. We have developed a system to estimate participants' focus of attention from gaze directions and sound sources. We employ an omnidirectional camera to simultaneously track participants' faces around a meeting table and use neural networks to estimate their head poses. In addition, we use microphones to detect who is speaking. The system predicts participants' focus of attention from acoustic and visual information separately. The system then combines the output of the audio- and video-based focus of attention predictors. We have evaluated the system using the data from three recorded meetings. The acoustic information has provided 8% relative error reduction on average compared to only using one modality. The focus of attention model can be used as an index for a multimedia meeting record. It can also be used for analyzing a meeting.
- B. Myers, Robert G. Malkin, M. Bett, A. Waibel, Benjamin Bostwick, Robert C. Miller, Jie Yang, Matthias Denecke, Edgar Seemann, Jie Zhu, Choon Hong Peck, Dave Kong, Jeffrey Nichols, W. Scherlis. 2002. Flexi-modal and multi-machine user interfaces. Abstract: We describe our system which facilitates collaboration using multiple modalities, including speech, handwriting, gestures, gaze tracking, direct manipulation, large projected touch-sensitive displays, laser pointer tracking, regular monitors with a mouse and keyboard, and wireless networked handhelds. Our system allows multiple, geographically dispersed participants to simultaneously and flexibly mix different modalities using the right interface at the right time on one or more machines. We discuss each of the modalities provided, how they were integrated in the system architecture, and how the user interface enabled one or more people to flexibly use one or more devices.
- Qin Jin, Tanja Schultz, A. Waibel. 2002. Speaker identification using multilingual phone strings. Abstract: Far-field speaker identification is very challenging since varying recording conditions often result in un-matching training and testing situations. Although the widely used Gaussian Mixture Models (GMM) approach achieves reasonable good results when training and testing conditions match, its performance degrades dramatically under un-matching conditions. In this paper we propose a new approach for far-field speaker identification: the usage of multilingual phone strings derived from phone recognizers in eight different languages. The experiments are carried out on a database of 30 speakers recorded with eight different microphone distances. The results show that the multi-lingual phone string approach is robust against un-matching conditions and significantly outperforms the GMMs. On 10-second test chunks, the average closed-set identification performance achieves 96.7% on variable distance data.
- Zhirong Wang, Umut Topkara, Tanja Schultz, A. Waibel. 2002. Towards universal speech recognition. Abstract: The increasing interest in multilingual applications like speech-to-speech translation systems is accompanied by the need for speech recognition front-ends in many languages that can also handle multiple input languages at the same time. We describe a universal speech recognition system that fulfills such needs. It is trained by sharing speech and text data across languages and thus reduces the number of parameters and overhead significantly at the cost of only slight accuracy loss. The final recognizer eases the burden of maintaining several monolingual engines, makes dedicated language identification obsolete and allows for code-switching within an utterance. To achieve these goals we developed new methods for constructing multilingual acoustic models and multilingual n-gram language models.
- K. Zechner, A. Waibel. 2002. Using Chunk Based Partial Parsing of Spontaneous Speech in Unrestricted Domains for Reducing Word Error Rate in Speech Recognition. Abstract: In this paper, we present a chunk based partial parsing system for spontaneous, conversational speech in unrestricted domains. We show that the chunk parses produced by this parsing system can be usefully applied to the task of reranking Nbest lists from a speech recognizer, using a combination of chunk-based n-gram model scores and chunk coverage scores. The input for the system is Nbest lists generated from speech recognizer lattices. The hypotheses from the Nbest lists are tagged for part of speech, "cleaned up" by a preprocessing pipe, parsed by a part of speech based chunk parser, and rescored using a backpropagation neural net trained oll the chunk based scores. Finally, the reranked Nbest lists are generated. The results of a system evaluation are promising in that a chunk accuracy of 87.4% is achieved and the best performance on a randomly selected test set is a decrease in word error rate of 0.3 percent (absolute), measured on the new first hypotheses in the reranked Nbest lists. 1 I n t r o d u c t i o n In the area of parsing spontaneous speech, most work so far has primarily focused on dealing with texts within a narrow, well-defined domain. Full scale parsers for spontaneous speech face severe difficulties due to the intrinsic nature of spoken language (e.g., false starts, hesitations, ungrammaticalities), in addition to the well-known complexities of large coverage parsing systems in general (Lavie, 1996; Light, 1996). An even more serious problem is the imperfect word accuracy of speech recognizers, particularly when faced with spontaneous speech over a large vocabuhtry and over a low bandwidth channel. This is particularly the case for the SWITCHBOARD database (Godfrey et al., 1992) which we mainly used for development, testing, and evaluation of our ~,;ystem. Current state-of-the-art recognizers exhibit word error rates (WER 1) for this corpus of approx1The word error rate (WER in %) is defined as follows: K l a u s Z e c h n e r and A l e x W a i b e l Language Technologies I n s t i t u t e Carnegie Mellon Univers i ty
- Manuel Kauers, S. Vogel, C. Fügen, A. Waibel. 2002. Interlingua based statistical machine translation. Abstract: In goal oriented spoken language translation, an interlingua based approach has proven quite useful as it (1) reduces overall effort when multiple language pairs are required, (2) can provide a para-phrase of semantic equivalence in the input language, (3) abstracts away from the dis ﬂ uencies of spoken language to express the speaker’s intention. On the other hand, interlingua based systems are cumbersome to develop as semantic grammars have to be labo-riously prepared for each input language. In this paper, we demonstrate that mappings from input text to interlingua can be learned automatically and that new input languages can be added by language projection. We show that the resulting system also delivers competitive performance.
- H. Holzapfel, C. Fügen, Matthias Denecke, A. Waibel. 2002. Integrating emotional cues into a framework for dialogue management. Abstract: Emotions are very important in human-human communication but are usually ignored in human-computer interaction. Recent work focuses on recognition and generation of emotions as well as emotion driven behavior. Our work focuses on the use of emotions in dialogue systems that can be used with speech input or as well in multi-modal environments. We describe a framework for using emotional cues in a dialogue system and their informational characterization. We describe emotion models that can be integrated into the dialogue system and can be used in different domains and tasks. Our application of the dialogue system is planned to model multi-modal human-computer-interaction with a humanoid robotic system.
- O. Kwon, A. Waibel. 2002. Korean Broadcast News Transcription Using Morpheme-based Recognition Units. Abstract: Broadcast news transcription is one of the hardest tasks in speech recognition because broadcast speech signals have much variability in speech quality, channel and background conditions. We developed a Korean broadcast news speech recognizer. We used a morpheme-based dictionary and a language model to reduce the out-of-vocabulary (OOV) rate. We concatenated the original morpheme pairs of short length or high frequency in order to reduce insertion and deletion errors due to short morphemes. We used a lexicon with multiple pronunciations to reflect inter-morpheme pron 나 nciation variations without severe modification of the search tree. By using the merged morpheme as recognition units, we achieved the OOV rate of 1.7% comparable to European languages with 64k vocabulary. We implemented a hidden Markov model-based recognizer with vocal tract length normalization and online speaker adaptation by maximum likelihood linear regression. Experimental results showed that the recognizer yielded 21.8% morpheme error rate for anchor speech and 31.6% for mostly noisy reporter speech.
- Chiori Hori, S. Furui, Robert G. Malkin, Hua Yu, A. Waibel. 2002. Automatic speech summarization applied to English broadcast news speech. Abstract: This paper reports an automatic speech summarization method and experimental results using English broadcast news speech. In our proposed method, a set of words maximizing a summarization score indicating an appropriateness of summarization is extracted from automatically transcribed speech. This extraction is performed using a Dynamic Programming (DP) technique according to a target compression ratio. We have previously tested the performance of our method using Japanese broadcast news speech. Since our method is based on a statistical approach, it could be applied to any language. In this paper, English broadcast news speech transcribed using a speech recognizer is automatically summarized. In order to apply our method to English, the model of estimating word concatenation probabilities based on a dependency structure in the original speech given by a Stochastic Dependency Context Free Grammar (SDCFG) is modified. A summarization method for multiple utterances using two-level DP technique is also proposed.
- A. Waibel, M. Bett, Florian Metze, K. Ries, Thomas Schaaf, Tanja Schultz, H. Soltau, Hua Yu, K. Zechner. 2001. Advances in automatic meeting record creation and access. Abstract: Oral communication is transient, but many important decisions, social contracts and fact findings are first carried out in an oral setup, documented in written form and later retrieved. At Carnegie Mellon University's Interactive Systems Laboratories we have been experimenting with the documentation of meetings. The paper summarizes part of the progress that we have made in this test bed, specifically on the question of automatic transcription using large vocabulary continuous speech recognition, information access using non-keyword based methods, summarization and user interfaces. The system is capable of automatically constructing a searchable and browsable audio-visual database of meetings and provide access to these records.
- J. McDonough, Florian Metze, H. Soltau, A. Waibel. 2001. Speaker compensation with sine-log all-pass transforms. Abstract: In previous work, we proposed the rational all-pass transform (RAPT) as the basis of a speaker adaptation scheme intended for use with a large vocabulary speech recognition system. It was shown that RAPT-based adaptation reduces to a linear transformation of cepstral means, much like the better known maximum likelihood linear regression (MLLR). In a set of speech recognition experiments conducted on the Switchboard Corpus, we obtained a word error rate (WER) of 37.9% using RAPT adaptation, a significant improvement over the 39.5% WER achieved with MLLR. In the present work, we propose the sine-log all-pass transform (SLAPT) as a replacement for the RAPT. Our findings indicate the SLAPT is just as effective as the RAPT at reducing WER when used as the basis for a variety of speaker compensation schemes, but in addition conduces to far more tractable computation of transformed cepstral sequences, and the estimation of optimal transform parameters.
- A. Waibel, Lori S. Levin. 2001. Published in "Proceedings of ICSLP-96" Translation of Conversational Speech with JANUS-II. Abstract: In this paper we investigate the possibility of translating continuous spoken conversations in a cross-talk environment. This is a task known to be difficult for human translators due to several factors. It is characterized by rapid and even overlapping turn-taking, a high degree of co-articulation, and fragmentary language. We describe experiments using both push-to-talk as well as cross-talk recording conditions. Our results indicate that conversational speech recognition and translation is possible, even in a free crosstalk environment. To date, our system has achieved performances of over 80% acceptable translations on transcribed input, and over 70% acceptable translations on speech input recognized with a 70-80% word accuracy. The system’s performance on spontaneous conversations recorded in a cross-talk environment is shown to be as good and even slightly superior to the simpler and easier push-to-talk scenario.
- L. Tomokiyo, A. Waibel. 2001. Adaptation Methods For Non-Native Speech. Abstract: T ,VCSR per-fonnanc:e is c:onsist.cnLly poor orr low-prolkiency non-nat.ive speech. \-Vhik gainH rrurn HpeakcT adapl.a tion can oft.en bring recognizer performance on high proficiency non-native speaker6 clo6e to that seen -for native speakers [12], recognition for lower-proficiency speakers remains low even after individual speaker adap tation [2]. The challenge for accent adaptation is to rnaxirni:,e rn:ogrriz.er pe,r-forrnanc:c wil.houL collcding large ;1rno1rnt.s or ;u:oustic dal.a ror caclr n,1tivc-l;rng11,1ge/t.;u·gct language pair. In this paper, we focus on adaptation for lower-proficiency speakers, exploring how acou6tic data from up to 15 adaptation speakers c~n be put to its most effective use.
- K. Ries, A. Waibel. 2001. Activity detection for information access to oral communication. Abstract: Oral communication is ubiquitous and carries important information yet it is also time consuming to document. Given the development of storage media and networks one could just record and store a conversation for documentation. The question is, however, how an interesting information piece would be found in a large database. Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance. An alternative index could be the activity such as discussing, planning, informing, story-telling, etc. This paper addresses the problem of the automatic detection of those activities in meeting situation and everyday rejoinders. Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger database and detect those automatically which is shown on a large database of TV shows. Emotions and other indices such as the dominance distribution of speakers might be available on the surface and could be used directly. Despite the small size of the databases used some results about the effectiveness of these indices can be obtained.
- Jie Yang, Jiang Gao, Y. Zhang, A. Waibel. 2001. Towards Automatic Sign Translation. Abstract: Signs are everywhere in our lives. They make our lives easier when we are familiar with them. But sometimes they also pose problems. For example, a tourist might not be able to understand signs in a foreign country. In this paper, we present our efforts towards automatic sign translation. We discuss methods for automatic sign detection. We describe sign translation using example based machine translation technology. We use a user-centered approach in developing an automatic sign translation system. The approach takes advantage of human intelligence in selecting an area of interest and domain for translation if needed. A user can determine which sign is to be translated if multiple signs have been detected within the image. The selected part of the image is then processed, recognized, and translated. We have developed a prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream.
- Tanja Schultz, A. Waibel, M. Bett, Florian Metze, Yue Pan, K. Ries, Thomas Schaaf, H. Soltau, M. Westphal, Hua Yu, K. Zechner. 2001. The ISL Meeting Room System. Abstract: Oral communication is transient but many important decisions, social contracts and fact findings are first carried out in an oral setup, documented in written form and later retrieved. At Carnegie Mellons University’s Interactive Systems Laboratories we have been experimenting with the documentation of meetings. This paper summarizes part of the progress that we have made in this test bed, specifically on the question of automatic transcription using LVCSR, information access using non-keyword based methods, summarization and user interfaces. The system is capable to automatically construct a searchable and browsable audiovisual database of meetings and provide access to these records.
- Jie Yang, Jiang Gao, Ying Zhang, Xilin Chen, A. Waibel. 2001. An automatic sign recognition and translation system. Abstract: A sign is something that suggests the presence of a fact, condition, or quality. Signs are everywhere in our lives. They make our lives easier when we are familiar with them. But sometimes they pose problems. For example, a tourist might not be able to understand signs in a foreign country. This paper discusses problems of automatic sign recognition and translation. We present a system capable of capturing images, detecting and recognizing signs, and translating them into a target language. We describe methods for automatic sign extraction and translation. We use a user-centered approach in system development. The approach takes advantage of human intelligence if needed and leverage human capabilities. We are currently working on Chinese sign translation. We have developed a prototype system that can recognize Chinese sign input from a video camera that is a common gadget for a tourist, and translate the signs into English or voice stream. The sign translation, in conjunction with spoken language translation, can help international tourists to overcome language barriers. The technology can also help a visually handicapped person to increase environmental awareness.
- R. Stiefelhagen, Jie Yang, A. Waibel. 2001. Tracking Focus of Attention for Human-Robot Communication. Abstract: In an intelligent working space, social robots should be capable of detecting and understanding human communicative cues. An important cue in human communication is focus of attention expressed by gaze direction. We have been developing technologies for gaze tracking and focus of attention modeling. In this paper we present our work on modeling focus of attention in meeting situations. We employ neural networks to estimate a persons head pose from camera images, and a probabilistic model to identify interesting targets in the scene based on the observed head pose. We are extending such technologies in building a gaze-aware human-friendly robot that is able to monitor a person's focus of attention.
- A. Waibel, Hua Yu, Tanja Schultz, Yue Pan, M. Bett, M. Westphal, H. Soltau, Thomas Schaaf, Florian Metze. 2001. Advances in meeting recognition. Abstract: Speech recognition has advanced considerably, but has been limited almost entirely either to situations in which close speaking microphones are natural and acceptable (telephone, dictation, command&control, etc.) or in which high-quality recordings are ensured. Furthermore, most recognition applications involve controlled recording environments, in which the user turns the recognition event on and off and speaks cooperatively for the purpose of being recognized.
- M. Westphal, A. Waibel. 2001. Model-combination-based acoustic mapping. Abstract: We propose a method for compensating distortions in the speech signal caused by environment changes. The basic method concentrates on additive noise, but can be extended to address also channel and to some extent speaker changes. By combining compensation with adaptation techniques it leads to high error rate reductions for mobile speech applications. Thereby, it is more efficient than adapting the acoustic model of the recognizer and more powerful than simple noise reduction techniques.
- R. Stiefelhagen, Jie Yang, A. Waibel. 2001. Estimating focus of attention based on gaze and sound. Abstract: Estimating a person's focus of attention is useful for various human-computer interaction applications, such as smart meeting rooms, where a user's goals and intent have to be monitored. In work presented here, we are interested in modeling focus of attention in a meeting situation. We have developed a system capable of estimating participants' focus of attention from multiple cues. We employ an omnidirectional camera to simultaneously track participants' faces around a meeting table and use neural networks to estimate their head poses. In addition, we use microphones to detect who is speaking. The system predicts participants' focus of attention from acoustic and visual information separately, and then combines the output of the audio- and video-based focus of attention predictors. We have evaluated the system using the data from three recorded meetings. The acoustic information has provided 8% error reduction on average compared to using a single modality.
- H. Soltau, Thomas Schaaf, Florian Metze, A. Waibel. 2001. The ISL evaluation system for Verbmobil-II. Abstract: Describes the 2000 ISL large vocabulary speech recognition system for fast decoding of conversational speech which was used in the German Verbmobil-II project. The challenge of this task is to build robust acoustic models to handle different dialects, spontaneous effects, and crosstalk as occur in conversational speech. We present speaker incremental normalization and adaptation experiments close to real-time constraints. To reduce the number of consequential errors caused by out-of-vocabulary words, we conducted filler-model experiments to handle unknown proper names. The overall improvements from 1998 to 2000 resulted in a word error reduction from 40% to 17% on our development test set.
- Jiang Gao, Jie Yang, Ying Zhang, A. Waibel. 2001. Text Detection and Translation from Natural Scenes. Abstract: Abstract : The authors present a system for automatic extraction and interpretation of signs from a natural scene. The system is capable of capturing images, detecting and recognizing signs, and translating them into a target language. The translation can be displayed on a hand-held wearable display or a head-mounted display. It can also be synthesized as a voice output message over the earphones. The paper addresses challenges in automatic sign extraction and translation, describes methods for automatic sign extraction, and extends example-based machine translation technology for sign translation. The authors use a user-centered approach in system development that takes advantage of human intelligence and leverages human capabilities. They are currently working on Chinese sign translation. So far, they have developed a prototype system that can recognize Chinese signs from a video camera and then translate them either into English text or a voice stream. They have built a database containing about 800 Chinese signs for development and evaluation. The authors hope that the sign translation, in conjunction with spoken language translation, will help international tourists overcome language barriers. The technology could also help a visually handicapped person increase his or her environmental awareness.
- Tanja Schultz, A. Waibel. 2001. ON CROSS-LANGUAGE ACOUSTIC MODELING. Abstract: With thedistribution of speechproductsall over theworld, theportability to new target languagesbecomesa practical concern. As a consequenceour researchfocuseson rapid transferof LVCSR systemsto other languages.In former studieswe evaluatedthe performanceif limited adaptation datais available.Particularlyfor verytimeconstrainedtasks andminority languages, it is evenreasonablethatno training datais availableat all. In this paperwe examinewhat performancecanbe expectedin this scenario.All experimentsarerunin theframeworkof theGlobalPhone project which investigatesLVCSRsystemsin 15 languages.
- B. Suhm, B. Myers, A. Waibel. 2001. Multimodal error correction for speech user interfaces. Abstract: Although commercial dictation systems and speech-enabled telephone voice user interfaces have become readily available, speech recognition errors remain a serious problem in the design and implementation of speech user interfaces. Previous work hypothesized that switching modality could speed up interactive correction of recognition errors. This article presents multimodal error correction methods that allow the user to correct recognition errors efficiently without keyboard input. Correction accuracy is maximized by novel recognition algorithms that use context information for recognizing correction input. Multimodal error correction is evaluated in the context of a prototype multimodal dictation system. The study shows that unimodal repair is less accurate than multimodal error correction. On a dictation task, multimodal correction is faster than unimodal correction by respeaking. The study also provides empirical evidence that system-initiated error correction (based on confidence measures) may not expedite error correction. Furthermore, the study suggests that recognition accuracy determines user choice between modalities: while users initially prefer speech, they learn to avoid ineffective correction modalities with experience. To extrapolate results from this user study, the article introduces a performance model of (recognition-based) multimodal interaction that predicts input speed including time needed for error correction. Applied to interactive error correction, the model predicts the impact of improvements in recognition technology on correction speeds, and the influence of recognition accuracy and correction method on the productivity of dictation systems. This model is a first step toward formalizing multimodal interaction.
- C. Fügen, M. Westphal, Mike Schneider, Tanja Schultz, A. Waibel. 2001. LingWear: A Mobile Tourist Information System. Abstract: In this paper, we describe LingWear, a mobile tourist information system that allows uninformed users to find their way around in foreign cities and to ask for information about sights, accommodations, and other places of interest. The user can communicate with LingWear either by means of spontaneous speech queries or via a touch screen. LingWear automatically decides whether to respond through the integrated speech synthesis or display messages. LingWear is currently available for the cities of Heidelberg and Karlsruhe. It was designed to run on wearable computer, e.g. the Xybernaut family, and is available in both Windows and Linux versions.
- H. Soltau, Florian Metze, C. Fugen, A. Waibel. 2001. A one-pass decoder based on polymorphic linguistic context assignment. Abstract: In this study, we examine how fast decoding of conversational speech with large vocabularies profits from efficient use of linguistic information, i.e. language models and grammars. Based on a re-entrant single pronunciation prefix tree, we use the concept of linguistic context polymorphism to allow an early incorporation of language model information. This approach allows us to use all available language model information in a one-pass decoder, using the same engine to decode with statistical n-gram language models as well as context free grammars or re-scoring of lattices in an efficient way. We compare this approach to our previous decoder, which needed three passes to incorporate all available information. The results on a very large vocabulary task show that the search can be speeded up by almost a factor of three, without introducing additional search errors.
- A. Lavie, C. Langley, A. Waibel, F. Pianesi, G. Lazzari, Paolo Coletti, Loredana Taddei, Franco Balducci. 2001. Architecture and Design Considerations in NESPOLE!: a Speech Translation System for E-commerce Applications. Abstract: NESPOLE! is a speech-to-speech machine translation research project funded jointly by the European Commission and the US NSF. The main goal of the NESPOLE! project is to advance the state-of-the-art of speech-to-speech translation in a real-world setting of common users involved in e-commerce applications. The project is a collaboration between three European research labs (IRST in Trento Italy, ISL at University of Karlsruhe in Germany, CLIPS at UJF in Grenoble France), a US research group (ISL at Carnegie Mellon in Pittsburgh) and two industrial partners (APT - the Trentino provincial tourism bureau, and Aethra - an Italian tele-communications commercial company). The speech-to-speech translation approach taken by the project builds upon previous work that the research partners conducted within the context of the C-STAR consortium (see http://www.c-star.org). The prototype system developed in NESPOLE! is intended to provide effective multi-lingual speech-to-speech communication between all pairs of four languages (Italian, German, French and English) within broad, but yet restricted domains. The first showcase currently under development is in the domain of tourism and travel information.
- Tanja Schultz, A. Waibel. 2001. Experiments on cross-language acoustic modeling. Abstract: With the distribution of speech products all over the world, the portability to new target languages becomes a practical concern. As a consequence our research focuses on rapid transfer of LVCSR systems to other languages. In former studies we evaluated the performance if limited adaptation data is available. Particularly for very time constrained tasks and minority languages, it is even reasonable that no training data is available at all. In this paper we examine what performance can be expected in this scenario. All experiments are run in the framework of the GlobalPhone project which investigates LVCSR systems in 15 languages.
- M. Bett, R. Gross, Hua Yu, Xiaojin Zhu, Yue Pan, Jie Yang, A. Waibel. 2000. Multimodal Meeting Tracker. Abstract: Face-to-face meetings usually encompass several modalities including speech, gesture, handwriting, and person identification. Recognition and integration of each of these modalities is important to create an accurate record of a meeting. However, each of these modalities presents recognition difficulties. Speech recognition must be speaker and domain independent, have low word error rates, and be close to real time to be useful. Gesture and handwriting recognition must be writer independent and support a wide variety of writing styles. Person identification has difficulty with segmentation in a crowded room. Furthermore, in order to produce the record automatically, we have to solve the assignment problem (who is saying what), which involves people identification and speech recognition. We follow a multimodal approach for people identification to increase the robustness (with the modules: color appearance id, face id and speaker id). This paper will examine a meeting room system under development at Carnegie Mellon University that enables us to track, capture and integrate the important aspects of a meeting from people identification to meeting transcription. Once a multimedia meeting record is created, it can be archived for later retrieval. This paper will review our meeting browser that we have developed which facilitates tracking and reviewing meetings.
- Qin Jin, A. Waibel. 2000. A na ve de-lambing method for speaker identification. Abstract: This paper addresses the issue of close-set text-independent speaker identification from speech samples recorded over telephone. We have known that the speaker identification performance variability can be attributed to many factors. One major factor is the inherent differences in the recognizability of different speakers. In speaker recognition systems such differences are characterized by the use of animal names for different types of speakers. In this paper we use lambs to refer to those speakers who are particularly easy to imitate in our closeset text-independent speaker identification system. That is, other speakers are much more likely to be recognized as these lamb speakers when they cannot be correctly recognized. Lambs adversely affect our close-set text-independent speaker identification performance a lot. In this paper we describe a naive de-lambing method to deal with these lamb speakers so as to improve our system performance. The speech data of our close-set speaker identification system is from the NIST 1999 Speaker Recognition Evaluation. Our experiments were conducted on 230 male speakers. We tried both testing from same telephone channels and sessions with training and different telephone channels and sessions with training for each speaker. Combined, the method developed in this paper result in a 15% relative improvement on the close-set 45-second training 10-second testing condition.
- H. Soltau, A. Waibel. 2000. Phone dependent modeling of hyperarticulated effects#. Abstract: In spoken dialogue systems, hyperarticulation occur as an effect to recover previous recognition errors. It is commonly observed that in particular real users ap ply similar recovery strategies as in human-human in teractions. Previous studies have shown that current speech rccogniwr cannot handle hypmarticulated speech. As a.n effect of higher ,vord error rates at hyperartic ulated speech, humans try to reinforce this speaking style which result in even more recognition errors. In this paper, we present approaches to build robust acoustic models for hypcrarticulatcd speech. The key point is that the changes of acoustic features at hyper articulation is a phone dependent effect. The idea is to use the likelihood criterion to decide, which phones should be treated separately. This can be done by in corporating dynamic questions about hyperarticula tion into the clustering stage. Based on such phonetic decision tree, we can generate appropriate acoustic models. \Vith this method, we achieved a word error reduct.ion about 9% relative at. hyperarticulation.
- U. Meier, R. Stiefelhagen, Jie Yang, A. Waibel. 2000. Towards Unrestricted Lip Reading. Abstract: Lip reading provides useful information in speech perception and language understanding, especially when the auditory speech is degraded. However, many current automatic lip reading systems impose some restrictions on users. In this paper, we present our research efforts in the Interactive System Laboratory, towards unrestricted lip reading. We first introduce a top–down approach to automatically track and extract lip regions. This technique makes it possible to acquire visual information in real-time without limiting the user's freedom of movement. We then discuss normalization algorithms to preprocess images for different lightning conditions (global illumination and side illumination). We also compare different visual preprocessing methods such as raw image, Linear Discriminant Analysis (LDA), and Principle Component Analysis (PCA). We demonstrate the feasibility of the proposed methods by the development of a modular system for flexible human–computer interaction via both visual and acoustic speech. The system is based on an extension of the existing state-of-the-art speech recognition system, a modular Multiple State–Time Delayed Neural Network (MS–TDNN) system. We have developed adaptive combination methods at several different levels of the recognition network. The system can automatically track a speaker and extract his/her lip region in real-time. The system has been evaluated under different noisy conditions such as white noise, music, and mechanical noise. The experimental results indicate that the system can achieve up to 55% error reduction using visual information in addition to the acoustic signal.
- Tanja Schultz, A. Waibel. 2000. Language Portability in Acoustic Modeling. Abstract: With the distribution of speech technology products all over the world, the portability to new target languages becomes a practical concern. As a consequence our research focuses on the question of how to port LVCSR systems in a fast and efﬁcient way. More speciﬁcally we want to estimate acoustic models for a new target language using speech data from varied source languages, but only limited data from the target language. For this purpose we introduce different methods for multilingual acoustic model combination and a polyphone decision tree specialization procedure. Recognition results using language dependent, independent and language adaptive acoustic models are presented and discussed in the framework of our GlobalPhone project which investigates LVCSR systems in 15 languages.
- K. Zechner, A. Waibel. 2000. DIASUMM: Flexible Summarization of Spontaneous Dialogues in Unrestricted Domains. Abstract: In this paper, we present a summarization system for spontaneous dialogues which consists of a novel multi-stage architecture. It is specifically aimed at addressing issues related to the nature of the texts being spoken vs. written and being dialogical vs. monological. The system is embedded in a graphical user interface and was developed and tested on transcripts of recorded telephone conversations in English and Spanish (CALLHOME).
- A. Waibel, P. Geutner, L. Tomokiyo, Tanja Schultz, M. Woszczyna. 2000. Multilinguality in speech and spoken language systems. Abstract: Building modern speech and language systems currently requires large data resources such as texts, voice recordings, pronunciation lexicons, morphological decomposition information and parsing grammars. Based on a study of the most important differences between language groups, we introduce approaches to efficiently deal with the enormous task of covering even a small percentage of the world's languages. For speech recognition, we have reduced the resource requirements by applying acoustic model combination, bootstrapping and adaption techniques. Similar algorithms have been applied to improve the recognition of foreign accents. Segmenting language into appropriate units reduces the amount of data required to robustly estimate statistical models. The underlying morphological principles are also used to automatically adapt the coverage of our speech recognition dictionaries with the Hypothesis-Driven Lexical Adaptation (HDLA) algorithm. This reduces the out-of-vocabulary problems encountered in agglutinative languages. Speech recognition results are reported for the read GlobalPhone database and some broadcast news data. For speech translation, using a task-oriented Interlingua allows to build a system with N languages with linear, rather than quadratic effort. We have introduced a modular grammar design to maximize reusability and portability. End-to-end translation results are reported on a travel-domain task in the framework of C-STAR.
- R. Gross, M. Bett, Hua Yu, Xiaojin Zhu, Yue Pan, Jie Yang, A. Waibel. 2000. Towards a multimodal meeting record. Abstract: Face-to-face meetings usually encompass several modalities including speech, gesture, handwriting, and person identification. Recognition and integration of each of these modalities is important to create an accurate record of a meeting. However, each of these modalities presents recognition difficulties. Speech recognition must be speaker and domain independent, have low word error rates, and be close to real time to be useful. Gesture and handwriting recognition must be writer independent and support a wide variety of writing styles. Person identification has difficulty with segmentation in a crowded room. Furthermore, in order to produce the record automatically, we have to solve the assignment problem (who is saying what), which involves people identification and speech recognition. This paper examines a multimodal meeting room system under development at Carnegie Mellon University that enables us to track, capture and integrate the important aspects of a meeting from people identification to meeting transcription. Once a multimedia meeting record is created, it can be archived for later retrieval.
- Xiaojin Zhu, Jie Yang, A. Waibel. 2000. Segmenting hands of arbitrary color. Abstract: Hand segmentation is a prerequisite for many gesture recognition tasks. Color has been widely used for hand segmentation. However, many approaches rely on predefined skin color models. It is very difficult to predefine a color model in a mobile application where the light condition may change dramatically over time. We propose a novel statistical approach to hand segmentation based on Bayes decision theory. The proposed method requires no predefined skin color model. Instead it generates a hand color model and a background color model for a given image, and uses these models to classify each pixel in the image as either a hand pixel or a background pixel. Models are generated using a Gaussian mixture model with the restricted EM algorithm. Our method is capable of segmenting hands of arbitrary color in a complex scene. It performs well even when there is a significant overlap between hand and background colors, or when the user wears gloves. We show that the Bayes decision method is superior to a commonly used method by comparing their upper bound performance. Experimental results demonstrate the feasibility of the proposed method.
- K. Ries, Lori S. Levin, Liza Valle, A. Lavie, A. Waibel. 2000. Shallow Discourse Genre Annotation in CallHome Spanish. Abstract: The classification of speech genre is not yet an established task in language technologies. However we believe that it is a task that will become fairly important as large amounts of audio (and video) data become widely available. The technological cability to easily transmit and store all human interactions in audio and video could have a radical impact on our social structure. The major open question is how this information can be used in practical and beneficial ways. As a first approach to this question we are looking at issues involving information access to databases of human-human interactions. Classification by genre is a first step in the process of retrieving a document out of a large collection. In this paper we introduce a local notion of speech activities that are exist side-by-side in conversations that belong to speech-genre: While the genre of CallHome Spanish is personal telephone calls between family members the actual instances of these calls contain activities such as storytelling, advising, interrogation and so forth. We are presenting experimental work on the detection of those activities using a variety of features. We have also observed that a limited number of distinguised activities can be defined that describes most of the activities in this database in a precise way. Proceedings of the Second International Conference On Language Ressources And Evaluation, LREC 2000, Athens, Greece, 31st May-2nd June 2000
- T. Kemp, Michael Schmidt, M. Westphal, A. Waibel. 2000. Strategies for automatic segmentation of audio data. Abstract: In many applications, like indexing of broadcast news or surveillance applications, the input data consists of a continuous, unsegmented audio stream. Speech recognition technology, however, usually requires segments of relatively short length as input. For such applications, effective methods to segment continuous audio streams into homogeneous segments are required. In this paper, three different segmenting strategies (model-based, metric-based and energy-based) are compared on the same broadcast news test data. It is shown that model-based and metric-based techniques outperform the simpler energy-based algorithms. While model based segmenters achieve very high level of segment boundary precision, the metric-based segmenter preforms better in terms of segment boundary recall (RCL). To combine the advantages of both strategies, a new hybrid algorithm is introduced. For this, the results of a preliminary metric-based segmentation are used to construct the models for the final model-based segmenter run. The new hybrid approach is shown to outperform the other segmenting strategies.
- R. Stiefelhagen, Jie Yang, A. Waibel. 2000. Simultaneous tracking of head poses in a panoramic view. Abstract: In this paper we present an approach to simultaneously estimate gaze directions of multiple people in the view of a panoramic camera. Human faces are located and tracked using a probabilistic skin-color model and motion detection. Neural networks are used to estimate head poses of the detected faces. With this approach, it is possible to simultaneously track the locations of multiple people around a meeting table and estimate their gaze directions using only a panoramic camera. We have achieved an accuracy of 9 degrees for head pan estimation and 6 degrees for tilt estimation for a multi-user system.
- T. Kemp, A. Waibel. 2000. End to end evaluation of the ISL View4You broadcast news transcription and retrieval system. Abstract: 7-Oxabicycloheptane substituted amino prostaglandin analogs are provided having the structural formula wherein n is 1 to 5; R1 is CO2H, CO2 lower alkyl or and R2 is wherein R3 is lower alkyl, aralkyl, -NHalkyl or -NHaryl, or R2 is wherein q is 1 to 12, and R4 is lower alkyl, lower alkenyl, lower alkynyl, aryl, arylalkyl, lower alkoxy, aryloxy, arylalkyloxy, amino, alkylamino arylamino, arylalkylamino, lower alkyl-S-, aryl-S-, arylalkyl-S-, (wherein n' is 0, 1 or 2), alkylaminoalkyl, arylaminoalkyl, arylalkylaminoalkyl, alkoxyalkyl, aryloxyalkyl or arylalkoxyalkyl. The compounds are cardiovascular agents useful, for example, in the treatment of thrombotic disease and are useful in the preparation of radiolabelled analogs.
- Fei Huang, Jie Yang, A. Waibel. 2000. Dialogue management for multimodal user registration. Abstract: User registration refers to associating certain personal information with a user. It is widely used in hospitals, hotels and conferences. In this paper, we propose an approach to interactive user registration by combining face recognition, speech recognition and speech synthesis technologies together through an efficient dialogue manager. In order to minimize a user’s effort, we employ a new dialogue management model based on a finite state automaton (FSA), which uses a Baysian network to fuse the user’s information from multiple channels (e.g., face image, speech, records stored in a pre-constructed database) to reliably estimate the confidence about user identity. Instead of fixing weights, the FSA adjusts its weights dynamically by integrating partial information from multiple information sources. This is achieved by maximizing an objective function to determine an optimal action at each succeeding state according to current confidence and information cues. Thus the transition between states can be done along the shortest path from the initial state to the goal state. We have developed a multimodal user registration system to demonstrate the feasibility of the proposed approach.
- R. Gross, Jie Yang, A. Waibel. 2000. Growing Gaussian mixture models for pose invariant face recognition. Abstract: A major challenge for face recognition algorithms lies in the variance faces undergo while changing pose. This problem is typically addressed by building view dependent models based on face images taken from predefined head poses. However, it is impossible to determine all head poses beforehand in an unrestricted setting such as a meeting room, where people can move and interact freely. We present an approach to pose invariant face recognition. We employ Gaussian mixture models to characterize human faces and model pose variance with different numbers of mixture components. The optimal number of mixture components for each person is automatically learned from training data by growing the mixture models. The proposed algorithm is tested on real data recorded in a meeting room. The experimental results indicate that the new method outperforms standard eigenface and Gaussian mixture model approaches. Our algorithm achieved as much as 42% error reduction compared to the standard eigenface approach on the same test data.
- T. Polzin, A. Waibel. 2000. EMOTION-SENSITIVE HUMAN-COMPUTER INTERFACES. Abstract: People are polite to their computers. They are flattered by them, form teams with them and even interact emotionally with them. In their experiments, Reeves and Nass (The Media Equation, 1996) showed that humans impose their interpersonal behavioral patterns onto their computers. Thus, the design of humancomputer interfaces should reflect this observation in order to facilitate an effective communication. In order to build a human-computer interface that is sensitive to the user's expressed emotion, we investigated spectral, prosodic, and verbal cues in the user's utterance. Based on these cues, we showed that the classification system achieved accuracies comparable to human performance. Finally, we demonstrate how to integrate information about the expressed emotion into a dialog system. The dialog system employs different discourse strategies depending on the expressed emotion allowing for a natural and effective communication between the user and the system.
- Hua Yu, Takashi Tomokiyo, Zhirong Wang, A. Waibel. 2000. New developments in automatic meeting transcription. Abstract: In this paper we report on new developments in the automatic meeting transcription task. Unlike other types of speech (such as those found in Broadcast News and Switchboard), meetings are unique in their richer dynamics of human-to-human interaction. An intuitive “ﬁngernail” plot is proposed to visualize such turn-taking behavior. We will also show how recognition of short turns can be improved by building a language model tailored speciﬁcally for short turns. Out-Of-Vocabulary (OOV) words become a more salient problem in the meeting transcription task, as they are mostly topic words and proper names, lack of which not only causes Word Error Rate (WER) increase, but also limits further use of recognition hypotheses. We describe a prototype system which uses the Web as a source for vocabulary expansion, and present preliminary OOV retrieval results.
- R. Gross, Jie Yang, A. Waibel. 2000. Face recognition in a meeting room. Abstract: We investigate the recognition of human faces in a meeting room. The major challenges of identifying human faces in this environment include low quality of input images, poor illumination, unrestricted head poses and continuously changing facial expressions and occlusion. In order to address these problems we propose a novel algorithm, dynamic space warping (DSW). The basic idea of the algorithm is to combine local features under certain spatial constraints. We compare DSW with the eigenface approach on data collected from various meetings. We have tested both front and profile face images and images with two stages of occlusion. The experimental results indicate that the DSW approach outperforms the eigenface approach in both cases.
- Yue Pan, A. Waibel. 2000. The effects of room acoustics on MFCC speech parameter. Abstract: Automatic speech recognition systems attain high performance for close-talking applications, but they deteriorate significantly in distant-talking environment. The reason is the mismatch between training and testing conditions. We have carried out a research work for a better understanding of the effects of room acoustics on speech feature by comparing simultaneous recordings of close talking and distant talking speech utterances. The characteristics of two degrading sources, background noise and room reverberation are discussed. Their impacts on the spectrum are different. The noise affects on the valley of the spectrum while the reverberation causes the distortion at the peaks at the pitch frequency and its multiples. In the situation of very few training data, we attempt to choose the efficient compensation approaches in the spectrum, spectrum subband or cepstrum domain. Vector Quantization based model is used to study the influence of the variation on feature vector distribution. The results of speaker identification experiments are presented for both close-talking and distant talking data.
- H. Soltau, A. Waibel. 2000. Specialized acoustic models for hyperarticulated speech. Abstract: This study aims to improve the performance of automatic speech recognizers at hyperarticulated speech. Hyperarticulation often occur as a strategy to recover previous recognition errors in spoken dialogue systems. Contrary to this intention a significant performance degradation can be observed at hyperarticulation. In this paper we present an analysis of features that caused the performance loss. The average phone duration is nearby 20% longer. Pitch contour and fundamental frequency change significantly at hyperarticulation. We report on adapting acoustic and transition models to hyperarticulated speech. We achieved a word error reduction about 23% at hyperarticulation.
- K. Zechner, A. Waibel. 2000. Minimizing Word Error Rate in Textual Summaries of Spoken Language. Abstract: Automatic generation of text summaries for spoken language faces the problem of containing incorrect words and passages due to speech recognition errors. This paper describes comparative experiments where passages with higher speech recognizer confidence scores are favored in the ranking process. Results show that a relative word error rate reduction of over 10% can be achieved while at the same time the accuracy of the summary improves markedly.
- Tanja Schultz, A. Waibel. 2000. Polyphone decision tree specialization for language adaptation. Abstract: With the distribution of speech technology products all over the world, the fast and efficient portability to new target languages becomes a practical concern. The authors explore the relative effectiveness of adapting multilingual LVCSR systems to a new target language with limited adaptation data. For this purpose they introduce a polyphone decision tree specialization method. Several recognition results are presented based on mono- and multilingual recognizers. These recognizers are developed in the framework of the project GlobalPhone. In this project we investigate speech recognition in 15 languages: Arabic, Mandarin and Shanghai Chinese, Croatian, English, French, German, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish.
- Tanja Schultz, A. Waibel. 2000. Language adaptive LVCSR through Polyphone Decision Tree Specialization. Abstract: Abstract : With the distribution of speech technology products all over the world, the fast and efficient portability to new target languages be comes a practical concern. In this paper we explore the relative effectiveness of porting multilingual recognition systems to new target languages with very limited adaptation data. For this purpose we introduce a polyphone decision tree specialization method. Several recognition results are presented based on mono- and multilingual recognizers developed in the framework of the project GlobalPhone which investigates LVCSR systems in 15 languages.
- Hua Yu, A. Waibel. 2000. Streamlining the front end of a speech recognizer. Abstract: In this paper we seek to streamline various operations within the front end of a speech recognizer, both to reduce unnecessary computation and to simplify the conceptual framework. First, a novel view of the front end in terms of linear transformations is presented. Then we study the invariance property of recognition performance with respect to linear transformations (LT) at the front end. Analysis reveals that several LT steps can be consolidated into a single LT, which effectively eliminates the Discrete Cosine Transform (DCT) step, part of the traditional MFCC (Mel-Frequency Cepstral Coefﬁcient) front end. Moreover, a highly simpliﬁed, data-driven front-end scheme is proposed as a direct generalization of this idea. The new setup has no Mel-scale ﬁltering, another part of the MFCC front end. Experimental results show a 5% relative improvement on the Broadcast News task.
- T. Kemp, Manfred Weber, A. Waibel. 2000. End to end evaluation of the ISL View4You broadcast news transcription system. Abstract: In this paper, we introduce the Interactive Systems Laboratories video indexing and retrieval system 'View4You'. The main components of the system, namely the segmenter, the speech recognizer and the information retrieval engine, are described in detail. 
 
In the View4You system, public television newscasts are recorded on a daily basis. The newscasts are automatically segmented and an index is created for each of the segments by means of automatic speech recognition. The user can query the system in natural language. The system returns a list of segments which is sorted by relevance with respect to the user query. By selecting a segment, the user can watch the corresponding part of the news show on his or her computer screen. 
 
Several end to end evaluations on real world data, using questions from naive users, are described. By substituting each of the components of the system with a perfect (manually simulated) one, the effect of the components' imperfection on the end to end result can be determined. We show, that the information retrieval component has the largest impact on the system performance, followed by the segmentation. The quality of the speech recognizer, as long as its error rate is below approximately 25%, is shown to have only a relatively small importance.
- M. Westphal, A. Waibel. 1999. Towards spontaneous speech recognition for on-board car navigation and information systems. Abstract: Speech recognition is seen to be of great benefit in on-board car navigation systems and assistance. The command word approach will be used for applications in the near future since the small active vocabulary and the hierarchical structure is much easier to cope with, from the developers’ side. An alternative approach, using spontaneous speech input, is far more complex but provides the user with an interface that is very intuitive and has fewer restrictions. The user can rely upon his or her experience in inter-human communication and utter spontaneous queries. In this paper, we describe the requirements and the collection of a continuous car speech data base and show first recognition results obtained under different environmental conditions in the car.
- Mari Ostendorf, B. Byrne, M. Bacchiani, M. Finke, A. Gunawardana, K. Ross, S. Roweis, Elizabeth Shriberg, D. Talkin, A. Waibel, B. Wheatley, T. Zeppenfeld. 1999. Modeling Systematic Variations in Pronunciation via a Language-Dependent Hidden Speaking Mode. Abstract: This paper describes the research efforts of the “Hidden Speaking Mode” group participating in the 1996 summer workshop on speech recognition. The goal of this project is to model pronunciation variations that occur in conversational speech in general and, more specifically, to investigate the use of a hidden speaking mode to represent systematic variations that are correlated with the word sequence (e.g. predictable from syntactic structure). This paper describes the theoretical formulation of hidden mode modeling, as well as some results in error analysis, language modeling and pronunciation modeling.
- W. Minker, Marsal Gavaldà, A. Waibel. 1999. Stochastically-based semantic analysis for machine translation. Abstract: Abstract We report our experience of applying a stochastic method for understanding natural language to a multilingual appointment scheduling task, in particular, to the English spontaneous speech task (ESST). The aim of the spoken language systems developed for this task is to translate spontaneous conversational speech among different languages. We have investigated the portability of a stochastic semantic analyser from a setting of human–machine interactions air travel information services (ATIS) and multimodal multimedia automated service kiosk (MASK) into the more open one of human-to-human interactions (ESST).
- J. McDonough, Thomas Schaaf, A. Waibel. 1999. Speaker adaptation with all-pass transforms. Abstract: In previous work, a class of transforms were proposed which achieve a remapping of the frequency axis much like conventional vocal tract length normalization. These mappings, known collectively as all-pass transforms (APT), were shown to produce substantial improvements in the performance of a large vocabulary speech recognition system when used to normalize incoming speech prior to recognition. In this application, the most advantageous characteristic of the APT was its cepstral-domain linearity; this linearity makes speaker normalization simple to implement, and provides for the robust estimation of the parameters characterizing individual speakers. In the current work, we exploit the APT to develop a speaker adaptation scheme in which the cepstral means of a speech recognition model are transformed to better match the speech of a given speaker. In a set of speech recognition experiments conducted on the Switchboard corpus, we report reductions in word error rate of 3.7% absolute.
- B. Suhm, B. Myers, A. Waibel. 1999. Model-based and empirical evaluation of multimodal interactive error correction. Abstract: Our research addresses the problem of error correction in speechuser interfaces. Previous work hypothesized that switching modalitycould speed up interactive correction of recognition errors(so-called multimodal error correction). We present a user studythat compares, on a dictation task, multimodal error correctionwith conventional interactive correction, such as speaking again,choosing Tom a list, and keyboard input. Results show thatmultimodal correction is faster than conventional correctionwithout keyboard input, but slower than correction by typing forusers with good typing skills. Furthermore, while users initiallyprefer speech, they learn to avoid ineffective correctionmodalities with experience. To extrapolate results from this userstudy we developed a performance model of multimodal interactionthat predicts input speed including time needed for errorcorrection. We apply the model to estimate the impact ofrecognition technology improvements on correction speeds and theinfluence of recognition accuracy and correction method on theproductivity of dictation systems. Our model is a first steptowards formalizing multimodal (recognition-based) interaction.
- M. Finke, J. Fritsch, D. Koll, A. Waibel. 1999. Modeling and efficient decoding of large vocabulary conversational speech. Abstract: Capturing the large variability of conversational speech in the framework of purely phone based speech recognizers is virtually impossible. It has been shown earlier that suprasegmental features such as speaking rate, duration and syllabic, syntactic and semantic structure are important predictors of pronunciation variation. In order to allow for a tighter coupling of these predictors of pronunciation, duration and acoustic modeling a new recognition toolkit has been developed. The phonetic transcription of speech has been generalized to an attribute based representation, thus enabling the integra-tion of suprasegmental, non-phonetic features. A pronunciation model is trained to augment the attribute transcription to mark possible pronunciation e(cid:11)ects which are then taken into account by the acoustic model induction algorithm. A (cid:12)nite state machine single-pre(cid:12)x-tree, one-pass, time-synchronous decoder is presented that ef-(cid:12)ciently decodes highly spontaneous speech within this new representational framework.
- T. Kemp, A. Waibel. 1999. Unsupervised training of a speech recognizer: recent experiments. Abstract: Current speech recognition systems require large amounts of transcribed data for parameter estimation. The transcription, however, is tedious and expensive. In this work we describe our experiments which are aimed at training a speech recognizer with only a minimal amount (30 minutes) of transcriptions and a large portion (50 hours) of un-transcribed data. A recognizer is bootstrapped on the transcribed part of the data and initial transcripts are generated with it for the remainder (the untranscribed part). Using a lattice-based con(cid:12)dence measure, the recognition errors are (partially) detected and the remainder of the hypotheses is used for training. Using this scheme, the word error rate on a broadcast news speech recognition task dropped from more than 32.0% to 21.4%. In a cheating experiment we show, that this performance cannot be signi(cid:12)cantly improved by improving the measure of con(cid:12)dence. By combining the unsupervisedly trained system with our currently best recognizer which is trained on 15.5 hours of transcribed data, an additional error reduction of 5% relative (as compared to the system trained in a standard fashion) is possible.
- Jie Yang, Weiyi Yang, Matthias Denecke, A. Waibel. 1999. Smart Sight: a tourist assistant system. Abstract: In this paper, we present our efforts towards developing an intelligent tourist system. The system is equipped with a unique combination of sensors and software. The hardware includes two computers, a GPS receiver, a lapel microphone plus an earphone, a video camera and a head-mounted display. This combination includes a multimodal interface to take advantage of speech and gesture input to provide assistance for a tourist. The software supports natural language processing, speech recognition, machine translation, handwriting recognition and multimodal fusion. A vision module is trained to locate and read written language, is able to adapt to to new environments, and is able to interpret intentions offered by the user such as a spoken clarification or pointing gesture. We illustrate the applications of the system using two examples.
- R. Stiefelhagen, Jie Yang, A. Waibel. 1999. Modeling focus of attention for meeting indexing. Abstract: Visual cues, such as gesturing, looking at each other or monitoring each others facial expressions, play an important role in meetings. Such information can be used for indexing of multimedia meeting recordings. In this paper, we present an approach to detect who is looking at whom during a meeting. Our proposal is to employ Hidden Markov Models to characterize participants’ focus of attention by using gaze information as well as knowledge about the number and positions of people present in a meeting. The number and positions of the participants faces are detected in the field of view of a panoramic camera. We use neural networks to estimate the directions of participants’ gaze from camera images. We discuss the implementation of the approach in detail including system architecture, data collection, and evaluation. The system has achieved an accuracy rate of up to 93 % in detecting focus of attention on test sequences taken from meetings. We have used focus of attention as an index in a multimedia meeting browser.
- P. Geutner, M. Finke, A. Waibel. 1999. Selection criteria for hypothesis driven lexical adaptation. Abstract: Adapting the vocabulary of a speech recognizer to the utterance to be recognized has proven to be successful both in reducing high out-of-vocabulary as well as word error rates. This applies especially to languages that have a rapid vocabulary growth due to a large number of inflections and composita. This paper presents various adaptation methods within the hypothesis driven lexical adaptation (HDLA) framework which allow speech recognition on a virtually unlimited vocabulary. Selection criteria for the adaptation process are either based on morphological knowledge or distance measures at phoneme or grapheme level. Different methods are introduced for determining distances between phoneme pairs and for creating the large fallback lexicon the adapted vocabulary is chosen from. HDLA reduces the out-of-vocabulary-rate by 55% for Serbo-Croatian, 35% for German and 27% for Turkish. The reduced out-of-vocabulary rate also decreases the word error rate by an absolute 4.1% to 25.4% on Serbo-Croatian broadcast news data.
- Jürgen Reichert, Tanja Schultz, A. Waibel. 1999. Mandarin large vocabulary speech recognition using the globalphone database. Abstract: This paper presents our recent efforts in developing a speaker independent LVCSR engine for Mandarin Chinese using our multilingual database GlobalPhone. We describe a two pass approach, in which the recognition first generates Pinyin hypotheses and second transform these into Chinese character hypotheses. We show how this approach can reduce complexity and increase flexibility. We evaluate and compare different systems including different base units for speech recognition as phoneme units versus syllables. Furthermore we analyze the influence of tonal information. Our currently best system shows very promising results achieving 15.0 % character error rate.
- A. Waibel. 1999. Translation systems under the C-STAR framework. Abstract: This talk will review our work on Speech Translation under the recent worldwide C-STAR demonstration. C-STAR is the Consortium for Speech Translation Advanced Research and now includes 6 partners and 20 partner/affiliate laboratories around the world. The work demonstrated concludes the second phase of the consortium, which has focused on translating conversational spontaneous speech as opposed to well formed, well structured text. As such, much of the work has focused on exploiting semantic and pragmatic constraints derived from the task domain and dialog situation to produce an understandable translation. Six partners have connected their respective systems with each other and allowed travel related spoken dialogs to provide communication between each of them. A common Interlingua representation was developed and used between the partners to make this multilingual deployment possible. The systems were also complemented by the introduction of Web based shared workspaces that allow one user in one country to communicate pictures, documents, sounds, tables, etc. to the other over the Web while referring to these documents in the dialog. Some of the partners' systems were also deployed in wearable situations, such as a traveler exploring a foreign city. In this case speech and language technology was installed on a wearable computer with a small hand-held display. It was used to provide language translation as well as human-machine information access for the purpose of navigation (using GPS localization) and tour guidance. This combination of human-machine and human-machine-human dialogs could allow a user explore a foreign environment more effectively by resorting to human-machine and human-human dialogs wherever most appropriate.
- R. Stiefelhagen, Jie Yang, A. Waibel. 1999. Modeling people's focus of attention. Abstract: In this paper, we present an approach to model focus of attention of participants in a meeting via hidden Markov models (HMM). We employ HMM to encode and track focus of attention, based on the participants' gaze information and knowledge of their positions. The positions of the participants are detected by face tracking in the view of a panoramic camera mounted on the meeting table. We use neural networks to estimate the participants' gaze from camera images. We discuss the implementation of the approach in detail, including system architecture, data collection, and evaluation. The system has achieved an accuracy rate of up to 93% in detecting focus of attention on test sequences taken from meetings. We have used focus of attention as an index in a multimedia meeting browser.
- W. Minker, Marsal Gavaldà, A. Waibel. 1999. Hidden Understanding Models for Machine Translation. Abstract: We demonstrate the portability of a stochastic method for understanding natural language from a setting of human-machine interactions (ATIS - Air Travel Information Services and MASK Multimodal Multimedia Automated Service Kiosk) into the more open one of human-to-human interactions. The application we use is the English Spontaneous Speech Task (ESST) for multilingual appointment scheduling. Spoken language systems developed for this task translate spontaneous conversational speech among different languages.
- Max Ritter, U. Meier, Jie Yang, A. Waibel. 1999. Face translation: A multimodal translation agent. Abstract: In this paper, we present Face Translation, a translation agent for people who speak different languages. The system can not only translate a spoken utterance into another language, but also produce an audio-visual output with the speaker’s face and synchronized lip movement. The visual output is synthesized from real images based on image morphing technology. Both mouth and eye movements are generated according to linguistic and social cues. An automatic feature extracting system can automatically initialize the system. After initialization, the system can generate synchronized visual output based on a few pre-stored images. The system is useful for a video conference application with a limited bandwidth. We have demonstrated the system in a travel planning application where a foreign tourist plans a trip with a travel agent over the Internet in a multimedia collaborative working space using a multimodal interface.
- Jie Yang, Xiaojin Zhu, R. Gross, J. Kominek, Yue Pan, A. Waibel. 1999. Multimodal people ID for a multimedia meeting browser. Abstract: A meeting browser is a system that allows users to review a multimedia meeting record from a variety of indexing methods. Identification of meeting participants is essential for creating such a multimedia meeting record. Moreover, knowing who is speaking can enhance the performance of speech recognition and indexing meeting transcription. In this paper, we present an approach that identifies meeting participants by fusing multimodal inputs. We use face ID, speaker ID, color appearance ID, and sound source directional ID to identify and track meeting. After describing the different modules in detail, we will discuss a framework for combining the information sources. Integration of the multimodal people ID into the multimedia meeting browser is in its preliminary stage.
- Tanja Schultz, A. Waibel. 1999. Experiments towards a multi-language LVCSR interface. Abstract: This paper describes experiments towards a multi language human-computer speech interface. Our in te1:face is designed for lar:qe vocabulary contin·uous 8peech inp11,t. For thi8 p11,rpose a m11,ltiling11,al dictation database has been collected under GlobalPhone, which is a project at the Interactive Systems Labs. This project investigates LVCSR systems in 15 langua_qes of the world, namely Arabic, Chinese, Croatian, Eng lish, French, German, Italian, Japanese, Korean, Por tug·aese, Russian, Spanish, Swedish, Tamil, and Turk ish. Based on a global phoneme set we build differ ent multilingual speech recognizer and present several pe1formance results in language independent and lan g11,age adaptive setups.
- Hua Yu, M. Finke, A. Waibel. 1999. Progress in automatic meeting transcription. Abstract: In this paper we report recent developments on the meeting transcription task, a large vocabulary conversational speech recognition task. Previous experiments showed this is a very challenging task, with about 50% word error rate (WER) using existing recognizers. The difficulty mostly comes from highly disfluent/conversational nature of meetings, and lack of domain specific training data. For the first problem, our SWB(Switchboard) system — a conversational telephone speech recognizer — was used to recognize wide-band meeting data; for the latter, we leveraged the large amount of Broadcast News (BN) data to build a robust system. This paper will especially focus on two experiments in the BN system development: model combination and HMM topology/duration modeling. Model combination can be done at various stages of recognition: post-processing schemes such as ROVER can lead to significant improvements; to reduce computation we tried model combination at acoustic score level. We will also show the importance of temporal constraints in decoding, present some HMM topology/duration modeling experiments. Finally, the meeting browser system and meeting room setup will be reviewed.
- H. Kabré, A. Waibel. 1999. Navigating German cities by spontaneous French queries. Abstract: This paper reports our e(cid:11)orts on the adaptation of a baseline system trained on clean speech to a task for which French native speakers ut-tered some Spontaneous French queries while driving a car. When the system is retrained on the new task acoustic data the Word Error Rate (WER) is decreased by 60% compared to our baseline system initial performance on the new task. We show that on spontaneous queries, 1/4 of this improvement could be achieved without prior system retraining by a more accurate Language Modelling which takes into account the noises and spontaneous speech e(cid:11)ects and by a carefull grapheme/phoneme transcription of foreign words. We also describe the integration of this French system in our Multilingual Navigation System.
- Wolfgang Hürst, Jie Yang, A. Waibel. 1998. Interactive error repair for an online handwriting interface. Abstract: Current online handwriting recognition systems have very limited error recovery mechanisms. In this paper, we discuss the problem of error repair in an online handwriting interface. Based on user study of common repair patterns found in human handwriting, we propose an approach that allows users to recover from recognition errors. The basic idea is to handle the error repair at the interface level by interacting with users. The method requires few modifications on original recognition engine and imposes few restrictions on users. We have developed a prototype system to demonstrate the proposed concept and perform user study when the system provides error recovery mechanisms.
- Jie Yang, R. Stiefelhagen, U. Meier, A. Waibel. 1998. Visual tracking for multimodal human computer interaction. Abstract: In this paper, we present visual tracking techniques for multimodal human computer interaction. First, we discuss techniques for tracking human faces in which human skin-color is used as a major feature. An adaptive stochastic model has been developed to characterize the skin-color distributions. Based on the maximum likelihood method, the model parameters can be adapted for different people and different lighting conditions. The feasibility of the model has been demonstrated by the development of a real-time face tracker. The system has achieved a rate of 30-t- frames/second using a low-end workstation with a framegrabber and a camera. We also present a top-down approach for tracking facial features such as eyes, nostrils, and lip comers. These real-time visual tracking techniques have been successfully applied to many applications such as gaze tracking, and lipreading. The face tracker has been combined with a microphone array for extracting speech signal from a specific person. The gaze tracker has been combined with a speech recognizer in a multimodal interface for controlling a panoramic image viewer.
