Anatole Gershman
Paper count: 94
- Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer. 2023. The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing. Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person’s illness is labeled “diagnosis”), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.
- Lu´ıs Marujo, Ricardo Ribeiro, David Martins de Matos, Jo˜ao P Neto, A. Gershman, Jaime Carbonell. 2023. Repositório ISCTE-IUL. Abstract: ,
- Steven Y. Feng, Vivek Khetan, Bogdan Sacaleanu, A. Gershman, E. Hovy. 2022. CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models. Abstract: We motivate and introduce CHARD: Clinical Health-Aware Reasoning across Dimensions, to investigate the capability of text generation models to act as implicit clinical knowledge bases and generate free-flow textual explanations about various health-related conditions across several dimensions. We collect and present an associated dataset, CHARDat, consisting of explanations about 52 health conditions across three clinical dimensions. We conduct extensive experiments using BART and T5 along with data augmentation, and perform automatic, human, and qualitative analyses. We show that while our models can perform decently, CHARD is very challenging with strong potential for further exploration.
- Dheeraj Rajagopal, Vivek Khetan, Bogdan Sacaleanu, A. Gershman, Andy E. Fano, E. Hovy. 2021. Template Filling for Controllable Commonsense Reasoning. Abstract: Large-scale sequence-to-sequence models have shown to be adept at both multiple-choice and open-domain commonsense reasoning tasks. However, the current systems do not provide the ability to control the various attributes of the reasoning chain. To enable better controllability, we propose to study the commonsense reasoning as a template ﬁlling task (TemplateCSR) — where the language models ﬁlls reasoning templates with the given constraints as control factors. As an approach to TemplateCSR, we (i) propose a dataset of commonsense reasoning template-expansion pairs and (ii) introduce POTTER , a pretrained sequence-to-sequence model using prompts to perform commonsense reasoning across concepts. Our experiments show that our approach outperforms baselines both in generation metrics and factuality metrics. We also present a detailed error analysis on our approach’s ability to reliably perform commonsense reasoning 1 .
- Dheeraj Rajagopal, Vivek Khetan, Bogdan Sacaleanu, A. Gershman, Andy E. Fano, E. Hovy. 2021. Cross-Domain Reasoning via Template Filling. Abstract: In this paper, we explore the ability of sequence to sequence models to perform cross-domain reasoning. Towards this, we present a prompt-template-ﬁlling approach to enable sequence to sequence models to perform cross-domain reasoning. We also present a case-study with commonsense and health and well-being domains, where we study how prompt-template-ﬁlling enables pretrained sequence to sequence models across domains. Our experiments across several pretrained encoder-decoder models show that cross-domain reasoning is challenging for current models. We also show an in-depth error analysis and avenues for future research for reasoning across domains 1 .
- E. Hovy, J. Carbonell, Hans Chalupsky, A. Gershman, Alexander Hauptmann, Florian Metze, T. Mitamura, Zaid A. W. Sheikh, Ankit Dangi, Aditi Chaudhary, Xianyang Chen, Xiang Kong, Bernie Huang, Salvador Medina, H. Liu, Xuezhe Ma, Maria Ryskina, Ramon Sanabria, Varun Gangal. 2019. OPERA: Operations-oriented Probabilistic Extraction, Reasoning, and Analysis. Abstract: The OPERA system of CMU and USC/ISI performs end-to-end information extraction from multiple media and languages (English, Russian, Ukrainian), integrates the results, builds Knowledge Bases about the domain, and does hypothesis creation and reasoning to answer questions. 
- E. Hovy, Taylor Berg-Kirkpatrick, J. Carbonell, Hans Chalupsky, A. Gershman, Alexander Hauptmann, Florian Metze, T. Mitamura, Aditi Chaudhary, Xianyang Chen, Bernie Huang, H. Liu, Xuezhe Ma, Shruti Palaskar, Dheeraj Rajagopal, Maria Ryskina, Ramon Sanabria. 2018. OPERA: Operations-oriented Probabilistic Extraction, Reasoning, and Analysis. Abstract: This paper describes CMU and USC/ISI’s OPERA system that performs endto-end information extraction from multiple media, integrates results across English, Russian, and Ukrainian, produces Knowledge Bases containing the extracted information, and performs hypothesis reasoning over the results.
- Sz-Rung Shiang, Stephanie Rosenthal, A. Gershman, J. Carbonell, Jean Oh. 2017. Vision-Language Fusion for Object Recognition. Abstract: 
 
 While recent advances in computer vision have caused object recognition rates to spike, there is still much room for improvement. In this paper, we develop an algorithm to improve object recognition by integrating human-generated contextual information with vision algorithms. Specifically, we examine how interactive systems such as robots can utilize two types of context information--verbal descriptions of an environment and human-labeled datasets. We propose a re-ranking schema, MultiRank, for object recognition that can efficiently combine such information with the computer vision results. In our experiments, we achieve up to 9.4% and 16.6% accuracy improvements using the oracle and the detected bounding boxes, respectively, over the vision-only recognizers. We conclude that our algorithm has the ability to make a significant impact on object recognition in robotics and beyond.
 

- Sz-Rung Shiang, A. Gershman, Jean Oh. 2017. A Generalized Model for Multimodal Perception. Abstract: In order for autonomous robots and humans to effectively collaborate on a task, robots need to be able to perceive their environments in a way that is accurate and consistent with their human teammates. To develop such cohesive perception, robots further need to be able to digest human teammates’ descriptions of an environment to combine those with what they have perceived through computer vision systems. In this context, we develop a graphical model for fusing object recognition results using two different modalities–computer vision and verbal descriptions. In this paper, we specifically focus on three types of verbal descriptions, namely, egocentric positions, relative positions using a landmark, and numeric constraints. We develop a Conditional Random Fields (CRF) based approach to fuse visual and verbal modalities where we model n-ary relations (or descriptions) as factor functions. We hypothesize that human descriptions of an environment will improve robot’s recognition if the information can be properly fused. To verify our hypothesis, we apply our model to the object recognition problem and evaluate our approach on NYU Depth V2 dataset and Visual Genome dataset. We report the results on sets of experiments demonstrating the significant advantage of multimodal perception, and discuss potential real world applications of our approach.
- E. Liongosari, A. Gershman, Mitu Singh. 2016. A New Generation of Digital Library to Support Drug Discovery Research. Abstract: The recent explosion of publicly available biomedical information gave drug discovery researchers unprecedented access to a wide variety of online repositories, but the sheer volume of the available data diminishes its utility. This is compounded by the fact that these repositories suffer from a silo effect: data from one cannot be easily linked to data in another. This is true for both publicly available sources and internal sources such as project reports. The ability to explore all aspects of biological data and to link data across sources is beneficial, as it allows researchers to discover new knowledge and to identify new collaboration opportunities by exploiting links. This paper presents an approach to solving this problem and an application that allows researchers to browse and analyze disparate bio-medical repositories as one semantically integrated knowledge space.
- Yun-Nung (Vivian) Chen, Ming Sun, Alexander I. Rudnicky, A. Gershman. 2016. Unsupervised user intent modeling by feature-enriched matrix factorization. Abstract: Spoken language interfaces are being incorporated into various devices such as smart phones and TVs. However, dialogue systems may fail to respond correctly when users' request functionality is not supported by currently installed apps. This paper proposes a feature-enriched matrix factorization (MF) approach to model open domain intents, which allows a system to dynamically add unexplored domains according to users' requests. First we leverage the structured knowledge from Wikipedia and Freebase to automatically acquire domain-related semantics to enrich features of input utterances, and then MF is applied to model automatically acquired knowledge, published app textual descriptions and users' spoken requests in a joint fashion; this generates latent feature vectors for utterances and user intents without need of prior annotations. Experiments show that the proposed MF models incorporated with rich features significantly improve intent prediction, achieving about 34% of mean average precision (MAP) for both ASR and manual transcripts.
- Junjie Hu, Jean Oh, A. Gershman. 2016. Learning Phrasal Lexicons for Robotic Commands using Crowdsourcing. Abstract: Robotic commands in natural language usually contain lots of spatial descriptions which are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language. The main purpose of this paper is to simulate the interaction process between human and robots using crowdsourcing platforms, and investigate the possibility of translating natural language to robot language with paraphrases.
- Junjie Hu, Jean Oh, A. Gershman. 2016. Learning Lexical Entries for Robotic Commands using Crowdsourcing. Abstract: Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language. The main purpose of this paper is to simulate the interaction process between human and robots using crowdsourcing platforms, and investigate the possibility of translating natural language to robot language with paraphrases.
- Luís Marujo, José Portêlo, Wang Ling, David Martins de Matos, J. Neto, A. Gershman, J. Carbonell, I. Trancoso, B. Raj. 2015. Privacy-Preserving Multi-Document Summarization. Abstract: State-of-the-art extractive multi-document summarization systems are usually designed without any concern about privacy issues, meaning that all documents are open to third parties. In this paper we propose a privacy-preserving approach to multi-document summarization. Our approach enables other parties to obtain summaries without learning anything else about the original documents’ content. We use a hashing scheme known as Secure Binary Embeddings to convert documents representation containing key phrases and bag-of-words into bit strings, allowing the computation of approximate distances, instead of exact ones. Our experiments indicate that our system yields similar results to its non-private counterpart on standard multi-document evaluation datasets.
- Luís Marujo, Wang Ling, I. Trancoso, Chris Dyer, A. Black, A. Gershman, David Martins de Matos, J. Neto, J. Carbonell. 2015. Automatic Keyword Extraction on Twitter. Abstract: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differences between this domain and the work performed on other domains, such as news, which makes existing approaches for automatic keyword extraction not generalize well on Twitter datasets. These datasets include the small amount of content in each tweet, the frequent usage of lexical variants and the high variance of the cardinality of keywords present in each tweet. We propose methods for addressing these issues, which leads to solid improvements on this dataset for this task.
- D. Gupta, J. Carbonell, A. Gershman, S. Klein, David Miller. 2015. Unsupervised Phrasal Near-Synonym Generation from Text Corpora. Abstract: 
 
 Unsupervised discovery of synonymous phrases is useful in a variety of tasks ranging from text mining and search engines to semantic analysis and machine translation. This paper presents an unsupervised corpus-based conditional model: Near-Synonym System (NeSS) for finding phrasal synonyms and near synonyms that requires only a large monolingual corpus. The method is based on maximizing information-theoretic combinations of shared contexts and is parallelizable for large-scale processing. An evaluation framework with crowd-sourced judgments is proposed and results are compared with alternate methods, demonstrating considerably superior results to the literature and to thesaurus look up for multi-word phrases. Moreover, the results show that the statistical scoring functions and overall scalability of the system are more important than language specific NLP tools. The method is language-independent and practically useable due to accuracy and real-time performance via parallel decomposition.
 

- Luís Marujo, Ricardo Ribeiro, David Martins de Matos, J. Neto, A. Gershman, J. Carbonell. 2015. Extending a Single-Document Summarizer to Multi-Document: a Hierarchical Approach. Abstract: The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.
- Yun-Nung (Vivian) Chen, William Yang Wang, A. Gershman, Alexander I. Rudnicky. 2015. Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding. Abstract: Spoken dialogue systems (SDS) typically require a predefined semantic ontology to train a spoken language understanding (SLU) module. In addition to the annotation cost, a key challenge for designing such an ontology is to define a coherent slot set while considering their complex relations. This paper introduces a novel matrix factorization (MF) approach to learn latent feature vectors for utterances and semantic elements without the need of corpus annotations. Specifically, our model learns the semantic slots for a domain-specific SDS in an unsupervised fashion, and carries out semantic parsing using latent MF techniques. To further consider the global semantic structure, such as inter-word and inter-slot relations, we augment the latent MF-based model with a knowledge graph propagation model based on a slot-based semantic graph and a word-based lexical graph. Our experiments show that the proposed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner.
- Yun-Nung, Ming Sun, Alexander I. Rudnicky, A. Gershman. 2015. Matrix Factorization with Domain Knowledge and Behavioral Patterns for Intent Modeling. Abstract: Spoken language interfaces are being incorporated into various devices such as smart-phones and TVs. However, dialog systems will fail to respond correctly when users request functionality not supported by currently installed apps. We propose a feature-enriched matrix factorization (MF) approach to model open domain intents that allow a system to dynamically add app-relevant domains according to users’ requests. We use MF to jointly model published app descriptions and users’ spoken requests; this generates latent feature vectors for utterances and user intents without need for prior annotation. The matrix can further incorporate user behavioral patterns found in their activity logs to learn user-specific intent prediction models. We show that the MF models enriched with multimodality significantly improve the intent prediction, achieving 34% and 55% of mean average precision (MAP) for unsupervised single-turn requests and for supervised multi-turn interactions on ASR transcripts respectively.
- Yun-Nung (Vivian) Chen, Ming Sun, Alexander I. Rudnicky, A. Gershman. 2015. Leveraging Behavioral Patterns of Mobile Applications for Personalized Spoken Language Understanding. Abstract: Spoken language interfaces are appearing in various smart devices (e.g. smart-phones, smart-TV, in-car navigating systems) and serve as intelligent assistants (IAs). However, most of them do not consider individual users' behavioral profiles and contexts when modeling user intents. Such behavioral patterns are user-specific and provide useful cues to improve spoken language understanding (SLU). This paper focuses on leveraging the app behavior history to improve spoken dialog systems performance. We developed a matrix factorization approach that models speech and app usage patterns to predict user intents (e.g. launching a specific app). We collected multi-turn interactions in a WoZ scenario; users were asked to reproduce the multi-app tasks that they had performed earlier on their smart-phones. By modeling latent semantics behind lexical and behavioral patterns, the proposed multi-model system achieves about 52% of turn accuracy for intent prediction on ASR transcripts.
- Lori S. Levin, T. Mitamura, B. MacWhinney, Davida Fromm, J. Carbonell, Wes Feely, R. Frederking, A. Gershman, Carlos Ramírez. 2014. Resources for the Detection of Conventionalized Metaphors in Four Languages. Abstract: This paper describes a suite of tools for extracting conventionalized metaphors in English, Spanish, Farsi, and Russian. The method depends on three significant resources for each language: a corpus of conventionalized metaphors, a table of conventionalized conceptual metaphors (CCM table), and a set of extraction rules. Conventionalized metaphors are things like “escape from poverty” and “burden of taxation”. For each metaphor, the CCM table contains the metaphorical source domain word (such as “escape”) the target domain word (such as “poverty”) and the grammatical construction in which they can be found. The extraction rules operate on the output of a dependency parser and identify the grammatical configurations (such as a verb with a prepositional phrase complement) that are likely to contain conventional metaphors. We present results on detection rates for conventional metaphors and analysis of the similarity and differences of source domains for conventional metaphors in the four languages.
- Luís Marujo, José Portêlo, David Martins de Matos, Joao P. Neto, A. Gershman, J. Carbonell, I. Trancoso, B. Raj. 2014. Privacy-Preserving Important Passage Retrieval. Abstract: State-of-the-art important passage retrieval methods obtain very good results, but do not take into account privacy issues. In this paper, we present a privacy preserving method that relies on creating secure representations of documents. Our approach allows for third parties to retrieve important passages from documents without learning anything regarding their content. We use a hashing scheme known as Secure Binary Embeddings to convert a key phrase and bag-of-words representation to bit strings in a way that allows the computation of approximate distances, instead of exact ones. Experiments show that our secure system yield similar results to its non-private counterpart on both clean text and noisy speech recognized text.
- Elena Demidova, Jos Portlo, David Martins de Matos, Joo P. Neto, A. Gershman, B. Raj, S. Bressan, Anisha T. J. Fernando, J. Du, H. Ashman, ChengXiang Zhai, Craig Macdonald, I. Ounis. 2014. Proceeding of the 1 st International Workshop on Privacy-Preserving IR : When Information Retrieval Meets Privacy and Security ( PIR 2014 ). Abstract: Many real world applications in the healthcare domain would gain a substantial advantage from sharing and search technologies available for P2P infrastructures if these technologies could provide required confidentiality guarantees. Currently, DHT-based indexes which are typically applied for effective and efficient information sharing and retrieval in P2P networks do not offer sufficient confidentiality for the patient data in a healthcare network and medical document archives. In this paper we discuss the challenges involved in securing patient data stored in a DHT-based index and discuss initial solutions to address these challenges.
- Luís Marujo, A. Gershman, J. Carbonell, J. Neto, David Martins de Matos. 2014. Ensemble Detection of Single & Multiple Events at Sentence-Level. Abstract: Event classification at sentence level is an important Information Extraction task with applications in several NLP, IR, and personalization systems. Multi-label binary relevance (BR) are the state-of-art methods. In this work, we explored new multi-label methods known for capturing relations between event types. These new methods, such as the ensemble Chain of Classifiers, improve the F1 on average across the 6 labels by 2.8% over the Binary Relevance. The low occurrence of multi-label sentences motivated the reduction of the hard imbalanced multi-label classification problem with low number of occurrences of multiple labels per instance to an more tractable imbalanced multiclass problem with better results (+ 4.6%). We report the results of adding new features, such as sentiment strength, rhetorical signals, domain-id (source-id and date), and key-phrases in both single-label and multi-label event classification scenarios.
- Yulia Tsvetkov, Leonid Boytsov, A. Gershman, Eric Nyberg, Chris Dyer. 2014. Metaphor Detection with Cross-Lingual Model Transfer. Abstract: We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction. Our model is constructed using English resources, and we obtain state-of-the-art performance relative to previous work in this language. Using a model transfer approach by pivoting through a bilingual dictionary, we show our model can identify metaphoric expressions in other languages. We provide results on three new test sets in Spanish, Farsi, and Russian. The results support the hypothesis that metaphors are conceptual, rather than lexical, in nature.
- Ekaterina Shutova, S. Argamon, J. Barnden, Gemma Boleda, Ted Briscoe, S. Clark, Anna Feldman, J. Feldman, Michael Flor, Eugenie Giesbrecht, Valia Kordoni, A. Korhonen, Mark G. Lee, Patricia Lichtenstein, James H. Martin, A. Musolff, S. Narayanan, T. Poibeau, T. Veale, Andreas Vlachos, Anja Jamrozik, Eyal Sagi, Micah B. Goldwater, Michael Mohler, D. Bracewell, Marc T. Tomlinson, David R Hinote, Y. Wilks, Adam Dalton, James F. Allen, Lucian, Dirk Hovy, Shashank Shrivastava, Sujay Kumar Jauhar, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whitney E. Sanders, Eduard, Ilana Heintz, Ryan Gabbard, M. Srivastava, D. Barner, Donald Black, Majorie Friedman, T. Strzalkowski, G. Broadwell, Sarah M. Taylor, L. Feldman, Samira Shaikh, Ting Liu, B. Yamrom, Kit Cho, Umit Boz, Ignacio Cases, Kyle Elliot, Yulia Badryzlova, N. Shekhtman, Yekaterina Isaeva, R. Kerimov, Vii, Beata Beigman Klebanov, D. Gentner, Lucian Galescu, Yulia Tsvetkov, E. Mukomel, A. Gershman, E. Hovy, Marissa Friedman, R. Weischedel, Jonathan Dunn. 2013. Relational Words Have High Metaphoric Potential Semantic Signatures for Example-based Linguistic Metaphor Detection Automatic Metaphor Detection Using Large-scale Lexical Resources and Conventional Metaphor Ex- Traction Cross-lingual Metaphor Detection Using Common Semantic Features Identifying Meta. Abstract: ii Introduction Characteristic to all areas of human activity (from poetic to ordinary to scientific) and, thus, to all types of discourse, metaphor becomes an important problem for natural language processing. Its ubiquity in language has been established in a number of corpus studies and the role it plays in human reasoning has been confirmed in psychological experiments. This makes metaphor an important research area for computational and cognitive linguistics, and its automatic identification and interpretation indispensable for any semantics-oriented NLP application. The work on metaphor in NLP and AI started in the 1980s, providing us with a wealth of ideas on the structure and mechanisms of the phenomenon. The last decade witnessed a technological leap in natural language computation, whereby manually crafted rules gradually give way to more robust corpus-based statistical methods. This is also the case for metaphor research. In the recent years, the problem of metaphor modeling has been steadily gaining interest within the NLP community, with a growing number of approaches exploiting statistical techniques. Compared to more traditional approaches based on hand-coded knowledge, these more recent methods tend to have a wider coverage, as well as be more efficient, accurate and robust. However, even the statistical metaphor processing approaches so far often focused on a limited domain or a subset of phenomena. At the same time, recent work on computational lexical semantics and lexical acquisition techniques, as well as a wide range of NLP methods applying machine learning to open-domain semantic tasks, open many new avenues for creation of large-scale robust tools for recognition and interpretation of metaphor. This workshop is the first one focused on modelling of metaphor using NLP techniques. Recent related events include workshops on Computational Approaches to Figurative Language (NAACL 2007) and on Computational Approaches to Linguistic Creativity (NAACL 2009, NAACL 2010). We received 14 submissions and accepted 10. Each paper was carefully reviewed by at least 3 members of the Program Committee. The selected papers offer explorations into the following directions: (1) creation of metaphor-annotated datasets; (2) identification of new features that are useful for metaphor identification; (3) cross-lingual metaphor identification. The papers represent a variety of approaches to utilization and creation of datasets. While existing annotated corpora were used in some papers (Dunn, Tsvetkov et al), most papers describe creation of new annotated materials. Along with annotation guidelines adapted from the MIP and MIPVU procedures (Badryzlova et al), more intuitive …
- Yulia Tsvetkov, E. Mukomel, A. Gershman. 2013. Cross-Lingual Metaphor Detection Using Common Semantic Features. Abstract: We present the CSF - Common Semantic Features method for metaphor detection. This method has two distinguishing characteristics: it is cross-lingual and it does not rely on the availability of extensive manually-compiled lexical resources in target languages other than English. A metaphor detecting classifier is trained on English samples and then applied to the target language. The method includes procedures for obtaining semantic features from sentences in the target language. Our experiments with Russian and English sentences show comparable results, supporting our hypothesis that a CSF-based classifier can be applied across languages. We obtain state-ofthe-art performance in both languages.
- Ricardo Ribeiro, Luís Marujo, David Martins de Matos, J. Neto, A. Gershman, J. Carbonell. 2013. Self reinforcement for important passage retrieval. Abstract: In general, centrality-based retrieval models treat all elements of the retrieval space equally, which may reduce their effectiveness. In the specific context of extractive summarization (or important passage retrieval), this means that these models do not take into account that information sources often contain lateral issues, which are hardly as important as the description of the main topic, or are composed by mixtures of topics. We present a new two-stage method that starts by extracting a collection of key phrases that will be used to help centrality-as-relevance retrieval model. We explore several approaches to the integration of the key phrases in the centrality model. The proposed method is evaluated using different datasets that vary in noise (noisy vs clean) and language (Portuguese vs English). Results show that the best variant achieves relative performance improvements of about 31% in clean data and 18% in noisy data.
- Luís Marujo, M. Bugalho, J. Neto, A. Gershman, J. Carbonell. 2013. Hourly Traffic Prediction of News Stories. Abstract: ABSTRACT The process of predicting news stories popularity from several news sources has become a challenge of great importance for both news producers and readers. In this paper, we investigate methods for automatically predicting the number of clicks on a news story during one hour. Our approach is a combination of additive regression and bagging applied over a M5P regression tree using a logarithmic scale (log 10 ). The features included are social-based (social network metadata from Facebook), content-based (automatically extracted keyphrases, and stylometric statistics from news titles), and time-based. In 1 st Sapo Data Challenge we obtained 11.99% as mean relative error value which put us in the 4 th place out of 26 participants. Categories and Subject Descriptors H.3.3 [ Information Search and Retrieval ]: Information filtering General Terms Algorithms , Measurement , Experimentation . Keywords Prediction, News, Clicks, Sapo Challenge, Traffic 1. INTRODUCTION
- Luís Marujo, A. Gershman, J. Carbonell, David Martins de Matos, J. Neto. 2013. Co-Multistage of Multiple Classifiers for Imbalanced Multiclass Learning. Abstract: In this work, we propose two stochastic architectural models (CMC and CMC-M) with two layers of classifiers applicable to datasets with one and multiple skewed classes. This distinction becomes important when the datasets have a large number of classes. Therefore, we present a novel solution to imbalanced multiclass learning with several skewed majority classes, which improves minority classes identification. This fact is particularly important for text classification tasks, such as event detection. Our models combined with pre-processing sampling techniques improved the classification results on six well-known datasets. Finally, we have also introduced a new metric SG-Mean to overcome the multiplication by zero limitation of G-Mean.
- Luís Marujo, Wang Ling, A. Gershman, J. Carbonell, J. Neto, David Martins de Matos. 2012. Recognition of Named-Event Passages in News Articles. Abstract: We extend the concept of Named Entities to Named Events - commonly occurring events such as battles and earthquakes. We propose a method for finding specific passages in news articles that contain information about such events and report our preliminary evaluation results. Collecting "Gold Standard" data presents many problems, both practical and conceptual. We present a method for obtaining such data using the Amazon Mechanical Turk service.
- Luís Marujo, A. Gershman, J. Carbonell, R. Frederking, J. Neto. 2012. Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization. Abstract: Fast and effective automated indexing is critical for search and personalized services. Key phrases that consist of one or more words and represent the main concepts of the document are often used for the purpose of indexing. In this paper, we investigate the use of additional semantic features and pre-processing steps to improve automatic key phrase extraction. These features include the use of signal words and freebase categories. Some of these features lead to significant improvements in the accuracy of the results. We also experimented with 2 forms of document pre-processing that we call light filtering and co-reference normalization. Light filtering removes sentences from the document, which are judged peripheral to its main content. Co-reference normalization unifies several written forms of the same named entity into a unique form. We also needed a Gold Standard ― a set of labeled documents for training and evaluation. While the subjective nature of key phrase selection precludes a true Gold Standard, we used Amazon's Mechanical Turk service to obtain a useful approximation. Our data indicates that the biggest improvements in performance were due to shallow semantic features, news categories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of deeper semantic features such as Freebase sub-categories was not beneficial by itself, but in combination with pre-processing, did cause slight improvements in the nDCG scores.
- Rushin Shah, Bo Lin, Kevin Dela Rosa, A. Gershman, R. Frederking. 2011. Improving cross-document co-reference with semi-supervised information extraction modelsi. Abstract: In this paper, we consider the problem of cross-document co-reference (CDC). Existing approaches tend to treat CDC as an information retrieval based problem and use features such as TF-IDF cosine similarity to cluster documents and/or co-reference chains. We augmented these features with features based on biographical attributes, such as occupation, nationality, gender, etc., obtained by using semisupervised attribute extraction models. Our results suggest that the addition of these features boosts the performance of our CDC system considerably. The extraction of such specific attributes allows us to use features, such as semantic similarity, mutual information and approximate name similarity which have not been used so far for CDC with traditional bag-of-words models. Our system achieves F0.5 scores of 0.82 and 0.81 on the WePS-1 and WePS-2 datasets, which rival the best reported scores for this problem.
- Kevin Dela Rosa, Rushin Shah, Bo Lin, A. Gershman, R. Frederking. 2011. Topical Clustering of Tweets. Abstract: In the emerging field of micro-blogging and social communication services, users post millions of short messages every day. Keeping track of all the messages posted by your friends and the conversation as a whole can become tedious or even impossible. In this paper, we presented a study on automatically clustering and classifying Twitter messages, also known as “tweets”, into different categories, inspired by the approaches taken by news aggregating services like Google News. Our results suggest that the clusters produced by traditional unsupervised methods can often be incoherent from a topical perspective, but utilizing a supervised methodology that utilize the hash-tags as indicators of topics produce surprisingly good results. We also offer a discussion on temporal effects of our methodology and training set size considerations. Lastly, we describe a simple method of finding the most representative tweet in a cluster, and provide an analysis of the results.
- A. Gershman, Travis Wolfe, Eugene Fink, J. Carbonell. 2011. News Personalization using Support Vector Machines. Abstract: We describe a system for recommending news articles, called NewsPer, which learns news-reading preferences of its users and suggests recently published articles that may be of interest to specific readers based on their interest profiles. The underlying algorithm is based on representing articles by bags of words and named entities, and applying support vector machines to this representation. We present this algorithm and give initial empirical results. We also discuss broader issues in the news personalization and the challenges of performance evaluation based on historical data.
- Haizhou Li, Min Zhang, A. Kumaran, Microsoft Research, Kalika Bali, Rafael E. Banchs, Barcelonamedia, Sivaji Bandyopadhyay, Marta Ruiz Costa-jussà, Upc, G. Grefenstette, Exalead, France, Mitesh M. Khapra, Olivia Kwong, Hong Kong, A. McCallum, Jong-Hoon Oh, Sunita Sarawagi, Iit-Bombay, India, S. Sarkar, Iit-Kharagpur, R. Sproat, Vasudeva Varma, Iiit-Hyderabad, Haifeng Wang, C. Wutiwiwatchai, Nectec, V. Pervouchine, Vladimir, Sittichai Jiampojamarn, Kenneth Dwyer, S. Bergsma, Aditya Bhargava, Qing Dou, Mi-Young, Yan Song, Chunyu Kit, Hai, Amitava Das, Tanik Saikh, Tapabrata Mondal, Asif Ekbal, Iman Saleh, Kareem Darwish, Aly, Eva Sourjikova, A. Frank, Simone Paolo, Yu Chen, Y. Ouyang, Wenjie Li, De-Kui Zheng, T. Zhao, Bo Lin, Rushin Shah, R. Frederking, A. Gershman, Mi-Young Kim, Grzegorz Kondrak, A. Finch, E. Sumita, Haizhen Zhao, A. Thangthai, Aly Fahmy, Simone Paolo Ponzetto, A. A. Hamid, Friday. 2010. Shared Task Organizing Committee -transliteration Mining: Whitepaper of News 2010 Shared Task on Transliteration Generation Transliteration Generation and Mining with Limited Training Resources Transliteration Using a Phrase-based Statistical Machine Translation System to Re-score the Output of a Jo. Abstract: ii Preface Named Entities play a significant role in Natural Language Processing and Information Retrieval. While identifying and analyzing named entities in a given natural language is a challenging research problem by itself, the phenomenal growth in the Internet user population, especially among the non-English speaking parts of the world, has extended this problem to the crosslingual arena. We specifically focus The purpose of the NEWS workshop is to bring together researchers across the world interested in identification, analysis, extraction, mining and transformation of named entities in monolingual or multilingual natural language text. The workshop scope includes many interesting specific research areas pertaining to the named entities, such as, orthographic and phonetic characteristics, corpus analysis, unsupervised and supervised named entities extraction in monolingual or multilingual corpus, transliteration modelling, and evaluation methodologies, to name a few. For this years edition, 11 research papers were submitted, each of which was reviewed by at least 3 reviewers from the program committee. 7 papers were chosen for publication, covering main research areas, from named entities recognition, extraction and categorization, to distributional characteristics of named entities, and finally a novel evaluation metrics for co-reference resolution. All accepted research papers are published in the workshop proceedings. This year, as parts of the NEWS workshop, we organized two shared tasks: one on Machine Transliteration Generation, and another on Machine Transliteration Mining, participated by research teams from around the world, including industry, government laboratories and academia. The transliteration generation task was introduced in NEWS 2009. While the focus of the 2009 shared task was on establishing the quality metrics and on baselining the transliteration quality based on those metrics, the 2010 shared task expanded the scope of the transliteration generation task to about dozen languages, and explored the quality depending on the direction of transliteration, between the languages. We collected significantly large, hand-crafted parallel named entities corpora in dozen different languages from 8 language families, and made available as common dataset for the shared task. We published the details of the shared task and the training and development data six months ahead of the conference that attracted an overwhelming response from the research community. Totally 7 teams participated in the transliteration generation task. The approaches ranged from traditional unsupervised learning methods (such as, Phrasal SMT-based, Conditional Random Fields, etc.) to somewhat unique approaches (such as, DirectTL approach), combined with several model combinations for results re-ranking. A report of …
- Bo Lin, Rushin Shah, R. Frederking, A. Gershman. 2010. ENCORE : Experiments with a Synthetic Entity Co-reference Resolution Tool. Abstract: We present ENCORE, a system for entity co-reference resolution that synthesizes the outputs of several off-the-shelf co-reference resolution systems. To boost precision, we filter the output using a named entity recognition tool called SYNERGY which itself is a synthesis of several off-the-shelf NER systems. ENCORE is designed to work under two conditions: NP-CR which resolves noun phrase co-reference and NE-CR which resolves co-references only for named entities. We report the results of our experiments with ENCORE that show 2% to 40% improvements in precision, recall and F-scores over the underlying systems. This opens a promising approach which leverages the existing “black box” state-of-the-art tools without attempting to re-create their achievements and focuses the development efforts on the differences in their output.
- Rushin Shah, Bo Lin, A. Gershman, R. Frederking. 2010. SYNERGY: A Named Entity Recognition System for Resource-scarce Languages such as Swahili using Online Machine Translation. Abstract: Developing Named Entity Recognition (NER) for a new language using standard techniques requires collecting and annotating large training resources, which is costly and time-consuming. Consequently, for many widely spoken languages such as Swahili, there are no freely available NER systems. We present here a new technique to perform NER for new languages using online machine translation systems. Swahili text is translated to English, the best off-the-shelf NER systems are applied to the resulting English text and the English named entities are mapped back to words in the Swahili text. Our system, called SYNERGY, addresses the problem of NER for a new language by breaking it into three relatively easier problems: Machine Translation to English, English NER and word alignment between English and the new language. SYNERGY achieves good precision as well as recall for Swahili. We also apply SYNERGY to Arabic, for which freely available NERs do exist, in order to compare its performance to other NERs. We ﬁnd that SYNERGY’s performance is close to the state-of-the-art in Arabic NER, with the advantage of requiring vastly less time and effort to build.
- Bo Lin, Rushin Shah, R. Frederking, A. Gershman. 2010. CONE: Metrics for Automatic Evaluation of Named Entity Co-Reference Resolution. Abstract: Human annotation for Co-reference Resolution (CRR) is labor intensive and costly, and only a handful of annotated corpora are currently available. However, corpora with Named Entity (NE) annotations are widely available. Also, unlike current CRR systems, state-of-the-art NER systems have very high accuracy and can generate NE labels that are very close to the gold standard for unlabeled corpora. We propose a new set of metrics collectively called CONE for Named Entity Co-reference Resolution (NE-CRR) that use a subset of gold standard annotations, with the advantage that this subset can be easily approximated using NE labels when gold standard CRR annotations are absent. We define CONE B3 and CONE CEAF metrics based on the traditional B3 and CEAF metrics and show that CONE B3 and CONE CEAF scores of any CRR system on any dataset are highly correlated with its B3 and CEAF scores respectively. We obtain correlation factors greater than 0.6 for all CRR systems across all datasets, and a best-case correlation factor of 0.8. We also present a baseline method to estimate the gold standard required by CONE metrics, and show that CONE B3 and CONE CEAF scores using this estimated gold standard are also correlated with B3 and CEAF scores respectively. We thus demonstrate the suitability of CONE B3 and CONE CEAF for automatic evaluation of NE-CRR.
- Eugene Fink, J. Carbonell, A. Gershman, Ganesh Mani, D. Dietrich. 2009. Representation and Analysis of Probabilities Intelligence Data (RAPID). Abstract: Abstract : Tools were developed for the representation and analysis of uncertainty in INTEL data and targeted uncertainty reduction. The purpose is to help INTEL analysts answer these questions: 1) What hypotheses can be validated/refuted based on available uncertain data and at what level of certainty? 2) What missing data is critical for verifying or refuting given hypotheses and increasing the certainty of current conclusions? 3) What are the tradeoffs between the value of specific missing data and cost and difficulty of obtaining it?
- A. Gershman, Eugene Fink, Bin Fu, J. Carbonell. 2009. Analysis of uncertain data: Selection of probes for information gathering. Abstract: We consider the problem of gathering data for evaluation of given hypotheses, and describe a method for analyzing tradeoffs between the expected utility and the cost of data collection.
- A. Gershman, Eugene Fink, Bin Fu, J. Carbonell. 2009. Analysis of uncertain data: Evaluation of given hypotheses. Abstract: We consider the problem of heuristic evaluation of given hypotheses based on limited observations, in situations when available data are insufficient for rigorous statistical analysis.
- A. Gershman, D. Roqueiro, V. Petrushin, Gang Wei, R. Ghani, Gang Wei. 2007. Role of Domain Knowledge and Sensor Quality in Robust Localization of Moving Objects. Abstract: Robust identification and localization of moving objects depends on the quality of sensors, sensor coverage and the fusion of the information obtained from the sensors. Sensor fusion algorithms use domain and context knowledge to calculate the most plausible interpretation of all available data. Little research has been done on the trade-offs between these factors. This paper presents an empirical study of these trade-offs for a sensor fusion algorithm BBP based on Bayesian forward and backward propagation. To test the robustness of this algorithm under various conditions, we created “virtual sensors” whose performance characteristics were based on the data gathered by tracking 31 people in 112 locations using a set of 34 cameras and 91 badge readers. Our Monte Carlo simulations show that given good domain knowledge, BBP performs robustly even in the presence of poor sensors, thus providing a promising alternative to the expensive practice of installing better sensors and calibration procedures in order to improve surveillance systems.
- V. Petrushin, Gang Wei, R. Ghani, A. Gershman, Gang Wei. 2006. Using Bayesian Reasoning from Sensor Network for Indoor Surveillance. Abstract: In this paper we define a Bayesian framework that uses noisy, but redundant data from a network of sensors that include multiple sensor streams of different types. It merges the data with the contextual and domain knowledge that is provided by both the physical constraints imposed by the local environment and by the people that are involved in the surveillance tasks. The paper also presents the results of applying the Bayesian framework to the people localization problem in indoor environment using a sensor network that consists of video cameras, infrared tag readers and a fingerprint reader.
- A. Schmidt, S. Spiekermann, A. Gershman, F. Michahelles. 2006. Real-World Challenges of Pervasive Computing. Abstract: At the Pervasive Technology Applied workshop (part of Pervasive 2006), practitioners and researchers discussed how to bridge the gap between academic research and the practical hurdles in pervasive technology. The wide range of submissions demonstrated the great potential of applied pervasive technologies. In the emerging discussions, participants highlighted the most important technical and cooperation issues.
- V. Petrushin, Gang Wei, Omer Shakil, D. Roqueiro, A. Gershman. 2006. Multiple-Sensor Indoor Surveillance System. Abstract: This paper describes a surveillance system that uses a network of sensors of different kind for localizing and tracking people in an office environment. The sensor network consists of video cameras, infrared tag readers, a fingerprint reader and a PTZ camera. The system implements a Bayesian framework that uses noisy, but redundant data from multiple sensor streams and incorporates it with the contextual and domain knowledge. The paper describes approaches to camera specification, dynamic background modeling, object modeling and probabilistic inference. The preliminary experimental results are presented and discussed.
- A. Gershman, A. Fano. 2005. Examples of commercial applications of ubiquitous computing. Abstract: Emerging tools will simply transform business practices---and customer expectations---in the near future.
- V. Petrushin, Gang Wei, R. Ghani, A. Gershman. 2005. Multiple Sensor Indoor Surveillance: Problems and Solutions. Abstract: The paper presents the Multiple Sensor Indoor Surveillance (MSIS) project which is a research project at the Accenture Technology Labs. It describes the objectives of the project, the problems it was designed to solve and solutions that have been currently obtained. The project environment includes 32 Web cameras, an infrared badge system, a PTZ camera, and a fingerprint reader. The solutions for the following two problems are described in details. The first problem is how to visualize events detected by 32 cameras during 24 hours. The solution is obtained using self-organizing maps. The second problem is how to localize people using fusion of multiple streams of noisy sensory data with the contextual and domain knowledge that is provided by both the physical constraints imposed by the local environment and by the people that are involved in the surveillance tasks. A Bayesian framework is suggested to solve this problem. The experimental data are provided and discussed
- V. Petrushin, Gang Wei, R. Ghani, A. Gershman. 2005. Multiple sensor integration for indoor surveillance. Abstract: Multiple Sensor Indoor Surveillance (MSIS) is a research project at Accenture Technology Labs aimed at exploring a variety of redundant sensors in a networked environment where each sensor is giving noisy information and the goal is to coherently reason about some aspect of the environment. We describe the objectives of the project, the problems it was designed to solve and some recent results. The environment includes 32 web cameras, an infrared badge ID system, a PTZ camera, and a fingerprint reader. We discuss two concrete problems that we have tackled in this project: (1) Visualizing events detected by 32 cameras during 24 hours, and (2) Localizing people using fusion of multiple streams of noisy sensory data with the contextual and domain knowledge that is provided by both the physical constraints imposed by the local environment and by the people that are involved in the surveillance tasks. We use Self-Organizing Maps to approach the first problem and suggest a Bayesian framework for the second one. The experimental data are provided and discussed.
- V. Petrushin, R. Ghani, A. Gershman. 2005. A Bayesian Framework for Robust Reasoning from Sensor Networks. Abstract: The work described in this paper defines a Bayesian framework to use noisy, but redundant data from multiple sensor streams and incorporate it with the contextual and domain knowledge that is provided by both the physical constraints imposed by the local environment where the sensors are located and by the people that are involved in the surveillance tasks. The paper also presents the preliminary results of applying the Bayesian framework to the people localization problem in indoor environment using a sensor network that consists of video cameras, infrared tag readers and a fingerprint reader.
- Dadong Wan, A. Gershman. 2004. Protecting People on the Move through Virtual Personal Security. Abstract: Ensuring personal safety for people on the move is becoming a heightened priority in today’s uncertain environment. Traditional approaches are no longer adequate in meeting rising demands in personal security. In this paper, we describe VIRTUAL PERSONAL SECURITY, a research prototype that demonstrates how technologies, such as ubiquitous surveillance cameras, location-aware PDAs and cell phones, wireless networks, and Web Services can be brought together to create a virtualized personal security service. By incorporating automatic service discovery, situated sensing and multimedia communication, the novel solution provides consumers with increasing availability, lower cost, and high flexibility. It also creates a new market for just-in-time micro security services, providing the owners of surveillance cameras a new revenue stream.
- E. Liongosari, A. Gershman, V. Gershman, Mitu Singh. 2004. Supporting Drug Discovery Research through Knowledge Modeling and Integration. Abstract: This paper describes a knowledge platform that is designed to support drug discovery researchers in pharmaceutical companies. The core of this platform is a knowledge model that provides a semantically integrated knowledge space for the researchers to easily learn and explore various aspects of biological data that originate from multiple disparate sources. By using domain-specific functional rules, the platform can assist researchers in exposing valuable hidden or unobvious linkages across multiple repositories. The rules can also be used to find collaboration opportunities among its users by monitoring the users’ navigation and interaction patterns. The results from collaboration can be further annotated and shared with other users. When combined with group shared spaces, this platform can be used as a key component to support cooperative learning.
- Charles Nebolsky, Nicholas K. Yee, V. Petrushin, A. Gershman. 2004. Corporate Training in Virtual Worlds. Abstract: This paper presents virtual training worlds that are relatively low-cost distributed collaborative learning environments suitable for corporate training. A virtual training world allows a facilitator, experts and trainees communicating and acting in the virtual environment for practicing skills during collaborative problem solving. Using these environments is beneficial to both trainees and corporations. Two system prototypes – the sales training and the leadership training virtual worlds – are described. The leadership training course design is discussed in details.
- Charles Nebolsky, Nicholas K. Yee, V. Petrushin, A. Gershman. 2003. Using virtual worlds for corporate training. Abstract: We present virtual training worlds that are relatively low-cost distributed collaborative learning environments suitable for corporate training. A virtual training world allows a facilitator, experts and trainees communicating and acting in the virtual environment for practicing skills during collaborative problem solving. Using these environments is beneficial to both trainees and corporations. The design of a leadership training course is discussed in details.
- Gang Wei, V. Petrushin, A. Gershman, N. Clark. 2002. A Learning Environment For Creating Media Processing Systems. Abstract: The Community of Multimedia Agents project (COMMA) is devoted to creating an open Web-based environment for developing, testing, learning and prototyping multimedia content analysis and annotation methods. Each method is represented as an agent that can communicate with the other agents registered in the environment using templates that are based on MPEG-7 descriptors and description schemes. The low-level agents can be combined to obtain more sophisticated ones. The paper discusses the educational aspects of the COMMA project. It describes both agent development tools that can be considered as learning environment in the narrow sense and the Webbased community as a collaborative learning environment.
- Gang Wei, V. Petrushin, A. Gershman. 2002. From Data To Insight: The Community Of Multimedia Agents. Abstract: Multimedia Data Mining requires the ability to automatically analyze and understand the content. The Community of Multimedia Agents project (COMMA) is devoted to creating an open environment for developing, testing, learning and prototyping multimedia content analysis and annotation methods. It serves as a medium for researchers to contribute and share their achievements while protecting their proprietary techniques. Each method is represented as an agent that can communicate with the other agents registered in the environment using templates that are based on the Descriptors and Description Schemes in the emerging MPEG-7 standard. This allows agents developed by different organizations to operate and communicate with each other seamlessly regardless of their programming languages and internal architecture. A Development Environment is provided to facilitate the construction of media analysis methods. The tool contains a Workbench using which the user can integrate the agents to build more sophisticated systems, and a Blackboard Browser that visualizes the processing results. It enables researchers to compare the performance of different agents and combine them to build more powerful and robust system prototypes. The COMMA can also serve as a learning environment for researchers and students to acquire and test cutting edge multimedia analysis algorithms. Thus the efficiency of research in this area can be improved by sharing of media agents.
- Gang Wei, V. Petrushin, A. Gershman. 2002. The Community of Multimedia Agents project. Abstract: Challenges in multimedia analysis are calling for the sharing of research efforts, while in practice collaboration is hindered by technical and proprietary issues. The Community of Multimedia Agents project (COMMA) attempts to solve this problem by creating an open environment for developing, testing, and prototyping multimedia content analysis and annotation methods. Each method is represented as an agent (an executable module) that can communicate with the other agents based on descriptors and description schemes in the coming MPEG-7 standard. This allows multimedia-processing agents developed by different organizations to operate and collaborate with each other, regardless of their programming languages and internal architecture. The researchers can compare the performance of agents and combine them to build more powerful and robust system prototypes. It can also serve as a learning environment for researchers and students to acquire and test cutting edge multimedia analysis algorithms. Through sharing of media agents, the Community can increase efficiency of research while protecting the intellectual property of the inventors.
- A. Fano, A. Gershman. 2002. The future of business services in the age of ubiquitous computing. Abstract: Redefining the key aspects of the business-customer relationship.
- J. Simeon, Chabane Simoff, Osmar R Djeraba, Zaїane, Simeon, J. Simoff, C. Djeraba, Osmar R Zaїane, W. Perrizo, William Jockheck, Amal Perera, Dongmei Ren, Weihua Wu, Yi Zhang, Mariana Ciucu, P. Héas, M. Datcu, James C, A. Benitez, Shih Fu, Lei Wang, L. Khan, A. Casey, P. Singh, S. Simoff, David, Xin Huang, Shu‐Ching Chen, Mei-Ling Shyu, Localchair Osmar, R. Zaїane, T. Caelli, A. Duffy, Howard J. Hamilton, Jiawei Han, Simon Fraser University, Canada, W. Hsu, Paul J. Kennedy, I. Kolyshkina, P. Coopers, Brian C Australia, Lovell, M. Maybury, O. Nielsen, M. Quafafou, Tuesday, K. Seidel, J. Tilton, Shih-Fu Chang, C. Breen, David Feng, Gang Wei, V. Petrushin, A. Gershman, N. Bianchi-Berthouze, Tomofumi Hayashi, Chengcui Zhang, J. Oh, Babitha Bandi. 2002. Multimedia Data Mining in Conjunction with Acm Sigkdd Eighth International Conference on Knowledge Discovery and Data Mining Table of Contents Multimedia Data Mining Using P-trees Scale Space Exploration for Mining Image Information Content Object Boundary Detection for Ontology-based Image Classifi. Abstract: Foreword Since the beginning of the century there have been two successful international workshops on multimedia data mining at the KDD forums: MDM/KDD2000 and MDM/KDD2001, in conjunction with KDD2000 (in Boston) and KDD2001 (in San Francisco), respectively. These workshops brought together numerous experts in spatial data analysis, digital media, multimedia information retrieval, state-of-art data mining and knowledge discovery in multimedia database systems, analysis of data in collaborative virtual environments. For more information about the workshops see the reports on the workshops in SIGKDD Explorations (2 (2), pp. 103-105 and 3 (2), pp. 65-67, respectively). Participants in both workshops were pleased with the event and there was consensus about the necessity of turning it into an annual meeting, where researchers, both from the academia and industry can exchange and compare both relatively mature and green house theories, methodologies, algorithms and frameworks for multimedia data mining. This workshop is organized in response to this interest. Being a third edition, the workshop this year is aiming to create a stimulating atmosphere for discussing the theoretical foundations of multimedia data mining, frameworks, methods and algorithms for integrated pattern extraction from multimedia data, multimedia data preprocessing, novel architectures for multimedia data mining, and applications of multimedia data mining in different areas. Consequently, the papers selected for presentation at the Third International Workshop on Multimedia Data Mining (MDM/KDD'2002) held in conjunction with the 7th and Applications of Multimedia Data Mining (with two subgroups of applications: in medical image analysis and in content-based multimedia processing). This grouping bears some similarity with the last year workshop, where there was similar emphasis on the research in the area of frameworks and methodologies, and on the research in the application area. The works selected for presentation at this workshop form more cohesive body of work, which indicates that the field has made a step forward towards achieving some level of maturity. As part of the SIGKDD conference series the workshop follows a rigid peer-review and paper selection process. Once again, we would like to thank all those, who supported this year's efforts on all stages – from the development and submission of the workshop proposal to the preparation of the final program and proceedings. We would like to thank all those who submitted their work to the workshop. In a good data mining tradition, a pattern is emerging – as in the previous workshop there were submissions from 10 different …
- A. Gershman. 2002. Ubiquitous commerce - always on, always aware, always pro-active. Abstract: The development of the infrastructure for ubiquitous computing is progressing rapidly, yet the applications that will be built on this new infrastructure remain largely ill defined. What will we do with these emerging capabilities? Stock quotes in 3-D on your heads-up display? Sports scores in stereo sound? We believe that the new kinds of services will result from three primary capabilities of ubiquitous devices: (1) to provide a service channel for remote service providers through an "always on" connection, (2) to inform these services about the local context of the user through an array of sensors, and (3) to enable these services to affect things in the user environment through actuators and local communication links. The new services enabled by these capabilities will lead to a new era of ubiquitous commerce, and change all business functions from customer relationship management to enterprise and supply chain management.
- Doug Bryan, A. Gershman. 2000. The aquarium: a novel user interface metaphor for large, online stores. Abstract: The advent of the Web has brought an unprecedented amount of information together with a large, diverse set of users. Online users are performing a wider variety of tasks than ever before. For example, not only is the Web being used to search conventional databases like Lexis/Nexus, it is also being used to broker Beanie Babies(R). Today's common information seeking metaphors (i.e., keyword search and hypertext) cannot be expected to support all these new tasks well. Our research investigates new metaphors for online information seeking tasks. We characterize a new user behavior called opportunistic exploration and show how it is significantly different than both browsing and searching. A novel visual metaphor for opportunistic exploration, an aquarium, is presented. The aquarium allows users to explore a large corpus at any level of granularity.
- A. Gershman, Joseph F. McCarthy, A. Fano. 1999. Situated computing: bridging the gap between intention and action. Abstract: Situated computing represents a new class of computing applications that bridges the gap between people's intentions and the actions they can take to achieve those intentions. These applications are contextually embedded in real-world situations, and are enabled by the proliferation of new kinds of computing devices, expanding communication capabilities and new kinds of digital content. Three types of discontinuities give rise to intention/action gaps and provide opportunities for situated computing applications: physical discontinuities, information discontinuities and and awareness discontinuities. Several examples of applications that overcome these discontinuities are presented.
- Doug Bryan, A. Gershman. 1999. Opportunistic exploration of large consumer product spaces. Abstract: The advent of the Web has brought an unprecedented amount of information together with a large, diverse set of users. Online users are performing a wider variety of tasks than ever before. For example, not only is the Web being used to search conventional databases like Lexis/Nexus, it is also being used to broker Beanie Babies. Today’s common information seeking metaphors (i.e., keyword search and hypertext) cannot be expected to support all these new tasks well. We characterize a new user behavior called opportunistic exploration. We show how it is significantly different than both browsing and searching. A novel visual metaphor for opportunistic exploration, an aquarium, is presented. In an aquarium users may explore a large corpus at any level of granularity. The aquarium’s implementation is discussed and demonstrated on a collection of 12,000 consumer products. The implementation automatically controls granularity based on the history of operations performed by a user.
- S. H. Sato, A. Gershman, Kishore S. Swaminathan. 1996. Prairie (video program) (abstract only): a conceptual framework for a virtual organization. Abstract: Prairie is a simulation prototype or vision, demonstrating how individuals may work together in a virtual work enviroment designed for a whole enterprise. Prairie addresses various organizational and social issues exacerbated by distance and time. By using the concept of communities and by extending physical interaction cues to others across distance and time, we demonstrate possible solutions to these issues. In Prairie, people and information are organized into mission-based (organizational units), goal-based (project teams) and interest-based (special interest groups) hierarchies for ease of navigation. A worker may alternately navigate to communities by using personal links from their private virtual desktops. Each community has two areas. One area contains the information germane to a community, that is pushed or pulled depending on the nature of the information. Each community also has an area with a shared view where community members can meet or congregate. Presence in these community areas range from seeing thumbnail photos to holding a video-conference. The shared view facilitates ad hoc, informal interactions which are important for maintaining and building social networks and organizational culture. We believe the framework for Prairie is flexible, integrated, and scaleable so it can be adapted to model other organizations, communities, and processes.
- B. Krulwich, Lucian Hughes, E. Gottsman, Michael Antonio, A. Gershman. 1994. Multimedia consumer applications on the information superhighway. Abstract: The recent frenzy of alliances among telephone companies, cable providers and hardware manufacturers is bringing the information superhighway from fantasy to reality faster than most of us expected. At Andersen Consulting, we believe that Electronic Forums and Malls-rich, sophisticated environments for home shopping and community activities-will be among the many superhighway services available through your TV set. Andersen Consulting's Center for Strategic Technology Research (CSTaR) is developing prototypes that explore how consumers will interact with the information superhighway.<<ETX>>
- B. M. Lange, James B. Treleaven, A. Gershman. 1993. OMNI: a model for focused collaborative work through issue management. Abstract: The bulk of the research done on collaborative ,work has been on improving the effectiveness of meetings. Little work has been done on providing support for knowledgeintensive, physically dispersed, asynchronous group tasks. These tasks have the essential characteristics of requiring collaboration among a team of participants, consisting of complex issue structures, requiring a significant amount of shared information, and necessitating negotiation in order to reach consensus. OMNI is a domain independent, general model of collaborative issue management designed to support such tasks. This collaboration model has four key elements which must be supported group processes, dynamics of group interactions, communications, and group memory. The central feature of the model is an issue management system capable of supporting complex, domain independent issue structures. OMNI has been implemented in the initial domain of venture capital investment decision-making.
- A. Gershman, Gottsman, Andersen. 1993. Use of hypermedia for corporate knowledge dissemination. Abstract: The authors report on an approach to building hypermedia systems for corporate knowledge dissemination based on a systematic transfer of the existing mechanisms of knowledge dissemination into hypermedia. This approach has been used to create several knowledge dissemination systems each using a different metaphor for knowledge organization and delivery. One such system, EAS (expert access system), is described. It is used to translate live workshops directly into an interactive multimedia platform so as to give users an experience similar to that of a live workshop. In order to achieve these results, digital video, topic- and question-based navigation, messaging, and electronic bulletin boards (Lotus Notes) are combined. This approach makes possible the economical development of effective knowledge dissemination applications in a corporate environment.<<ETX>>
- B. M. Lange, A. Gershman. 1992. OMNI: a corporate knowledge enviornment for collaborative work. Abstract: The goal of the OMNI project is to explore strategies for managing a dynamic knowledge environment for supporting a complex task involving multiple participants working asynchronously. The authors describe the collaboration model and illustrate its application in a knowledge-rich domain. OMNI implementation is discussed along with intended results and future directions.<<ETX>>
- Barbara E. Myers, R. Flast, A. Gershman, E. Gottsman. 1992. Making sense of competitive intelligence with TIGER: an information visualization tool. Abstract: This paper describes a joint research project in data visualization between MetLife and the CSTAR group at Andersen Consulting. The goal of the project was to produce a tool for retrieval, display, and analyzing competitive information that would run on conventional PC platforms. As part of the visualization scheme, icon clusters provide a graphical representation of each case's salient features, while 1, 2, or 3 dimensional maps of clusters reveal relationships among the cases. A graphical query facility allows users to specify cluster maps dynamically. In addition, the tool supports an ever-present 'bird's-eye-view' of the information space being analyzed. The tool is particularly valuable for exploring patterns among stories grouped and viewed in the 2 and 3 dimensional views which would be difficult to discern from a hardcopy version.
- A. Gershman. 1981. Figuring Out What the User Wants: Steps Toward an Automatic Yellow Pages Assistant. Abstract: An experimental system, AYPA, for automatic Yellow Pages assistance is described. The system, which operates in the domain of automobiles, automobile parts, and related objects, reads the user's request in simple English, analyzes it and represents it in terms of the system's conceptual primitives. From this, the system tries to figure out the intent of the request and formulate a Yellow Pages query. It paraphrases the request back to the user in English and searches its data base for the relevant Yellow Pages categories. The system serves as a research vehicle for experiments with its various components and user interfaces.
- J. Carbonell, R. E. Cullingford, A. Gershman. 1981. Steps Toward Knowledge-Based Machine Translation. Abstract: This paper considers the possibilities for knowledge-based automatic text translation in the light of recent advances in artificial intelligence. It is argued that competent translation requires some reasonable depth of understanding of the source text, and, in particular, access to detailed contextual information. The following machine translation paradigm is proposed. First, the source text is analyzed and mapped into a language-free conceptual representation. Inference mechanisms then apply contextual world knowledge to augment the representation in various ways, adding information about items that were only implicit in the input text. Finally, a natural-language generator maps appropriate sections of the language-free representation into the target language. We discuss several difficult translation problems from this viewpoint with examples of English-to-Spanish and English-to-Russian translations; and illustrate possible solutions as embodied in a computer understander called SAM, which reads certain kinds of newspaper stories, then summarizes or paraphrases them in a variety of languages.
- A. Gershman. 1979. Knowledge-based parsing.. Abstract: Abstract : A model for knowledge-based natural language analysis is described. The model is applied to parsing English into Conceptual Dependency representations. The model processes sentences from left to right, one word at a time, using linguistic and non-linguistic knowledge to find the meaning of the input. It operates in three modes: structure-driven, position-driven, and situation-driven. The first two modes are expectation-based. In structure driven mode concepts underlying new input are expected to fill slots in the previously built conceptual structures. Noun groups are handled in position-driven mode which uses position-based pooling of expectations. When the first two modes fail to account for a new input, the parser goes into the third, situation-driven mode which tries to handle a situation by applying a series of appropriate experts. Four general kinds of knowledge are identified as necessary for language understanding: lexical knowledge, world knowledge, linguistic knowledge, and contextual knowledge.
- J. Carbonell, Richard E Cullinford, A. Gershman. 1978. Knowledge-Based Machine Translation.. Abstract: Abstract : This paper discusses knowledge-based machine translation research at Yale University Artificial Intelligence Laboratory. Our paradigm, illustrated by several working computer programs, is to analyze the source text into a language-free representation, apply world knowledge to infer information implicit in the input text, and generate the translation in various target languages. (Author)
- A. Gershman. 1977. Conceptual Analysis of Noun Groups in English. Abstract: An expectation-based system, NGP, f o r parsing Engl ish noun groups i n to the Conceptual Dependency representat ion is descr ibed. The system is a par t of ELI (Engl ish Language I n t e rp re te r ) which is used as the f ron t end to several na tu ra l language understanding systems and is capable of handl ing a wide range of sentences of considerable complex i ty . NGP processes the input from l e f t to r i g h t , one word at a t ime, using l i n g u i s t i c and world knowledge to f i nd the meaning of a noun group. D ic t ionary en t r i es fo r i n d i v i d u a l words contain much of the program's knowledge. In a d d i t i o n , a l i m i t e d a b i l i t y f o r the handl ing of s l i g h t l y i nco r rec t sentences and unknown words is incorpora ted. 0. I n t roduc t i on Every na tu ra l language processor has to have the a b i l i t y to i n t e r p r e t noun phrases. This paper describes a set of programs ca l led NGP (Noun Group Processor) which is an i n t e g r a l par t of ELI , the Engl ish Language In te rp re te r (Riesbeck and Schank 1976) which serves as the f r o n t end to three of the Yale na tu ra l language understanding systems, SAM, PAM and WEIS. SAM is a system capable of understanding s to r i es such as var ious newspaper repor ts by using s c r i p t s (Schank and Abelson 1975, 1977; Cu l l i ng fo rd 1975, 1977). PAM is an understanding system which uses general knowledge about peoples' goals and plans (Wilensky 1976). WEIS is a system which understands and c l a s s i f i e s a great v a r i e t y of i so la ted newspaper headlines on i n t e r n a t i o n a l r e l a t i o n s . Thus, our task was to process not only noun phrases of considerable complexity but also to i n t e r p r e t newspaper headl ines, which are not always grammatical ly c o r r e c t . The fo l l ow ing two examples i l l u s t r a t e the kind of sentences our system is able to handle. 1. A CONNECTICUT MAN, JOHN DOE, AGE 23, OF 342 COLLEGE AVENUE, NEW HAVEN WAS PRONOUNCED DEAD AT THE SCENE BY DR. DANA BLAUCHARD, MEDICAL EXAMINER. 2. FUNERAL OF INDIA 'S SHASTRI ATTENDED BY USSR KOSYGIN AND USA HUMPHREY. This work was supported in par t by the Advanced Research Projects Agency of the Department of Defense and monitored under the Of f i ce of Naval Research under cont rac t N00014-75-C-1111 To process such a la rge scope of sentences the program makes extensive use of i t s knowledge of the problem domain and the redundancy of n a t u r a l language expressions. This saves e f f o r t and permits co r rec t processing of such i r r e g u l a r i t i e s of input t ex t as missing commas and a r t i c l e s , or s l i g h t l y i nco r rec t word order. I t also provides f o r the a b i l i t y to ignore unknown words or ( i n some cases) to make p laus ib le i n t e r p r e t a t i o n s of unknown words. This knowledge is kept in the d i c t i o n a r y . The con t ro l mechanisms remain domain independent. NGP is a p roduc t i on l i ke system which uses expectat ions as i t s basic con t ro l mechanism. The problem wi th every p roduc t i on l i ke system is the tendency f o r the accumulation of a la rge number of expectat ions f i g h t i n g fo r a chance to be tes ted . In t h i s work I have t r i e d to develop a theory of how var ious expectat ions are organized and processed, which, I be l i eve , is in f ac t a theory of how people process na tu ra l language. The basic gu id ing p r i n c i p l e fo r t h i s theory was i t s i n t u i t i v e p l a u s i b i l i t y . !• Noun Group Semantics In t h i s paper we w i l l discuss two classes of noun groups according to the conceptual s t r uc tures they generate: PP P ic tu re Producers and CTP Concept Producers. PP's are def ined by Schank (Schank 1975) as concepts which tend to produce p ic tu res of r e a l wor ld items in the mind of a hearer. For example,
- A. Gershman. 1977. Analysing English Noun Groups for Their Conceptual Content.. Abstract: Abstract : An expectation-based system, Noun Group Processor(NGP), for parsing English noun groups into the Conceptual Dependency representation is described. The system is a part of ELI (English Language Interpreter) which is used as the front end to several natural language understanding systems and is capable of handling a wide range of sentences of considerable complexity. NGP processes the input from left to right, one word at a time, using linguistic and world knowledge to find the meaning of a noun group. Dictionary entries for individual words contain much of the program's knowledge. In addition, a limited ability for the handling of slightly incorrect sentences and unknown words is incorporated.
