Bhiksha Raj
Paper count: 404
- Thanh-Dat Truong, Ngan T. H. Le, B. Raj, J. Cothren, Khoa Luu. 2023. FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding. Abstract: Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, i.e., SYNTHIA $\rightarrow$ Cityscapes and GTA5 $\rightarrow$ Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance11The implementation of FREDOM is available at https://github.com/uark-cviu/FREDOM
- Pha Nguyen, Kha Gia Quach, J. Gauch, S. Khan, B. Raj, Khoa Luu. 2023. UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation. Abstract: Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.
- Hao Chen, R. Tao, Yue Fan, Yidong Wang, Jindong Wang, B. Schiele, Xingxu Xie, B. Raj, M. Savvides. 2023. SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning. Abstract: The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.
- Muhammad A Shah, B. Raj. 2023. Fixed Inter-Neuron Covariability Induces Adversarial Robustness. Abstract: The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \textit{without being trained on adversarially perturbed data
- Samiran Gode, Supreeth Bare, B. Raj, H. Yoo. 2023. Understanding political polarization using language models: A dataset and method. Abstract: Our paper aims to analyze political polarization in US political system using language models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates' views on the economy, healthcare, education, and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model‐based method that helps analyze how polarized a candidate is. Our data are divided into two parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, and so forth. We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization, we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer‐based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background. The code and data for the project will be available here: “https://github.com/samirangode/Understanding_Polarization”
- Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj. 2023. BASS: Block-wise Adaptation for Speech Summarization. Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
- Samiran Gode, Supreeth Bare, B. Raj, H. Yoo. 2023. Understanding Political Polarisation using Language Models: A dataset and method. Abstract: Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.
- Ankit Shah, Larry Tang, Po Hao Chou, Yilun Zheng, Ziqian Ge, B. Raj. 2023. An Approach to Ontological Learning from Weak Labels. Abstract: Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the "Is A" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.
- Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj. 2023. Rethinking Voice-Face Correlation: A Geometry View. Abstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.
- Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj. 2023. Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement. Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.
- Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Hojeong Lee, Ankit Shah, Shuo Han, YUNYANG ZENG, Amanda Shu, Haohui Liu, Xuankai Chang, Hamza Khalid, Minseon Gwak, Kawon Lee, Minjeong Kim, B. Raj. 2023. Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms. Abstract: In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.
- Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, B. Raj. 2023. Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms. Abstract: General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including audio tasks. In order to understand representations better, we conducted a thorough error analysis and visualization of HEAR 2021 submission results. Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks. Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021
- Muhammad A Shah, B. Raj. 2023. Training on Foveated Images Improves Robustness to Adversarial Attacks. Abstract: Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop \RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\% higher accuracy on perturbed data.
- Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj. 2023. Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.
- Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj. 2023. The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features. Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.
- Thanh-Dat Truong, Hoang-Quan Nguyen, B. Raj, Khoa Luu. 2023. Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments. Abstract: Continual semantic segmentation aims to learn new classes while maintaining the information from the previous classes. Although prior studies have shown impressive progress in recent years, the fairness concern in the continual semantic segmentation needs to be better addressed. Meanwhile, fairness is one of the most vital factors in deploying the deep learning model, especially in human-related or safety applications. In this paper, we present a novel Fairness Continual Learning approach to the semantic segmentation problem. In particular, under the fairness objective, a new fairness continual learning framework is proposed based on class distributions. Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning. Moreover, the proposed Conditional Structural Consistency loss further regularized the structural constraint of the predicted segmentation. Our proposed approach has achieved State-of-the-Art performance on three standard scene understanding benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness of the segmentation model.
- Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, B. Raj. 2023. PaintSeg: Training-free Segmentation via Painting. Abstract: The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation.
- YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj. 2023. TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement. Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.
- B. Raj. 2023. Improving sound event detection with ontologies. Abstract: Sound event recognition is the task of identifying and categorizing sounds in audio data. Automated algorithms for sound event recognition depend on having explicit models for individual sound event types to be recognized, which are trained on data tagged explicitly for those classes. The approach is data hungryand is fundamentally limited by the number of classes for which such data may be obtained. It also ignores the relationship between sounds being modeled. In this work, we attempt to address these deficiencies through the use of a human-generated sound ontology which represents sibling and parent–child relations between sound classes. We incorporate the relationships in the ontology through the design of an appropriate “loss” function (the objective function optimized to train sound-classifier models) that incorporates the relationships in the ontology, and through appropriate model update rules which utilize data from a class to update parameters (of both ontological siblings and parents). Through experiments run on the “Audioset” (a popular, large-scale dataset of 600 sound categories), we find that better-performing models can be trained for sound classes with a given dataset, and that the amount of new data required to train models for a novel sound class can be significantly reduced.
- L. Heller, Benjamin Elizalde, B. Raj, Soham Deshmukh. 2023. Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session. Abstract: Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on"Synergy between human and machine approaches to sound/scene recognition and processing"at the 2023 ICASSP meeting.
- Francisco Teixeira, A. Abad, B. Raj, I. Trancoso. 2022. Privacy-Preserving Automatic Speaker Diarization. Abstract: Automatic Speaker Diarization (ASD) is an enabling technology with numerous applications, which deals with recordings of multiple speakers, raising special concerns in terms of privacy. In fact, in remote settings, where recordings are shared with a server, clients relinquish not only the privacy of their conversation, but also of all the information that can be inferred from their voices. However, to the best of our knowledge, the development of privacy-preserving ASD systems has been overlooked thus far. In this work, we tackle this problem using a combination of two cryptographic techniques, Secure Multiparty Computation (SMC) and Secure Modular Hashing, and apply them to the two main steps of a cascaded ASD system: speaker embedding extraction and agglomerative hierarchical clustering. Our system is able to achieve a reasonable trade-off between performance and efficiency, presenting real-time factors of 1.1 and 1.6, for two different SMC security settings.
- Chonghan Chen, Qi Jiang1, Chih-Hao Wang, Noel Chen, Haohan Wang, Xiang Lorraine Li, B. Raj. 2022. Bear the Query in Mind: Visual Grounding with Query-conditioned Convolution. Abstract: Visual grounding is a task that aims to locate a target object according to a natural language expression. As a multi-modal task, feature interaction between textual and visual inputs is vital. However, previous solutions mainly handle each modality independently before fusing them together, which does not take full advantage of relevant textual information while extracting visual features. To better leverage the textual-visual relationship in visual grounding, we propose a Q uery-conditioned C onvolution M odule (QCM) that extracts query-aware visual features by incorporating query information into the generation of convolutional kernels. With our proposed QCM, the downstream fusion module receives visual features that are more discriminative and focused on the desired object described in the expression, leading to more accurate predictions. Extensive experiments on three popu-lar visual grounding datasets demonstrate that our method achieves state-of-the-art performance. In addition, the query-aware visual features are informative enough to achieve comparable performance to the latest methods when directly used for prediction without further multi-modal fusion.
- Muqiao Yang, Joseph Konan, David Bick, Anurag Kumar, Shinji Watanabe, B. Raj. 2022. Improving Speech Enhancement through Fine-Grained Speech Characteristics. Abstract: While deep learning based speech enhancement systems have made rapid progress in improving the quality of speech signals, they can still produce outputs that contain artifacts and can sound unnatural. We propose a novel approach to speech enhancement aimed at improving perceptual quality and naturalness of enhanced signals by optimizing for key characteristics of speech. We first identify key acoustic parameters that have been found to correlate well with voice quality (e.g. jitter, shimmer, and spectral flux) and then propose objective functions which are aimed at reducing the difference between clean speech and enhanced speech with respect to these features. The full set of acoustic features is the extended Geneva Acoustic Parameter Set (eGeMAPS), which includes 25 different attributes associated with perception of speech. Given the non-differentiable nature of these feature computation, we first build differentiable estimators of the eGeMAPS and then use them to fine-tune existing speech enhancement systems. Our approach is generic and can be applied to any existing deep learning based enhancement systems to further improve the enhanced speech signals. Experimental results conducted on the Deep Noise Suppression (DNS) Challenge dataset shows that our approach can improve the state-of-the-art deep learning based enhancement systems.
- Roshan Sharma, Tyler Vuong, Mark Lindsey, Hira Dhamyal, Rita Singh, B. Raj. 2022. Self-supervision and Learnable STRFs for Age, Emotion, and Country Prediction. Abstract: This work presents a multitask approach to the simultaneous estimation of age, country of origin, and emotion given vocal burst audio for the 2022 ICML Expressive Vocalizations Challenge E X V O -M ULTI T ASK track. The method of choice utilized a combination of spectro-temporal modulation and self-supervised features, followed by an encoder-decoder network organized in a multitask paradigm. We evaluate the complementarity between the tasks posed by examining independent task-speciﬁc and joint models, and explore the relative strengths of different feature sets. We also introduce a simple score fusion mechanism to leverage the complementarity of different feature sets for this task. We ﬁnd that robust data preprocessing in con-junction with score fusion over spectro-temporal receptive ﬁeld and HuBERT models achieved our best E X V O -M ULTI T ASK test score of 0.412. framework may be This is the hypothesis which to
- Xiang Li, H. Cao, Shijie Zhao, Junlin Li, Li Zhang, B. Raj. 2022. Panoramic Video Salient Object Detection with Ambisonic Audio Guidance. Abstract: Video salient object detection (VSOD), as a fundamental computer vision problem, has been extensively discussed in the last decade. However, all existing works focus on addressing the VSOD problem in 2D scenarios. With the rapid development of VR devices, panoramic videos have been a promising alternative to 2D videos to provide immersive feelings of the real world. In this paper, we aim to tackle the video salient object detection problem for panoramic videos, with their corresponding ambisonic audios. A multimodal fusion module equipped with two pseudo-siamese audio-visual context fusion (ACF) blocks is proposed to effectively conduct audio-visual interaction. The ACF block equipped with spherical positional encoding enables the fusion in the 3D context to capture the spatial correspondence between pixels and sound sources from the equirectangular frames and ambisonic audios. Experimental results verify the effectiveness of our proposed components and demonstrate that our method achieves state-of-the-art performance on the ASOD60K dataset.
- R. Olivier, B. Raj. 2022. There is more than one kind of robustness: Fooling Whisper with adversarial examples. Abstract: Whisper is a recent Automatic Speech Recognition (ASR) model displaying impressive robustness to both out-of-distribution inputs and random noise. In this work, we show that this robustness does not carry over to adversarial noise. We show that we can degrade Whisper performance dramatically, or even transcribe a target sentence of our choice, by generating very small input perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling the Whisper language detector we can very easily degrade the performance of multilingual models. These vulnerabilities of a widely popular open-source model have practical security implications and emphasize the need for adversarially robust ASR.
- Francisco Teixeira, A. Abad, B. Raj, I. Trancoso. 2022. Towards End-to-End Private Automatic Speaker Recognition. Abstract: The development of privacy-preserving automatic speaker veriﬁcation systems has been the focus of a number of studies with the intent of allowing users to authenticate themselves without risking the privacy of their voice. However, current privacy-preserving methods assume that the template voice representations (or speaker embeddings) used for authentication are extracted locally by the user. This poses two important issues: ﬁrst, knowledge of the speaker embedding extraction model may create security and robustness liabilities for the authentication system, as this knowledge might help attackers in crafting adversarial examples able to mislead the system; second, from the point of view of a service provider the speaker embedding extraction model is arguably one of the most valuable compo-nents in the system and, as such, disclosing it would be highly undesirable. In this work, we show how speaker embeddings can be extracted while keeping both the speaker’s voice and the service provider’s model private, using Secure Multiparty Computation. Further, we show that it is possible to obtain reasonable trade-offs between security and computational cost. This work is complementary to those showing how authentication may be performed privately, and thus can be considered as another step towards fully private automatic speaker recognition.
- Shentong Mo, Jingfei Xia, Xiaoqing Tan, B. Raj. 2022. Point3D: tracking actions as moving points with 3D CNNs. Abstract: Spatio-temporal action recognition has been a challenging task that involves detecting where and when actions occur. Current state-of-the-art action detectors are mostly anchor-based, requiring sensitive anchor designs and huge computations due to calculating large numbers of anchor boxes. Motivated by nascent anchor-free approaches, we propose Point3D, a flexible and computationally efficient network with high precision for spatio-temporal action recognition. Our Point3D consists of a Point Head for action localization and a 3D Head for action classification. Firstly, Point Head is used to track center points and knot key points of humans to localize the bounding box of an action. These location features are then piped into a time-wise attention to learn long-range dependencies across frames. The 3D Head is later deployed for the final action classification. Our Point3D achieves state-of-the-art performance on the JHMDB, UCF101-24, and AVA benchmarks in terms of frame-mAP and video-mAP. Comprehensive ablation studies also demonstrate the effectiveness of each module proposed in our Point3D.
- Joseph P. Turian, Jordie Shier, H. Khan, B. Raj, Björn Schuller, C. Steinmetz, C. Malloy, G. Tzanetakis, Gissel Velarde, K. McNally, Max Henry, Nicolas Pinto, Camille Noufi, Christian Clough, Dorien Herremans, Eduardo Fonseca, Jesse Engel, J. Salamon, P. Esling, Pranay Manocha, Shinji Watanabe, Zeyu Jin, Yonatan Bisk. 2022. HEAR: Holistic Evaluation of Audio Representations. Abstract: What audio embedding approach generalizes best to a wide range of downstream tasks across a variety of everyday domains without fine-tuning? The aim of the HEAR benchmark is to develop a general-purpose audio representation that provides a strong basis for learning in a wide variety of tasks and scenarios. HEAR evaluates audio representations using a benchmark suite across a variety of domains, including speech, environmental sound, and music. HEAR was launched as a NeurIPS 2021 shared challenge. In the spirit of shared exchange, each participant submitted an audio embedding model following a common API that is general-purpose, open-source, and freely available to use. Twenty-nine models by thirteen external teams were evaluated on nineteen diverse downstream tasks derived from sixteen datasets. Open evaluation code, submitted models and datasets are key contributions, enabling comprehensive and reproducible evaluation, as well as previously impossible longitudinal studies. It still remains an open question whether one single general-purpose audio representation can perform as holistically as the human ear.
- Xiang Li, Jinglu Wang, Xiaohao Xu, B. Raj, Yan Lu. 2022. Online Video Instance Segmentation via Robust Context Fusion. Abstract: Video instance segmentation (VIS) aims at classifying, segmenting and tracking object instances in video sequences. Recent transformer-based neural networks have demonstrated their powerful capability of modeling spatio-temporal correlations for the VIS task. Relying on video- or clip-level input, they suffer from high latency and computational cost. We propose a robust context fusion network to tackle VIS in an online fashion, which predicts instance segmentation frame-by-frame with a few preceding frames. To acquire the precise and temporal-consistent prediction for each frame efficiently, the key idea is to fuse effective and compact context from reference frames into the target frame. Considering the different effects of reference and target frames on the target prediction, we first summarize contextual features through importance-aware compression. A transformer encoder is adopted to fuse the compressed context. Then, we leverage an order-preserving instance embedding to convey the identity-aware information and correspond the identities to predicted instance masks. We demonstrate that our robust fusion network achieves the best performance among existing online VIS methods and is even better than previously published clip-level methods on the Youtube-VIS 2019 and 2021 benchmarks. In addition, visual objects often have acoustic signatures that are naturally synchronized with them in audio-bearing video recordings. By leveraging the flexibility of our context fusion network on multi-modal data, we further investigate the influence of audios on the video-dense prediction task, which has never been discussed in existing works. We build up an Audio-Visual Instance Segmentation dataset, and demonstrate that acoustic signals in the wild scenarios could benefit the VIS task.
- Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, B. Raj, Rita Singh. 2022. Describing emotions with acoustic property prompts for speech emotion recognition. Abstract: Emotions lie on a broad continuum and treating emotions as a discrete number of classes limits the ability of a model to capture the nuances in the continuum. The challenge is how to describe the nuances of emotions and how to enable a model to learn the descriptions. In this work, we devise a method to automatically create a description (or prompt) for a given audio by computing acoustic properties, such as pitch, loudness, speech rate, and articulation rate. We pair a prompt with its corresponding audio using 5 different emotion datasets. We trained a neural network model using these audio-text pairs. Then, we evaluate the model using one more dataset. We investigate how the model can learn to associate the audio with the descriptions, resulting in performance improvement of Speech Emotion Recognition and Speech Audio Retrieval. We expect our ﬁndings to motivate research describing the broad continuum of emotion.
- Joseph P. Turian, Jordie Shier, H. Khan, B. Raj, Björn Schuller, C. Steinmetz, C. Malloy, G. Tzanetakis, Gissel Velarde, K. McNally, Max Henry, Nicolas Pinto, Camille Noufi, Christian Clough, Dorien Herremans, Eduardo Fonseca, Jesse Engel, J. Salamon, P. Esling, Pranay Manocha, Shinji Watanabe, Zeyu Jin, Yonatan Bisk. 2022. HEAR 2021: Holistic Evaluation of Audio Representations. Abstract: What audio embedding approach generalizes best to a wide range of downstream tasks across a variety of everyday domains without ﬁne-tuning? The aim of the HEAR 2021 NeurIPS challenge is to develop a general-purpose audio representation that provides a strong basis for learning in a wide variety of tasks and scenarios. HEAR 2021 evaluates audio representations using a benchmark suite across a variety of domains, including speech, environmental sound, and music. In the spirit of shared exchange, each participant submitted an audio embedding model following a common API that is general-purpose, open-source, and freely available to use. Twenty-nine models by thirteen external teams were evaluated on nineteen diverse downstream tasks derived from sixteen datasets. Open evaluation code, submitted models and datasets are key contributions, enabling comprehensive and reproducible evaluation, as well as previously impossible longitudinal studies. It still remains an open question whether one single general-purpose audio representation can perform as holistically as the human ear.
- R. Olivier, B. Raj. 2022. Recent improvements of ASR models in the face of adversarial attacks. Abstract: Like many other tasks involving neural networks, Speech Recognition models are vulnerable to adversarial attacks. However recent research has pointed out differences between attacks and defenses on ASR models compared to image models. Improving the robustness of ASR models requires a paradigm shift from evaluating attacks on one or a few models to a systemic approach in evaluation. We lay the ground for such research by evaluating on various architectures a representative set of adversarial attacks: targeted and untargeted, optimization and speech processing-based, white-box, black-box and targeted attacks. Our results show that the relative strengths of different attack algorithms vary considerably when changing the model architecture, and that the results of some attacks are not to be blindly trusted. They also indicate that training choices such as self-supervised pretraining can significantly impact robustness by enabling transferable perturbations. We release our source code as a package that should help future research in evaluating their attacks and defenses.
- Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Yan Lu, B. Raj. 2022. R^2VOS: Robust Referring Video Object Segmentation via Relational Multimodal Cycle Consistency. Abstract: Referring video object segmentation (R-VOS) aims to segment the object masks in a video given a referring linguistic expression to the object. It is a recently introduced task attracting growing research attention. However, all existing works make a strong assumption: The object depicted by the expression must exist in the video, namely, the expression and video must have an object-level semantic consensus. This is often violated in real-world applications where an expression can be queried to false videos, and existing methods always fail in such false queries due to abusing the assumption. In this work, we emphasize that studying semantic consensus is necessary to improve the robustness of R-VOS. Accordingly, we pose an extended task from R-VOS without the semantic consensus assumption, named Robust R-VOS ( R 2 -VOS). The R 2 -VOS task is essentially related to the joint modeling of the primary R-VOS task and its dual problem (text reconstruction). We embrace the observation that the embedding spaces have relational consistency through the cycle of text-video-text transformation, which connects the primary and dual problems. We leverage the cycle consistency to discriminate the semantic consensus, thus advancing the primary task. Parallel optimization of the primary and dual problems are enabled by introducing an early grounding medium. A new evaluation dataset, R 2 -Youtube-VOS, is collected to measure the robustness of R-VOS models against unpaired videos and expressions. Extensive experiments demonstrate that our method not only identiﬁes negative pairs of unrelated expressions and videos, but also improves the segmentation accuracy for positive pairs with a superior disambiguating ability. Our model achieves the state-of-the-art performance on Ref-DAVIS17, Ref-Youtube-VOS, and the novel R 2 -Youtube-VOS dataset.
- Haoxing Chen, Yue Fan, Yidong Wang, Jindong Wang, B. Schiele, Xingxu Xie, M. Savvides, B. Raj. 2022. An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. Abstract: Semi-supervised learning (SSL) has shown great promise in leveraging unlabeled data to improve model performance. While standard SSL assumes uniform data distribution, we consider a more realistic and challenging setting called imbalanced SSL, where imbalanced class distributions occur in both labeled and unlabeled data. Although there are existing endeavors to tackle this challenge, their performance degenerates when facing severe imbalance since they can not reduce the class imbalance sufficiently and effectively. In this paper, we study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance by simply supplementing labeled data with pseudo-labels, according to the difference in class distribution from the most frequent class. Such a simple baseline turns out to be highly effective in reducing class imbalance. It outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and 16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127 respectively. The reduced imbalance results in faster convergence and better pseudo-label accuracy of SimiS. The simplicity of our method also makes it possible to be combined with other re-balancing techniques to improve the performance further. Moreover, our method shows great robustness to a wide range of data distributions, which holds enormous potential in practice. Code will be publicly available.
- Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, M. Savvides, T. Shinozaki, B. Raj, Zhen Wu, Jindong Wang. 2022. FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning. Abstract: Semi-supervised Learning (SSL) has witnessed great success owing to the impressive performances brought by various methods based on pseudo labeling and consistency regularization. However, we argue that existing methods might fail to utilize the unlabeled data more effectively since they either use a pre-defined / fixed threshold or an ad-hoc threshold adjusting scheme, resulting in inferior performance and slow convergence. We first analyze a motivating example to obtain intuitions on the relationship between the desirable threshold and model's learning status. Based on the analysis, we hence propose FreeMatch to adjust the confidence threshold in a self-adaptive manner according to the model's learning status. We further introduce a self-adaptive class fairness regularization penalty to encourage the model for diverse predictions during the early training stage. Extensive experiments indicate the superiority of FreeMatch especially when the labeled data are extremely rare. FreeMatch achieves 5.78%, 13.59%, and 1.28% error rate reduction over the latest state-of-the-art method FlexMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class, and ImageNet with 100 labels per class, respectively. Moreover, FreeMatch can also boost the performance of imbalanced SSL. The codes can be found at https://github.com/microsoft/Semi-supervised-learning.
- R. Olivier, B. Raj. 2022. How Many Perturbations Break This Model? Evaluating Robustness Beyond Adversarial Accuracy. Abstract: Robustness to adversarial attacks is typically evaluated with adversarial accuracy. While essential, this metric does not capture all aspects of robustness and in particular leaves out the question of how many perturbations can be found for each point. In this work, we introduce an alternative approach, adversarial sparsity, which quantifies how difficult it is to find a successful perturbation given both an input point and a constraint on the direction of the perturbation. We show that sparsity provides valuable insight into neural networks in multiple ways: for instance, it illustrates important differences between current state-of-the-art robust models them that accuracy analysis does not, and suggests approaches for improving their robustness. When applying broken defenses effective against weak attacks but not strong ones, sparsity can discriminate between the totally ineffective and the partially effective defenses. Finally, with sparsity we can measure increases in robustness that do not affect accuracy: we show for example that data augmentation can by itself increase adversarial robustness, without using adversarial training.
- Roshan Sharma, Hira Dhamyal, B. Raj, Rita Singh. 2022. Unifying the Discrete and Continuous Emotion labels for Speech Emotion Recognition. Abstract: Traditionally, in paralinguistic analysis for emotion detection from speech, emotions have been identified with discrete or dimensional (continuous-valued) labels. Accordingly, models that have been proposed for emotion detection use one or the other of these label types. However, psychologists like Russell and Plutchik have proposed theories and models that unite these views, maintaining that these representations have shared and complementary information. This paper is an attempt to validate these viewpoints computationally. To this end, we propose a model to jointly predict continuous and discrete emotional attributes and show how the relationship between these can be utilized to improve the robustness and performance of emotion recognition tasks. Our approach comprises multi-task and hierarchical multi-task learning frameworks that jointly model the relationships between continuous-valued and discrete emotion labels. Experimental results on two widely used datasets (IEMOCAP and MSPPodcast) for speech-based emotion recognition show that our model results in statistically significant improvements in performance over strong baselines with non-unified approaches. We also demonstrate that using one type of label (discrete or continuous-valued) for training improves recognition performance in tasks that use the other type of label. Experimental results and reasoning for this approach (called the mismatched training approach) are also presented.
- Larry Tang, Po-Hao Chou, Yilun Zheng, Ziqian Ge, Ankit Shah, B. Raj. 2022. Ontological Learning from Weak Labels. Abstract: Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the"Is A"relations between the concepts. We first re-implemented the model proposed by soundevent_ontology with modification to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Siamese does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. In our experiments, we also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best Siamese-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline Siamese but about the same as our models that do not use ontology information.
- R. Olivier, H. Abdullah, B. Raj. 2022. Watch What You Pretrain For: Targeted, Transferable Adversarial Examples on Self-Supervised Speech Recognition models. Abstract: A targeted adversarial attack produces audio samples that can force an Automatic Speech Recognition (ASR) system to output attacker-chosen text. To exploit ASR models in real-world, black-box settings, an adversary can leverage the transferability property, i.e. that an adversarial sample produced for a proxy ASR can also fool a different remote ASR. However recent work has shown that transferability against large ASR models is very difficult. In this work, we show that modern ASR architectures, specifically ones based on Self-Supervised Learning, are in fact vulnerable to transferability. We successfully demonstrate this phenomenon by evaluating state-of-the-art self-supervised ASR models like Wav2Vec2, HuBERT, Data2Vec and WavLM. We show that with low-level additive noise achieving a 30dB Signal-Noise Ratio, we can achieve target transferability with up to 80% accuracy. Next, we 1) use an ablation study to show that Self-Supervised learning is the main cause of that phenomenon, and 2) we provide an explanation for this phenomenon. Through this we show that modern ASR architectures are uniquely vulnerable to adversarial security threats.
- Kashu Yamazaki, Khoa T. Vo, Sang Truong, B. Raj, Ngan T. H. Le. 2022. VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning. Abstract: Video Paragraph Captioning aims to generate a multi-sentence description of an untrimmed video with multiple temporal event locations in a coherent storytelling. 
Following the human perception process, where the scene is effectively understood by decomposing it into visual (e.g. human, animal) and non-visual components (e.g. action, relations) under the mutual influence of vision and language, we first propose a visual-linguistic (VL) feature. In the proposed VL feature, the scene is modeled by three modalities including (i) a global visual environment; (ii) local visual main agents; (iii) linguistic scene elements. We then introduce an autoregressive Transformer-in-Transformer (TinT) to simultaneously capture the semantic coherence of intra- and inter-event contents within a video. Finally, we present a new VL contrastive loss function to guarantee the learnt embedding features are consistent with the captions semantics. Comprehensive experiments and extensive ablation studies on the ActivityNet Captions and YouCookII datasets show that the proposed Visual-Linguistic Transformer-in-Transform (VLTinT) outperforms previous state-of-the-art methods in terms of accuracy and diversity. The source code is made publicly available at: https://github.com/UARK-AICV/VLTinT.
- R. Olivier, B. Raj. 2022. Not all broken defenses are equal: The dead angles of adversarial accuracy. Abstract: Robustness to adversarial attack is typically evaluated with adversarial accuracy. This metric is however too coarse to properly capture all robustness properties of machine learning models. Many defenses, when evaluated against a strong attack, do not provide accuracy improvements while still con-tributing partially to adversarial robustness. Popular certiﬁcation methods suffer from the same issue, as they provide a lower bound to accuracy. To capture ﬁner robustness properties we propose a new metric for L 2 robustness, adversarial angular sparsity , which partially answers the question “how many adversarial examples are there around an in-put”. We demonstrate its usefulness by evaluating both “strong” and “weak” defenses. We show that some state-of-the-art defenses, delivering very similar accuracy, can have very different sparsity on the inputs that they are not robust on. We also show that some weak defenses actually decrease robustness, while others strengthen it in a measure that accuracy cannot capture. These differences are predictive of how useful such defenses can become when combined with adversarial training.
- Hira Dhamyal, B. Raj, Rita Singh. 2022. Positional Encoding for Capturing Modality Specific Cadence for Emotion Detection. Abstract: Emotion detection from a single modality, such as an audio or text stream, has been known to be a challenging task. While encouraging results have been obtained by using joint evidence from multiple streams, combining such evidence in optimal ways is an open challenge. In this paper, we claim that al-though the multi-modalities like audio, phoneme sequence ids and word sequence ids are related to each other, they also have their individual local ‘cadence’, which is important to be modelled for the task of emotion recognition. We model the local cadence by using separate ‘positional encodings’ for each modality in a transformer architecture. Our results show that emotion detection based on this strategy is better than when the modality specific cadence is ignored or normalized out by using a shared positional encoding. We also find that capturing the modality interdependence is not as important as is capturing of the local cadence of individual modalities. We conduct our experiments on the IEMOCAP and CMU-MOSI datasets to demonstrate the effectiveness of the proposed methodology for combining multi-modal evidence.
- Roshan Sharma, B. Raj. 2022. Cross-utterance context for multimodal video transcription. Abstract: In multimodal speech transcription of videos, audio and video are segmented using the concept of utterances, and transcription is performed independently on these segments. However, for real-world video transcription, multimodal information from past utterances can be leveraged to contextualize and improve video transcription. In this work, we first build multimodal speech recognition systems on instructional YouTube videos using the How2 corpus. We examine different visual representations and multimodal fusion techniques to infuse visual information into audio-only models. Then we explore methods to embed past context by training on longer input sequences. Experiments on the How2- 300h data demonstrate the importance of multi-modality and long-term context, which result in a 1 % WER absolute improvement over audio-only speech recognizers.
- Ankit Shah, Hira Dhamyal, Yang Gao, Rita Singh, B. Raj. 2022. On the pragmatism of using binary classifiers over data intensive neural network classifiers for detection of COVID-19 from voice. Abstract: Lately, there has been a global effort by multiple research groups to detect COVID-19 from voice. Different researchers use different kinds of information from the voice signal to achieve this. Various types of phonated sounds and the sound of cough and breath have all been used with varying degrees of success in automated voice-based COVID-19 detection apps. In this paper, we show that detecting COVID-19 from voice does not require custom-made non-standard features or complicated neural network classifiers rather it can be successfully done with just standard features and simple binary classifiers. In fact, we show that the latter is not only more accurate and interpretable and also more computationally efficient in that they can be run locally on small devices. We demonstrate this from a human-curated dataset collected and calibrated in clinical settings. On this dataset which comprises over 1000 speakers, a simple binary classifier is able to achieve 94% detection accuracy.
- Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, B. Raj, Yan Lu. 2022. Robust Referring Video Object Segmentation with Cyclic Structural Consensus. Abstract: Referring Video Object Segmentation (R-VOS) is a challenging task that aims to segment an object in a video based on a linguistic expression. Most existing R-VOS methods have a critical assumption: the object referred to must appear in the video. This assumption, which we refer to as "semantic consensus", is often violated in real-world scenarios, where the expression may be queried against false videos. In this work, we highlight the need for a robust R-VOS model that can handle semantic mismatches. Accordingly, we propose an extended task called Robust R-VOS (R2-VOS), which accepts unpaired video-text inputs. We tackle this problem by jointly modeling the primary R-VOS problem and its dual (text reconstruction). A structural text-to-text cycle constraint is introduced to discriminate semantic consensus between video-text pairs and impose it in positive pairs, thereby achieving multi-modal alignment from both positive and negative pairs. Our structural constraint effectively addresses the challenge posed by linguistic diversity, overcoming the limitations of previous methods that relied on the point-wise constraint. A new evaluation dataset, R2-Youtube-VOS is constructed to measure the model robustness. Our model achieves state-of-the-art performance on R-VOS benchmarks, Ref-DAVIS17 and Ref-Youtube-VOS, and also our R2-Youtube-VOS dataset.
- Roshan Sharma, B. Raj. 2022. XNOR-FORMER: Learning Accurate Approximations in Long Speech Transformers. Abstract: Transformers are among the state of the art for many tasks in speech, vision, and natural language processing, among others. Self-attentions, which are crucial contributors to this performance have quadratic computational complexity, which makes training on longer input sequences challenging. Prior work has produced state-of-the-art transformer variants with linear attention, however, current models sacrifice performance to achieve efficient implementations. In this work, we develop a novel linear transformer by examining the properties of the key-query product within self-attentions. Our model outperforms state of the art approaches on speech recognition and speech summarization, resulting in 1 % absolute WER improvement on the Librispeech-100 speech recognition benchmark and a new INTERVIEW speech recognition benchmark, and 5 points on ROUGE for summarization with How2.
- Yidong Wang, Hao Chen, Yue Fan, Wangbin Sun, R. Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe Guo, Heli Qi, Zhen Wu, Yu-Feng Li, Satoshi Nakamura, Weirong Ye, M. Savvides, B. Raj, T. Shinozaki, B. Schiele, Jindong Wang, Xingxu Xie, Yue Zhang. 2022. USB: A Unified Semi-supervised Learning Benchmark. Abstract: Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) for classification by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with TorchSSL.
- Yandong Wen, Weiyang Liu, Adrian Weller, B. Raj, Rita Singh. 2021. SphereFace2: Binary Classification is All You Need for Deep Face Recognition. Abstract: State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we first identify the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the “competitive” nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this “one-vs-all” binary classification framework so that it can outperform current competitive methods. We conduct comprehensive experiments on popular benchmarks to demonstrate that SphereFace2 can consistently outperform current state-of-the-art deep face recognition methods.
- Muhammad A Shah, R. Olivier, B. Raj. 2021. Optimal Strategies For Comparing Covariates To Solve Matching Problems. Abstract: Many machine learning tasks can be posed as matching problems in which we are given a “probe” entry that we expect matches some of the entries in our “gallery”. The general solution to these problems is to retrieve matching entries based on statistical dependencies between the probe and the gallery data that are learned using complex models. Often, however, there are other common covariates to the probe and gallery data which might be easily inferred and may explain some of the statistical dependencies between the two. In this paper we present a probabilistic framework to derive optimal matching strategies based only on covariate features for three broad tasks, namely N-way classification, pairwise verification and ranking. We use canonical metrics to determine the maximum performance that can be expected if only covariate features are used and determine the marginal gain of using complex models. We find that covariate matching achieves an EER within 10% of a CNN in the verification task, and an MAP within 22% of the a DNN based model in the ranking task.
- M. J. Correia, Francisco Teixeira, Catarina Botelho, I. Trancoso, B. Raj. 2021. The in-the-Wild Speech Medical Corpus. Abstract: Automatic detection of speech affecting (SA) diseases has received significant attention, particularly in clinical scenarios. However, the same task in in-the-wild conditions is often neglected, in part, due to the lack of appropriate datasets.In this work, we present the in-the-Wild Speech Medical (WSM) Corpus, a collection of in-the-wild videos, featuring subjects potentially affected by a SA disease - specifically, depression or Parkinson’s disease. The WSM Corpus contains a total 928 videos, and over 131 hours of speech. Each video is accompanied by a crowdsourced annotation for perceived age/gender, and self-reported health status of the speaker. The WSM Corpus is balanced over all the labels.In this work we present a detailed description of the collection, and annotation processes of the WSM corpus. Furthermore, we present present several baseline systems for the detection of SA diseases using speech alone, thus motivating the use of this type of in-the-wild data in paralinguistic audiovisual tasks.
- Benjamin Elizalde, Radu Revutchi, Samarjit Das, B. Raj, Ian Lane, L. Heller. 2021. Identifying Actions for Sound Event Classification. Abstract: In Psychology, actions are paramount for humans to identify sound events. In Machine Learning (ML), action recognition achieves high accuracy; however, it has not been asked whether identifying actions can benefit Sound Event Classification (SEC), as opposed to mapping the audio directly to a sound event. Therefore, we propose a new Psychology-inspired approach for SEC that includes identification of actions via human listeners. To achieve this goal, we used crowdsourcing to have listeners identify 20 actions that in isolation or in combination may have produced any of the 50 sound events in the well-studied dataset ESC-50. The resulting annotations for each audio recording relate actions to a database of sound events for the first time. The annotations were used to create semantic representations called Action Vectors (AVs). We evaluated SEC by comparing the AVs with two types of audio features - log-mel spectrograms and state-of-the-art audio embeddings. Because audio features and AVs capture different abstractions of the acoustic content, we combined them and achieved one of the highest reported accuracies (88%).
- Muhammad A Shah, R. Olivier, B. Raj. 2021. Towards Adversarial Robustness Via Compact Feature Representations. Abstract: Deep Neural Networks (DNNs), while providing state-of-the-art performance in a wide variety of tasks, have been shown to be vulnerable to adversarial attacks. Recent studies have posited that this vulnerability arises because DNNs operate over a grossly overspecified input space with very sparse human supervision due to which they tend to learn spurious features that humans would ignore. These spurious features provide an attack vector for the adversary because perturbing these features would not alter the human’s decision but may alter the model’s prediction. In this paper we explore hypothesis that reducing the size of the model’s feature representation while maintaining its generalizability would discard spurious features while retaining perceptually relevant ones. We find that after the size of the feature representation has been reduced the models exhibit increased adversarial robustness, while suffering only a minimal loss in accuracy. In addition to being more robust, models with compact feature representations have the benefit of being more resource efficient.
- B. R. Chernyak, B. Raj, Tamir Hazan, Joseph Keshet. 2021. Constant Random Perturbations Provide Adversarial Robustness with Minimal Effect on Accuracy. Abstract: This paper proposes an attack-independent (non-adversarial training) technique for improving adversarial robustness of neural network models, with minimal loss of standard accuracy. We suggest creating a neighborhood around each training example, such that the label is kept constant for all inputs within that neighborhood. Unlike previous work that follows a similar principle, we apply this idea by extending the training set with multiple perturbations for each training example, drawn from within the neighborhood. These perturbations are model independent, and remain constant throughout the entire training process. We analyzed our method empirically on MNIST, SVHN, and CIFAR-10, under different attacks and conditions. Results suggest that the proposed approach improves standard accuracy over other defenses while having increased robustness compared to vanilla adversarial training.
- Kaiqin Hu, Jie Shao, Yuan Liu, B. Raj, M. Savvides, Zhiqiang Shen. 2021. Contrast and Order Representations for Video Self-supervised Learning. Abstract: This paper studies the problem of learning self-supervised representations on videos. In contrast to image modality that only requires appearance information on objects or scenes, video needs to further explore the relations between multiple frames/clips along the temporal dimension. However, the recent proposed contrastive-based self-supervised frameworks do not grasp such relations explicitly since they simply utilize two augmented clips from the same video and compare their distance without referring to their temporal relation. To address this, we present a contrast-and-order representation (CORP) framework for learning self-supervised video representations that can automatically capture both the appearance information within each frame and temporal information across different frames. In particular, given two video clips, our model first predicts whether they come from the same input video, and then predict the temporal ordering of the clips if they come from the same video. We also propose a novel decoupling attention method to learn symmetric similarity (contrast) and anti-symmetric patterns (order). Such design involves neither extra parameters nor computation, but can speed up the learning process and improve accuracy compared to the vanilla multi-head attention. We extensively validate the representation ability of our learned video features for the downstream action recognition task on Kinetics-400 and Something-something V2. Our method outperforms previous state-of-the-arts by a significant margin.
- Weiyang Liu, Yandong Wen, B. Raj, Rita Singh, Adrian Weller. 2021. SphereFace Revived: Unifying Hyperspherical Face Recognition. Abstract: This paper addresses the deep face recognition problem under an open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. To this end, hyperspherical face recognition, as a promising line of research, has attracted increasing attention and gradually become a major focus in face recognition research. As one of the earliest works in hyperspherical face recognition, SphereFace explicitly proposed to learn face embeddings with large inter-class angular margin. However, SphereFace still suffers from severe training instability which limits its application in practice. In order to address this problem, we introduce a unified framework to understand large angular margin in hyperspherical face recognition. Under this framework, we extend the study of SphereFace and propose an improved variant with substantially better training stability – SphereFace-R. Specifically, we propose two novel ways to implement the multiplicative margin, and study SphereFace-R under three different feature normalization schemes (no feature normalization, hard feature normalization and soft feature normalization). We also propose an implementation strategy – “characteristic gradient detachment” – to stabilize training. Extensive experiments on SphereFace-R show that it is consistently better than or competitive with state-of-the-art methods.
- Anxiang Zhang, Ankit Shah, B. Raj. 2021. Training image classifiers using Semi-Weak Label Data. Abstract: In Multiple Instance learning (MIL), weak labels are provided at the bag level with only presence/absence information known. However, there is a considerable gap in performance in comparison to a fully supervised model, limiting the practical applicability of MIL approaches. Thus, this paper introduces a novel semi-weak label learning paradigm as a middle ground to mitigate the problem. We define semi-weak label data as data where we know the presence or absence of a given class and the exact count of each class as opposed to knowing the label proportions. We then propose a two-stage framework to address the problem of learning from semi-weak labels. It leverages the fact that counting information is non-negative and discrete. Experiments are conducted on generated samples from CIFAR-10. We compare our model with a fully-supervised setting baseline, a weakly-supervised setting baseline and learning from pro-portion (LLP) baseline. Our framework not only outperforms both baseline models for MIL-based weakly super-vised setting and learning from proportion setting, but also gives comparable results compared to the fully supervised model. Further, we conduct thorough ablation studies to analyze across datasets and variation with batch size, losses architectural changes, bag size and regularization
- R. Olivier, B. Raj. 2021. Sequential Randomized Smoothing for Adversarially Robust Speech Recognition. Abstract: While Automatic Speech Recognition has been shown to be vulnerable to adversarial attacks, defenses against these attacks are still lagging. Existing, naive defenses can be partially broken with an adaptive attack. In classification tasks, the Randomized Smoothing paradigm has been shown to be effective at defending models. However, it is difficult to apply this paradigm to ASR tasks, due to their complexity and the sequential nature of their outputs. Our paper overcomes some of these challenges by leveraging speech-specific tools like enhancement and ROVER voting to design an ASR model that is robust to perturbations. We apply adaptive versions of state-of-the-art attacks, such as the Imperceptible ASR attack, to our model, and show that our strongest defense is robust to all attacks that use inaudible noise, and can only be broken with very high distortion.
- Soham Deshmukh, B. Raj, Rita Singh. 2021. Improving weakly supervised sound event detection with self-supervised auxiliary tasks. Abstract: While multitask and transfer learning has shown to improve the performance of neural networks in limited data settings, they require pretraining of the model on large datasets beforehand. In this paper, we focus on improving the performance of weakly supervised sound event detection in low data and noisy settings simultaneously without requiring any pretraining task. To that extent, we propose a shared encoder architecture with sound event detection as a primary task and an additional secondary decoder for a self-supervised auxiliary task. We empirically evaluate the proposed framework for weakly supervised sound event detection on a remix dataset of the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 dB SNR. To ensure we retain the localisation information of multiple sound events, we propose a two-step attention pooling mechanism that provides a time-frequency localisation of multiple audio events in the clip. The proposed framework with two-step attention outperforms existing benchmark models by 22.3%, 12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an ablation study to determine the contribution of the auxiliary task and two-step attention pooling to the SED performance improvement.
- Yandong Wen, Weiyang Liu, B. Raj, Rita Singh. 2021. Self-Supervised 3D Face Reconstruction via Conditional Estimation. Abstract: We present a conditional estimation (CEST) framework to learn 3D facial parameters from 2D single-view images by self-supervised training from videos. CEST is based on the process of analysis by synthesis, where the 3D facial parameters (shape, reflectance, viewpoint, and illumination) are estimated from the face image, and then recombined to reconstruct the 2D face image. In order to learn semantically meaningful 3D facial parameters without explicit access to their labels, CEST couples the estimation of different 3D facial parameters by taking their statistical dependency into account. Specifically, the estimation of any 3D facial parameter is not only conditioned on the given image, but also on the facial parameters that have already been derived. Moreover, the reflectance symmetry and consistency among the video frames are adopted to improve the disentanglement of facial parameters. Together with a novel strategy for incorporating the reflectance symmetry and consistency, CEST can be efficiently trained with in-the-wild video clips. Both qualitative and quantitative experiments demonstrate the effectiveness of CEST.
- Wenbo Liu, Ming Li, Xiaobing Zou, B. Raj. 2021. Discriminative Dictionary Learning for Autism Spectrum Disorder Identification. Abstract: Autism Spectrum Disorder (ASD) is a group of lifelong neurodevelopmental disorders with complicated causes. A key symptom of ASD patients is their impaired interpersonal communication ability. Recent study shows that face scanning patterns of individuals with ASD are often different from those of typical developing (TD) ones. Such abnormality motivates us to study the feasibility of identifying ASD children based on their face scanning patterns with machine learning methods. In this paper, we consider using the bag-of-words (BoW) model to encode the face scanning patterns, and propose a novel dictionary learning method based on dual mode seeking for better BoW representation. Unlike k-means which is broadly used in conventional BoW models to learn dictionaries, the proposed method captures discriminative information by finding atoms which maximizes both the purity and coverage of belonging samples within one class. Compared to the rich literature of ASD studies from psychology and neural science, our work marks one of the relatively few attempts to directly identify high-functioning ASD children with machine learning methods. Experiments demonstrate the superior performance of our method with considerable gain over several baselines. Although the proposed work is yet too preliminary to directly replace existing autism diagnostic observation schedules in the clinical practice, it shed light on future applications of machine learning methods in early screening of ASD.
- Thanh-Dat Truong, C. Duong, T. D. Vu, H. Pham, B. Raj, Ngan T. H. Le, Khoa Luu. 2021. The Right to Talk: An Audio-Visual Transformer Approach. Abstract: Turn-taking has played an essential role in structuring the regulation of a conversation. The task of identifying the main speaker (who is properly taking his/her turn of speaking) and the interrupters (who are interrupting or reacting to the main speaker’s utterances) remains a challenging task. Although some prior methods have partially addressed this task, there still remain some limitations. Firstly, a direct association of Audio and Visual features may limit the correlations to be extracted due to different modalities. Secondly, the relationship across temporal segments helping to maintain the consistency of localization, separation and conversation contexts is not effectively exploited. Finally, the interactions between speakers that usually contain the tracking and anticipatory decisions about transition to a new speaker is usually ignored. Therefore, this work introduces a new Audio-Visual Transformer approach to the problem of localization and highlighting the main speaker in both audio and visual channels of a multi-speaker conversation video in the wild. The proposed method exploits different types of correlations presented in both visual and audio signals. The temporal audio-visual relationships across spatial-temporal space are anticipated and optimized via the self-attention mechanism in a Transformer structure. Moreover, a newly collected dataset is introduced for the main speaker detection. To the best of our knowledge, it is one of the first studies that is able to automatically localize and highlight the main speaker in both visual and audio channels in multi-speaker conversation videos.
- M. J. Correia, I. Trancoso, B. Raj. 2020. Automatic In-the-wild Dataset Annotation with Deep Generalized Multiple Instance Learning. Abstract: The automation of the diagnosis and monitoring of speech affecting diseases in real life situations, such as Depression or Parkinson’s disease, depends on the existence of rich and large datasets that resemble real life conditions, such as those collected from in-the-wild multimedia repositories like YouTube. However, the cost of manually labeling these large datasets can be prohibitive. In this work, we propose to overcome this problem by automating the annotation process, without any requirements for human intervention. We formulate the annotation problem as a Multiple Instance Learning (MIL) problem, and propose a novel solution that is based on end-to-end differentiable neural networks. Our solution has the additional advantage of generalizing the MIL framework to more scenarios where the data is stil organized in bags but does not meet the MIL bag label conditions. We demonstrate the performance of the proposed method in labeling the in-the-Wild Speech Medical (WSM) Corpus, using simple textual cues extracted from videos and their metadata. Furthermore we show what is the contribution of each type of textual cues for the final model performance, as well as study the influence of the size of the bags of instances in determining the difficulty of the learning problem
- Yuichiro Koyama, B. Raj. 2020. Exploring Optimal DNN Architecture for End-to-End Beamformers Based on Time-frequency References. Abstract: Acoustic beamformers have been widely used to enhance audio signals. Currently, the best methods are the deep neural network (DNN)-powered variants of the generalized eigenvalue and minimum-variance distortionless response beamformers and the DNN-based filter-estimation methods that are used to directly compute beamforming filters. Both approaches are effective; however, they have blind spots in their generalizability. Therefore, we propose a novel approach for combining these two methods into a single framework that attempts to exploit the best features of both. The resulting model, called the W-Net beamformer, includes two components; the first computes time-frequency references that the second uses to estimate beamforming filters. The results on data that include a wide variety of room and noise conditions, including static and mobile noise sources, show that the proposed beamformer outperforms other methods on all tested evaluation metrics, which signifies that the proposed architecture allows for effective computation of the beamforming filters.
- Yuichiro Koyama, Oluwafemi Azeez, B. Raj. 2020. Efficient Integration of Multi-channel Information for Speaker-independent Speech Separation. Abstract: Although deep-learning-based methods have markedly improved the performance of speech separation over the past few years, it remains an open question how to integrate multi-channel signals for speech separation. We propose two methods, namely, early-fusion and late-fusion methods, to integrate multi-channel information based on the time-domain audio separation network, which has been proven effective in single-channel speech separation. We also propose channel-sequential-transfer learning, which is a transfer learning framework that applies the parameters trained for a lower-channel network as the initial values of a higher-channel network. For fair comparison, we evaluated our proposed methods using a spatialized version of the wsj0-2mix dataset, which is open-sourced. It was found that our proposed methods can outperform multi-channel deep clustering and improve the performance proportionally to the number of microphones. It was also proven that the performance of the late-fusion method is consistently higher than that of the single-channel method regardless of the angle difference between speakers.
- Jie Shao, Kaiqin Hu, Changhu Wang, X. Xue, B. Raj. 2020. Is normalization indispensable for training deep neural network?. Abstract: Normalization operations are widely used to train deep neural networks, and they can improve both convergence and generalization in most tasks. The theories for normalization’s effectiveness and new forms of normalization have always been hot topics in research. To better understand normalization, one question can be whether normalization is indispensable for training deep neural networks? In this paper, we analyze what would happen when normalization layers are removed from the networks, and show how to train deep neural networks without normalization layers and without performance degradation . Our proposed method can achieve the same or even slightly better performance in a variety of tasks: image classiﬁcation in ImageNet, object detection and segmentation in MS-COCO, video classiﬁcation in Kinetics, and machine translation in WMT English-German, etc. Our study may help better understand the role of normalization layers and can be a competitive alternative to normalization layers. Codes are available at https://github.com/ hukkai/rescaling .
- Yuichiro Koyama, Tyler Vuong, S. Uhlich, B. Raj. 2020. Exploring the Best Loss Function for DNN-Based Low-latency Speech Enhancement with Temporal Convolutional Networks. Abstract: Recently, deep neural networks (DNNs) have been successfully used for speech enhancement, and DNN-based speech enhancement is becoming an attractive research area. While time-frequency masking based on the short-time Fourier transform (STFT) has been widely used for DNN-based speech enhancement over the last years, time domain methods such as the time-domain audio separation network (TasNet) have also been proposed. The most suitable method depends on the scale of the dataset and the type of task. In this paper, we explore the best speech enhancement algorithm on two different datasets. We propose a STFT-based method and a loss function using problem-agnostic speech encoder (PASE) features to improve subjective quality for the smaller dataset. Our proposed methods are effective on the Voice Bank + DEMAND dataset and compare favorably to other state-of-the-art methods. We also implement a low-latency version of TasNet, which we submitted to the DNS Challenge and made public by open-sourcing it. Our model achieves excellent performance on the DNS Challenge dataset.
- Rowland Chen, R. Dannenberg, B. Raj, Rita Singh. 2020. Artificial Creative Intelligence: Breaking the Imitation Barrier. Abstract: Not all knowledge is created equal. A hierarchical architecture is a method to classify knowledge for use in the field of human cognition and computational creativity. This paper introduces an Insight-Knowledge Object (IKO) model as a framework for Artificial Creative Intelligence (ACI), a step forward in the pursuit of replicating general human intelligence with computing machinery. To achieve ACI, it is hypothesized that a fundamental rethinking of the architecture of human cognition and knowledge processing is required. One possible novel architecture could be the IKO model. The authors include a description of on-going work at Carnegie Mellon University that applies the IKO model in practice with an artificial music improvisation embodiment.
- Soham Deshmukh, B. Raj, Rita Singh. 2020. Multi-Task Learning for Interpretable Weakly Labelled Sound Event Detection. Abstract: Weakly Labelled learning has garnered lot of attention in recent years due to its potential to scale Sound Event Detection (SED) and is formulated as Multiple Instance Learning (MIL) problem. This paper proposes a Multi-Task Learning (MTL) framework for learning from Weakly Labelled Audio data which encompasses the traditional MIL setup. To show the utility of proposed framework, we use the input TimeFrequency representation (T-F) reconstruction as the auxiliary task. We show that the chosen auxiliary task de-noises internal T-F representation and improves SED performance under noisy recordings. Our second contribution is introducing two step Attention Pooling mechanism. By having 2-steps in attention mechanism, the network retains better T-F level information without compromising SED performance. The visualisation of first step and second step attention weights helps in localising the audio-event in T-F domain. For evaluating the proposed framework, we remix the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 db SNR resulting in a multi-class Weakly labelled SED problem. The proposed total framework outperforms existing benchmark models over all SNRs, specifically 22.3 %, 12.8 %, 5.9 % improvement over benchmark model on 0, 10 and 20 dB SNR respectively. We carry out ablation study to determine the contribution of each auxiliary task and 2-step Attention Pooling to the SED performance improvement. The code is publicly released
- Jiachen Lian, A. V. Kumar, Hira Dhamyal, B. Raj, Rita Singh. 2020. Masked Proxy Loss for Text-Independent Speaker Verification. Abstract: Open-set speaker recognition can be regarded as a metric learning problem, which is to maximize inter-class variance and minimize intra-class variance. Supervised metric learning can be categorized into entity-based learning and proxy-based learning. Most of the existing metric learning objectives like Contrastive, Triplet, Prototypical, GE2E, etc all belong to the former division, the performance of which is either highly dependent on sample mining strategy or restricted by insufficient label information in the mini-batch. Proxy-based losses mitigate both shortcomings, however, fine-grained connections among entities are either not or indirectly leveraged. This paper proposes a Masked Proxy (MP) loss which directly incorporates both proxy-based relationships and pair-based relationships. We further propose Multinomial Masked Proxy (MMP) loss to leverage the hardness of speaker pairs. These methods have been applied to evaluate on VoxCeleb test set and reach state-of-the-art Equal Error Rate(EER).
- Yang Gao, Jiachen Lian, B. Raj, Rita Singh. 2020. Detection and Evaluation of Human and Machine Generated Speech in Spoofing Attacks on Automatic Speaker Verification Systems. Abstract: Automatic speaker verification (ASV) systems utilize the biometric information in human speech to verify the speaker’s identity. The techniques used for performing speaker verification are often vulnerable to malicious attacks that attempt to induce the ASV system to return wrong results, allowing an impostor to bypass the system and gain access. Attackers use a multitude of spoofing techniques for this, such as voice conversion, audio replay, speech synthesis, etc. In recent years, easily available tools to generate deepfaked audio have increased the potential threat to ASV systems. In this paper, we compare the potential of human impersonation (voice disguise) based attacks with attacks based on machinegenerated speech, on black-box and white-box ASV systems. We also study countermeasures by using features that capture the unique aspects of human speech production, under the hypothesis that machines cannot emulate many of the finelevel intricacies of the human speech production mechanism. We show that fundamental frequency sequence-related entropy, spectral envelope, and aperiodic parameters are promising candidates for robust detection of deepfaked speech generated by unknown methods.
- Jiachen Lian, A. V. Kumar, Hira Dhamyal, B. Raj, Rita Singh. 2020. Mask Proxy Loss for Text-Independent Speaker Recognition. Abstract: Open-set speaker recognition can be regarded as a metric learning problem, which is to maximize inter-class variance and minimize intra-class variance. Supervised metric learning can be categorized into entity-based learning and proxy-based learning\protect\footnote{Different from the definition in \cite{Proxyanchor}, we adopt the concept of entity-based learning rather than pair-based learning to illustrate the data-to-data relationship. Entity refers to real data point.}. Most of existing metric learning objectives like Contrastive, Triplet, Prototypical, GE2E, etc all belong to the former division, the performance of which is either highly dependent on sample mining strategy or restricted by insufficient label information in the mini-batch. Proxy-based losses mitigate both shortcomings, however, fine-grained connections among entities are either not or indirectly leveraged. This paper proposes a Mask Proxy (MP) loss which directly incorporates both proxy-based relationship and entity-based relationship. We further propose Multinomial Mask Proxy (MMP) loss to leverage the hardness of entity-to-entity pairs. These methods have been applied to evaluate on VoxCeleb test set and reach state-of-the-art Equal Error Rate(EER).
- Muhammad A Shah, Khaled A. Harras, B. Raj. 2020. Sherlock: A Crowd-sourced System For Automatic Tagging Of Indoor Floor Plans. Abstract: Having knowledge of the users’ indoor location and the semantics of their environment can facilitate the development of many indoor context-aware applications. For such applications, an accurate indoor map is often needed. While current techniques are capable of producing such maps, these maps are not labeled and hence are of limited utility for many applications. To address this shortcoming, we propose Sherlock, a crowdsourced system for automatically tagging indoor floor plans. Sherlock leverages the myriad of sensors embedded in modern smartphones to intelligently gather audio and visual data, and upload it to the Sherlock Server. At the Sherlock Server, acoustic monitoring and object recognition techniques are used to classify these data samples. The classification scores of current and past samples are then aggregated in a probabilistic framework to determine the confidence with which we can apply as label to a given space. We evaluate Sherlock on a dataset of more than 11,000 audio recordings and 1,200 images, that we collected in three different university campuses. In our evaluation, the confidence for the true label generally outstripped the confidence for all other labels and, in some cases, even reached as high as 100% with as little as 30 data samples.
- Muhammad A Shah, R. Olivier, B. Raj. 2020. Exploiting Non-Linear Redundancy for Neural Model Compression. Abstract: Deploying deep learning models with millions, even billions, of parameters is challenging given real world memory, power and compute constraints. In an effort to make these models more practical, in this paper, we propose a novel model compression approach that exploits linear dependence between the activations in a layer to eliminate entire structural units (neurons/convolutional filters). Our approach also adjusts the weights of the layer in a manner that is provably lossless while training if the removed neuron was perfectly predictable. We combine this approach with an annealing algorithm that may be applied during training, or even on a trained model, and demonstrate, using popular datasets, that our technique can reduce the parameters of VGG and AlexNet by more than 97% on CIFAR-10, 85% on Caltech-256, and 19% on ImageNet at less than 2% loss in accuracy. Furthermore, we provide theoretical results showing that in overparametrized, locally linear (ReLU) neural networks where redundant features exist, and with correct hyperparameter selection, our method is indeed able to capture and suppress those dependencies.
- A. Shamsabadi, Francisco Teixeira, A. Abad, B. Raj, A. Cavallaro, I. Trancoso. 2020. FoolHD: Fooling Speaker Identification by Highly Imperceptible Adversarial Disturbances. Abstract: Speaker identification models are vulnerable to carefully designed adversarial perturbations of their input signals that induce misclassification. In this work, we propose a white-box steganography-inspired adversarial attack that generates imperceptible adversarial perturbations against a speaker identification model. Our approach, FoolHD, uses a Gated Convolutional Autoencoder that operates in the DCT domain and is trained with a multi-objective loss function, to generate and conceal the adversarial perturbation within the original audio files. In addition to hindering speaker identification performance, this multi-objective loss accounts for human perception through a frame-wise cosine similarity between MFCC feature vectors extracted from the original and adversarial audio files. We validate the effectiveness of FoolHD with a 250-speaker identification x-vector network, trained using VoxCeleb, in terms of accuracy, success rate, and imperceptibility. Our results show that FoolHD generates highly imperceptible adversarial audio files (average PESQ scores above 4.30), while achieving a success rate of 99.6% and 99.2% in misleading the speaker identification model, for untargeted and targeted settings, respectively.
- Muhammad A Shah, B. Raj. 2020. Deriving Compact Feature Representations Via Annealed Contraction. Abstract: It is common practice to use pretrained image recognition models to compute feature representations for visual data. The size of the feature representations can have a noticeable impact on the complexity of the models that use these representations, and by extension on their deployablity and scalability. Therefore it would be beneficial to have compact visual representations that carry as much information as their high-dimensional counterparts. To this end we propose a technique that shrinks a layer by an iterative process in which neurons are removed from the and network is fine tuned. Using this technique we are able to remove 99% of the neurons from the penultimate layer of AlexNet and VGG16, while suffering less than 5% drop in accuracy on CIFAR10, Caltech101 and Caltech256. We also show that our method can reduce the size of AlexNet by 95% while only suffering a 4% reduction in accuracy on Caltech101.
- Hira Dhamyal, Shahan Ali Memon, B. Raj, Rita Singh. 2019. The phonetic bases of vocal expressed emotion: natural versus acted. Abstract: Can vocal emotions be emulated? This question has been a recurrent concern of the speech community, and has also been vigorously investigated. It has been fueled further by its link to the issue of validity of acted emotion databases. Much of the speech and vocal emotion research has relied on acted emotion databases as valid proxies for studying natural emotions. To create models that generalize to natural settings, it is crucial to work with valid prototypes -- ones that can be assumed to reliably represent natural emotions. More concretely, it is important to study emulated emotions against natural emotions in terms of their physiological, and psychological concomitants. In this paper, we present an on-scale systematic study of the differences between natural and acted vocal emotions. We use a self-attention based emotion classification model to understand the phonetic bases of emotions by discovering the most 'attended' phonemes for each class of emotions. We then compare these attended-phonemes in their importance and distribution across acted and natural classes. Our tests show significant differences in the manner and choice of phonemes in acted and natural speech, concluding moderate to low validity and value in using acted speech databases for emotion classification tasks.
- Shahan Ali Memon, Hira Dhamyal, Oren Wright, Daniel Justice, Vijaykumar Palat, William Boler, Yandong Wen, B. Raj, Rita Singh. 2019. Detecting gender differences in perception of emotion in crowdsourced data. Abstract: Do men and women perceive emotions differently? Popular convictions place women as more emotionally perceptive than men. Empirical findings, however, remain inconclusive. Most prior studies focus on visual modalities. In addition, almost all of the studies are limited to experiments within controlled environments. Generalizability and scalability of these studies has not been sufficiently established. In this paper, we study the differences in perception of emotion between genders from speech data in the wild, annotated through crowdsourcing. While we limit ourselves to a single modality (i.e. speech), our framework is applicable to studies of emotion perception from all such loosely annotated data in general. Our paper addresses multiple serious challenges related to making statistically viable conclusions from crowdsourced data. Overall, the contributions of this paper are two fold: a reliable novel framework for perceptual studies from crowdsourced data; and the demonstration of statistically significant differences in speech-based emotion perception between genders.
- Felix Kreuk, Yossi Adi, B. Raj, Rita Singh, Joseph Keshet. 2019. Hide and Speak: Deep Neural Networks for Speech Steganography. Abstract: Steganography is the science of hiding a secret message within an ordinary public message, which referred to as Carrier. Traditionally, digital signal processing techniques, such as least significant bit encoding, were used for hiding messages. In this paper, we explore the use of deep neural networks as steganographic functions for speech data. To this end, we propose to jointly optimize two neural networks: the first network encodes the message inside a carrier, while the second network decodes the message from the modified carrier. We demonstrated the effectiveness of our method on several speech data-sets and analyzed the results quantitatively and qualitatively. Moreover, we showed that our approach could be applied to conceal multiple messages in a single carrier using multiple decoders or a single conditional decoder. Qualitative experiments suggest that modifications to the carrier are unnoticeable by human listeners and that the decoded messages are highly intelligible.
- Abelino Jiménez, B. Raj. 2019. Time Signal Classification Using Random Convolutional Features. Abstract: In this paper we present a transformation to convert time signals into a randomized low-dimensional vectors such that the inner product between these new features provides information about the similarity of the signals. We show that the described inner product approximates a cross-correlation based kernel. This is very useful at the moment of use Kernel Machines, such as Non-linear Support Vector Machines. Indeed, this allows to apply simpler and faster linear methods on the generated random features. Our proposed scheme improves computational storage and time cost over the direct kernel approach, while performing the classification performance with minimal loss. We support our statements by providing theoretical guarantees as well as empirical evaluation across different data sets.
- Yuichiro Koyama, B. Raj. 2019. W-Net BF: DNN-based Beamformer Using Joint Training Approach. Abstract: Acoustic beamformers have been widely used to enhance audio signals. The best current methods are DNN-powered variants of the generalized eigenvalue beamformer, and DNN-based filterestimation methods that directly compute beamforming filters. Both approaches, while effective, have blindspots in their generalizability. We propose a novel approach that combines both approaches into a single framework that attempts to exploit the best features of both. The resulting model, called a W-Net beamformer, includes two components: the first computes a noise-masked reference which the second uses to estimate beamforming filters. Results on data that include a wide variety of room and noise conditions, including static and mobile noise sources, show that the proposed beamformer outperforms other methods in all tested evaluation metrics.
- Daanish Ali Khan, Linhong Li, Ninghao Sha, Zhuoran Liu, Abelino Jiménez, B. Raj, Rita Singh. 2019. Non-Determinism in Neural Networks for Adversarial Robustness. Abstract: Recent breakthroughs in the field of deep learning have led to advancements in a broad spectrum of tasks in computer vision, audio processing, natural language processing and other areas. In most instances where these tasks are deployed in real-world scenarios, the models used in them have been shown to be susceptible to adversarial attacks, making it imperative for us to address the challenge of their adversarial robustness. Existing techniques for adversarial robustness fall into three broad categories: defensive distillation techniques, adversarial training techniques, and randomized or non-deterministic model based techniques. In this paper, we propose a novel neural network paradigm that falls under the category of randomized models for adversarial robustness, but differs from all existing techniques under this category in that it models each parameter of the network as a statistical distribution with learnable parameters. We show experimentally that this framework is highly robust to a variety of white-box and black-box adversarial attacks, while preserving the task-specific performance of the traditional neural network model.
- Felix Kreuk, Yossi Adi, B. Raj, Rita Singh, Joseph Keshet. 2019. Hide and Speak: Towards Deep Neural Networks for Speech Steganography. Abstract: Steganography is the science of hiding a secret message within an ordinary public message, which is referred to as Carrier. Traditionally, digital signal processing techniques, such as least significant bit encoding, were used for hiding messages. In this paper, we explore the use of deep neural networks as steganographic functions for speech data. We showed that steganography models proposed for vision are less suitable for speech, and propose a new model that includes the short-time Fourier transform and inverse-short-time Fourier transform as differentiable layers within the network, thus imposing a vital constraint on the network outputs. We empirically demonstrated the effectiveness of the proposed method comparing to deep learning based on several speech datasets and analyzed the results quantitatively and qualitatively. Moreover, we showed that the proposed approach could be applied to conceal multiple messages in a single carrier using multiple decoders or a single conditional decoder. Lastly, we evaluated our model under different channel distortions. Qualitative experiments suggest that modifications to the carrier are unnoticeable by human listeners and that the decoded messages are highly intelligible.
- Yandong Wen, Rita Singh, B. Raj. 2019. Reconstructing faces from voices. Abstract: Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling - reconstructing someone's face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task - cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance.
- A. Mesaros, Aleksandr Diment, Benjamin Elizalde, T. Heittola, E. Vincent, B. Raj, T. Virtanen. 2019. Sound Event Detection in the DCASE 2017 Challenge. Abstract: Each edition of the challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) contained several tasks involving sound event detection in different setups. DCASE 2017 presented participants with three such tasks, each having specific datasets and detection requirements: Task 2, in which target sound events were very rare in both training and testing data, Task 3 having overlapping events annotated in real-life audio, and Task 4, in which only weakly labeled data were available for training. In this paper, we present three tasks, including the datasets and baseline systems, and analyze the challenge entries for each task. We observe the popularity of methods using deep neural networks, and the still widely used mel frequency-based representations, with only few approaches standing out as radically different. Analysis of the systems behavior reveals that task-specific optimization has a big role in producing good performance; however, often this optimization closely follows the ranking metric, and its maximization/minimization does not result in universally good performance. We also introduce the calculation of confidence intervals based on a jackknife resampling procedure to perform statistical analysis of the challenge results. The analysis indicates that while the 95% confidence intervals for many systems overlap, there are significant differences in performance between the top systems and the baseline for all tasks.
- Joana Correia, I. Trancoso, B. Raj. 2019. In-the-Wild End-to-End Detection of Speech Affecting Diseases. Abstract: Speech is a complex bio-signal that has the potential to provide a rich bio-marker for health. It enables the development of non-invasive routes to early diagnosis and monitoring of speech affecting diseases, such as the ones studied in this work: Depression, and Parkinson's Disease. However, the major limitation of current speech based diagnosis and monitoring tools is the lack of large and diverse datasets. Existing datasets are small, and collected under very controlled conditions. As such, there is an upper bound in the complexity of the models that can be trained using these datasets. There is also limited applicability in real life scenarios where the channel and noise conditions, among others, are impossible to control. In this work, we show that datasets collected from in-the-wild sources, such as collections of vlogs, can contribute to improve the performance of diagnosis tools both in controlled and in-the-wild conditions, even though the data are noisier. Moreover, we show that it is possible to successfully move away from hand-crafted features (i.e. features that are computed based on predefined algorithms, that based on human expertise) and adopt end-to-end modeling paradigms, such as CNN-LSTMs, that extract data driven features from the raw spectrograms of the speech signal, and capture temporal information from the speech signals.
- Yandong Wen, B. Raj, Rita Singh. 2019. Face Reconstruction from Voice using Generative Adversarial Networks. Abstract: Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling - reconstructing someone's face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task - cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance. The code is publicly available in https://github.com/cmu-mlsp/reconstructing_faces_from_voices
- Wenbo Zhao, Yang Gao, Shahan Ali Memon, B. Raj, Rita Singh. 2019. Hierarchical Routing Mixture of Experts. Abstract: In regression tasks, the data distribution is often too complex to be fitted by a single model. In contrast, partition-based models are developed where data is divided and fitted by local models. These models partition the input space and do not leverage the input-output dependency of multimodal-distributed data, and strong local models are needed to make good predictions. Addressing these problems, we propose a binary tree-structured hierarchical routing mixture of experts (HRME) model that has classifiers as non-leaf node experts and simple regression models as leaf node experts. The classifier nodes jointly soft-partition the input-output space based on the natural separateness of multimodal data. This enables simple leaf experts to be effective for prediction. Further, we develop a probabilistic framework for the HRME model and propose a recursive Expectation-Maximization (EM) based algorithm to learn both the tree structure and the expert models. Experiments on a collection of regression tasks validate our method's effectiveness compared to various other regression models.
- Daanish Ali Khan, Saquib Razak, B. Raj, Rita Singh. 2019. Human Behaviour Recognition Using Wifi Channel State Information. Abstract: Device-Free Human Behaviour Recognition is automatically recognizing physical activity from a series of observations, without directly attaching sensors to the subject. Behaviour Recognition has applications in security, health-care, and smart homes. The ubiquity of WiFi devices has generated recent interest in Channel State Information (CSI) that describes the propagation of RF signals for behaviour recognition, leveraging the relationship between body movement and variations in CSI streams. Existing work on CSI based behaviour recognition has established the efficacy of deep neural network classifiers, yielding performance that surpasses traditional techniques. In this paper, we propose a deep Recurrent Neural Network (RNN) model for CSI based Behaviour Recognition that utilizes a Convolutional Neural Network (CNN) feature extractor with stacked Long Short-Term Memory (LSTM) networks for sequence classification. We also examine CSI de-noising techniques that allow faster training and model convergence. Our model has yielded significant improvement in classification accuracy, compared to existing techniques.
- Benjamin Elizalde, Shuayb Zarar, B. Raj. 2019. Cross Modal Audio Search and Retrieval with Joint Embeddings Based on Text and Audio. Abstract: Existing audio search engines use one of two approaches: matching text-text or audio-audio pairs. In the former, text queries are matched to semantically similar words in an index of audio metadata to retrieve corresponding audio clips or segments, while in the latter, audio signals are directly used to retrieve acoustically-similar recordings from an audio database. However, independent treatment of text and audio has precluded information exchange between the two modalities. This is a problem because similarity in language does not always imply similarity in acoustics, and vice versa. Moreover, independent modeling can be error prone especially for ad hoc, user-generated recordings, which are noisy in both audio and their associated textual labels. To overcome this limitation, we propose a framework that learns joint embeddings from a shared lexico-acoustic space, where vectors from either modality can be mapped together and compared directly. Thus, we improve semantic knowledge and enable the use of either text or audio queries to search and retrieve audio. Our results break new ground for a cross-modal audio search engine, and further exploration of lexico-acoustic spaces.
- Chirag Nagpal, R. Sangave, Amit Chahar, Parth Shah, A. Dubrawski, B. Raj. 2019. Nonlinear Semi-Parametric Models for Survival Analysis. Abstract: Semi-parametric survival analysis methods like the Cox Proportional Hazards (CPH) regression (Cox, 1972) are a popular approach for survival analysis. These methods involve fitting of the log-proportional hazard as a function of the covariates and are convenient as they do not require estimation of the baseline hazard rate. Recent approaches have involved learning non-linear representations of the input covariates and demonstrate improved performance. In this paper we argue against such deep parameterizations for survival analysis and experimentally demonstrate that more interpretable semi-parametric models inspired from mixtures of experts perform equally well or in some cases better than such overly parameterized deep models.
- Hira Dhamyal, Tianyan Zhou, B. Raj, Rita Singh. 2019. Optimizing Neural Network Embeddings Using a Pair-Wise Loss for Text-Independent Speaker Verification. Abstract: This paper proposes a new loss function called the “quartet” loss for the better optimization of the neural networks for matching tasks. For such tasks, where neural network embeddings are the key component, the optimization of the network for better embeddings is critical. The embeddings are required to be class discriminative, resulting in minimal inter-class variation and maximal intra-class variation even for unseen classes for better generalization of the network. The quartet loss explicitly computes the distance metric between pairs of inputs and increases the gap between the similarity score distributions between the same class pairs and the different class pairs. We evaluate on the speaker verification task and demonstrate the performance of the loss on our proposed neural network.
- Ankit Shah, Anurag Kumar, Alexander Hauptmann, B. Raj. 2018. A Closer Look at Weak Label Learning for Audio Events. Abstract: Audio content analysis in terms of sound events is an important research problem for a variety of applications. Recently, the development of weak labeling approaches for audio or sound event detection (AED) and availability of large scale weakly labeled dataset have finally opened up the possibility of large scale AED. However, a deeper understanding of how weak labels affect the learning for sound events is still missing from literature. In this work, we first describe a CNN based approach for weakly supervised training of audio events. The approach follows some basic design principle desirable in a learning method relying on weakly labeled audio. We then describe important characteristics, which naturally arise in weakly supervised learning of sound events. We show how these aspects of weak labels affect the generalization of models. More specifically, we study how characteristics such as label density and corruption of labels affects weakly supervised training for audio events. We also study the feasibility of directly obtaining weak labeled data from the web without any manual label and compare it with a dataset which has been manually labeled. The analysis and understanding of these factors should be taken into picture in the development of future weak label learning methods. Audioset, a large scale weakly labeled dataset for sound events is used in our experiments.
- Sabit Hassan, Shaden Shaar, B. Raj, Saquib Razak. 2018. Interactive Evaluation of Classifiers Under Limited Resources. Abstract: In this paper, we propose strategies to estimate the accuracy of classifiers on a dataset when resource limitations restrict the number of instances for which true labels can be obtained. Our target scenarios include situations where the classifier output labels, but no scores, e.g. when the "classifier" is not an automated classifier but an inexpert human labeller who only outputs labels. Our objective is to optimally select a subset of the data to obtain true labels for, such that they provide the best estimate of classifier accuracy. We use techniques based on stratified sampling to address this problem. However, stratified sampling poses two challenges: i) how best to stratify the data, and ii) how to allocate samples among the strata. We propose a method of stratifying data and then present two novel interactive algorithms to approximate optimal allocation of samples to the strata. Our proposed methods for stratification and allocation are seen to outperform other popular approaches to the problem.
- Yandong Wen, Mahmoud Al Ismail, Weiyang Liu, B. Raj, Rita Singh. 2018. Disjoint Mapping Network for Cross-modal Matching of Voices and Faces. Abstract: We propose a novel framework, called Disjoint Mapping Network (DIMNet), for cross-modal biometric matching, in particular of voices and faces. Different from the existing methods, DIMNet does not explicitly learn the joint relationship between the modalities. Instead, DIMNet learns a shared representation for different modalities by mapping them individually to their common covariates. These shared representations can then be used to find the correspondences between the modalities. We show empirically that DIMNet is able to achieve better performance than other current methods, with the additional benefits of being conceptually simpler and less data-intensive.
- Kaiqin Hu, B. Raj. 2018. Higher-order Network for Action Recognition. Abstract: Capturing spatiotemporal dynamics is an essential topic in video recognition. In this paper, we present learnable higher-order operations as a generic family of building blocks for capturing spatiotemporal dynamics from RGB input video space. Similar to higher-order functions, the weights of higher-order operations are themselves derived from the data with learnable parameters. Classical architectures such as residual learning and network-in-network are first-order operations where weights are directly learned from the data. Higher-order operations make it easier to capture context-sensitive patterns, such as motion. Self-attention models are also higher-order operations, but the attention weights are mostly computed from an affine operation or dot product. Learnable higher-order operations can be more generic and flexible. Experimentally, we show that on the task of video recognition, our higher-order models can achieve results on par with or better than the existing state-of-the-art methods on Something-Something (V1 and V2), Kinetics and Charades datasets.
- Abelino Jiménez, Benjamin Elizalde, B. Raj. 2018. CLASSIFICATION USING ONTOLOGY-BASED NEURAL NETWORKS. Abstract: State of the art sound event classification relies in neural networks to learn the associations between class labels and audio recordings within a dataset. These datasets typically define an ontology to create a structure that relates these sound classes with more abstract super classes. Hence, the ontology serves a source of domain knowledge representation of sounds. However, the ontology information is rarely considered, and specially under explored to model neural network architectures. We propose ontology-based neural network architectures for sound event classification. We defined a framework to design simple network architectures that preserve an ontological structure. The networks are trained and evaluated using the MSoS dataset. Results show an improvement in accuracy demonstrating the benefits of the ontology.
- Yang Gao, Rita Singh, B. Raj. 2018. Voice Impersonation Using Generative Adversarial Networks. Abstract: Voice impersonation is not the same as voice transformation, although the latter is an essential element of it. In voice impersonation, the resultant voice must convincingly convey the impression of having been naturally produced by the target speaker, mimicking not only the pitch and other perceivable signal qualities, but also the style of the target speaker. In this paper, we propose a novel neural-network based speech quality- and style-mimicry framework for the synthesis of impersonated voices. The framework is built upon a fast and accurate generative adversarial network model. Given spectrographic representations of source and target speakers' voices, the model learns to mimic the target speaker's voice quality and style, regardless of the linguistic content of either's voice, generating a synthetic spectrogram from which the time-domain signal is reconstructed using the Griffin-Lim method. In effect, this model reframes the well-known problem of style-transfer for images as the problem of style-transfer for speech signals, while intrinsically addressing the problem of durational variability of speech sounds. Experiments demonstrate that the model can generate extremely convincing samples of impersonated speech. It is even able to impersonate voices across different genders effectively. Results are qualitatively evaluated using standard procedures for evaluating synthesized voices.
- Yandong Wen, Mahmoud Al Ismail, B. Raj, Rita Singh. 2018. Optimal Strategies for Matching and Retrieval Problems by Comparing Covariates. Abstract: In many retrieval problems, where we must retrieve one or more entries from a gallery in response to a probe, it is common practice to learn to do by directly comparing the probe and gallery entries to one another. In many situations the gallery and probe have common covariates -- external variables that are common to both. In principle it is possible to perform the retrieval based merely on these covariates. The process, however, becomes gated by our ability to recognize the covariates for the probe and gallery entries correctly. 
In this paper we analyze optimal strategies for retrieval based only on matching covariates, when the recognition of the covariates is itself inaccurate. We investigate multiple problems: recovering one item from a gallery of $N$ entries, matching pairs of instances, and retrieval from large collections. We verify our analytical formulae through experiments to verify their correctness in practical settings.
- Abelino Jiménez, Benjamin Elizalde, B. Raj. 2018. DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant Kernels and Random Features. Abstract: Acoustic scene recordings are represented by different types of handcrafted or Neural Network-derived features. These features, typically of thousands of dimensions, are classified in state of the art approaches using kernel machines, such as the Support Vector Machines (SVM). However, the complexity of training these methods increases with the dimensionality of these input features and the size of the dataset. A solution is to map the input features to a randomized lower-dimensional feature space. The resulting random features can approximate non-linear kernels with faster linear kernel computation. In this work, we computed a set of 6,553 input features and used them to compute random features to approximate three types of kernels, Gaussian, Laplacian and Cauchy. We compared their performance using an SVM in the context of the DCASE Task 1 - Acoustic Scene Classification. Experiments show that both, input and random features outperformed the DCASE baseline by an absolute 4%. Moreover, the random features reduced the dimensionality of the input by more than three times with minimal loss of performance and by more than six times and still outperformed the baseline. Hence, random features could be employed by state of the art approaches to compute low-storage features and perform faster kernel computations.
- Abelino Jiménez, Benjamin Elizalde, B. Raj. 2018. Acoustic Scene Classification Using Discrete Random Hashing for Laplacian Kernel Machines. Abstract: State of the art acoustic scene classification techniques often employ features of large dimensionality, which are then used to train and perform inferences with kernel machines such as Support Vector Machines. However, the complexity of computing the non-linear kernel matrix for these methods increases with the dimensionality of the features and the size of the dataset. In this work, we introduce a new scheme that hashes features, which combined with a linear function approximates a non-linear Laplacian kernel. Each hash typically has lower dimensionality than the input features and each component is represented by one bit instead of floating values. Hence, allowing efficient computation of the kernel matrix using XOR operations rather than dot-products. Our scheme is demonstrated mathematically and tested in the 2017 DCASE: Acoustic Scene Classification. The hashes reduce up to six powers of two the feature representation with minimal loss of accuracy.
- Anurag Kumar, Ankit Shah, Alexander Hauptmann, B. Raj. 2018. Learning Sound Events From Webly Labeled Data. Abstract: In the last couple of years, weakly labeled learning has turned out to be an exciting approach for audio event detection. In this work, we introduce webly labeled learning for sound events which aims to remove human supervision altogether from the learning process. We first develop a method of obtaining labeled audio data from the web (albeit noisy), in which no manual labeling is involved. We then describe methods to efficiently learn from these webly labeled audio recordings. In our proposed system, WeblyNet, two deep neural networks co-teach each other to robustly learn from webly labeled data, leading to around 17% relative improvement over the baseline method. The method also involves transfer learning to obtain efficient representations.
- Yandong Wen, Tianyan Zhou, Rita Singh, B. Raj. 2018. A Corrective Learning Approach for Text-Independent Speaker Verification. Abstract: We present a conceptually plausible approach for text-independent speaker verification (TISV) which treats speech recordings as a collection of segments providing incremental evidence. This approach, called corrective learning, gradually improves an initial prediction of speaker identity based on incoming speech and the latest prediction. Specifically, we propose deep corrective learning networks (CLNets) that explicitly learn a mapping from a new speech segment and the current predictions, to a correction. Intuitively, the predictions eventually converge to the ground truth after several corrections. Trained on NIST SRE datasets, CLNets outperform current CNN and the i-vector baselines. Moreover, CLNets and i-vectors are complementary, and their fusion leads to significant performance improvements compared to what can be achieved by each of them individually.
- Abelino Jiménez, Benjamin Elizalde, B. Raj. 2018. Sound event classification using ontology-based neural networks. Abstract: State of the art sound event classification relies in neural networks to learn the associations between class labels and audio recordings within a dataset. These datasets typically define an ontology to create a structure that relates these sound classes with more abstract super classes. Hence, the ontology serves as a source of domain knowledge representation of sounds. However, the ontology information is rarely considered, and specially under explored to model neural network architectures. We propose two ontology-based neural network architectures for sound event classification. We defined a framework to design simple network architectures that preserve an ontological structure. The networks are trained and evaluated using two of the most common sound event classification datasets. Results show an improvement in classification performance demonstrating the benefits of including the ontological information.
- Benjamin Elizalde, Rohan Badlani, Ankit Shah, Anurag Kumar, B. Raj. 2018. NELS - Never-Ending Learner of Sounds. Abstract: Sounds are essential to how humans perceive and interact with the world and are captured in recordings and shared on the Internet on a minute-by-minute basis. These recordings, which are predominantly videos, constitute the largest archive of sounds we know. However, most of these recordings have undescribed content making necessary methods for automatic sound analysis, indexing and retrieval. These methods have to address multiple challenges, such as the relation between sounds and language, numerous and diverse sound classes, and large-scale evaluation. We propose a system that continuously learns from the web relations between sounds and language, improves sound recognition models over time and evaluates its learning competency in the large-scale without references. We introduce the Never-Ending Learner of Sounds (NELS), a project for continuously learning of sounds and their associated knowledge, available on line in nels.cs.cmu.edu
- M. J. Correia, B. Raj, I. Trancoso. 2018. Querying Depression Vlogs. Abstract: Speech based diagnosis-aid tools for depression typically depend on few and small datasets, that are expensive to collect. The limited availability of training data poses a limitation to the quality that these systems can achieve. An unexplored alternative for large scale source of data are vlogs collected from online multimedia repositories. Along with the automation of the mining process, it is necessary to automate the labeling process too.In this work, we propose a framework to automatically label a corpus of in-the-wild vlogs of possibly depressed subjects, and we estimate the quality of the predicted labels, without ever having access to a ground truth for the majority of the corpus. The framework uses a small subset to train a model and estimate the labels for the remainder of the corpus. Then, using the predicted labels, we train a noisy model and attempt to reconstruct the labels of the original labeled subset. We hypothesize that the quality of the estimated labels for the unlabelled subset of the corpus is correlated to the quality of the label reconstruction of the labeled subset.The results of the bi-modal experiment using in-the-wild data are compared to the ones obtained using controlled data.
- Shahan Ali Memon, Wenbo Zhao, B. Raj, Rita Singh. 2018. Neural Regression Trees. Abstract: Regression-via-Classification (RvC) is the process of converting a regression problem to a classification one. Current approaches for RvC use ad-hoc discretization strategies and are suboptimal. We propose a neural regression tree model for RvC. In this model, we employ a joint optimization framework where we learn optimal discretization thresholds while simultaneously optimizing the features for each node in the tree. We empirically show the validity of our model by testing it on two challenging regression tasks where we establish the state of the art.
- M. J. Correia, B. Raj, I. Trancoso, Francisco Teixeira. 2018. Mining Multimodal Repositories for Speech Affecting Diseases. Abstract: The motivation for this work is to contribute to the collection of large in-the-wild multimodal datasets in which the speech of the subject is affected by certain medical conditions. Our mining effort is focused on video blogs (vlogs), and as a proof-of-concept we have selected three target diseases: Depression, Parkinson’s disease, and cold. Given the large scale nature of the online repositories, we take advantage of existing retrieval algorithms to narrow the pool of candidate videos for a given query related with the disease (e.g. depression vlog), and on top of that we apply several ﬁltering techniques. These techniques explore both audio, video, text and metadata cues, in order to retrieve vlogs that include a single speaker which, at some point, admits that he/she is currently affected by a given disease. The use of straightforward NLP techniques on the automatically transcribed data showed that distinguishing between narratives of present and past experiences is harder than distinguishing between narratives of self experiences and of someone else’s. The three resulting speech datasets were tested with neural networks trained with speech data collected in controlled conditions, yielding results only slightly below the ones achieved with the original test datasets.
- A. Mesaros, T. Heittola, Aleksandr Diment, Benjamin Elizalde, Ankit Shah, E. Vincent, B. Raj, T. Virtanen. 2017. DCASE2017 Challenge Setup: Tasks, Datasets and Baseline System. Abstract: DCASE 2017 Challenge consists of four tasks: acoustic scene classification , detection of rare sound events, sound event detection in real-life audio, and large-scale weakly supervised sound event detection for smart cars. This paper presents the setup of these tasks: task definition, dataset, experimental setup, and baseline system results on the development dataset. The baseline systems for all tasks rely on the same implementation using multilayer perceptron and log mel-energies, but differ in the structure of the output layer and the decision making process, as well as the evaluation of system output using task specific metrics.
- K. Osako, Yuki Mitsufuji, Rita Singh, B. Raj. 2017. Supervised monaural source separation based on autoencoders. Abstract: In this paper, we propose a new supervised monaural source separation based on autoencoders. We employ the autoencoder for the dictionary training such that the nonlinear network can encode the target source with high expressiveness. The dictionary is trained by each target source without the mixture signal, which makes the system independent from the context where the dictionaries will be used. In separation process, the decoder portions of the trained autoencoders are used as dictionaries to find the activations in a iterative manner such that a summation of the decoder outputs approximates the original mixture. The results of the instruments source separation experiments revealed that the separation performance of the proposed method was superior to that of the NMF.
- Muhammad A Shah, B. Raj, Khaled A. Harras. 2017. Inferring room semantics using acoustic monitoring. Abstract: Having knowledge of the environmental context of the user i.e. the knowledge of the users' indoor location and the semantics of their environment, can facilitate the development of many of location-aware applications. In this paper, we propose an acoustic monitoring technique that infers semantic knowledge about an indoor space over time, using audio recordings from it. Our technique uses the impulse response of these spaces as well as the ambient sounds produced in them in order to determine a semantic label for them. As we process more recordings, we update our confidence in the assigned label. We evaluate our technique on a dataset of single-speaker human speech recordings obtained in different types of rooms at three university buildings. In our evaluation, the confidence for the true label generally outstripped the confidence for all other labels and in some cases converged to 100% with less than 30 samples.
- Abelino Jiménez, B. Raj. 2017. A two factor transformation for speaker verification through ℓ1 comparison. Abstract: In a speaker verification task, speech is used as a unique biometrie identifier of an individual. A speaker presents his credentials along with a voice sample. The system matches the voice sample to its own model for the speaker to accept or reject him. This has many pitfalls. First, speech by itself, is not a sufficiently "strong" biometric, and false acceptance is a problem. Second, the user must provide the system with voice samples. This puts the speaker's privacy at risk. The system may infer personal information about the user, such as gender, age, ethnicity, health, etc. Finally, if a malicious entity pilfers the speaker's models from the system, the loss is permanent. The speaker cannot change their voice to re-enroll. In this paper, we present a two-factor transformation that addresses all the above issues. It combines a personal password with speech features in order to increase the performance of a verification system. At the same time it is guaranteed not to not reveal any information about the speech or the password to the system. Finally, it is cancelable; if a model is compromised, the user can re-enroll without risk. In particular, we study a transformation that preserves the ℓ1 distance between features as long as this is smaller than some threshold and the user uses the correct password. Experimental results confirm the theory of the proposal in term of improvement in the system's accuracy, finding conditions to get zero error. Security consequences and feasibility of its implementation are discussed.
- Nia Peters, B. Raj, Griffin D. Romigh. 2017. Topic and Prosodic Modeling for Interruption Management in Multi-User Multitasking Communication Interactions. Abstract: When to send system-mediated interruptions within collaborative multi-human-machine environments has been widely debated in the development of interruption management systems. Unfortunately, these studies do not address when to send interruptions in multi-user, multitasking scenarios or predictors of interruptibility within communication tasks. This paper addresses the issue of predicting interruptibility within these interactions with special attention to which users are engaged in which tasks or task engagement and where users are within a current task or task structure as predictors of interruptibility. Using natural human speech from these interactions, we attempt to model task engagement and task structure to predict candidate points of interruptions. The motivation for these models and their performance in a multi-user, multitasking environment are discussed as proposals in developing communication interruption management systems. To model task structure, a task breakpoint model is proposed which performs with a 90% accuracy within a multi-user, multitasking dataset. Integrating this task breakpoint model into a real-time interaction results in an average accuracy of 93% using the proposed task breakpoint model and a rule-based model. To determine the current task in which users are engaged or task engagement, a proposed task topic model performs with an accuracy between 76-88% depending on the topic within the dataset. Closely examining task structure and task engagement as predictors of interruptibility sheds new light on a rarely explored area for system-mediated interruption timings within multi-user, multitasking communication tasks.
- Pranay Manocha, Rohan Badlani, Anurag Kumar, Ankit Shah, Benjamin Elizalde, B. Raj. 2017. Content-Based Representations of Audio Using Siamese Neural Networks. Abstract: In this paper, we focus on the problem of content-based retrieval for audio, which aims to retrieve all semantically similar audio recordings for a given audio clip query. This problem is similar to the problem of query by example of audio, which aims to retrieve media samples from a database, which are similar to the user-provided example. We propose a novel approach which encodes the audio into a vector representation using Siamese Neural Networks. The goal is to obtain an encoding similar for files belonging to the same audio class, thus allowing retrieval of semantically similar audio. Using simple similarity measures such as those based on simple euclidean distance and cosine similarity we show that these representations can be very effectively used for retrieving recordings similar in audio content.
- Anders Øland, Aayush Bansal, R. Dannenberg, B. Raj. 2017. Be Careful What You Backpropagate: A Case For Linear Output Activations & Gradient Boosting. Abstract: In this work, we show that saturating output activation functions, such as the softmax, impede learning on a number of standard classification tasks. Moreover, we present results showing that the utility of softmax does not stem from the normalization, as some have speculated. In fact, the normalization makes things worse. Rather, the advantage is in the exponentiation of error gradients. This exponential gradient boosting is shown to speed up convergence and improve generalization. To this end, we demonstrate faster convergence and better performance on diverse classification tasks: image classification using CIFAR-10 and ImageNet, and semantic segmentation using PASCAL VOC 2012. In the latter case, using the state-of-the-art neural network architecture, the model converged 33% faster with our method (roughly two days of training less) than with the standard softmax activation, and with a slightly better performance to boot.
- Rohan Badlani, Ankit Shah, Benjamin Elizalde, Anurag Kumar, B. Raj. 2017. Framework for Evaluation of Sound Event Detection in Web Videos. Abstract: The largest source of sound events is web videos. Most videos lack sound event labels at segment level, however, a significant number of them do respond to text queries, from a match found using metadata by search engines. In this paper we explore the extent to which a search query can be used as the true label for detection of sound events in videos. We present a framework for large-scale sound event recognition on web videos. The framework crawls videos using search queries corresponding to 78 sound event labels drawn from three datasets. The datasets are used to train three classifiers, and we obtain a prediction on 3.7 million web video segments. We evaluated performance using the search query as true label and compare it with human labeling. Both types of ground truth exhibited close performance, to within 10%, and similar performance trend with increasing number of evaluated segments. Hence, our experiments show potential for using search query as a preliminary true label for sound event recognition in web videos.
- Haohan Wang, B. Raj. 2017. On the Origin of Deep Learning On the Origin of Deep Learning. Abstract: This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks, and extends to popular recent models like variational autoencoder and generative adversarial nets. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning. 1 ar X iv :1 70 2. 07 80 0v 1 [ cs .L G ] 2 4 Fe b 20 17
- Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, B. Raj, Le Song. 2017. SphereFace: Deep Hypersphere Embedding for Face Recognition. Abstract: This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter m. We further derive specific m to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge 1 show the superiority of A-Softmax loss in FR tasks.
- Anurag Kumar, B. Raj. 2017. Deep CNN Framework for Audio Event Recognition using Weakly Labeled Web Data. Abstract: The development of audio event recognition models requires labeled training data, which are generally hard to obtain. One promising source of recordings of audio events is the large amount of multimedia data on the web. In particular, if the audio content analysis must itself be performed on web audio, it is important to train the recognizers themselves from such data. Training from these web data, however, poses several challenges, the most important being the availability of labels : labels, if any, that may be obtained for the data are generally {\em weak}, and not of the kind conventionally required for training detectors or classifiers. We propose that learning algorithms that can exploit weak labels offer an effective method to learn from web data. We then propose a robust and efficient deep convolutional neural network (CNN) based framework to learn audio event recognizers from weakly labeled data. The proposed method can train from and analyze recordings of variable length in an efficient manner and outperforms a network trained with {\em strongly labeled} web data by a considerable margin.
- Nikolas Wolfe, Aditya Sharma, Lukas Drude, B. Raj. 2017. The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning. Abstract: How much can pruning algorithms teach us about the fundamentals of learning representations in neural networks? A lot, it turns out. Neural network model compression has become a topic of great interest in recent years, and many different techniques have been proposed to address this problem. In general, this is motivated by the idea that smaller models typically lead to better generalization. At the same time, the decision of what to prune and when to prune necessarily forces us to confront our assumptions about how neural networks actually learn to represent patterns in data. In this work we set out to test several long-held hypotheses about neural network learning representations and numerical approaches to pruning. To accomplish this we first reviewed the historical literature and derived a novel algorithm to prune whole neurons (as opposed to the traditional method of pruning weights) from optimally trained networks using a second-order Taylor method. We then set about testing the performance of our algorithm and analyzing the quality of the decisions it made. As a baseline for comparison we used a first-order Taylor method based on the Skeletonization algorithm and an exhaustive brute-force serial pruning algorithm. Our proposed algorithm worked well compared to a first-order method, but not nearly as well as the brute-force method. Our error analysis led us to question the validity of many widely-held assumptions behind pruning algorithms in general and the trade-offs we often make in the interest of reducing computational complexity. We discovered that there is a straightforward way, however expensive, to serially prune 40-70\% of the neurons in a trained network with minimal effect on the learning representation and without any re-training.
- G. Chollet, Abelino Jiménez, D. Petrovska-Delacrétaz, B. Raj. 2017. Privacy Preserving Biometric Identity Verification. Abstract: Monday, June 13, 2016 08:30 – 09:00 Registration 09:00 – 12:00 Morning Tutorials Tutorial I: Privacy Preserving Biometric Identity Verification Gérard Chollet, CNRS-LTCI, IMT, France; Dijana Petrovska-Delacrétaz, Mines Telecom / Telecom SudParis, France; Bhiksha Raj, Carnegie Mellon University, USA Tutorial II: 3D Morphable Face Models and Their Applications Josef Kittler, University of Surrey, UK; Paul Koppen, University of Surrey, UK 13:30 – 16:30: Afternoon Tutorials Tutorial III: Contactless 3D Fingerprint Acquisition and Matching Ajay Kumar, Hong Kong Polytechnic University, Hong Kong Tutorial IV: BEAT With Hands-On: An Online Web-Platform For Reproducible Research In Computational Science Andre Anjos, Idiap Research Institute; Switzerland; Laurent El Shafey, Idiap Research Institute; Switzerland; Sebastien Marcel, Idiap Research Institute; Switzerland 17:30 WELCOME RECEPTION Halmstad Castle and Scandic Hallandia Hotel
- Janek Ebbers, Jahn Heymann, Lukas Drude, Thomas Glarner, Reinhold Häb-Umbach, B. Raj. 2017. Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery. Abstract: Variational Autoencoders (VAEs) have been shown to provide efficient neural-network-based approximate Bayesian inference for observation models for which exact inference is intractable. Its extension, the so-called Structured VAE (SVAE) allows inference in the presence of both discrete and continuous latent variables. Inspired by this extension, we developed a VAE with Hidden Markov Models (HMMs) as latent models. We applied the resulting HMM-VAE to the task of acoustic unit discovery in a zero resource scenario. Starting from an initial model based on variational inference in an HMM with Gaussian Mixture Model (GMM) emission probabilities, the accuracy of the acoustic unit discovery could be significantly improved by the HMM-VAE. In doing so we were able to demonstrate for an unsupervised learning task what is well-known in the supervised learning case: Neural networks provide superior modeling power compared to GMMs.
- Haohan Wang, B. Raj, E. Xing. 2017. On the Origin of Deep Learning. Abstract: This paper is a review of the evolutionary history of deep learning models. It covers from the genesis of neural networks when associationism modeling of the brain is studied, to the models that dominate the last decade of research in deep learning like convolutional neural networks, deep belief networks, and recurrent neural networks. In addition to a review of these models, this paper primarily focuses on the precedents of the models above, examining how the initial ideas are assembled to construct the early models and how these preliminary models are developed into their current forms. Many of these evolutionary paths last more than half a century and have a diversity of directions. For example, CNN is built on prior knowledge of biological vision system; DBN is evolved from a trade-off of modeling power and computation complexity of graphical models and many nowadays models are neural counterparts of ancient linear models. This paper reviews these evolutionary paths and offers a concise thought flow of how these models are developed, and aims to provide a thorough background for deep learning. More importantly, along with the path, this paper summarizes the gist behind these milestones and proposes many directions to guide the future research of deep learning.
- G. Friedland, P. Smaragdis, Josh H. McDermott, B. Raj. 2017. Audition for multimedia computing. Abstract: What do the fields of robotics, human-computer interaction, AI, video retrieval, privacy, cybersecurity, Internet of Things, and big data all have in common? They all work with various sources of data: visual, textual, time stamps, links, records. But there is one source of data that has been almost completely ignored by the academic community---sound. 
 
Our comprehension of the world relies critically on audition---the ability to perceive and interpret the sounds we hear. Sound is ubiquitous, and is a unique source of information about our environment and the events occurring in it. Just by listening, we can determine whether our child's laughter originated inside or outside our house, how far away they were when they laughed, and whether the window through which the sound passed was open or shut. The ability to derive information about the world from sound is a core aspect of perceptual intelligence. 
 
Auditory inferences are often complex and sophisticated despite their routine occurrence. The number of possible inferences is typically not enumerable, and the final interpretation is not merely one of selection from a fixed set. And yet humans perform such inferences effortlessly, based only on sounds captured using two sensors, our ears. 
 
Electronic devices can also "perceive" sound. Every phone and tablet has at least one microphone, as do most cameras. Any device or space can be equipped with microphones at minimal expense. Indeed, machines can not only "listen"; they have potential advantages over humans as listening devices, in that they can communicate and coordinate their experiences in ways that biological systems simply cannot. Collections of devices that can sense sound and communicate with each other could instantiate a single electronic entity that far surpasses humans in its ability to record and process information from sound. 
 
And yet machines at present cannot truly hear. Apart from well-developed efforts to recover structure in speech and music, the state of the art in machine hearing is limited to relatively impoverished descriptions of recorded sounds: detecting occurrences of a limited pre-specified set of sound types, and their locations. Although researchers typically envision artificially intelligent agents such as robots to have human-like hearing abilities, at present the rich descriptions and inferences humans can make about sound are entirely beyond the capability of machine systems. 
 
In this chapter, we suggest establishing the field of Computer Audition to develop the theory behind artificial systems that extract information from sound. Our objective is to enable computer systems to replicate and exceed human abilities. This chapter describes the challenges of this field.
- Sebastian Säger, Damian Borth, Benjamin Elizalde, Christian Schulze, B. Raj, Ian Lane, A. Dengel. 2016. AudioSentibank: Large-scale Semantic Ontology of Acoustic Concepts for Audio Content Analysis. Abstract: Audio carries substantial information about the content of our surroundings. The content has been explored at the semantic level using acoustic concepts, but rarely on concept pairs such as happy crowd and angry crowd. Concept pairs convey unique information and complement other audio and multimedia applications. Hence, in this work we explored for the first time the classification's performance of acoustic concepts pairs. For this study, we introduce the AudioSentiBank corpus, which is a large-scale folksology containing over 1,123 adjective and verb noun pairs. Our contribution consists on providing the research corpus, the benchmark for classification of acoustic concept pairs, and an analysis on the pairs.
- Agha Ali Raza, Rajat Kulshreshtha, Spandana Gella, S. Blagsvedt, Maya Chandrasekaran, B. Raj, R. Rosenfeld. 2016. Viral Spread via Entertainment and Voice-Messaging Among Telephone Users in India. Abstract: We explore how development-related, voice-based, information services could organically spread among low-literate masses in the developing world. We report lessons learned from a remote deployment of "Polly" in India (from the US) to spread job-related information. Polly is an entertainment driven, voice-based service, available over simple phones that is aimed at familiarizing people with speech interfaces and mass-dissemination of development related information to low-literate users. In 2012, Polly had become viral in Pakistan and successfully spread recorded newspaper job ads to thousands of mobile phone users. Remotely deployed in India, Polly did not take off immediately as it did in Pakistan. Instead, it initially entered a six-month long phase of fluctuating, intermittent activity. We experimented with various forms of seeding and it eventually transitioned into a viral phase, with sustained transmission that continued for five months but without (exponential) growth. Finally, interface adjustments in response to user feedback enabling plain-voice asynchronous voice-messaging resulted in an abrupt exponential and viral growth amassing 10,349 phone calls by 1,613 users over a span of seven days. Of these, 299 users also transitioned to the job service. User feedback and surveys suggest possible reasons for each phase. We study the challenges of remote deployment and the interplay of user interface; language of the system; seeding mechanisms and active response to user feedback towards the uptake of the service. We also report a detailed comparison of viral spread in the two countries.
- Anurag Kumar, B. Raj. 2016. Features and Kernels for Audio Event Recognition. Abstract: One of the most important problems in audio event detection research is absence of benchmark results for comparison with any proposed method. Different works consider different sets of events and datasets which makes it difficult to comprehensively analyze any novel method with an existing one. In this paper we propose to establish results for audio event recognition on two recent publicly-available datasets. In particular we use Gaussian Mixture model based feature representation and combine them with linear as well as non-linear kernel Support Vector Machines.
- Abelino Jiménez, B. Raj. 2016. Privacy preserving Distance computation using somewhat-trusted third parties. Abstract: A critically important component of most signal processing procedures is that of computing the distance between signals. In multiparty processing applications where these signals belong to different parties, this introduces privacy challenges. The signals may themselves be private, and the parties to the computation may not be willing to expose them. Solutions proposed to the problem in the literature generally invoke homomorphic encryption schemes, secure multi-party computation, or other cryptographic methods which introduce significant computational complexity into the proceedings, often to the point of making more complex computations requiring repeated computations unfeasible. Other solutions invoke third parties, making unrealistic assumptions about their trustworthiness. In this paper we propose an alternate approach, also based on third party computation, but without assuming as much trust in the third party. Individual participants to the computation “secure” their data through a proposed secure hashing scheme with shared keys, prior to sharing it with the third party. The hashing ensures that the third party cannot recover any information about the individual signals or their statistics, either from analysis of individual computations or their long-term aggregate patterns. We provide theoretical proof of these properties and empirical demonstration of the feasibility of the computation.
- Benjamin Elizalde, Ankit Shah, Siddharth Dalmia, Min Hun Lee, Rohan Badlani, Anurag Kumar, B. Raj, Ian Lane. 2016. An approach for self-training audio event detectors using web data. Abstract: Audio Event Detection (AED) aims to recognize sounds within audio and video recordings. AED employs machine learning algorithms commonly trained and tested on annotated datasets. However, available datasets are limited in number of samples and hence it is difficult to model acoustic diversity. Therefore, we propose combining labeled audio from a dataset and unlabeled audio from the web to improve the sound models. The audio event detectors are trained on the labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever the detectors recognized any of the known sounds with high confidence, the unlabeled audio was use to re-train the detectors. The performance of the re-trained detectors is compared to the one from the original detectors using the annotated test set. Results showed an improvement of the AED, and uncovered challenges of using web audio from videos.
- Anurag Kumar, B. Raj, Ndapandula Nakashole. 2016. Discovering sound concepts and acoustic relations in text. Abstract: In this paper we describe approaches for discovering acoustic concepts and relations in text. The first major goal is to be able to identify text phrases which contain a notion of audibility and can be termed as a sound or an acoustic concept. We also propose a method to define an acoustic scene through a set of sound concepts. We use pattern matching and parts of speech tags to generate sound concepts from large scale text corpora. We use dependency parsing and LSTM recurrent neural network to predict a set of sound concepts for a given acoustic scene. These methods are not only helpful in creating an acoustic knowledge base but in the future can also directly help acoustic event and scene detection research.
- Rita Singh, B. Raj, D. Gençaga. 2016. Forensic anthropometry from voice: An articulatory-phonetic approach. Abstract: This paper addresses a problem that is of paramount importance in solving crimes wherein voice may be key evidence, or the only evidence: that of describing the perpetrator. The term Forensic anthropometry from voice refers to the deduction of the speaker's physical dimensions from voice. There are multiple studies in the literature that approach this problem in different ways, many of which depend on the availability of sufficient volumes of speech for analysis. However, in the case of many voice-based crimes, the voice evidence available may be limited. In such cases it is especially advantageous to regard the recorded signal as comprising multiple pieces of evidence. In this paper, we show how this can be done. We explain why, for any anthropometric measurement from speech, it makes sense to consider the contributions of each articulatory-phonetic unit independently of others, and to aggregate the deductions from them only in the aftermath. This approach is based on the hypothesis that the relative evidence given by different compositional units of speech can be more indicative of the anthropometric factor being deduced, than the evidence derived from the aggregate voice signal. We explain the applicability of this approach through experiments on standard speech databases.
- M. J. Correia, I. Trancoso, B. Raj. 2016. Adaptation of SVM for MIL for inferring the polarity of movies and movie reviews. Abstract: Polarity detection is a research topic of major interest, with many applications including detecting the polarity of product reviews. However, in some cases, the polarity of the product reviews might not be available while the polarity of the product itself might be, prohibiting the use of any form of fully supervised learning technique. This scenario, while different, is close to that of multiple instance learning (MIL). In this work we propose two new adaptations of support vector machines (SVM) for MIL, θ-MIL, to suit this new scenario, and infer the polarity of products and product reviews. We perform experiments on the proposed methods using the IMDb movie review corpus, and compare the performance of the proposed methods to the traditional SVM for MIL approach. Although we make weaker assumptions about the data, the proposed methods achieve a comparable performance to the SVM for MIL in accurately detecting the polarity of movies and movie reviews.
- Benjamin Elizalde, Anurag Kumar, Ankit Shah, Rohan Badlani, E. Vincent, B. Raj, Ian Lane. 2016. Experiments on the DCASE Challenge 2016: Acoustic Scene Classification and Sound Event Detection in Real Life Recording. Abstract: In this paper we present our work on Task 1 Acoustic Scene Classi- fication and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments we have low-level and high-level features, classifier optimization and other heuristics specific to each task. Our performance for both tasks improved the baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9% compared to the baseline of 72.6% and for Task 3 we achieved a Segment-Based Error Rate of 0.76 compared to the baseline of 0.91.
- Lukas Drude, B. Raj, Reinhold Häb-Umbach. 2016. On the Appropriateness of Complex-Valued Neural Networks for Speech Enhancement. Abstract: Although complex-valued neural networks (CVNNs) â?? networks which can operate with complex arithmetic â?? have been around for a while, they have not been given reconsideration since the breakthrough of deep network architectures. This paper presents a critical assessment whether the novel tool set of deep neural networks (DNNs) should be extended to complex-valued arithmetic. Indeed, with DNNs making inroads in speech enhancement tasks, the use of complex-valued input data, specifically the short-time Fourier transform coefficients, is an obvious consideration. In particular when it comes to performing tasks that heavily rely on phase information, such as acoustic beamforming, complex-valued algorithms are omnipresent. In this contribution we recapitulate backpropagation in CVNNs, develop complex-valued network elements, such as the split-rectified non-linearity, and compare real- and complex-valued networks on a beamforming task. We find that CVNNs hardly provide a performance gain and conclude that the effort of developing the complex-valued counterparts of the building blocks of modern deep or recurrent neural networks can hardly be justified.
- R. Iyer, Sanjeel Parekh, Vikas Mohandoss, Anush Ramsurat, B. Raj, Rita Singh. 2016. Content-based Video Indexing and Retrieval Using Corr-LDA. Abstract: Existing video indexing and retrieval methods on popular web-based multimedia sharing websites are based on user-provided sparse tagging. This paper proposes a very specific way of searching for video clips, based on the content of the video. We present our work on Content-based Video Indexing and Retrieval using the Correspondence-Latent Dirichlet Allocation (corr-LDA) probabilistic framework. This is a model that provides for auto-annotation of videos in a database with textual descriptors, and brings the added benefit of utilizing the semantic relations between the content of the video and text. We use the concept-level matching provided by corr-LDA to build correspondences between text and multimedia, with the objective of retrieving content with increased accuracy. In our experiments, we employ only the audio components of the individual recordings and compare our results with an SVM-based approach.
- Anurag Kumar, B. Raj. 2016. Audio Event Detection using Weakly Labeled Data. Abstract: Acoustic event detection is essential for content analysis and description of multimedia recordings. The majority of current literature on the topic learns the detectors through fully-supervised techniques employing strongly labeled data. However, the labels available for majority of multimedia data are generally weak and do not provide sufficient detail for such methods to be employed. In this paper we propose a framework for learning acoustic event detectors using only weakly labeled data. We first show that audio event detection using weak labels can be formulated as an Multiple Instance Learning problem. We then suggest two frameworks for solving multiple-instance learning, one based on support vector machines, and the other on neural networks. The proposed methods can help in removing the time consuming and expensive process of manually annotating data to facilitate fully supervised learning. Moreover, it can not only detect events in a recording but can also provide temporal locations of events in the recording. This helps in obtaining a complete description of the recording and is notable since temporal information was never known in the first place in weakly labeled data.
- Anurag Kumar, B. Raj. 2016. Weakly supervised scalable audio content analysis. Abstract: Audio Event Detection is an important task for content analysis of multimedia data. Most of the current works on detection of audio events is driven through supervised learning approaches. We propose a weakly supervised learning framework which can make use of the tremendous amount of web multimedia data with significantly reduced annotation effort and expense. Specifically, we use several multiple instance learning algorithms to show that audio event detection through weak labels is feasible. We also propose a novel scalable multiple instance learning algorithm and show that its competitive with other multiple instance learning algorithms for audio event detection tasks.
- B. Raj. 2016. APPLYING RECURRENT NEURAL NETWORK TO ARABIC NAMED ENTITY RECOGNITION. Abstract: This technical report collects the final reports of the undergraduate Computer Science majors from the Qatar Campus of Carnegie Mellon University who elected to complete a senior research thesis in the academic year 2015–16 as part of their degree. These projects have spanned the students’ entire senior year, during which they have worked closely with their faculty advisors to plan and carry out their projects. This work counts as 18 units of academic credit each semester. In addition to doing the research, the students presented a brief midterm progress report each semester, presented a public poster session in December, presented an oral summary in the year-end campuswide Meeting of the Minds and submitted a written thesis in May.
- B. Raj, Anurag Kumar. 2016. Audio event and scene recognition: A unified approach using strongly and weakly labeled data. Abstract: In this paper we propose a novel learning framework called Supervised and Weakly Supervised Learning where the goal is to learn simultaneously from weakly and strongly labeled data. Strongly labeled data can be simply understood as fully supervised data where all labeled instances are available. In weakly supervised learning only data is weakly labeled which prevents one from directly applying supervised learning methods. Our proposed framework is motivated by the fact that a small amount of strongly labeled data can give considerable improvement over only weakly supervised learning. The primary problem domain focus of this paper is acoustic event and scene detection in audio recordings. We first propose a naive formulation for leveraging labeled data in both forms. We then propose a more general framework for Supervised and Weakly Supervised Learning (SWSL). Based on this general framework, we propose a graph based approach for SWSL. Our main method is based on manifold regularization on graphs in which we show that the unified learning can be formulated as a constraint optimization problem which can be solved by iterative concave-convex procedure (CCCP). Our experiments show that our proposed framework can address several concerns of audio content analysis using weakly labeled data.
- Suyoun Kim, B. Raj, Ian Lane. 2016. Environmental Noise Embeddings for Robust Speech Recognition. Abstract: We propose a novel deep neural network architecture for speech recognition that explicitly employs knowledge of the background environmental noise within a deep neural network acoustic model. A deep neural network is used to predict the acoustic environment in which the system in being used. The discriminative embedding generated at the bottleneck layer of this network is then concatenated with traditional acoustic features as input to a deep neural network acoustic model. Through a series of experiments on Resource Management, CHiME-3 task, and Aurora4, we show that the proposed approach significantly improves speech recognition accuracy in noisy and highly reverberant environments, outperforming multi-condition training, noise-aware training, i-vector framework, and multi-task learning on both in-domain noise and unseen noise.
- Anurag Kumar, Benjamin Elizalde, B. Raj. 2016. Audio Content Based Geotagging in Multimedia. Abstract: In this paper we propose methods to extract geographically relevant information in a multimedia recording using its audio. Our method primarily is based on the fact that urban acoustic environment consists of a variety of sounds. Hence, location information can be inferred from the composition of sound events/classes present in the audio. More specifically, we adopt matrix factorization techniques to obtain semantic content of recording in terms of different sound classes. These semantic information are then combined to identify the location of recording.
- Rita Singh, Joseph Keshet, D. Gençaga, B. Raj. 2016. The relationship of voice onset time and Voice Offset Time to physical age. Abstract: In a speech signal, Voice Onset Time (VOT) is the period between the release of a plosive and the onset of vocal cord vibrations in the production of the following sound. Voice Offset Time (VOFT), on the other hand, is the period between the end of a voiced sound and the release of the following plosive. Traditionally, VOT has been studied across multiple disciplines and has been related to many factors that influence human speech production, including physical, physiological and psychological characteristics of the speaker. The mechanism of extraction of VOT has however been largely manual, and studies have been carried out over small ensembles of individuals under very controlled conditions, usually in clinical settings. Studies of VOFT follow similar trends, but are more limited in scope due to the inherent difficulty in the extraction of VOFT from speech signals. In this paper we use a structured-prediction based mechanism for the automatic computation of VOT and VOFT. We show that for specific combinations of plosives and vowels, these are relatable to the physical age of the speaker. The paper also highlights the ambiguities in the prediction of age from VOT and VOFT, and consequently in the use of these measures in forensic analysis of voice.
- Rita Singh, D. Gençaga, B. Raj. 2016. Formant manipulations in voice disguise by mimicry. Abstract: The human voice can be disguised in many ways. The purpose of disguise could either be to impersonate another person, or to conceal the identity of the original speaker, or both. On the other hand, the goal of any biometric analysis on disguised voices could also be twofold: either to find out if the originator of the disguised voice is a given speaker, or to know how a speaker's voice can be manipulated so that the extent and type of disguise that the speaker can perform can be guessed a-priori. Any analysis toward the former goal must rely on the knowledge of what characteristics of a person's voice are least affected or unaffected by attempted disguise. Analysis towards the latter goal must use the knowledge of what sounds are typically most amenable to voluntary variation by the speaker, so that the extent to which given speakers can successfully disguise their voice can be estimated. Our paper attempts to establish a simple methodology for analysis of voice for both goals. We study the voice impersonations performed by an expert mimic, focusing specifically on formants and formant-related measurements, to find out the extent and type of formant manipulations that are performed by the expert at the level of individual phonemes. Expert mimicry is an extreme form of attempted disguise. Our study is presented with the expectation that non-expert attempts at voice disguise by mimicry will fall within the gold standard of manipulation patterns set by an expert mimic, and that it is therefore useful to establish this gold standard.
- Afsaneh Asaei, M. Taghizadeh, Saeid Haghighatshoar, B. Raj, H. Bourlard, V. Cevher. 2016. Binary Sparse Coding of Convolutive Mixtures for Sound Localization and Separation via Spatialization. Abstract: We propose a sparse coding approach to address the problem of source-sensor localization and speech reconstruction. This approach relies on designing a dictionary of spatialized signals by projecting the microphone array recordings into the array manifolds characterized for different locations in a reverberant enclosure using the image model. Sparse representation over this dictionary enables identifying the subspace of the actual recordings and its correspondence to the source and sensor locations. The speech signal is reconstructed by inverse filtering the acoustic channels associated to the array manifolds. We provide rigorous analysis on the optimality of speech reconstruction by elucidating the links between inverse filtering and source separation followed by deconvolution. This procedure is evaluated for localization, reconstruction and recognition of simultaneous speech sources using real data recordings. The results demonstrate the effectiveness of the proposed approach and compare favorably against beamforming and independent component analysis techniques.
- Rita Singh, B. Raj, J. Baker. 2016. Short-term analysis for estimating physical parameters of speakers. Abstract: Conventional approaches to estimating speakers' physiometric parameters such as height, age, weight etc. from their voice analyze the speech signal at relatively coarse time resolutions, typically with analysis windows of 25ms or longer. At these resolutions the analysis effectively captures the structure of the supra-glottal vocal tract. In this paper we hypothesize that by analyzing the signal at a finer temporal resolution that is lower than a pitch period, it may be possible to analyze segments of the speech signal that are obtained entirely when the glottis is open, and thereby capture some of the sub-glottal structure that may be represented in the voice. To explore this hypothesis we propose an analysis approach that combines signal analysis techniques suited to fine-temporal-resolution analysis and well-known regression models. We test it on the prediction of heights and ages of speakers from a standard speech database. Our findings show that the higher-resolution analysis does provide benefits over conventional analysis for estimating speaker height, although it is less useful in predicting age.
- Luís Marujo, José Portêlo, Wang Ling, David Martins de Matos, J. Neto, A. Gershman, J. Carbonell, I. Trancoso, B. Raj. 2015. Privacy-Preserving Multi-Document Summarization. Abstract: State-of-the-art extractive multi-document summarization systems are usually designed without any concern about privacy issues, meaning that all documents are open to third parties. In this paper we propose a privacy-preserving approach to multi-document summarization. Our approach enables other parties to obtain summaries without learning anything else about the original documents’ content. We use a hashing scheme known as Secure Binary Embeddings to convert documents representation containing key phrases and bag-of-words into bit strings, allowing the computation of approximate distances, instead of exact ones. Our experiments indicate that our system yields similar results to its non-private counterpart on standard multi-document evaluation datasets.
- Wenbo Liu, Li Yi, Zhiding Yu, Xiaobing Zou, B. Raj, Ming Li. 2015. Efficient autism spectrum disorder prediction with eye movement: A machine learning framework. Abstract: We propose an autism spectrum disorder (ASD) prediction system based on machine learning techniques. Our work features the novel development and application of machine learning methods over traditional ASD evaluation protocols. Specifically, we are interested in discovering the latent patterns that possibly indicate the symptom of ASD underneath the observations of eye movement. A group of subjects (either ASD or non-ASD) are shown with a set of aligned human face images, with eye gaze locations on each image recorded sequentially. An image-level feature is then extracted from the recorded eye gaze locations on each face image. Such feature extraction process is expected to capture discriminative eye movement patterns related to ASD. In this work, we propose a variety of feature extraction methods, seeking to evaluate their prediction performance comprehensively. We further propose an ASD prediction framework in which the prediction model is learned on the labeled features. At testing stage, a test subject is also asked to view the face images with eye gaze locations recorded. The learned model predicts the image-level labels and a threshold is set to determine whether the test subject potentially has ASD or not. Despite the inherent difficulty of ASD prediction, experimental results indicates statistical significance of the predicted results, showing promising perspective of this framework.
- José Portêlo, B. Raj, I. Trancoso. 2015. Logsum Using Garbled Circuits. Abstract: Secure multiparty computation allows for a set of users to evaluate a particular function over their inputs without revealing the information they possess to each other. Theoretically, this can be achieved using fully homomorphic encryption systems, but so far they remain in the realm of computational impracticability. An alternative is to consider secure function evaluation using homomorphic public-key cryptosystems or Garbled Circuits, the latter being a popular trend in recent times due to important breakthroughs. We propose a technique for computing the logsum operation using Garbled Circuits. This technique relies on replacing the logsum operation with an equivalent piecewise linear approximation, taking advantage of recent advances in efficient methods for both designing and implementing Garbled Circuits. We elaborate on how all the required blocks should be assembled in order to obtain small errors regarding the original logsum operation and very fast execution times.
- M. Usmani, Ramón Cepeda, T. M. Sullivan, B. Raj. 2015. Improving headphone spatialization for stereo music. Abstract: Music is mixed and mastered for playback on near- and far-field speakers, which presents a problem to the growing population of listeners who listen to music primarily on headphones. Playing legacy stereo mixes on headphones places the stereo image inside the listener's head and makes the image appear ultra-wide. While this is helpful for separating the center and stereo content, it has a detrimental effect on the spatialization of the music. It makes headphone listening unnatural and uncomfortable, although listeners have learned to accept it. In this work, we develop a system that processes stereo signals to provide a better sound image to headphone listeners. The sound image is improved by adding the necessary cues to the signal so as to externalize the perceived soundstage by making it similar to the soundstage experienced inside a mixing studio. In order to work with all headphones, our system tries to maintain the mastering equalization curve of the original stereo content. We employ head-related tr...
- Anurag Kumar, B. Raj. 2015. Unsupervised Fusion Weight Learning in Multiple Classifier Systems. Abstract: In this paper we present an unsupervised method to learn the weights with which the scores of multiple classifiers must be combined in classifier fusion settings. We also introduce a novel metric for ranking instances based on an index which depends upon the rank of weighted scores of test points among the weighted scores of training points. We show that the optimized index can be used for computing measures such as average precision. Unlike most classifier fusion methods where a single weight is learned to weigh all examples our method learns instance-specific weights. The problem is formulated as learning the weight which maximizes a clarity index; subsequently the index itself and the learned weights both are used separately to rank all the test points. Our method gives an unsupervised method of optimizing performance on actual test data, unlike the well known stacking-based methods where optimization is done over a labeled training set. Moreover, we show that our method is tolerant to noisy classifiers and can be used for selecting N-best classifiers.
- Anders Øland, B. Raj. 2015. Reducing communication overhead in distributed learning by an order of magnitude (almost). Abstract: Large-scale distributed learning plays an ever-more increasing role in modern computing. However, whether using a compute cluster with thousands of nodes, or a single multi-GPU machine, the most significant bottleneck is that of communication. In this work, we explore the effects of applying quantization and encoding to the parameters of distributed models. We show that, for a neural network, this can be done - without slowing down the convergence, or hurting the generalization of the model. In fact, in our experiments we were able to reduce the communication overhead by nearly an order of magnitude - while actually improving the generalization accuracy.
- Wenbo Liu, Zhiding Yu, B. Raj, Ming Li. 2015. Locality constrained transitive distance clustering on speech data. Abstract: The idea of developing unsupervised learning methods has received signiﬁcant attention in recent years. An important application is whether one can train a high quality speaker veri-ﬁcation model given large quantities of unlabeled speech data. Unsupervised learning methods such as data clustering often play a central role since they are able to analyze the underlying latent patterns without any supervision information. In this paper, we focus on developing an effective clustering method for speech data. We propose the locality constrained transitive distance, a distance measure which better models speech data with arbitrarily shaped clusters. We also propose a robust top-down clustering framework on top of the distance measure to generate accurate cluster labels. Experimental results show the good performance of the proposed method.
- Soham De, Indradyumna Roy, Tarunima Prabhakar, Kriti Suneja, Sourish Chaudhuri, Rita Singh, B. Raj. 2015. Plagiarism Detection in Polyphonic Music using Monaural Signal Separation. Abstract: Given the large number of new musical tracks released each year, automated approaches to plagiarism detection are essential to help us track potential violations of copyright. Most current approaches to plagiarism detection are based on musical similarity measures, which typically ignore the issue of polyphony in music. We present a novel feature space for audio derived from compositional modelling techniques, commonly used in signal separation, that provides a mechanism to account for polyphony without incurring an inordinate amount of computational overhead. We employ this feature representation in conjunction with traditional audio feature representations in a classification framework which uses an ensemble of distance features to characterize pairs of songs as being plagiarized or not. Our experiments on a database of about 3000 musical track pairs show that the new feature space characterization produces significant improvements over standard baselines.
- K. Osako, Rita Singh, B. Raj. 2015. Complex recurrent neural networks for denoising speech signals. Abstract: Effective denoising of noise-corrupted speech signals remains a challenging problem. Existing solutions typically employ some combination of noise estimation and noise elimination, either by subtraction or by filtering. The estimation of noise and the denoising are generally treated as independent aspects of the problem. In this paper we propose a new neural-network-based approach for de-noising of speech signals. The approach integrates noise estimation and denoising into a single network design, while maintaining many of the aspects of conventional noise estimation and signal denoising through a recurrent gated structure. The network thus operates as a single integrated process that can be trained to jointly estimate noise and denoise the speech signal with minimal artifacts. Noise reduction experiments on noisy speech, both with digitally added synthetic noise and real car noise, show that the proposed algorithm can recover much of the degradation caused by the noise.
- Zhenzhong Lan, Shoou-I Yu, Ming Lin, B. Raj, Alexander Hauptmann. 2015. Handcrafted Local Features are Convolutional Neural Networks. Abstract: Image and video classification research has made great progress through the development of handcrafted local features and learning based features. These two architectures were proposed roughly at the same time and have flourished at overlapping stages of history. However, they are typically viewed as distinct approaches. In this paper, we emphasize their structural similarities and show how such a unified view helps us in designing features that balance efficiency and effectiveness. As an example, we study the problem of designing efficient video feature learning algorithms for action recognition. 
We approach this problem by first showing that local handcrafted features and Convolutional Neural Networks (CNNs) share the same convolution-pooling network structure. We then propose a two-stream Convolutional ISA (ConvISA) that adopts the convolution-pooling structure of the state-of-the-art handcrafted video feature with greater modeling capacities and a cost-effective training algorithm. Through custom designed network structures for pixels and optical flow, our method also reflects distinctive characteristics of these two data sources. 
Our experimental results on standard action recognition benchmarks show that by focusing on the structure of CNNs, rather than end-to-end training methods, we are able to design an efficient and powerful video feature learning algorithm.
- Haohan Wang, B. Raj. 2015. A Survey: Time Travel in Deep Learning Space: An Introduction to Deep Learning Models and How Deep Learning Models Evolved from the Initial Ideas. Abstract: This report will show the history of deep learning evolves. It will trace back as far as the initial belief of connectionism modelling of brain, and come back to look at its early stage realization: neural networks. With the background of neural network, we will gradually introduce how convolutional neural network, as a representative of deep discriminative models, is developed from neural networks, together with many practical techniques that can help in optimization of neural networks. On the other hand, we will also trace back to see the evolution history of deep generative models, to see how researchers balance the representation power and computation complexity to reach Restricted Boltzmann Machine and eventually reach Deep Belief Nets. Further, we will also look into the development history of modelling time series data with neural networks. We start with Time Delay Neural Networks and move further to currently famous model named Recurrent Neural Network and its extension Long Short Term Memory. We will also briefly look into how to construct deep recurrent neural networks. Finally, we will conclude this report with some interesting open-ended questions of deep neural networks.
- T. Virtanen, J. Gemmeke, B. Raj, P. Smaragdis. 2015. Compositional Models for Audio Processing: Uncovering the structure of sound mixtures. Abstract: Many classes of data are composed as constructive combinations of parts. By constructive combination, we mean additive combination that does not result in subtraction or diminishment of any of the parts. We will refer to such data as compositional data. Typical examples include population or counts data, where the total count of a population is obtained as the sum of counts of subpopulations. To characterize such data, various mathematical models have been developed in the literature. These models, in conformance with the nature of the data, represent them as nonnegative linear combinations of parts, which themselves are also nonnegative to ensure that such a combination does not result in subtraction or diminishment. We will refer to such models as compositional models.
- Nikolas Wolfe, Juneki Hong, Agha Ali Raza, B. Raj, R. Rosenfeld. 2015. Rapid development of public health education systems in low-literacy multilingual environments: combating ebola through voice messaging. Abstract: One of the main challenges in combating the spread of the Ebola outbreak in West Africa is a lack of effective public health education among affected populations in Guinea, Sierra Leone, and Liberia. Difﬁculties include resistance to ofﬁcial sources of information, mistrust of government, cultural norms, linguistic barriers, and illiteracy. In this paper we describe the development and initial deployment of a voice-based, multilingual mobile phone application to spread reliable public health information about Ebola via peer-to-peer sharing. Our hypothesis is that we can overcome mistrust and disseminate important health information via the power of social learning and suggestion from friends, family, and local communities. In collaboration with partners on the ground in Conakry, Guinea, we have launched two parallel mobile phone services known as Polly Game and Polly Health to enable message sharing in several Guinean languages. We discuss a variety of strategies we have tried to encourage the spread of the application and data on uptake to date.
- Anurag Kumar, B. Raj. 2015. A novel ranking method for multiple classifier systems. Abstract: We introduce an unsupervised optimization method for optimal fusion of multiple classifiers in retrieval problems. The method is based on a ranking loss called the “clarity” index, which does not depend on the label of the test instances. The technique optimizes the weights with which individual classifier scores must be combined to maximize this clarity. Our method is instance-specific; the weights are optimized individually for each test instance. The proposed schema can also be used for instance-specific ranking of classifiers. We also show that the method is highly tolerant to the introduction of noise in classifier outputs.
- José Portêlo, A. Abad, B. Raj, I. Trancoso. 2015. Privacy-preserving Query-by-Example Speech Search. Abstract: This paper investigates a new privacy-preserving paradigm for the task of Query-by-Example Speech Search using Secure Binary Embeddings, a hashing method that converts vector data to bit strings through a combination of random projections followed by banded quantization. The proposed method allows performing spoken query search in an encrypted domain, by analyzing ciphered information computed from the original recordings. Unlike other hashing techniques, the embeddings allow the computation of the distance between vectors that are close enough, but are not perfect matches. This paper shows how these hashes can be combined with Dynamic Time Warping based on posterior derived features to perform secure speech search. Experiments performed on a sub-set of the Speech-Dat Portuguese corpus showed that the proposed privacy-preserving system obtains similar results to its non-private counterpart.
- Abelino Jiménez, B. Raj, José Portêlo, I. Trancoso. 2015. Secure Modular Hashing. Abstract: In many situations, such as in biometric applications, there is need to encrypt and “hide” data, while simultaneously permitting restricted computations on them. We present a method to securely determine the ℓ2 distance between two signals if they are close enough. This method relies on a locality sensitive hashing scheme based on a secure modular embedding, computed using quantized random projections, being a generalization of previous work in the area. Secure Modular Hashes (SMH) extracted from the signals preserve information about the distance between the signals, hiding other characteristic from the signals. Theoretical properties state that the described scheme provides a mechanism to threshold how much information to reveal, and is also information theoretically secure above this threshold. Finally, experimental results reveal that distances computed from SMH vectors can effectively replace the actual Euclidean distances with minimal degradation.
- José Portêlo, B. Raj, A. Abad, I. Trancoso. 2014. Privacy-preserving speaker verification using secure binary embeddings. Abstract: Remote speaker verification services typically rely on the system having access to the users recordings, or features derived from them, and/or a model for the users voice. This conventional approach raises several privacy concerns. In this work, we address this privacy problem in the context of a speaker verification system using a factor analysis based front-end extractor, the so-called i-vectors. Preserving privacy in our context means that neither the system observes voice samples or speech models from the user, nor the user observes the universal model owned by the system. This is achieved by transforming speaker i-vectors to bit strings in a way that allows for the computation of approximate distances, instead of exact ones. The key to the transformation uses a hashing scheme known as secure binary embeddings. Then, an SVM classifier with a modified kernel operates on the hashes. Experiments showed that the secure system yielded similar results as its non-private counterpart. The approach may be extended to other types of biometric authentication.
- Zhenzhong Lan, Ming Lin, Xuanchong Li, Alexander Hauptmann, B. Raj. 2014. Beyond Gaussian Pyramid: Multi-skip Feature Stacking for action recognition. Abstract: Most state-of-the-art action feature extractors involve differential operators, which act as highpass filters and tend to attenuate low frequency action information. This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices. The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation. However, at the core of the Gaussian Pyramid is a convolutional smoothing operation, which makes it incapable of generating new features at coarse scales. In order to address this problem, we propose a novel feature enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space. MIFS compensates for information lost from using differential operators by recapturing information at coarse scales. This recaptured information allows us to match actions at different speeds and ranges of motion. We prove that MIFS enhances the learnability of differential-based features exponentially. The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods. Experimental results show significantly improved performance on challenging action recognition and event detection tasks. Specifically, our method exceeds the state-of-the-arts on Hollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts on HMDB51 and Olympics Sports datasets. MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost.
- Luís Marujo, José Portêlo, David Martins de Matos, Joao P. Neto, A. Gershman, J. Carbonell, I. Trancoso, B. Raj. 2014. Privacy-Preserving Important Passage Retrieval. Abstract: State-of-the-art important passage retrieval methods obtain very good results, but do not take into account privacy issues. In this paper, we present a privacy preserving method that relies on creating secure representations of documents. Our approach allows for third parties to retrieve important passages from documents without learning anything regarding their content. We use a hashing scheme known as Secure Binary Embeddings to convert a key phrase and bag-of-words representation to bit strings in a way that allows the computation of approximate distances, instead of exact ones. Experiments show that our secure system yield similar results to its non-private counterpart on both clean text and noisy speech recognized text.
- José Portêlo, B. Raj, A. Abad, I. Trancoso. 2014. Privacy-preserving speaker verification using garbled GMMS. Abstract: In this paper we present a privacy-preserving speaker verification system using a UBM-GMM technique. Remote speaker verification services rely on the system having access to the user's recordings, or features derived from them, and a model representing the user's voice. Preserving privacy in our context means that neither the system observes voice samples or speech models from the user nor the user observes the universal model owned by the system. Our approach uses Garbled Circuits for obtaining an implementation that simultaneously is secure, has high accuracy and is efficient. To the best of our knowledge this is the first privacy-preserving speaker verification system that accomplishes all these three goals.
- Elena Demidova, Jos Portlo, David Martins de Matos, Joo P. Neto, A. Gershman, B. Raj, S. Bressan, Anisha T. J. Fernando, J. Du, H. Ashman, ChengXiang Zhai, Craig Macdonald, I. Ounis. 2014. Proceeding of the 1 st International Workshop on Privacy-Preserving IR : When Information Retrieval Meets Privacy and Security ( PIR 2014 ). Abstract: Many real world applications in the healthcare domain would gain a substantial advantage from sharing and search technologies available for P2P infrastructures if these technologies could provide required confidentiality guarantees. Currently, DHT-based indexes which are typically applied for effective and efficient information sharing and retrieval in P2P networks do not offer sufficient confidentiality for the patient data in a healthcare network and medical document archives. In this paper we discuss the challenges involved in securing patient data stored in a DHT-based index and discuss initial solutions to address these challenges.
- Shoou-I Yu, Lu Jiang, Zhongwen Xu, Zhenzhong Lan, Shicheng Xu, Xiaojun Chang, Xuanchong Li, Zexi Mao, Chuang Gan, Yajie Miao, Xingzhong Du, Yang Cai, Lara J. Martin, Nikolas Wolfe, Anurag Kumar, Huan Li, Ming Lin, Zhigang Ma, Yi Yang, Deyu Meng, S. Shan, P. D. Sahin, Susanne Burger, Florian Metze, Rita Singh, B. Raj, T. Mitamura, R. Stern, Alexander Hauptmann. 2014. Informedia@TrecVID 2014: MED and MER. Abstract: We report on our system used in the TRECVID 2014 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. On the MED task, the CMU team achieved leading performance in the Semantic Query (SQ), 000Ex, 010Ex and 100Ex settings. Furthermore, SQ and 000Ex runs are significantly better than the submissions from the other teams. We attribute the good performance to 4 main components: 1) our large-scale semantic concept detectors trained on video shots for SQ/000Ex systems, 2) better features such as improved trajectories and deep learning features for 010Ex/100Ex systems, 3) a novel Multistage Hybrid Late Fusion method for 010Ex/100Ex systems and 4) our developed reranking methods for Pseudo Relevance Feedback for 000Ex/010Ex systems. On the MER task, our system utilizes a subset of features and detection results from the MED system from which the recounting is then generated. Recounting evidence is presented by selecting the most likely concepts detected in the salient shots of a video. Salient shots are detected by searching for shots which have high response when predicted by the video level event detector.
- Amir R. Moghimi, B. Raj, R. Stern. 2014. Post-masking: a hybrid approach to array processing for speech recognition. Abstract: In the context of array processing for speech and audio ap-plications, linear beamforming has long been the approach of choice, for reasons including good performance, robustness and analytical simplicity. Nevertheless, various nonlinear techniques, typically based on the study of auditory scene analysis, have also been of interest. The class of techniques known as time-frequency (T-F) masking , in particular, shows promise; T-F masking is based on accepting or rejecting individual time-frequency cells based on some estimate of local signal quality. While these approaches have been shown to outperform linear beamforming in two-sensor arrays, extensions to larger arrays have been few and unsuccessful. This paper seeks to gain a deeper understanding of the limitations of T-F masking in larger arrays and to develop an approach to overcome them. It is shown that combining beamforming and masking can bring the beneﬁts of masking to larger arrays. As a result, a hybrid beamforming-masking approach, called post-masking, is devel-oped that improves upon the performance of MMSE beamforming (and can be used with any beamforming technique). Post-masking extends the beneﬁts of masking up to arrays of six elements or more, with the potential for even greater improvement in the future.
- R. Ahuja, B. Raj, N. Bondale, X. Furtado, J. Parikh, M. N. Murty, P.V.S. Rao, Gabungan Komputer, Nasional Malaysia, Management Science, U. Bhattacharya, T. K. Das, A. Datta, S. K. Parui, B. B. Chaudhuri, Ray Chaudhuri, A.K.Mandal, K. Jithesh, K. Sulochana, Ravindra Kumar, U. Pal, T. Faruquie, C. Neti, Nitendra Rajput, L. V. Subramaniam, Ashish Verma, Niloy Mukherjee, Sheetal S. Sethi, S. R. Anjaneyulu, Rohit Sinha, S. Umesh, R. B. Thosar. 2014. Text Input into Computers. Abstract: .
- T. Virtanen, B. Raj, J. Gemmeke, H. V. hamme. 2014. Active-set newton algorithm for non-negative sparse coding of audio. Abstract: We propose a new algorithm to efficiently obtain non-negative sparse representations for audio. The spectrum of an audio signal is represented as a sparse linear combination of atoms taken from an overcomplete dictionary. The algorithm is based on minimizing the generalized Kullback-Leibler divergence between an observed magnitude spectrum and a non-negative linear combination of atoms, plus an ℓ1 regularization term. The proposed method consists of an active-set method that iteratively updates a set of active atoms that have non-zero weights, using a Newton step where the weights of the active atoms are updated. The proposed method was evaluated using mixtures of two speakers, and it was shown to yield more than 10 times faster convergence in comparison to an established algorithm based on multiplicative update rules. Moreover, the ℓ1 regularization was found to decrease the computation time and to improve the source separation performance.
- Jahn Heymann, Oliver Walter, Reinhold Häb-Umbach, B. Raj. 2014. Iterative Bayesian word segmentation for unsupervised vocabulary discovery from phoneme lattices. Abstract: In this paper we present an algorithm for the unsupervised segmentation of a lattice produced by a phoneme recognizer into words. Using a lattice rather than a single phoneme string accounts for the uncertainty of the recognizer about the true label sequence. An example application is the discovery of lexical units from the output of an error-prone phoneme recognizer in a zero-resource setting, where neither the lexicon nor the language model (LM) is known. We propose a computationally efficient iterative approach, which alternates between the following two steps: First, the most probable string is extracted from the lattice using a phoneme LM learned on the segmentation result of the previous iteration. Second, word segmentation is performed on the extracted string using a word and phoneme LM which is learned alongside the new segmentation. We present results on lattices produced by a phoneme recognizer on the WSJ-CAM0 dataset. We show that our approach delivers superior segmentation performance than an earlier approach found in the literature, in particular for higher-order language models.
- Anurag Kumar, Rita Singh, B. Raj. 2014. Detecting sound objects in audio recordings. Abstract: In this paper we explore the idea of defining sound objects and how they may be detected. We try to define sound objects and demonstrate by our experiments the existence of these objects. Most of current works on acoustic event detection focus on detecting a finite set of audio events and the detection of a generic object in sound is not done. The major reason for proposing the idea of sound objects is to work with a generic sound concept instead of working with a small set of acoustic events for detection as is the norm. Our definition tries to conform to notions present in human auditory perception. Our experimental results are promising, and show that the idea of sound objects is worth pursuing and that it could give a new direction to semi-supervised or unsupervised learning of acoustic event detection mechanisms.
- Oliver Walter, R. Haeb-Umbach, Sourish Chaudhuri, B. Raj. 2013. Unsupervised Word Discovery from Phonetic Input Using Nested Pitman-Yor Language Modeling. Abstract: In this paper we consider the unsupervised word discovery from phonetic input. We employ a word segmentation algorithm which simultaneously develops a lexicon, i.e., the transcription of a word in terms of a phone sequence, learns a n-gram language model describing word and word sequence probabilities, and carries out the segmentation itself. The underlying statistical model is that of a Pitman-Yor process, a concept known from Bayesian non-parametrics, which allows for an a priori unknown and unlimited number of different words. Using a hierarchy of Pitman-Yor processes, language models of different order can be employed and nesting it with another hierarchy of Pitman-Yor processes on the phone level allows for backing off unknown word unigrams by phone m-grams. We present results on a large-vocabulary task, assuming an error-free phone sequence is given. We finish by discussing options how to cope with noisy phone sequences.
- B. Lambert, B. Raj, Rita Singh. 2013. Discriminatively trained dependency language modeling for conversational speech recognition. Abstract: We present a discriminatively trained dependency parser-based language model. The model operates on utterances, rather than words, and so can utilize long-distance structural features of each sentence. We train the model discriminatively on n -best lists, using the perceptron algorithm to tune the model weights. Our features include standard n -gram style features, long-distance co-occurrence features, and syntactic structural features. We evaluate this model by re-ranking n best lists of recognized speech from the Fisher dataset of informal telephone conversations. We compare various combinations of feature types, and methods of training the model.
- Afsaneh Asaei, B. Raj, H. Bourlard, V. Cevher. 2013. A Multipath Sparse Beamfroming Method. Abstract: A novel formulation of beamforming is proposed for acquisition of the signals in reverberant acoustic clutter of interferences and noise. We derive the beamforming methods which incorporate the sparsity structure pertained to the acoustic source distribution and multipath propagation model. The quantitative assessments demonstrate that sparse beamforming enables effective beampattern steering from far fewer samples than the conventional beamformers. In addition, linear constraint on the desired channel rather than the desired direction improves the signal estimation performance in reverberant enclosures.
- T. Virtanen, J. Gemmeke, B. Raj. 2013. Active-Set Newton Algorithm for Overcomplete Non-Negative Representations of Audio. Abstract: This paper proposes a computationally efficient algorithm for estimating the non-negative weights of linear combinations of the atoms of large-scale audio dictionaries, so that the generalized Kullback-Leibler divergence between an audio observation and the model is minimized. This linear model has been found useful in many audio signal processing tasks, but the existing algorithms are computationally slow when a large number of atoms is used. The proposed algorithm is based on iteratively updating a set of active atoms, with the weights updated using the Newton method and the step size estimated such that the weights remain non-negative. Algorithm convergence evaluations on representing audio spectra that are mixtures of two speakers show that with all the tested dictionary sizes the proposed method reaches a much lower value of the divergence than can be obtained by conventional algorithms, and is up to 8 times faster. A source separation evaluation revealed that when using large dictionaries, the proposed method produces a better separation quality in less time.
- A. Juárez, B. Raj, Rita Singh. 2013. Semi-supervised context-aware discovery of unknown audio concepts. Abstract: Both defining new audio categories and annotating data that belong to these is a problem yet hardly tackled, much less resolved. These problems need to be solved, however, if we are to escalate the labeling of audio data from subjective manual annotation onto automatic audio discovery upon the vast amounts of audio data available today. The lack of work on this matter is understandable: audio data overlaps different semantic categories through a single channel, it is most often noisy, and any application that works on large datasets needs to deal with these problems. Additionally, relating semantic concepts to audio data is a problem in itself: how can a system associate newlyencountered acoustic data to concepts of which there is no data available? In this paper we describe how we used a labeled dataset to train reliable concept detectors for several semantic categories, how we augmented unlabeled data with contextual features through co-occurrence to and duration of known concepts, and results that indicate the feasibility of this task. We believe the design presented can serve as a general discovery framework for audio-like sequential data in general.
- José Portêlo, A. Abad, B. Raj, I. Trancoso. 2013. Secure binary embeddings of front-end factor analysis for privacy preserving speaker verification. Abstract: Remote speaker verification services typically rely on the system to have access to the users recordings, or features derived from them, and also a model of the users voice. This conventional scheme raises several privacy concerns. In this work, we address this privacy problem in the context of a speaker verification system using a factor analysis based front-end extractor, the so-called i-vectors. Speaker verification without exposing speaker data is achieved by transforming speaker i-vectors to bit strings in a way that allows the computation of approximate distances, instead of exact ones. The key to the transformation uses a hashing scheme known as Secure Binary Embeddings. Then, a modified SVM kernel permits operating on the i-vector hashes. Experiments on sub-sets of NIST SRE 2008 showed that the secure system yielded similar results as its non-private counterpart.
- Leibny Paola García-Perera, B. Raj, J. Nolazco-Flores. 2013. Optimization of the DET curve in speaker verification under noisy conditions. Abstract: The increasing need for secure authentication systems has motivated recent interest in effective algorithms for Speaker Verification (SV). In particular, there is increasing need for noise robust algorithms for SV, which will allow SV systems to operate successfully in real conditions, which are typically noisy. Speaker verification addresses a pattern classification problem, in which there is a tradeoff between false acceptance and false rejections. Traditional approaches optimize the parameters of a classifier for a single operating point emobidied by the proportions of positive and negative examples in the training data, or by learning the parameters without considering the tradeoff. In a real situation where noise is present, the operating point is effectively unknown and may not match training conditions. We believe that for such situations the optimization of the parameters should not be limited to a single operating point, and that a more robust strategy is to optimize the parameters for all operating points by minimizing the area under the detection error tradeoff curve. In this paper we investigate the minimization of the area under the detection error curve in noisy conditions. Experiments performed on the database NIST2008 show our method improves the performance with respect to conventional methods.
- Manas A. Pathak, B. Raj, S. Rane, P. Smaragdis. 2013. Privacy-preserving speech processing: cryptographic and string-matching frameworks show promise. Abstract: Speech is one of the most private forms of communication. People do not like to be eavesdropped on. They will frequently even object to being recorded; in fact, in many places it is illegal to record people speaking in public, even when it is acceptable to capture their images on video [1]. Yet, when a person uses a speech-based service such as a voice authentication system or a speech recognition service, they must grant the service complete access to their voice recordings. This exposes the user to abuse, with security, privacy and economic implications. For instance, the service could extract information such as gender, ethnicity, and even the emotional state of the user from the recording-factors not intended to be exposed by the user-and use them for undesired purposes. The recordings may be edited to create fake recordings that the user never spoke, or to impersonate them for other services. Even derivatives from the voice are risky to expose. For example, a voice-authentication service could make unauthorized use of the models or voice prints it has for users to try to identify their presence in other media such as YouTube.
- Pranay Dighe, Parul Agarwal, H. Karnick, Siddartha Thota, B. Raj. 2013. Scale independent raga identification using chromagram patterns and swara based features. Abstract: In Indian classical music a raga describes the constituent structure of notes in a musical piece. In this work, we investigate the problem of scale independent automatic raga identification by achieving state-of-the-art results using GMM based Hidden Markov Models over a collection of features consisting of chromagram patterns, mel-cepstrum coefficients and timbre features. We also perform the above task using 1) discrete HMMs and 2) classification trees over swara based features created from chromagrams using the concept of vadi of a raga. On a dataset of 4 ragas- darbari, khamaj, malhar and sohini; we have achieved an average accuracy of ~ 97%. This is a certain improvement over previous works because they use the knowledge of scale used in the raga performance. We believe that with a more careful selection of features and by fusing results from multiple classifiers we should be able to improve results further.
- Sourish Chaudhuri, B. Raj. 2013. Unsupervised hierarchical structure induction for deeper semantic analysis of audio. Abstract: Current audio analysis techniques rely on fairly shallow analysis of audio content, using symbols or patterns extracted directly from the observed acoustics. We hypothesize that the observed acoustics actually map to semantics in a hierarchical manner, and that the higher levels of this hierarchy correspond to increasingly higher-level semantics. In this paper, we present a model for deeper analysis of the observed acoustics, that induces a probabilistic tree structure depending on estimated constituent identities and contexts. Audio characterization using the deeper structure outperforms the standard shallow-feature based characterizations.
- Leibny Paola García-Perera, B. Raj, J. Nolazco-Flores. 2013. Ensemble approach in speaker verification. Abstract: The speech signal is a combination of attributes that contain information of the speaker, channel and noise. Conventional speaker verification systems train a single generic model for all cases, and handle all variations from these attributes either by factor analysis, or by not considering the variations explicitly. We propose a new methodology to partition the data space according to these factors and train separate models for each partition. The partitions may be obtained according to any attribute. We train models for the partitions discriminatively to maximize the separation between them. For classification we suggest multiple ways of combining scores from partitions. Experiments performed on the database NIST2008 show that our method improves the performance with respect to conventional methods when partitions are formed according to speakers. On noisy speech, partitions by noise result in the best performance.
- J. McDonough, K. Kumatani, T. Arakawa, Kazumasa Yamamoto, B. Raj. 2013. Speaker tracking with spherical microphone arrays. Abstract: In prior work, we investigated the application of a spherical microphone array to a distant speech recognition task. In that work, the relative positions of a fixed loud speaker and the spherical array required for beamforming were measured with an optical tracking device. In the present work, we investigate how these relative positions can be determined automatically for real, human speakers based solely on acoustic evidence. We first derive an expression for the complex pressure field of a plane wave scattering from a rigid sphere. We then use this theoretical field as the predicted observation in an extended Kalman filter whose state is the speaker's current position, the direction of arrival of the plane wave. By minimizing the squared-error between the predicted pressure field and that actually recorded, we are able to infer the position of the speaker.
- Jahn Heymann, Oliver Walter, Reinhold Häb-Umbach, B. Raj. 2013. Unsupervised word segmentation from noisy input. Abstract: In this paper we present an algorithm for the unsupervised segmentation of a character or phoneme lattice into words. Using a lattice at the input rather than a single string accounts for the uncertainty of the character/phoneme recognizer about the true label sequence. An example application is the discovery of lexical units from the output of an error-prone phoneme recognizer in a zero-resource setting, where neither the lexicon nor the language model is known. Recently a Weighted Finite State Transducer (WFST) based approach has been published which we show to suffer from an issue: language model probabilities of known words are computed incorrectly. Fixing this issue leads to greatly improved precision and recall rates, however at the cost of increased computational complexity. It is therefore practical only for single input strings. To allow for a lattice input and thus for errors in the character/phoneme recognizer, we propose a computationally efficient suboptimal two-stage approach, which is shown to significantly improve the word segmentation performance compared to the earlier WFST approach.
- Sourish Chaudhuri, Rita Singh, B. Raj. 2013. BLOCK-SPARSE BASIS SETS FOR IMPROVED AUDIO CONTENT ESTIMATION. Abstract: Unsupervised lexicon learning techniques for audio-in-the-wild typically assume that only one of the lexical units is active at any given point in time (hard quantization) or use soft counts to avoid committing to one unit (soft quantization). In reality, the audio will usually be produced as a mixture of the different audio concepts in the lexicon. In this paper, we propose a model where the audio content is assumed to be generated by a mixture of a sparse subset of the lexical units thus guiding the system toward a better estimate of presence of the concepts. We present an approach that builds on current lexicon learning frameworks, and develop a novel algorithm to estimate the contribution of different sources by imposing block-sparsity constraints on the lexicon. Our proposed framework shows significant improvement over the standard lexicon learning framework on a retrieval task for audio-in-the-wild.
- José Portêlo, B. Raj, P. Boufounos, I. Trancoso, A. Abad. 2013. Speaker verification using Secure Binary Embeddings. Abstract: This paper addresses privacy concerns in voice biometrics. Conventional remote speaker verification systems rely on the system to have access to the user's recordings, or features derived from them, and also a model of the user's voice. In the proposed approach, the system has access to none of them. The supervectors extracted from the user's recordings are transformed to bit strings in a way that allows the computation of approximate distances, instead of exact ones. The key to the transformation uses a hashing scheme known as Secure Binary Embeddings. An SVM classifier with a modified kernel operates on the hashes. This allows speaker verification to be performed without exposing speaker data. Experiments showed that the secure system yielded similar results as its non-private counterpart. The approach may be extended to other types of biometric authentication.
- Shubhranshu Barnwal, Rohit Barnwal, R. Hegde, Rita Singh, B. Raj. 2013. Doppler based speed estimation of vehicles using passive sensor. Abstract: This paper aims to develop a system for estimating a vehicle's speed by analyzing its drive by acoustics with a passive audio microphone. Analysis of the vehicle's acoustics would primarily use the phenomenon of Doppler shift, and the instant at which vehicle is at closest-point-of approach. This approach uses a technique called Seam carving to track harmonics formed by vehicle particularly its engine noise. The method proposed is computationally inexpensive and can very easily be developed into mobile application.
- Anurag Kumar, R. Hegde, Rita Singh, B. Raj. 2013. Event detection in short duration audio using Gaussian Mixture Model and Random Forest Classifier. Abstract: The amount of online multimedia files is increasing day by day with the ever increasing popularity of video sharing websites. This has led to a huge interest in content analysis of multimedia files. Audio being a major component of multimedia has the potential to help analyze different events occurring in a multimedia recording. In this paper we present an audio event detection mechanism based on Gaussian Mixture Model (GMM) and Random Forest Classifier. Experiments show that our proposed mechanism shows significant improvement in detection of specifically finer audio events in short duration recordings.
- Manas A. Pathak, B. Raj. 2013. Privacy-Preserving Speaker Verification and Identification Using Gaussian Mixture Models. Abstract: Speech being a unique characteristic of an individual is widely used in speaker verification and speaker identification tasks in applications such as authentication and surveillance respectively. In this article, we present frameworks for privacy-preserving speaker verification and speaker identification systems, where the system is able to perform the necessary operations without being able to observe the speech input provided by the user. In a speech-based authentication setting, this privacy constraint protect against an adversary who can break into the system and use the speech models to impersonate legitimate users. In surveillance applications, we require the system to first identify if the speech recording belongs to a suspect while preserving the privacy constraints. This prevents the system from listening in on conversations of innocent individuals. In this paper we formalize the privacy criteria for the speaker verification and speaker identification problems and construct Gaussian mixture model-based protocols. We also report experiments with a prototype implementation of the protocols on a standardized dataset for execution time and accuracy.
- Manas A. Pathak, B. Raj, S. Rane, P. Smaragdis. 2013. Privacy Preserving Speech Processing. Abstract: • It is generally expected that intelligent devices will respond to voice. The voice will often not be processed locally, but relegated to a remote server, as data owners may not have the resources to process their own data. This poses serious privacy risks to the user. •A person’s voice is a legally-accepted biometric, and carries information about their identity, gender, nationality, health, emotional state and a variety of other factors. •The remote voice service could potentially make undesired inferences about any of these factors, which may be unrelated to the actual service provided. •This poster discuss ”privacy preserving” computational approaches for voice processing that prevent such undesired inferences through cleverly-designed cryptographic and hashing schemes.
- Zhenzhong Lan, Lu Jiang, Shoou-I Yu, Chenqiang Gao, Shourabh Rawat, Yang Cai, Shicheng Xu, Haoquan Shen, Xuanchong Li, Yipei Wang, Waito Sze, Yan Yan, Zhigang Ma, Nicolas Ballas, Deyu Meng, Wei Tong, Yi Yang, Susanne Burger, Florian Metze, Rita Singh, B. Raj, R. Stern, T. Mitamura, Eric Nyberg, Alexander Hauptmann, A. Hauptmann. 2013. Informedia@TRECVID 2013. Abstract: In the first part of this three-part report we describe our system and novel approaches used in the TRECVID 2013 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. A separate section of the report (SIN) details methods and results for the Semantic Indexing task. The final section (SED) describes our approaches and results on the Surveillance Event Detection task.
- B. Raj, P. Boufounos, I. Trancoso, A. Abad. 2013. SPEAKER VERIFICATION USING SECURE BINARY EMBEDDINGS JosPortˆ. Abstract: This paper addresses privacy concerns in voice biometrics. Conventional remote speaker verification systems rely on the system to have access to the user’s recordings, or features derived from them, and also a model of the user’s voice. In the proposed approach, the system has access to none of them. The supervectors extracted from the user’s recordings are transformed to bit strings in a way that allows the computation of approximate distances, instead of exact ones. The key to the transformation uses a hashing scheme known as Secure Binary Embeddings. An SVM classifier with a modified kernel operates on the hashes. This allows speaker verification to be performed without exposing speaker data. Experiments showed that the secure system yielded similar results as its non-private counterpart. The approach may be extended to other types of biometric authentication.
- Parul Agarwal, H. Karnick, B. Raj. 2013. A Comparative Study Of Indian And Western Music Forms. Abstract: Music in India has very ancient roots. Indian classical music is considered to be one of the oldest musical traditions in the world but compared to Western music very little work has been done in the areas of genre recognition, classification, automatic tagging, comparative studies etc. In this work, we investigate the structural differences between Indian and Western music forms and compare the two forms of music in terms of harmony, rhythm, microtones, timbre and other spectral features. To capture the temporal and static structure of the spectrogram, we form a set of global and local frame-wise features for 5genres of each music form. We then apply Adaboost classification and GMM based Hidden Markov Models for four types of feature sets and observe that Indian Music performs better as compared to Western Music. We have achieved a best accuracy of 98.0% and 77.5% for Indian and Western musical genres respectively. Our comparative analysis indicates that features that work well with one form of music may not necessarily perform well with the other form. The results obtained on Indian Music Genres are better than the previous state-of-the-art.
- Oliver Walter, Timo Korthals, Reinhold Häb-Umbach, B. Raj. 2013. A hierarchical system for word discovery exploiting DTW-based initialization. Abstract: Discovering the linguistic structure of a language solely from spoken input asks for two steps: phonetic and lexical discovery. The first is concerned with identifying the categorical subword unit inventory and relating it to the underlying acoustics, while the second aims at discovering words as repeated patterns of subword units. The hierarchical approach presented here accounts for classification errors in the first stage by modelling the pronunciation of a word in terms of subword units probabilistically: a hidden Markov model with discrete emission probabilities, emitting the observed subword unit sequences. We describe how the system can be learned in a completely unsupervised fashion from spoken input. To improve the initialization of the training of the word pronunciations, the output of a dynamic time warping based acoustic pattern discovery system is used, as it is able to discover similar temporal sequences in the input data. This improved initialization, using only weak supervision, has led to a 40% reduction in word error rate on a digit recognition task.
- Pranay Dighe, H. Karnick, B. Raj. 2013. Swara Histogram Based Structural Analysis And Identification Of Indian Classical Ragas. Abstract: This work is an attempt towards robust automated analysis of Indian classical ragas through machine learning and signal processing tools and techniques. Indian classical music has a definite heirarchical structure where macro level concepts like thaats and raga are defined in terms of micro entities like swaras and shrutis. Swaras or notes in Indian music are defined only in terms of their relation to one another (akin to the movable do-re-mi-fa system), and an inference must be made from patterns of sounds, rather than their absolute frequency structure. We have developed methods to perform scale-independent raga identification using a random forest classifier on swara histograms and achieved state-of-the-art results for the same. The approach is robust as it directly works on partly noisy raga recordings from Youtube videos without knowledge of the scale used, whereas previous work in this direction often use audios generated in a controlled environment with the desired scale. The current work demonstrates the approach for 8 ragas namely Darbari, Khamaj, Malhar, Sohini, Bahar, Basant, Bhairavi and Yaman and we have achieved an average identification accuracy of 94.28% through the framework.
- S. Bahmani, P. Boufounos, B. Raj. 2013. Robust 1-bit Compressive Sensing via Gradient Support Pursuit. Abstract: This paper studies a formulation of 1-bit Compressed Sensing (CS) problem based on the maximum likelihood estimation framework. In order to solve the problem we apply the recently proposed Gradient Support Pursuit algorithm, with a minor modification. Assuming the proposed objective function has a Stable Restricted Hessian, the algorithm is shown to accurately solve the 1-bit CS problem. Furthermore, the algorithm is compared to the state-of-the-art 1-bit CS algorithms through numerical simulations. The results suggest that the proposed method is robust to noise and at mid to low input SNR regime it achieves the best reconstruction SNR vs. execution time trade-off.
- Leibny Paola García-Perera, J. Nolazco-Flores, B. Raj, R. Stern. 2012. Optimization of the DET curve in speaker verification. Abstract: Speaker verification systems are, in essence, statistical pattern detectors which can trade off false rejections for false acceptances. Any operating point characterized by a specific tradeoff between false rejections and false acceptances may be chosen. Training paradigms in speaker verification systems however either learn the parameters of the classifier employed without actually considering this tradeoff, or optimize the parameters for a particular operating point exemplified by the ratio of positive and negative training instances supplied. In this paper we investigate the optimization of training paradigms to explicitly consider the tradeoff between false rejections and false acceptances, by minimizing the area under the curve of the detection error tradeoff curve. To optimize the parameters, we explicitly minimize a mathematical characterization of the area under the detection error tradeoff curve, through generalized probabilistic descent. Experiments on the NIST 2008 database show that for clean signals the proposed optimization approach is at least as effective as conventional learning. On noisy data, verification performance obtained with the proposed approach is considerably better than that obtained with conventional learning methods.
- Afsaneh Asaei, B. Raj, H. Bourlard, V. Cevher. 2012. Structured sparse coding for microphone array location calibration. Abstract: We address the problem of microphone location cali- bration where the sensor positions have a sparse spatial approximation on a discretized grid. We characterize the microphone signals as a sparse vector represented over a codebook of multi-channel signals where the support of the representation encodes the microphone locations. The codebook is constructed of multi-channel signals obtained by inverse filtering the acoustic channel and projecting the signals onto a array manifold matrix of the hypothesized geometries. This framework requires that the position of a speaker or the track of its movement to be known without any further assumption about the source signal. The sparse position encoding vector is approximated by model-based sparse recovery algorithm exploiting the block-dependency structure underlying the broadband speech spectrum. The experiments conducted on real data recordings demonstrate the effectiveness of the proposed approach and the importance of the joint sparsity models in multi-channel speech processing tasks.
- José Portêlo, B. Raj, I. Trancoso. 2012. Attacking a privacy preserving music matching algorithm. Abstract: Secure multi-party computation based techniques are often used to perform audio database search tasks, such as music matching, with privacy. However, in spite of the security of individual components of the matching schemes, the overall scheme may still not be secure. This paper explains how such flaws may occur, using a privacy preserving music matching problem as a template, and provides a solution, and analyzes the resulting tradeoff between privacy and computational complexity. Although the paper focus on a music matching application, the principles can be easily adapted to perform other tasks, such as speaker verification and keyword spotting.
- G. Gweon, Mahaveer Jain, J. McDonough, B. Raj, C. Rosé. 2012. Predicting Idea Co-Construction in Speech Data using Insights from Sociolinguistics. Abstract: Automatic assessment of group processes in collaborative groups is one of the holy grails of the computer supported collaborative learning community. In this paper, we present work towards detecting one type of group process which provides an important window into the inner workings of a group, namely “idea co-construction (ICC)”. What is unique about our approach in relation to other educational data mining techniques is that we adopt insights from sociolinguistic theories by modeling stylistic convergence of speech. We present an unsupervised machine learning technique that is able to generate a predictor of the prevalence of ICC in face-to-face debates with an R 2 value of .13, p < .05.
- T. Virtanen, Rita Singh, B. Raj. 2012. Techniques for Noise Robustness in Automatic Speech Recognition. Abstract: Automatic speech recognition (ASR) systems are finding increasing use in everyday life. Many of the commonplace environments where the systems are used are noisy, for example users calling up a voice search system from a busy cafeteria or a street. This can result in degraded speech recordings and adversely affect the performance of speech recognition systems. As the use of ASR systems increases, knowledge of the state-of-the-art in techniques to deal with such problems becomes critical to system and application engineers and researchers who work with or on ASR technologies. This book presents a comprehensive survey of the state-of-the-art in techniques used to improve the robustness of speech recognition systems to these degrading external influences. Key features: Reviews all the main noise robust ASR approaches, including signal separation, voice activity detection, robust feature extraction, model compensation and adaptation, missing data techniques and recognition of reverberant speech. Acts as a timely exposition of the topic in light of more widespread use in the future of ASR technology in challenging environments. Addresses robustness issues and signal degradation which are both key requirements for practitioners of ASR. Includes contributions from top ASR researchers from leading research units in the field
- Manas A. Pathak, B. Raj. 2012. Privacy-preserving speaker verification as password matching. Abstract: We present a text-independent privacy-preserving speaker verification system that functions similar to conventional password-based authentication. Our privacy constraints require that the system does not observe the speech input provided by the user, as this can be used by an adversary to impersonate the user in the same system or elsewhere. We represent the speech input using supervectors and apply locality sensitive hashing (LSH) to transform these into bit strings, where two supervectors, and therefore inputs, are likely to be similar if they map to the same string. This transformation, therefore, reduces the problem of identifying nearest neighbors to string comparison. The users then apply a cryptographic hash function to the strings obtained from their enrollment and verification data, thereby obfuscating it from the server, who can only check if two hashed strings match without being able to reconstruct their content. We present execution time and accuracy experiments with the system on the YOHO dataset, and observe that the system achieves acceptable accuracy with minimal computational overhead needed to satisfy the privacy constraints.
- Manas A. Pathak, B. Raj. 2012. Large Margin Gaussian Mixture Models with Differential Privacy. Abstract: As increasing amounts of sensitive personal information is aggregated into data repositories, it has become important to develop mechanisms for processing the data without revealing information about individual data instances. The differential privacy model provides a framework for the development and theoretical analysis of such mechanisms. In this paper, we propose an algorithm for learning a discriminatively trained multiclass Gaussian mixture model-based classifier that preserves differential privacy using a large margin loss function with a perturbed regularization term. We present a theoretical upper bound on the excess risk of the classifier introduced by the perturbation.
- J. McDonough, K. Kumatani, B. Raj. 2012. Microphone array processing for distant speech recognition: Spherical arrays. Abstract: Distant speech recognition (DSR) holds out the promise of the most natural human computer interface because it enables man-machine interactions through speech, without the necessity of donning intrusive body- or head-mounted microphones. With the advent of the Microsoft Kinect, the application of non-uniform linear arrays to the DSR problem has become commonplace. Performance analysis of such arrays is well-represented in the literature. Recently, spherical arrays have become the subject of intense research interest in the acoustic array processing community. Such arrays have heretofore been analyzed solely with theoretical metrics under idealized conditions. In this work, we analyze such arrays under realistic conditions. Moreover, we compare a linear array with 64-channel arrays and a total length of 126 cm to a spherical array with 32 channels and a radius of 4.2 cm; we found that these provided word error rates of 9.3% and 10.2%, respectively, on a DSR task. For a speaker positioned at an oblique angle with respect to the linear array, we recorded error rates of 12.8% and 9.7%, respectively, for the linear and spherical arrays. The compact size and outstanding performance of the spherical array recommends itself well to space-limited and mobile applications such as homegaming consoles and humanoid robots.
- Shubhranshu Barnwal, Kamal Sahni, Rita Singh, B. Raj. 2012. Spectrographic seam patterns for discriminative word spotting. Abstract: This paper presents a novel method for deriving patterns for classification of speech sounds. In contrast to conventional methods that attempt to capture time-frequency patterns as represented by spectral envelopes or peaks, our method captures patterns of high-energy tracks, or seams, of maximum “whiteness” across frequency in spectrograms. Our hypothesis is that these seams could potentially carry relatively invariant signatures of underlying sounds. We present a method to derive feature vectors from seam patterns for discriminative word spotting. We show experimentally that spectrographic seam patterns are indeed distinctive for different spoken words, and are effective for word spotting.
- K. Kumatani, B. Raj, Rita Singh, J. McDonough. 2012. Microphone Array Post-filter based on Spatially-Correlated Noise Measurements for Distant Speech Recognition. Abstract: This paper presents a new microphone-array post-ﬁltering algorithm for distant speech recognition (DSR). Convention-ally, post-ﬁltering methods assume static noise ﬁeld models, and using this assumption, employ a Wiener ﬁlter mechanism for estimating the noise parameters. In contrast to this, we show how we can build the Wiener post-ﬁlter based on actual noise observations without any noise-ﬁeld assumption. The algorithm is framed within a state-of-the-art beamforming technique, namely maximum negentropy (MN) beamforming with super directivity. We investigate the effectiveness of the proposed post-ﬁlter on DSR through experiments on noisy data collected in a car under different acoustic conditions. Experiments show that the new post-ﬁltering mechanism is able to achieve up to 20% relative reduction of word error rates (WER) under the represented noise conditions, as compared to a single distant microphone. In contrast, super-directive (SD) beamforming followed by Zelinski post-ﬁltering achieves a relative WER reduction of only up to 11%. Other post-ﬁlters evaluated perform similarly in comparison to the proposed post-ﬁlter.
- Anurag Kumar, Pranay Dighe, Rita Singh, Sourish Chaudhuri, B. Raj. 2012. Audio event detection from acoustic unit occurrence patterns. Abstract: In most real-world audio recordings, we encounter several types of audio events. In this paper, we develop a technique for detecting signature audio events, that is based on identifying patterns of occurrences of automatically learned atomic units of sound, which we call Acoustic Unit Descriptors or AUDs. Experiments show that the methodology works as well for detection of individual events and their boundaries in complex recordings.
- Mahaveer Jain, J. McDonough, G. Gweon, B. Raj, C. Rosé. 2012. An Unsupervised Dynamic Bayesian Network Approach to Measuring Speech Style Accommodation. Abstract: Speech style accommodation refers to shifts in style that are used to achieve strategic goals within interactions. Models of stylistic shift that focus on specific features are limited in terms of the contexts to which they can be applied if the goal of the analysis is to model socially motivated speech style accommodation. In this paper, we present an unsupervised Dynamic Bayesian Model that allows us to model stylistic style accommodation in a way that is agnostic to which specific speech style features will shift in a way that resembles socially motivated stylistic variation. This greatly expands the applicability of the model across contexts. Our hypothesis is that stylistic shifts that occur as a result of social processes are likely to display some consistency over time, and if we leverage this insight in our model, we will achieve a model that better captures inherent structure within speech.
- K. Kumatani, T. Arakawa, Kazumasa Yamamoto, J. McDonough, B. Raj, Rita Singh, I. Tashev. 2012. Microphone array processing for distant speech recognition: Towards real-world deployment. Abstract: Distant speech recognition (DSR) holds out the promise of providing a natural human computer interface in that it enables verbal interactions with computers without the necessity of donning intrusive body- or head-mounted devices. Recognizing distant speech robustly, however, remains a challenge. This paper provides a overview of DSR systems based on microphone arrays. In particular, we present recent work on acoustic beamforming for DSR, along with experimental results verifying the effectiveness of the various algorithms described here; beginning from a word error rate (WER) of 14.3% with a single microphone of a 64-channel linear array, our state-of-the-art DSR system achieved a WER of 5.3%, which was comparable to that of 4.2% obtained with a lapel microphone. Furthermore, we report the results of speech recognition experiments on data captured with a popular device, the Kinect [1]. Even for speakers at a distance of four meters from the Kinect, our DSR system achieved acceptable recognition performance on a large vocabulary task, a WER of 24.1%, beginning from a WER of 42.5% with a single array channel.
- Sourish Chaudhuri, Rita Singh, B. Raj. 2012. Exploiting Temporal Sequence Structure for Semantic Analysis of Multimedia. Abstract: Automatic deduction of semantic labels for audiovisual data requires awareness of context, which in turn requires processing sequences of audiovisual scenes or events. The representation of such sequences is important for semantic analysis tasks. Whereas, conventionally, sequences of specific short-duration event labels, often hand-annotated for learning detectors or classifiers, have been used, we propose a new technique for audiovisual event categorization in this paper, wherein units of audio and image scenes are discovered automatically from data in a likelihood-maximization process. We show how these units for audio and video, respectively called AUDs and VIDs, can be used to learn the salient characteristics of broad-category semantic labels without requiring explicit error recovery measures. Experiments with the MED-11 dataset show that AUDs and VIDs are better able to retrieve semantic categories from mixed-content data as compared to vector quantization-based systems and systems that use library-based descriptors. Index: multimedia analysis, semantic labels, unsupervised lexicon learning, audiovisual data retrieval
- Sourish Chaudhuri, B. Raj. 2012. Unsupervised Structure Discovery for Semantic Analysis of Audio. Abstract: Approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models. We submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex. We present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics. Our model has two layers with the first layer modeling generalized sound units with no clear semantic associations, while the second layer models local patterns over these sound units. We evaluate our model on a large-scale retrieval task from TRECVID 2011, and report significant improvements over standard baselines.
- B. Raj, Kaustubh Kalgaonkar, Chris Harrison, P. Dietz. 2012. Properties and Applications of Ultrasonic Doppler Sensing in Human-Computer Interaction. Abstract: an overview of our work on ultrasonic Doppler sensing. This is a technique that captures data on the relative velocities of objects in the field of detection. We draw upon our experiences using the tech- nology to characterize several unique properties that significantly differentiate it from other sensing tech- niques, and we believe merit attention from the HCI community. These include high frame rate, low com- putational overhead, instantaneous velocity readings (instead of e.g., frame differencing), and some degree of range independence. Additionally, because it is not vision-based, the technique may sidestep privacy concerns found in many camera-driven approaches, potentially opening the door to sensing in once taboo locations, such as homes, restrooms, hospitals, and schools.
- Y. Chiu, B. Raj, R. Stern. 2012. Learning-Based Auditory Encoding for Robust Speech Recognition. Abstract: This paper describes an approach to the optimization of the nonlinear component of a physiologically motivated feature extraction system for automatic speech recognition. Most computational models of the peripheral auditory system include a sigmoidal nonlinear function that relates the log of signal intensity to output level, which we represent by a set of frequency dependent logistic functions. The parameters of these rate-level functions are estimated to maximize the a posteriori probability of the correct class in training data. The performance of this approach was verified by the results of a series of experiments conducted with the CMU S phinx-III speech recognition system on the DARPA Resource Management, Wall Street Journal databases, and on the AURORA 2 database. In general, it was shown that feature extraction that incorporates the learned rate-nonlinearity, combined with a complementary loudness compensation function, results in better recognition accuracy in the presence of background noise than traditional MFCC feature extraction without the optimized nonlinearity when the system is trained on clean speech and tested in noise. We also describe the use of lattice structure that constraints the training process, enabling training with much more complicated acoustic models.
- Sixie Yu, Zhongwen Xu, Duo Ding, Waito Sze, F. Vicente, Zhenzhong Lan, Yang Cai, Shourabh Rawat, Peter F. Schulam, N. Markandaiah, S. Bahmani, A. Juárez, Wei Tong, Yi Yang, Susanne Burger, Florian Metze, Rita Singh, B. Raj, R. Stern, T. Mitamura, Eric Nyberg, A. Hauptmann. 2012. Informedia @TRECVID 2012. Abstract: We report on our system used in the TRECVID 2012 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. For MED, it consists of three main steps: extracting features, training detectors and fusion. In the feature extraction part, we extract many low-level, high-level, and text features. Those features are then represented in three different ways which are spatial bag-of words with standard tiling, spatial bag-of-words with feature and event specific tiling and the Gaussian Mixture Model Super Vector. In the detector training and fusion, two classifiers and three fusion methods are employed. The results from both the official sources and our internal evaluations show good performance of our system. Our MER system utilizes a subset of features and detection results from the MED system from which the recounting is generated. 1. MED System 1.1 Features In order to encompass all aspects of a video, we extracted a wide variety of low-level and highlevel features. Table 1 summarizes the features used in our system. Among those features, most of them are widely used features in the community, for example, SIFT, STIP and MFCC. We extracted those features using standard code available from the authors with default parameters. Table 1: Features used for MED’12 system Visual Features Audio Features Low-level features 1. SIFT (Sande, Gevers, & Snoek, 2010) 2. Color SIFT (CSIFT) (Sande, Gevers, & Snoek, 2010) 3. Motion SIFT (MoSIFT) (Chen & Hauptmann, 2009) 4. Transformed Color Histogram (TCH) (Sande, Gevers, & Snoek, 2010) 5. STIP (Wang, Ullah, Klaser, Laptev, & Schmid, 2009) 6. Dense Trajectory (Wang, Klaser, Schmid, & Liu, 2011) 1. MFCC 2. Acoustic Unit Descriptors (AUDs) (Chaudhuri, Harvilla, & Raj, 2011) High-level features 1. Semantic Indexing Concepts (SIN) (Over, et al., 2012) 2. Object Bank (Li, Su, Xing, & Fei-Fei, 2010) 1. Acoustic Scene Analysis Text Features 1. Optical Character Recognition 1. Automatic Speech Recognition Besides those common features, we have two home-grown features which are Motion SIFT (MoSIFT) and Acoustic Unit Descriptors (AUDs). We will introduce these two features in the following subsections. 1.1 .1 Motion SIFT (MoSIFT) Feature The goal of developing the MoSIFT feature is to combine the features from the spatial domain and the temporal domain. Local spatio-temporal features around interest points provide compact and descriptive representations for video analysis and motion recognition. Current approaches tend to extend spatial descriptions by adding a temporal component to the appearance descriptor, which only implicitly captures motion information. MoSIFT detects interest points and encodes not only their local appearance but also explicitly models local motion. The idea is to detect distinctive local features through local appearance and motion. Figure 1 demonstrates the MoSIFT algorithm. Figure 1: System flow chart of the MoSIFT algorithm. The algorithm takes a pair of video frames to find spatio-temporal interest points at multiple scales. Two major computations are applied: SIFT point detection and optical flow computation according to the scale of the SIFT points. For the descriptor, MoSIFT adapts the idea of grid aggregation in SIFT to describe motions. Optical flow detects the magnitude and direction of a movement. Thus, optical flow has the same properties as appearance gradients. The same aggregation can be applied to optical flow in the neighborhood of interest points to increase robustness to occlusion and deformation. The two aggregated histograms (appearance and optical flow) are combined into the MoSIFT descriptor, which now has 256 dimensions. 1.1 .2 Acoustic Unit Descriptors (AUDs) We have developed an unsupervised lexicon learning algorithm that automatically learns units of sound. Each unit is such that it spans a set of audio frames, thereby taking local acoustic context into account. Using a maximum-likelihood estimation process, we can learn a set of such acoustic units unsupervised from audio data. Each of these units can be thought of as low-level fundamental units of sound, and each audio frame is generated by these units. We refer to these units as Acoustic Unit Descriptors (AUDs) and we expect that the distribution of these units will carry information about the semantic content of the audio stream. Each AUD is represented by a 5-state Hidden Markov Model (HMM) with a 4-gaussian mixture output density function. Ideally, with a perfect learning process, we would like to learn semantically interpretable lowerlevel units, such as a clap, a thud sound, a bang, etc. Naturally, it is hard to enforce semantic interpretability on the audio learning process at that level of detail. Further, because the space of all possible sounds is so large, many different sounds will be mapped into single sounds at learning time, since we can only learn a finite set of units. 1.2 Feature Representat ions In the previous section, we briefly describe the features we used in the system. In this section, we will describe the representations we used for the raw features extracted in Section 1. Three representations were used in our system. They were K-means based spatial bag-ofwords model with standard tiling (Lazebnik, Schmid, & Ponce, 2006), K-means based spatial bag-of-words with feature and event specific tiling (Viitaniemil & Laaksonen, 2009) and Gaussian Mixture Model Super Vector (Campbell & Sturim, 2006). Since the K-means based spatial bag-of-words model with standard tiling and Gaussian Mixture Model Super Vector are standard technology, we will focus on the K-means based spatial bag-of-words model with feature and event specific tiling. For simplicity, we will refer to it as tiling. Spatial bag-of-words model is a widely used representation of the low-level image/video features. The central idea of the spatial bag-of-words model is to divide the image into some small tiles and compute bag-of-words for each tile. Figure 2 shows a couple of tiling examples. Figure 2: Examples of tiling In general, the standard spatial bag-of-words tiling uses the 1x1, 2x2 and 4x4 tiling. However the use of those tilings is ad-hoc and some preliminary works have shown that other tilings might produce better performance (Viitaniemil & Laaksonen, 2009). In our system, we systematically tested 80 different tilings to select the best one for each feature and each event. Table 2 shows the performance of feature specific tiling v.s. the standard tiling. The scores are computed from our internal experiments and are the average over 20 MED12 pre-specified events. The PMiss @ TER=12.5 metric is an official evaluation metric specified in the MED 2012 Evaluation Plan. A smaller PMiss score signifies better performance. From the table, we can see clearly that for all of the five features, the feature specific tiling performs consistently at least 1% better than the standard tiling. Table 2: The performance of feature specific tiling and standard tiling Feature SIFT CSIFT TCH STIP MOSIFT Feature Specific Tiling 0.4209 0.4496 0.4914 0.5178 0.4330 Standard Tiling 0.4325 0.4618 0.5052 0.5234 0.4456 Figure 3 shows an example of the performance of event specific tiling v.s. standard tiling on Event 25 (marriage proposal), which is a difficult event identified in our experiments. It can be seen clearly that the event specific tiling can noticeably improve the performance over standard tiling. Figure 3: The comparison of event specific tiling and standard tiling on Event 25 1.3 Training and Fusion We used the standard MED’12 training dataset for our internal evaluation and the training of the models for our submission. For our internal evaluation, the MED’12 training dataset was further divided into the training set and testing set by randomly selecting half of the positive examples into the training set and the other half into the testing set. The negative examples consisted of only NULL videos which do not have label information. The two classifiers used in the system were kernel SVM and kernelized rigid regression. For simplicity, we will refer to it as kernel regression. For the K-means based feature representations, we used the Chi-squared kernel. For the GMM based representation RBF kernel was used. The parameters of the model were tuned by 5-fold cross validation and the PMiss @ TER = 12.5 metric was used as the evaluation metric. For combining features from multiple modalities and the outputs of different classifiers, we used fusion and ensemble methods. More specifically, for the same classifier, we used three fusion methods to fuse different features. The fusion methods were early fusion, late fusion and double fusion (Lan, Bao, Yu, Liu, & Hauptmann, 2012). In early fusion, the kernel matrices from different features were normalized first and then combined together. In late fusion, the prediction scores from the models trained using different features were combined. In our system, we also used a fusion method called double fusion, which combines early fusion and late fusion together. Finally, the results from different classifiers were ensembled together. Figure 4 shows the diagram of our system. Figure 4: The diagram of the system 0.55 0.57 0.59 0.61 0.63 0.65 0.67 0.69 0.71 0.73 0.75 CSIFT SIFT MOSIFT STIP TCH PM is s@ 12 .5 E025 Marriage_proposal baseline 1.4 Submiss ion In the following section we describe in detail the runs we submitted to NIST. Table 3 shows the official performance of each submission. 1.4 .1 Pre-Specified Submission 1.4.1.1 Submission 1: CMU_MED12_MED12TEST_PS_MEDFull_EKFull_AutoEAG_p_ensembleKRSVM_1 In this submission, using the features described in the previous section, we did the following to generate this run: 1. For each feature, train a SVM classifier and a kernel regression model. 2. Late fusion of all the results from SVM classifiers and kernel regression respectively. 3. Early fusion of all features except ASR. 4. Train a SVM classifier and a kernel
- S. Bahmani, P. Boufounos, B. Raj. 2012. Learning Model-Based Sparsity via Projected Gradient Descent. Abstract: Several convex formulation methods have been proposed previously for statistical estimation with structured sparsity as the prior. These methods often require a carefully tuned regularization parameter, often a cumbersome or heuristic exercise. Furthermore, the estimate that these methods produce might not belong to the desired sparsity model, albeit accurately approximating the true parameter. Therefore, greedy-type algorithms could often be more desirable in estimating structured-sparse parameters. So far, these greedy methods have mostly focused on linear statistical models. In this paper, we study the projected gradient descent with a non-convex structured-sparse parameter model as the constraint set. Should the cost function have a stable model-restricted Hessian, the algorithm produces an approximation for the desired minimizer. As an example, we elaborate on application of the main results to estimation in generalized linear models.
- B. Raj, Kaustubh Kalgaonkar, Chris Harrison, P. Dietz. 2012. Ultrasonic Doppler Sensing in HCI. Abstract: Several properties differentiate ultrasonic Doppler sensing from other sensing techniques-high frame rate, low computational overhead, instantaneous velocity readings, and range independence. Also, because it isn't vision-based, it might open doors to sensing in once taboo locations.
- Kamal Sahni, Pranay Dighe, Rita Singh, B. Raj. 2012. Language identification using spectro-temporal patch features. Abstract: We present a novel approach for automatic Language Identification (LID) using spectro-temporal patch features. Our approach is based on the premise that speech and spoken phenomena are characterized by typical visible patterns in timefrequency representations of the signal, and that the manner of occurrence of these patterns is language specific. To model this, we derive a randomly selected library of spectro-temporal patterns from spoken examples from a language, and derive features from the correlations of this library to spectrograms derive from the speech signal. Under our hypothesis, the relative frequency of correlation peaks must be different for different languages. We model this by learning a discriminative classifier based on these features to detect the presence of the language in a recording. The proposed approach has been tested on two different datasets: the VoxForge multilingual speech data and the LDC2005S26 corpus. Preliminary results indicate that our proposed approach can achieve an accuracy of 85-93%, and perform significantly better than a non-phonetic HMM-based classifier.
- Gang Chen, K. Kumatani, J. McDonough, B. Raj. 2011. DISTANT MULTI-SPEAKER VOICE ACTIVITY DETECTION USING RELATIVE ENERGY RATIO. Abstract: While single-speaker voice activity detection is a well-studied problem, multi-speaker voice activity detection (MSVAD) for distant speech recognition remains a challenging task. In this work, we propose a new MSVAD system for identifying voice activity of an individual speaker from multi-speaker distant speech data captured with a microphone array. In contrast to normal energy-based approaches, our MSVAD algorithm employs information from the interfering channels in a hierarchical manner in order to adaptively adjust the threshold. We demonstrate the effectiveness of our MSVAD algorithm through experiments on the Speech Separation Challenge corpus [1]. A MSVAD technique with the cross-meeting normalized energy criterion [2] provided a missed detection rate (MDR) of 7.4% with a false alarm rate (FAR) of 28.0%. By incorporating the proposed criterion in the algorithm, the MDR and FAR were further reduced to 4.3% and 19.6%, respectively. Our algorithm also achieved speech recognition performance comparable to manual segmentation results. Moreover, our method requires no parameter training and has low computational complexity.
- S. Bahmani, B. Raj, P. Boufounos. 2011. Greedy sparsity-constrained optimization. Abstract: Finding optimal sparse solutions to estimation problems, particularly in underdetermined regimes has recently gained much attention. Most existing literature study linear models in which the squared error is used as the measure of discrepancy to be minimized. However, in many applications discrepancy is measured in more general forms such as log-likelihood. Regularization by ℓ1-norm has been shown to induce sparse solutions, but their sparsity level can be merely suboptimal. In this paper we present a greedy algorithm, dubbed Gradient Support Pursuit (GraSP), for sparsity-constrained optimization. Quantifiable guarantees are provided for GraSP when cost functions have the “Stable Hessian Property”.
- P. Boufounos, P. Smaragdis, B. Raj. 2011. Joint sparsity models for wideband array processing. Abstract: Recent work has demonstrated the power of sparse models and representations in signal processing applications and has provided the community with computational tools to use it. In this paper we explore the use of sparsity in localization and beamforming when capturing multiple broadband sources using a sensor array. Specifically, we reformulate the wideband signal acquisition as a joint/group sparsity problem in a combined frequency-space domain. Under this formulation the signal is sparse in the spatial domain but has common support in all frequencies. Using techniques from the model-based compressive sensing literature we demonstrate that it is possible to robustly capture, localize and often reconstruct multiple signals present in the scene.
- S. A. Thati, B. Raj, B. Yegnanarayana, K. Prahallad. 2011. A Comparison of Prosody Modification using Instants of Significant Excitation and Mel-Cepstral Vocoder. Abstract: In this paper, we compare two methods for prosody (duration and pitch) modification. These are prosody modification using instants of Significant Excitation and Mel-Cepstral vocoder. We show that duration modifications are better using Mel- Cepstral vocoder for higher modification factor while pitch modifications are better using instants of Significant Excitations. In the end we show that Mel-Cepstral vocoder provides flexibility for non-uniform prosody manipulation. method using Instants of Significant Excitation (epochs) was proposed (3). This method operates on the linear prediction (LP) residual of signal and incorporates desired features by using the knowledge of epochs. Prosody modification in the residual domain is believed to reduce the spectral and phase distortions. Most of the above methods are non-parametric, as they rely heavily on the speech production model and there is no explicit estimation of the model parameters. Other techniques have been proposed in which the parameters of a speech production model are estimated, and explicitly used in the modification/synthesis stages. The most straightforward of such approaches was the Linear Predictive Vocoder (4), but has now been abandoned because of its inability to provide high-quality modifications. Another such approach using Mel- Cepstral Vocoder represents a more promising approach be- cause the estimated parameters are considered to be highly
- Manas A. Pathak, B. Raj. 2011. Efficient Protocols for Principal Eigenvector Computation over Private Data. Abstract: In this paper we present a protocol for computing the principal eigenvector of a collection of data matrices belonging to multiple semi-honest parties with privacy constraints. Our proposed protocol is based on secure multi-party computation with a semi-honest arbitrator who deals with data encrypted by the other parties using an additive homomorphic cryptosystem. We augment the protocol with randomization and oblivious transfer to make it difficult for any party to estimate properties of the data belonging to other parties from the intermediate steps. The previous approaches towards this problem were based on expensive QR decomposition of correlation matrices, we present an efficient algorithm using the power iteration method. We present an analysis of the correctness, security, and efficiency of protocol along with experimental results using a prototype implementation over simulated data and USPS handwritten digits dataset.
- B. Raj, Rita Singh, J. Baker. 2011. A paired test for recognizer selection with untranscribed data. Abstract: Traditionally, the use of untranscribed speech has been restricted to unsupervised or semi-supervised training of acoustic models. Comparison of recognizers has required labeled data. In this paper we show how recognizers may be rank-ordered in terms of their performance using only a large quantity of untranscribed data, given a third “reference” recognizer. We develop statistical tests for comparing recognizers in this scenario. The accuracy of the reference system need not be known. Also, while the accuracy of the reference system affects the amount of data required, with enough data it only needs to perform better than chance. We show through detailed experiments that the rank ordering predicted from untranscribed data is indeed correct.
- Sourish Chaudhuri, B. Raj, Tony Ezzat. 2011. A Paradigm for Limited Vocabulary Speech Recognition Based on Redundant Spectro-Temporal Feature Sets. Abstract: Speech recognition techniques have come to rely almost completely on HMM based frameworks. In this paper, we present a novel paradigm for small-vocabulary speech recognition based on a recently proposed word spotting technique. Recent work using discriminative classiﬁers with ordered spectro-temporal features to detect the presence of keywords obtained encouraging improvements over HMM-based models. We propose to extend this approach to recognize continuous speech in our work. Our method uses discriminative models to predict which words are present in a speech signal and hypothesize their locations. A graph search using dynamic programming is then used to obtain the most likely sequence of words from the hypothesis set produced as a result of combining the results from the discriminative word classiﬁers. While this approach doesn’t perform as well as state-of-the-art ASR systems, it can be particularly useful for languages with small amounts of annotated data available.
- K. Kumatani, J. McDonough, B. Raj. 2011. Block-wise incremental adaptation algorithm for maximum kurtosis beamforming. Abstract: In prior work, the current authors investigated beamforming algorithms that exploit the non-Gaussianity of human speech. The beamformers proposed in [1, 2, 3] are designed to maximize the kurtosis or negentropy of the subband output subject to the distortionless constraint for the direction of interest. Such techniques are able to suppress interference signals as well as reverberation effects without signal cancellation. They require, however, multiple passes of processing for each utterance in order to estimate the active weight vector. Hence, they are unsuitable for online implementation. In this work, we propose an online implementation of the maximum kurtosis beamformer. In a set of distant speech recognition experiments on far-field data, we demonstrate the effectiveness of the proposed technique. Compared to a single channel of the array, the proposed algorithm reduced word error rate from 15.4% to 6.5%.
- Manas A. Pathak, S. Rane, Wei Sun, B. Raj. 2011. Privacy preserving probabilistic inference with Hidden Markov Models. Abstract: Alice possesses a sample of private data from which she wishes to obtain some probabilistic inference. Bob possesses Hidden Markov Models (HMMs) for this purpose, but he wants the model parameters to remain private. This paper develops a framework that enables Alice and Bob to collaboratively compute the so-called forward algorithm for HMMs while satisfying their privacy constraints. This is achieved using a public-key additively homomorphic cryptosystem. Our framework is asymmetric in the sense that a larger computational overhead is incurred by Bob who has higher computational resources at his disposal, compared with Alice who has limited computing resources. Practical issues such as the encryption of probabilities and the effect of finite precision on the accuracy of probabilistic inference are considered. The protocol is implemented in software and used for secure keyword recognition.
- Sourish Chaudhuri, Mark Harvilla, B. Raj. 2011. Unsupervised Learning of Acoustic Unit Descriptors for Audio Content Representation and Classification. Abstract: In this paper, we attempt to represent audio as a sequence of acoustic units using unsupervised learning and use them for multi-class classiﬁcation. We expect the acoustic units to represent sounds or sound sequences to automatically create a sound alphabet. We use audio from multi-class Youtube-quality multimedia data to converge on a set of sound units, such that each audio ﬁle is represented as a sequence of these units. We then try to learn category language models over sequences of the acoustic units, and use them to generate acoustic and language model scores for each category. Finally, we use a margin based classiﬁcation algorithm to weight the category scores to predict the class that each test data point belongs to. We compare different settings and report encouraging results on this task.
- Manas A. Pathak, B. Raj. 2011. Privacy Preserving Speaker Verification Using Adapted GMMs. Abstract: In this paper we present an adapted UBM-GMM based privacy preserving speaker veriﬁcation (PPSV) system, where the system is not able to observe the speech data provided by the user and the user does not observe the models trained by the system. These privacy criteria are important in order to prevent an adversary having unauthorized access to the user’s client device from impersonating a user and also from another adversary who can break into the veriﬁcation system can learn about the user’s speech patterns to impersonate the user in another system. We present protocols for speaker enrollment and veriﬁcation which preserve privacy according to these requirements and report experiments with a prototype implementation on the YOHO dataset.
- José Portêlo, B. Raj, A. Abad, I. Trancoso. 2011. On the implementation of a secure musical database matching. Abstract: This paper presents an implementation of a privacy-preserving music database matching algorithm, showing how privacy is achieved at the cost of computational complexity and execution time. The paper presents not only implementation details but also an analysis of the obtained results in terms of communication between the two parties, computational complexity, execution time and correctness of the matching algorithm. Although the paper focus on a music matching application, the principles can be easily adapted to perform other tasks, such as speaker verification and keyword spotting.
- K. Kumatani, J. McDonough, J. Lehman, B. Raj. 2011. Channel selection based on multichannel cross-correlation coefficients for distant speech recognition. Abstract: In theory, beamforming performance can be improved by using as many microphones as possible, but in practice it has been shown that using all possible channels does not always improve speech recognition performance [1, 2, 3, 4, 5]. In this work, we present a new channel selection method in order to increase the computational efficiency of beamforming for distant speech recognition (DSR) without sacrficing performance.
- J. McDonough, B. Raj, K. Kumatani. 2011. On the combination of voice prompt suppression with maximum kurtosis beamforming. Abstract: In earlier work, we proposed a voice prompt suppression (VPS) algorithm based on a Kalman filter, in which the temporal update or correction step is performed in information space. The advantage of this approach is that the information matrix can be diagonally loaded in order to control the magnitude of the subband filter coefficients, which provides for better robustness. In this work, we extend that earlier work by proposing a square root implementation of the information filter VPS algorithm, as well as a technique for diagonally loading the Cholesky factor of the error covariance matrix used in this implementation. We also investigate the effectiveness of cascading VPS after maximum kurtosis beamforming, which has been shown to provide performance superior to all conventional beamforming techniques. In a set of distant speech recognition experiments we demonstrate that VPS can reduce word error rate from 19.9% to 16.1% for an adult speaker, and from 44.4% to 40.0% for a child.
- Manas A. Pathak, Mehrbod Sharifi, B. Raj. 2011. Privacy Preserving Spam Filtering. Abstract: Email is a private medium of communication, and the inherent privacy constraints form a major obstacle in developing effective spam filtering methods which require access to a large amount of email data belonging to multiple users. To mitigate this problem, we envision a privacy preserving spam filtering system, where the server is able to train and evaluate a logistic regression based spam classifier on the combined email data of all users without being able to observe any emails using primitives such as homomorphic encryption and randomization. We analyze the protocols for correctness and security, and perform experiments of a prototype system on a large scale spam filtering task. 
State of the art spam filters often use character n-grams as features which result in large sparse data representation, which is not feasible to be used directly with our training and evaluation protocols. We explore various data independent dimensionality reduction which decrease the running time of the protocol making it feasible to use in practice while achieving high accuracy.
- Sourish Chaudhuri, B. Raj. 2011. Learning contextual relevance of audio segments using discriminative models over AUD sequences. Abstract: Effective retrieval of multimodal data involves performing accurate segmentation and analysis of such data. With easy access to a number of audio and video sharing platforms online, user-generated content with considerably less than ideal recording conditions has increased rapidly. One major issue with such content is the presence of semantically irrelevant segments in such recordings. This leads to the presence of considerable contextual noise in such recordings that makes analysis difficult. In this paper, we present a discriminative large-margin based approach that uses annotated data to understand which parts of the audio are relevant (while noting that the notion of relevance could be extremely subjective and potentially challenging to define), and can automatically extract such segments from new audio.
- Kshitiz Kumar, Rita Singh, B. Raj, R. Stern. 2011. Gammatone sub-band magnitude-domain dereverberation for ASR. Abstract: We present an algorithm for dereverberation of speech signals for automatic speech recognition (ASR) applications. Often ASR systems are presented with speech that has been recorded in environments that include noise and reverberation. The performance of ASR systems degrades with increasing levels of noise and reverberation. While many algorithms have been proposed for robust ASR in noisy environments, reverberation is still a challenging problem. In this paper, we present 1 an approach for dereverberation that models reverberation as a convolution operation in the speech spectral domain. Using a least-squares error criterion we decompose reverberated spectra into clean spectra convolved with a filter. We incorporate non-negativity and sparsity of the speech spectra as constraints within a non-negative matrix factorization (NMF) framework to achieve the decomposition. In ASR experiments where the system is trained with unreverberated and reverberated speech, we show that the proposed approach can provide upto 40% and 19% relative reduction respectively in performance.
- Kshitiz Kumar, B. Raj, Rita Singh, R. Stern. 2011. An iterative least-squares technique for dereverberation. Abstract: Some recent dereverberation approaches that have been effective for automatic speech recognition (ASR) applications, model reverberation as a linear convolution operation in the spectral domain, and derive a factorization to decompose spectra of reverberated speech in to those of clean speech and room-response filter. Typically, a general non-negative matrix factorization (NMF) framework is employed for this. In this work1 we present an alternative to NMF and propose an iterative least-squares deconvolution technique for spectral factorization. We propose an efficient algorithm for this and experimentally demonstrate it's effectiveness in improving ASR performance. The new method results in 40–50% relative reduction in word error rates over standard baselines on artificially reverberated speech.
- G. Gweon, Pulkit Agrawal, Mikesh Udani, B. Raj, C. Rosé. 2011. The automatic assessment of knowledge integration processes in project teams. Abstract: : Automatic assessment of group processes in collaborative groups is one of the holy grails of the computer supported collaborative learning community. The conversation in collaborative work provides an important window into the inner workings of a group. In this paper we present work towards detecting where students are displaying “reasoning” in conversational speech and how others are building upon those expressions of reasoning (“idea co-construction (ICC)”). Such technology would add to the body of work in educational data mining another means of monitoring student work as well as contributing to the area of automatic collaborative process analysis. We begin by discussing our operationalization of targeted group processes, namely reasoning and ICC. We then discuss the level of success we are able to achieve applying machine learning technology to replicate this human analysis using simple audio signal processing techniques.
- B. Raj, Rita Singh, T. Virtanen. 2011. Phoneme-Dependent NMF for Speech Enhancement in Monaural Mixtures. Abstract: The problem of separating speech signals out of monaural mixtures (with other non-speech or speech signals) has become increasingly popular in recent times. Among the various solutions proposed, the most popular methods are based on compositional models such as non-negative matrix factorization (NMF) and latent variable models. Although these techniques are highly effective they largely ignore the inherently phonetic nature of speech. In this paper we present a phoneme-dependent NMFbased algorithm to separate speech from monaural mixtures. Experiments performed on speech mixed with music indicate that the proposed algorithm can result in significant improvement in separation performance, over conventional NMF-based separation.
- K. Kumatani, J. McDonough, B. Raj. 2011. MAXIMUM KURTOSIS BEAMFORMING. Abstract: In prior work, the current authors investigated the use of optimization criteria for beamforming that exploit the non-Gaussianity of human speech. In particular, we examined beamforming algorithms designed to maximize the kurtosis or negentropy of the subband output of a generalized sidelobe canceller. These techniques, while effective, require making multiple passes through the data, and hence are unsuitable for online implementation. Thus, in this work, we propose an online implementation of the maximum kurtosis beamformer. In a set of distant speech recognition experiments, we compare the effectiveness of the proposed technique to several common beamformer designs. Compared to a single channel of the array, the proposed algorithm reduced word error rate from 24.0% to 10.3%, which is the best performance yet achieved on this task.
- K. Kumatani, J. McDonough, B. Raj. 2011. Maximum kurtosis beamforming with a subspace filter for distant speech recognition. Abstract: This paper presents a new beamforming method for distant speech recognition (DSR). The dominant mode subspace is considered in order to efficiently estimate the active weight vectors for maximum kurtosis (MK) beamforming with the generalized sidelobe canceler (GSC). We demonstrated in [1], [2], [3] that the beamforming method based on the maximum kurtosis criterion can remove reverberant and noise effects without signal cancellation encountered in the conventional beamforming algorithms. The MK beamforming algorithm, however, required a relatively large amount of data for reliably estimating the active weight vector because it relies on a numerical optimization algorithm. In order to achieve efficient estimation, we propose to cascade the subspace (eigenspace) filter [4, §6.8] with the active weight vector. The subspace filter can decompose the output of the blocking matrix into directional signals and ambient noise components. Then, the ambient noise components are averaged and would be subtracted from the beamformer's output, which leads to reliable estimation as well as significant computational reduction. We show the effectiveness of our method through a set of distant speech recognition experiments on real microphone array data captured in the real environment. Our new beamforming algorithm provided the best recognition performance among conventional beamforming techniques, a word error rate (WER) of 5.3 %, which is comparable to the WER of 4.2 % obtained with a close-talking microphone. Moreover, it achieved better recognition performance with a fewer amounts of adaptation data than the conventional MK beamformer.
- J. McDonough, Wei Chu, K. Kumatani, B. Raj, J. Lehman. 2011. An information filter for voice prompt suppression. Abstract: Modern speech enabled applications provide for dialog between a machine and one or more human users. The machine prompts the user with queries that are either prerecorded or synthesized on the fly. The human users respond with their own voices, and their speech is then recognized and understood by a human language understanding module. In order to achieve as natural an interaction as possible, the human user(s) must be allowed to interrupt the machine during a voice prompt. In this work, we compare two techniques for such voice prompt suppression. The first is a straightforward adaptation of a conventional Kalman filter, which has certain advantages over the normalized least squares algrithm in terms of robustness and speed of convergence. The second algorithm, which is novel in this work, is also based on a Kalman filter, but differs from the first in that the update or correction step is performed in information space and hence allows for the use of diagonal loading in order to control the growth of the subband filter coefficients, and thereby add robustness to the VPS.
- B. Lambert, Rita Singh, B. Raj. 2010. Creating a linguistic plausibility dataset with non-expert annotators. Abstract: We describe the creation of a linguistic plausibility dataset that contains annotated examples of language judged to be linguistically plausible, implausible, and every-thing in between. To create the dataset we randomly generate sentences and have them annotated by crowd sourcing over the Amazon Mechanical Turk. Obtaining inter-annotator agreement is a difﬁcult problem because linguistic plausibility is highly subjective. The annotations obtained depend, among other factors, on the manner in which annotators are questioned about the plausibility of sentences. We describe our experiments on posing a number of different questions to the annotators, in order to elicit the responses with greatest agreement, and present several methods for analyzing the resulting responses. The generated dataset and annotations are being made available to public.
- S. Srinivasan, B. Raj, Tony Ezzat. 2010. Ultrasonic sensing for robust speech recognition. Abstract: In this paper, we present our work using ultrasonic sensing of speech for digit recognition. First, a set of spectral ultrasonic features are developed and tuned in order to achieve optimal performance for the digit recognition task. Using these features, we demonstrate an overall accuracy of 33.00% on a digit recognition task using HMMs with recordings from 6 speakers. The results indicate that ultrasonic sensing of speech is viable, but that further work is needed to achieve word accuracies that match those of audio. Finally, experimental results are presented which demonstrate that fusing information from ultrasound and audio sources show marginal improvements over audio-only performances.
- Rita Singh, B. Raj, P. Smaragdis. 2010. Latent-variable decomposition based dereverberation of monaural and multi-channel signals. Abstract: We present an algorithm to dereverberate single- and multi-channel audio recordings. The proposed algorithm models the magnitude spectrograms of clean audio signals as histograms drawn from a multinomial process. Spectrograms of reverberated signals are obtained as histograms of draws from the PDF of the sum of two random variables, one representing the spectrogram of clean speech and the second the frequency decomposition of the room response. The spectrogram of the clean signal is computed as a maximum-likelihood estimate from the spectrogram of reverberant speech using an EM algorithm. Experimental evaluations show that the proposed algorithm is able to greatly reduce the reverberation effects in even highly reverberant signals captured in auditoria and other open spaces.
- Manas A. Pathak, S. Rane, B. Raj. 2010. Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers. Abstract: As increasing amounts of sensitive personal information finds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classifier using classifiers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classifier. We also present a detailed theoretical analysis containing a proof of differential privacy of the perturbed aggregate classifier and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset.
- G. Gweon, Pulkit Agrawal, Nitish Srivastava, Shantanu Agrawal, Abhilash Jindal, Marietta Sionti, B. Raj, C. Rosé. 2010. Automatic assessment of student “reasoning” processes in face-to-face interactions using speech data. Abstract: Assessment of student reasoning processes is one of the holy grails of intelligent tutoring. One way in which students display their reasoning is through conversation. In this paper we present work towards detecting where students are displaying “reasoning” in conversational speech. Such technology would add to the body of work in educational data mining another means of monitoring student work. Because the concept of reasoning is somewhat abstract, we first discuss how we have operationalized it, achieving an agreement of 0.67 kappa between human raters. We then discuss how we have used machine learning technology to predict whether a given speech segment contains a reasoning statement using features that can be extracted automatically using simple audio signal processing techniques. The result is promising with an f-score of 0.63.
- Arthur R. Toth, Kaustubh Kalgaonkar, B. Raj, Tony Ezzat. 2010. Synthesizing speech from Doppler signals. Abstract: It has long been considered a desirable goal to be able to construct an intelligible speech signal merely by observing the talker in the act of speaking. Past methods at performing this have been based on camera-based observations of the talker's face, combined with statistical methods that infer the speech signal from the facial motion captured by the camera. Other methods have included synthesis of speech from measurements taken by electro-myelo graphs and other devices that are tethered to the talker - an undesirable setup. In this paper we present a new device for synthesizing speech from characterizations of facial motion associated with speech - a Doppler sonar. Facial movement is characterized through Doppler frequency shifts in a tone that is incident on the talker's face. These frequency shifts are used to infer the underlying speech signal. The setup is farfield and untethered, with the sonar acting from the distance of a regular desktop microphone. Preliminary experimental evaluations show that the mechanism is very promising - we are able to synthesize reasonable speech signals, comparable to those obtained from tethered devices such as EMGs.
- Ziad Al Bawab, B. Raj, R. Stern. 2010. A hybrid physical and statistical dynamic articulatory framework incorporating analysis-by-synthesis for improved phone classification. Abstract: In this paper, we present a dynamic articulatory model for phone classification. The model integrates real articulatory information derived from ElectroMagnetic Articulograph (EMA) data into its inner states. It maps from the articulatory space to the acoustic one using an adapted vocal tract model for each speaker and a physiologically-motivated articulatory synthesis approach. We apply the analysis-by-synthesis paradigm in a statistical fashion. We first present a fast approach for deriving analysis-by-synthesis distortion features. Next, the distortion between the speech synthesized from the articulatory states and the incoming speech signal is used to compute the output observation probabilities of the Hidden Markov Model (HMM) used for classification. Experiments with the novel framework show improvements over baseline in phone classification accuracy.
- K. Wilson, B. Raj. 2010. Spectrogram dimensionality reductionwith independence constraints. Abstract: We present an algorithm to find a low-dimensional decomposition of a spectrogram by formulating this as a regularized non-negative matrix factorization (NMF) problem with a regularization term chosen to encourage independence. This algorithm provides a better decomposition than standard NMF when the underlying sources are independent. It is directly applicable to non-square matrices, and it makes better use of additional observation streams than previous nonnegative ICA algorithms.
- B. Raj, K. Wilson, A. Krueger, Reinhold Häb-Umbach. 2010. Ungrounded independent non-negative factor analysis. Abstract: We describe an algorithm that performs regularized non-negative matrix factorization (NMF) to find independent components in non-negative data. Previous techniques proposed for this purpose require the data to be grounded, with support that goes down to 0 along each dimension. In our work, this requirement is eliminated. Based on it, we present a technique to find a low-dimensional decomposition of spectrograms by casting it as a problem of discovering independent non-negative components from it. The algorithm itself is implemented as regularized non-negative matrix factorization (NMF). Unlike other ICA algorithms, this algorithm computes the mixing matrix rather than an unmixing matrix. This algorithm provides a better decomposition than standard NMF when the underlying sources are independent. It makes better use of additional observation streams than previous non-negative ICA algorithms.
- B. Raj, P. Smaragdis, M. Slaney, Chung-Hsien Wu, Liming Chen, Hyoung‐Gook Kim. 2010. Scalable Audio-Content Analysis. Abstract: 1Carnegie Mellon University, PA 15213, USA 2Advanced Technology Laboratories, Adobe Systems Inc., Newton, MA 02466, USA 3Yahoo! Research, Santa Clara, CA 95054, USA 4Center for Computer Research in Music and Acoustics (CCRMA), Stanford University, CA 94305-8180, USA 5Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan 6Department of Mathematics and Informatics, Ecole Centrale de Lyon, University of Lyon, 69006 Lyon, France 7 Intelligent Multimedia Signal Processing Laboratory, Kwangwoon University, Seoul 139-701, Republic of Korea
- B. Raj, T. Virtanen, Sourish Chaudhuri, Rita Singh. 2010. Non-negative matrix factorization based compensation of music for automatic speech recognition. Abstract: This paper proposes to use non-negative matrix factorization based speech enhancement in robust automatic recognition of mixtures of speech and music. We represent magnitude spectra of noisy speech signals as the non-negative weighted linear combination of speech and noise spectral basis vectors, that are obtained from training corpora of speech and music. We use overcomplete dictionaries consisting of random exemplars of the training data. The method is tested on theWall Street Journal large vocabulary speech corpus which is artificially corrupted with polyphonic music from the RWC music database. Various music styles and speech-tomusic ratios are evaluated. The proposed methods are shown to produce a consistent, significant improvement on the recognition performance in the comparison with the baseline method. Audio demonstrations of the enhanced signals are available at http://www.cs.tut.fi/ tuomasv.
- Rita Singh, B. Lambert, B. Raj. 2010. The use of sense in unsupervised training of acoustic models for ASR systems. Abstract: In unsupervised training of ASR systems, no annotated data are assumed to exist. Word-level annotations for training audio are generated iteratively using an ASR system. At each iteration a subset of data judged as having the most reliable transcriptions is selected to train the next set of acoustic models. Data selection however remains a difficult problem, particularly when the error rate of the recognizer providing the initial annotation is very high. In this paper we propose an iterative algorithm that uses a combination of likelihoods and a simple model of sense to select data. We show that the algorithm is effective for unsupervised training of acoustic models, particularly when the initial annotation is highly erroneous. Experiments conducted on Fisher-1 data using initial models from Switchboard, and a vocabulary and LM derived from the Google N-grams, show that performance on a selected held-out test set from Fisher data improves more with iterations relative to likelihood-based data selection.
- Lu Jiang, Zhongwen Xu, Zhenzhong Lan, Shicheng Xu, Xiaojun Chang, Xuanchong Li, Zexi, Mao, Chuang Gan, Yajie Miao, Xingzhong Du, Yang Cai, Lara Martin, Nikolas Wolfe, Anurag Kumar, Huan Li, Ming Lin, Yezhou Yang, Deyu Meng, S. Shan, P. D. Sahin, Susanne, Burger, Florian Metze, Rita Singh, B. Raj, T. Mitamura, R. Stern, Alexander, Hauptmann, Pinar Duygulu-Sahin, Alexander Hauptmann, Yicheng Zhao. 2010. MMM-TJU at TRECVID 2010. Abstract: Surveillance Event Detection Semantic event detection in the huge amount of surveillance video in both retrospective and real-time styles is essential to a variety of higher-level applications in the public security. In TRECVID 2010, to overcome the limitations of the traditional human action analysis method with human detection/tracking and domain knowledge, we evaluate the general framework for multiple human behaviors modeling with the philosophy of bag of spatiotemporal feature (BoSTF). The brief
- Y. Chiu, B. Raj, R. Stern. 2010. Learning-based auditory encoding for robust speech recognition. Abstract: This paper describes ways of speeding up the optimization process for learning physiologically-motivated components of a feature computation module directly from data. During training, word lattices generated by the speech decoder and conjugate gradient descent were included to train the parameters of logistic functions in a fashion that maximizes the a posteriori probability of the correct class in the training data. These functions represent the rate-level nonlinearities found in most mammalian auditory systems. Experiments conducted using the CMU SPHINX-III system on the DARPA Resource Management and Wall Street Journal tasks show that the use of discriminative training to estimate the shape of the rate-level nonlinearity provides better recognition accuracy in the presence of background noise than traditional procedures which do not employ learning. More importantly, the inclusion of conjugate gradient descent optimization and a word lattice to reduce the number of hypotheses considered greatly increases the training speed, which makes training with much more complicated models possible.
- Arthur R. Toth, Michael Wand, S. Jou, Tanja Schultz, B. Raj, Kaustubh Kalgaonkar, Tony Ezzat. 2010. Synthesizing speech from surface electromyography and acoustic Doppler sonar.. Abstract: Numerous techniques have been devised to process speech audio in noise, but automatic speech recognition is difficult when the noise is too great. An alternative approach is to collect data that represent the speech production process but is less affected by noise in the speech audio range. Two such types of data come from surface electromyography (EMG) and acoustic Doppler sonar (ADS). EMG records muscle activation potentials. ADS records reflected ultrasound tones. Both can be used to measure facial movements related to speech, but they present their own challenges for automatic speech recognition. This work investigates the alternative approach of using these data sources for speech synthesis. The synthesis techniques explored in this work are based on Gaussian mixture model mapping techniques, which are commonly used for voice transformation. Voice transformation is traditionally concerned with changing the identity of speech audio signals, but others have demonstrated that such techniques can be used...
- Y. Chiu, B. Raj, R. Stern. 2009. Towards fusion of feature extraction and acoustic model training: a top down process for robust speech recognition. Abstract: This paper presents a strategy to learn physiologicallymotivated components in a feature computation module discriminatively, directly from data, in a manner that is inspired by the presence of efferent processes in the human auditory system. In our model a set of logistic functions which represent the rate-level nonlinearities found in most mammal hearing system are put in as part of the feature extraction process. The parameters of these rate-level functions are estimated to maximize the a posteriori probability of the correct class in the training data. The estimated feature computation is observed to be robust against environmental noise. Experiments conducted with the CMU Sphinx-III on the DARPA Resource Management task show that the discriminatively estimated rate-nonlinearity results in better performance in the presence of background noise than traditional procedures which separate the feature extraction and model training into two distinct parts without feed back from the latter to the former.
- P. Smaragdis, B. Raj, M. Shashanka. 2009. Missing data imputation for spectral audio signals. Abstract: With the recent attention to audio processing in the time -frequency domain we increasingly encounter the problem of missing data. In this paper we present an approach that allows for imputing missing values in the time-frequency domain of audio signals. The presented approach is able to deal with real-world polyphonic signals by performing imputation even in the presence of complex mixtures. We show that this approach outperforms generic imputation approaches, and we present a variety of situations that highlight its utility.
- Kaustubh Kalgaonkar, B. Raj. 2009. One-handed gesture recognition using ultrasonic Doppler sonar. Abstract: This paper presents a new device based on ultrasonic sensors to recognize one-handed gestures. The device uses three ultrasonic receivers and a single transmitter. Gestures are characterized through the Doppler frequency shifts they generate in reflections of an ultrasonic tone emitted by the transmitter. We show that this setup can be used to classify simple one-handed gestures with high accuracy. The ultrasonic doppler based device is very inexpensive - $20 USD for the whole setup including the acquisition system, and computationally efficient as compared to most traditional devices (e.g. video). These gestures, could potentially be used to control and drive a device.
- Dhananjay Bansal, N. Nair, Rita Singh, B. Raj. 2009. A joint decoding algorithm for multiple-example-based addition of words to a pronunciation lexicon. Abstract: We propose an algorithm that enables joint Viterbi decoding of multiple independent audio recordings of a word to derive its pronunciation. Experiments show that this method results in better pronunciation estimation and word recognition accuracy than that obtained either with a single example of the word or using conventional approaches to pronunciation estimation using multiple examples.
- M. Shashanka, B. Raj, P. Smaragdis, P. I. 2009. Probabilistic Latent Variable Model for Sparse Decompositions of Non-negative Data. Abstract: An important problem in data-analysis tasks is to find suitable representations that make hidden structure inthe data explicit. In this paper, we present a probabilistic latent variable model that is equivalent to a matrix decomposition of no nnegative data. Data is modeled as histograms of multiple dra ws from an underlying generative process. The model expresses the generative distribution as a mixture of hidden distributions which capture the latent structure. We extend the model to incorporate sparsity constraints by using an entropic prior. We derive algorithms for parameter estimation and show how the model can be applied for unsupervised feature extraction an d supervised classification tasks.
- B. Raj, R. Stern. 2009. Missing-Feature Approaches in Speech Recognition [ Improving recognition accuracy in noise by using partial spectrographic information ]. Abstract: D espite decades of focused research on the problem, the accuracy of automatic speech recognition (ASR) systems is still adversely affected by noise and other sources of acoustical variability. For example, there are presently dozens, if not hundreds, of algorithms that have been developed to cope with the effects of quasistationary additive noise and linear filtering of the channel [16], [23]. While these approaches are reasonably effective in the context of their intended purposes, they are generally ineffective in improving recognition accuracy in many more difficult environments. Some of these more challenging conditions, which are frequently encountered in some of the most important potential speech recognition applications today, include speech in the presence of transient and nonstationary disturbances (as in many factory, military, and telematic environments), speech in the presence of background music or background speech (as in the automatic transcription of broadcast program material as well as in many natural environments), and speech at very low signal-to-noise ratios (SNRs). Conventional environmental compensation provides only limited benefit for these problems even today. For example, Raj et al. [29] showed that while codeword-dependent cepstral normalization is highly effective in dramatically reducing the impact of additive broadband noise on speech recognition accuracy, it is relatively ineffective when speech is presented to the system in the presence of background music, for reasons that are believed to be a consequence of the nonstationary nature of the acoustical degradation. [Bhiksha Raj and Richard M. Stern]
- Ziad Al Bawab, L. Turicchia, R. Stern, B. Raj. 2009. Deriving vocal tract shapes from electromagnetic articulograph data via geometric adaptation and matching. Abstract: In this paper, we present our efforts towards deriving vocal tract shapes from ElectroMagnetic Articulograph data (EMA) via geometric adaptation and matching. We describe a novel approach for adapting Maeda’s geometric model of the vocal tract to one speaker in the MOCHA database. We show how we can rely solely on the EMA data for adaptation. We present our search technique for the vocal tract shapes that best fit the given EMA data. We then describe our approach of synthesizing speech from these shapes. Results on Mel-cepstral distortion reflect improvement in synthesis over the approach we used before without adaptation.
- P. Smaragdis, M. Shashanka, B. Raj. 2009. A Sparse Non-Parametric Approach for Single Channel Separation of Known Sounds. Abstract: In this paper we present an algorithm for separating mixed sounds from a monophonic recording. Our approach makes use of training data which allows us to learn representations of the types of sounds that compose the mixture. In contrast to popular methods that attempt to extract compact generalizable models for each sound from training data, we employ the training data itself as a representation of the sources in the mixture. We show that mixtures of known sounds can be described as sparse combinations of the training data itself, and in doing so produce significantly better separation results as compared to similar systems based on compact statistical models.
- Chanwoo Kim, Kshitiz Kumar, B. Raj, R. Stern. 2009. Signal separation for robust speech recognition based on phase difference information obtained in the frequency domain. Abstract: In this paper, we present a new two-microphone approach that improves speech recognition accuracy when speech is masked by other speech. The algorithm improves on previous systems that have been successful in separating signals based on differences in arrival time of signal components from two microphones. The present algorithm differs from these efforts in that the signal selection takes place in the frequency domain. We observe that additional smoothing of the phase estimates over time and frequency is needed to support adequate speech recognition performance. We demonstrate that the algorithm described in this paper provides better recognition accuracy than timedomain-based signal separation algorithms, and at less than 10 percent of the computation cost.
- K. Wilson, B. Raj, P. Smaragdis. 2008. Factorization With Temporal Dependencies for Speech Denoising. Abstract: We present a technique for denoising speech using temporally regularized nonnegative matrix factorization (NMF). In previous work [1], we used a regularized NMF update to impose structure within each audio frame. In this paper, we add frame-to-frame regularization across time and show that this additional regularization can also improve our speech denoising results. We evaluate our algorithm on a range of nonstationary noise types and outperform a state-of-the-art Wiener filter implementation. Interspeech
- K. Wilson, B. Raj, P. Smaragdis, Ajay Divakaran. 2008. Speech denoising using nonnegative matrix factorization with priors. Abstract: We present a technique for denoising speech using nonnegative matrix factorization (NMF) in combination with statistical speech and noise models. We compare our new technique to standard NMF and to a state-of-the-art Wiener filter implementation and show improvements in speech quality across a range of interfering noise types.
- Kaustubh Kalgaonkar, B. Raj. 2008. Recognizing talking faces from acoustic Doppler reflections. Abstract: Face recognition algorithms typically deal with the classification of static images of faces that are obtained using a camera. In this paper we propose a new sensing mechanism based on the Doppler effect to capture the patterns of motion of talking faces. We incident an ultrasonic tone on subjects' faces and capture the reflected signal. When the subject talks, different parts of their face move with different velocities in a characteristic manner. Each of these velocities imparts a different Doppler shift to the reflected ultrasonic signal. Thus, the set of frequencies in the reflected ultrasonic signal is characteristic of the subject. We show that even using a simple feature computation scheme to characterize the spectrum of the reflected signal, and a simple GMM based Bayesian classifier, we are able to recognize talkers with an accuracy of over 90%. Interestingly, we are also able to identify the gender of the talker with an accuracy of over 90%.
- Kaustubh Kalgaonkar, B. Raj. 2008. Ultrasonic Doppler sensor for speaker recognition. Abstract: In this paper we present a novel use of an acoustic Doppler sonar for multi-modal speaker identification. An ultrasonic emitter directs a 40 kHz tone toward the speaker. Reflections from the speaker's face are recorded as the speaker talks. The frequency of the tone is modified by the velocity of the facial structures it is reflected by. The received ultrasonic signal thus contains an entire spectrum of frequencies representing the set of all velocities of facial components. The pattern of frequencies in the reflected signal is observed to be typical of the speaker. The captured ultrasonic signal is synchronously analyzed with the corresponding voice signal to extract specific characteristics that can be used to identify the speaker. Experiments show that the information this can result in significant improvements in speaker identification accuracy both under clean conditions and in noise.
- K. Wilson, B. Raj, P. Smaragdis. 2008. Regularized non-negative matrix factorization with temporal dependencies for speech denoising. Abstract: We present a technique for denoising speech using temporally regularized nonnegative matrix factorization (NMF). In previous work [1], we used a regularized NMF update to impose structure within each audio frame. In this paper, we add frame-to-frame regularization across time and show that this additional regularization can also improve our speech denoising results. We evaluate our algorithm on a range of nonstationary noise types and outperform a state-of-the-art Wiener filter implementation. Interspeech 2008
- M. Shashanka, B. Raj, P. Smaragdis. 2008. Probabilistic Latent Variable Models as Nonnegative Factorizations. Abstract: This paper presents a family of probabilistic latent variable models that can be used for analysis of nonnegative data. We show that there are strong ties between nonnegative matrix factorization and this family, and provide some straightforward extensions which can help in dealing with shift invariances, higher-order decompositions and sparsity constraints. We argue through these extensions that the use of this approach allows for rapid development of complex statistical models for analyzing nonnegative data.
- B. Schmidt-Nielsen, B. Harsham, B. Raj, C. Forlines. 2008. Speech-Based UI Design for the Automobile. Abstract: In this chapter we discuss a variety of topics relating to speech-based user interfaces for use in an automotive environment. We begin by presenting a number of design principles for the design of such interfaces, derived from several decades of combined experience in the development and evaluation of spoken user interfaces (UI) for automobiles, along with three case studies of current automotive navigation interfaces. Finally, we present a new model for speech-based user interfaces in automotive environments that recasts the goal of the UI from supporting the navigation among and selection from multiple states to that of selecting the desired command from a short list. We also present experimental evidence that UIs based on this approach can impose significantly lower cognitive load on a driver than conventional UIs.
- P. Smaragdis, B. Raj. 2008. Inferring missing spectral data.. Abstract: In this talk we will present a methodology that allows us to infer missing portions of spectrograms. We will present an approach that constructs models of sounds by observing either examples or the existing portions of a spectrogram with missing data. Once the model is learned we can use it to reconstruct missing areas of a spectrogram with inaudible artifacts. This process is very useful when trying to correct errors from spectral editing or when dealing with processes that corrupt a signal in the time/frequency domain. We will show how this approach is appropriate for polyphonic audio signals and that it significantly outperforms approaches using generic statistical models which are ill suited for spectral data.
- Ziad Al Bawab, B. Raj, R. Stern. 2008. Analysis-by-synthesis features for speech recognition. Abstract: We present a framework for speech recognition that accounts for hidden articulatory information. We model the articulatory space using a codebook of articulatory configurations geometrically derived from EMA measurements available in the MOCHA database. The articulatory parameter set we derive is in the form of Maeda parameters. In turn, these parameters are used in a physiologically- motivated articulatory speech synthesizer based on the model by Sondhi and Schroeter. We use the distortion between the speech synthesized from each of the articulatory configurations and the original speech as features for recognition. We setup a segmented phoneme recognition task on the MOCHA database using Gaussian mixture models (GMMs). Improvements are achieved when combining the probability scores generated using the distortion features with the scores using acoustic features.
- P. Smaragdis, B. Raj, M. Shashanka. 2008. Sparse and shift-invariant feature extraction from non-negative data. Abstract: In this paper we describe a technique that allows the extraction of multiple local shift-invariant features from analysis of non-negative data of arbitrary dimensionality. Our approach employs a probabilistic latent variable model with sparsity constraints. We demonstrate its utility by performing feature extraction in a variety of domains ranging from audio to images and video.
- Rita Singh, B. Raj. 2008. Discovery of temporal patterns in continuous nonrandom sound sequences.. Abstract: The problem addressed is that of automatically determining the minimal structures in structured sounds such as human speech. It is well established that human speech comprises consistent sound units (such as phonemes), a fact that is exploited by speech recognition systems, which only model the units, characterizing all speech as sequences of units. Currently, the units are typically manually defined, and their statistical models are assigned structures based on subjective judgments. In this work it is attempted to identify these units automatically through an analysis of data. The problem is treated as one of entropy minimization. The minimum entropy estimation principle assumes a structured universe and dictates the estimation of the most predictable model that the observations used to train the model will allow. For the current problem, this amounts to identifying a set of units such that every word can be represented by a minimum‐perplexity network over them. Experiments demonstrate that this procedur...
- Kaustubh Kalgaonkar, B. Raj. 2007. Acoustic Doppler sonar for gait recogination. Abstract: A person's gait is a characteristic that might be employed to identify him/her automatically. Conventionally, automatic for gait-based identification of subjects employ video and image processing to characterize gait. In this paper we present an Acoustic Doppler Sensor(ADS) based technique for the characterization of gait. The ADS is very inexpensive sensor that can be built using off-the-shelf components, for under $20 USD at today's prices. We show that remarkably good gait recognition is possible with the ADS sensor.
- Rita Singh, E. Gouvêa, B. Raj. 2007. Probabilistic deduction of symbol mappings for extension of lexicons. Abstract: This paper proposes a statistical mapping-based technique for guessing pronunciations of novel words from their spellings. The technique is based on the automatic determination and utilization of unidirectional mappings between n-tuples of characters and n-tuples of phonemes, and may be viewed as a statistical extension of analogy-based pronunciation guessing algorithms.
- A. Reddy, B. Raj. 2007. Soft Mask Methods for Single-Channel Speaker Separation. Abstract: The problem of single-channel speaker separation attempts to extract a speech signal uttered by the speaker of interest from a signal containing a mixture of acoustic signals. Most algorithms that deal with this problem are based on masking, wherein unreliable frequency components from the mixed signal spectrogram are suppressed, and the reliable components are inverted to obtain the speech signal from speaker of interest. Most current techniques estimate this mask in a binary fashion, resulting in a hard mask. In this paper, we present two techniques to separate out the speech signal of the speaker of interest from a mixture of speech signals. One technique estimates all the spectral components of the desired speaker. The second technique estimates a soft mask that weights the frequency subbands of the mixed signal. In both cases, the speech signal of the speaker of interest is reconstructed from the complete spectral descriptions obtained. In their native form, these algorithms are computationally expensive. We also present fast factored approximations to the algorithms. Experiments reveal that the proposed algorithms can result in significant enhancement of individual speakers in mixed recordings, consistently achieving better performance than that obtained with hard binary masks.
- Kaustubh Kalgaonkar, P. Smaragdis, B. Raj. 2007. Sensor and Data Systems, Audio-Assisted Cameras and Acoustic Doppler Sensors. Abstract: In this chapter we present two technologies for sensing and surveillance -audio-assisted cameras and acoustic Doppler sensors for gait recognition.
- Kaustubh Kalgaonkar, Rongquiang Hu, B. Raj. 2007. Ultrasonic Doppler Sensor for Voice Activity Detection. Abstract: This letter describes a robust voice activity detector using an ultrasonic Doppler sonar device. An ultrasonic beam is incident on the talker's face. Facial movements result in Doppler frequency shifts in the reflected signal that are sensed by an ultrasonic sensor. Speech-related facial movements result in identifiable patterns in the spectrum of the received signal that can be used to identify speech activity. These sensors are not affected by even high levels of ambient audio noise. Unlike most other non-acoustic sensors, the device need not be taped to a talker. A simple yet robust method of extracting the voice activity information from the ultrasonic Doppler signal is developed and presented in this letter. The algorithm is seen to be very effective and robust to noise, and it can be implemented in real time.
- M. Shashanka, B. Raj, P. Smaragdis. 2007. Sparse Overcomplete Decomposition for Single Channel Speaker Separation. Abstract: We present an algorithm for separating multiple speakers from a mixed single channel recording. The algorithm is based on a model proposed by Raj and Smaragdis (2005). The idea is to extract certain characteristic spectra-temporal basis functions from training data for individual speakers and decompose the mixed signals as linear combinations of these learned bases. In other words, their model extracts a compact code of basis functions that can explain the space spanned by spectral vectors of a speaker. In our model, we generate a sparse-distributed code where we have more basis functions than the dimensionality of the space. We propose a probabilistic framework to achieve sparsity. Experiments show that the resulting sparse code better captures the structure in data and hence leads to better separation.
- B. Raj, Rita Singh, M. Shashanka, P. Smaragdis. 2007. Bandwidth Expansionwith a pólya URN Model. Abstract: We present a new statistical technique for the estimation of the high frequency components (4-8 kHz) of speech signals from narrow-band (0-4 kHz) signals. The magnitude spectra of broadband speech are modelled as the outcome of a Polya Urn process, that represents the spectra as the histogram of the outcome of several draws from a mixture multinomial distribution over frequency indices. The multinomial distributions that compose this process are learnt from a corpus of broadband (0-8 kHz) speech. To estimate high-frequency components of narrow-band speech, its spectra are also modelled as the outcome of draws from a mixture-multinomial process that is composed of the learnt multinomials, where the counts of the indices of higher frequencies have been obscured. The obscured high-frequency components are then estimated as the expected number of draws of their indices from the mixture-multinomial. Experiments conducted on bandlimited signals derived from the WSJ corpus show that the proposed procedure is able to accurately estimate the high frequency components of these signals.
- Yunbin Deng, X. Li, C. Kwan, B. Raj, R. Stern. 2007. Continuous Feature Adaptation for Non-Native Speech Recognition. Abstract: The current speech interfaces in many military applications may be adequate for native speakers. However, the recognition rate drops quite a lot for non-native speakers (people with foreign accents). This is mainly because the nonnative speakers have large temporal and intra-phoneme variations when they pronounce the same words. This problem is also complicated by the presence of large environmental noise such as tank noise, helicopter noise, etc. In this paper, we proposed a novel continuous acoustic feature adaptation algorithm for on-line accent and environmental adaptation. Implemented by incremental singular value decomposition (SVD), the algorithm captures local acoustic variation and runs in real-time. This feature-based adaptation method is then integrated with conventional model-based maximum likelihood linear regression (MLLR) algorithm. Extensive experiments have been performed on the NATO non-native speech corpus with baseline acoustic model trained on native American English. The proposed feature-based adaptation algorithm improved the average recognition accuracy by 15%, while the MLLR model based adaptation achieved 11% improvement. The corresponding word error rate (WER) reduction was 25.8% and 2.73%, as compared to that without adaptation. The combined adaptation achieved overall recognition accuracy improvement of 29.5%, and WER reduction of 31.8%, as compared to that without adaptation. Keywords—speaker adaptation; environment adaptation; robust speech recognition; SVD; non-native speech recognition
- B. Raj, L. Turicchia, B. Schmidt-Nielsen, R. Sarpeshkar. 2007. An FFT-Based Companding Front End for Noise-Robust Automatic Speech Recognition. Abstract: We describe an FFT-based companding algorithm for preprocessing speech before recognition. The algorithm mimics tone-to-tone suppression and masking in the auditory system to improve automatic speech recognition performance in noise. Moreover, it is also very computationally efficient and suited to digital implementations due to its use of the FFT. In an automotive digits recognition task with the CU-Move database recorded in real environmental noise, the algorithm improves the relative word error by 12.5% at -5 dB signal-to-noise ratio (SNR) and by 6.2% across all SNRs (-5 dB SNR to +5 dB SNR). In the Aurora-2 database recorded with artificially added noise in several environments, the algorithm improves the relative word error rate in almost all situations.
- P. Smaragdis, B. Raj. 2007. Shift-Invariant Probabilistic Latent Component Analysis. Abstract: In this paper we present a model which can decompose a probability densities or count data into a set of shift invariant components. We begin by introducing a regular latent variable model and subsequently extend it to deal with shift invariance in order to model more complex inputs. We develop an expectation maximization algorithm for estimating components and present various results on challenging real-world data. We show that this approach is a probabilistic generalization of well known algorithms such as Non-Negative Matrix Factorization and multi-way decompositions, and discuss its advantages over such approaches.
- C. Kwan, Xiaokun Li, D. Lao, Yunbin Deng, Z. Ren, B. Raj, Rita Singh, R. Stern. 2007. Voice driven applications in non-stationary and chaotic environment. Abstract: Automated operations based on voice commands would become more and more important in many applications, including robotics, maintenance operations, etc. However, voice command recognition rates drop quite a lot under nonstationary and chaotic noise environments. In this research, we tried to significantly improve the speech recognition rates under non-stationary noise environments. First, 298 Navy acronyms have been selected for automatic speech recognition. Data sets were collected under 4 types of non-stationary noisy environments: factory, buccaneer jet, babble noise in a canteen, and destroyer. Within each noisy environment, 4 levels (5 dB, 15 dB, 25 dB, and clean) of signal-to-noise ratio (SNR) were introduced to corrupt the speech. Second, a new algorithm to estimate speech or no speech regions has been developed, implemented, and evaluated. Third, extensive simulations were carried out. It was found that the combination of the new algorithm, the proper selection of language model and a customized training of the speech recognizer based on clean speech yielded very high recognition rates, which are from 80% to 90% for the four different noisy conditions. Fourth, extensive comparative studies have also been carried out
- M. Shashanka, B. Raj, P. Smaragdis. 2007. Sparse Overcomplete Latent Variable Decomposition of Counts Data. Abstract: An important problem in many fields is the analysis of counts data to extract meaningful latent components. Methods like Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA) have been proposed for this purpose. However, they are limited in the number of components they can extract and lack an explicit provision to control the "expressiveness" of the extracted components. In this paper, we present a learning formulation to address these limitations by employing the notion of sparsity. We start with the PLSA framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity. We show that this allows the extraction of overcomplete sets of latent components which better characterize the data. We present experimental evidence of the utility of such representations.
- P. Smaragdis, B. Raj. 2007. Example-Driven Bandwidth Expansion. Abstract: In this paper we present an example-driven algorithm that allows the recovery of wide regions of lost spectral components in bandlimited signals. We present a generative spectral model which allows the extraction of salient information from audio snippets, and then apply this information to enhance the bandwidth of bandlimited signals.
- B. Raj. 2006. SEPARATING A FOREGROUND SINGER FROM. Abstract: In this paper we present a algorithm for separating singing voices from background music in popular songs. The algorithm is derived by modelling the magnitude spectrogram of audio signals as the outcome of draws from a discrete bi-variate random process that generates time-frequency pairs. The spectrogram of a song is assumed to have been obtained through draws from the distributions underlying the music and the vocals, respectively. The parameters of the underlying distribuiton are learnt from the observed spectrogram of the song. The spectrogram of the separated vocals is then derived by estimating the fraction of draws that were obtained from its distribution. In the paper we present the algorithm within a framework that allows personalization of popular songs, by separating out the vocals, processing them appropriately to one’s own tastes, and remixing them. Our experiments reveal that we are effectively able to separate out the vocals in a song and personalize them to our tastes.
- B. Raj, M. Shashanka, P. Smaragdis. 2006. Latent Dirichlet Decomposition for Single Channel Speaker Separation. Abstract: We present an algorithm for the separation of multiple speakers from mixed single-channel recordings by latent variable decomposition of the speech spectrogram. We model each magnitude spectral vector in the short-time Fourier transform of a speech signal as the outcome of a discrete random process that generates frequency bin indices. The distribution of the process is modeled as a mixture of multinomial distributions, such that the mixture weights of the component multinomials vary from analysis window to analysis window. The component multinomials are assumed to be speaker specific and are learned from training signals for each speaker. We model the prior distribution of the mixture weights for each speaker as a Dirichlet distribution. The distributions representing magnitude spectral vectors for the mixed signal are decomposed into mixtures of the multinomials for all component speakers. The frequency distribution, i.e the spectrum for each speaker, is reconstructed from this decomposition
- G. Weinberg, B. Raj, Kaustubh Kalgaonkar. 2006. Two New Techniques for Natural Spoken User Interfaces. Abstract: Speech can be an excellent modality for simple information retrieval tasks, such as finding points of interest in an automotive navigation system or selecting music from a personal player. However, to achieve natural yet robust retrieval from spoken inputs in noisy, real-world environments, new techniques are needed. We will demonstrate two such techniques, one software-based and one hardware-based. In the former we use the whole search space of the speech recognition system to perform retrieval; in the latter, we use a new device, an ultrasonic Doppler microphone, to detect the start and end of utterances, thus enabling hands free operation. Together these two techniques can significantly improve the usability of speech interaction for our target applications.
- Yunbin Deng, Xiaokun Li, C. Kwan, R. Xu, B. Raj, R. Stern, D. Williamson. 2006. An integrated approach to improve speech recognition rate for non-native speakers. Abstract: The current speech interfaces in many military applications may be adequate for native speakers. However, the recognition rate drops quite a lot for non-native speakers (people with foreign accents). This is mainly because the non-native speakers have large temporal and intra-phoneme variations when they pronounce the same words. This problem is also complicated by the presence of loud environmental noise such as tank noise, helicopter noise, etc. In this paper, we proposed a novel speech feature adaptation algorithm for continuous accent and environmental adaptation. This feature-based adaptation method is then integrated with conventional model-based maximum likelihood linear regression (MLLR) algorithm. Extensive experiments have been performed on the NATO non-native speech corpus with baseline acoustic model trained on native American English. The proposed feature-based adaptation algorithm improved the average recognition accuracy by 15%, while the MLLR model-based adaptation achieved 11% improvement. The combined adaptation achieved overall recognition accuracy improvement of 29.5%, and word error rate reduction of 31.8%.
- D. Ellis, B. Raj, Judith C. Brown, M. Slaney, P. Smaragdis. 2006. Editorial: Special Section on Statistical and Perceptual Audio Processing. Abstract: H UMAN perception has always been an inspiration for automatic processing systems, not least because tasks such as speech recognition only exist because people do them—and, indeed, without that example we might wonder if they were possible at all. As computational power grows, we have increasing opportunities to model and duplicate perceptual abilities with greater fidelity, and, most importantly, based on larger and larger amounts of raw data describing both what signals exist in the real world, and how people respond to them. The power to deal with large data sets has meant that approaches that were once mere theoretical possibilities, such as exhaustive search of exponentially-sized codebooks, or real-time direct convolution of long sequences, have become increasingly practical and even unremarkable. A major consequence of this is the growth of statistical or corpus-based approaches, where complex relations, discriminations, or structures are inferred directly from example data (for instance by optimizing the parameters of a very general algorithm). An increasing number of complex tasks can be given empirically optimal solutions based on large, representative datasets. The traditional idea of perceptually-inspired processing is to develop a machine algorithm for a complex task such as melody recognition or source separation through inspiration and intro-spection about how individuals perform the task, and on the basis of direct psychological or neurophysiological data. The results can appear to be at odds with the statistical perspective, since perceptually-motivated work is often ad-hoc, comprising many stages whose individual contributions are difficult to separate. We believe that it is important to unify these two approaches: to employ rigorous, exhaustive techniques taking advantage of the statistics of large data sets to develop and solve perceptually based and subjectively-defined problems. With this in mind, we organized a one-day workshop on Statistical and Perceptual Audio Processing as a satellite to the International Although independent of the workshops, this special issue is based on the same insight and goal. In our call for papers, and in our editorial choices of which papers to consider for the special issue, we were looking specifically for rigorous, statistical techniques applied to perceptually-defined tasks or processing in innovative and interesting ways. As can be seen from the papers that follow, this seemingly narrow focus can be interpreted in a wide variety of ways. In fact, we received more than 50 submissions for the special issue—many more than we ex-pected—which says something about the timeliness and resonance …
- P. Smaragdis, B. Raj, Madhusudana Shashanka. 2006. A Probabilistic Latent Variable Model for Acoustic Modeling. Abstract: In this paper we describe a model developed for the analysis of acoustic spectra. Unlike decompositions techniques that can result in difficult to interpret results this model explicitly models spectra as distributions and extracts sets of additive and semantically useful components that facilitate a variety of applications ranging from source separation, denoising, music transcription and sound recognition. This model is probabilistic in nature and is easily extended to produce sparse codes, and discover transform invariant components which can be optimized for particular applications.
- Kaustubh Kalgaonkar, B. Raj. 2006. AN ACOUSTIC DOPPLER-BASED FRONT END FOR HANDS FREE SPOKEN USER INTERFACES. Abstract: Two major problems facing hands-free spoken user interfaces are first, to be able to identify accurately when it is addressed and second to effectively determine the start and end points of utterances so that proper sections of speech can be used for further processing and recognition tasks. Accurate determination of end points not only aids accurate noise estimation but also improves speech recognition accuracy. This paper presents an acoustic Doppler sensor based front end for a hands-free spoken user device which provides an efficient and effective solution to both the aforementioned problems.
- Jethran Guinness, B. Raj, B. Schmidt-Nielsen, L. Turicchia, R. Sarpeshkar. 2005. A companding front end for noise-robust automatic speech recognition. Abstract: Feature computation modules for automatic speech recognition (ASR) systems have long been modeled on the human auditory system. Most current ASR systems model the critical band response and equal loudness characteristics of the auditory system. It has been postulated that more detailed models of the human auditory system can lead to more noise-robust speech recognition. An auditory phenomenon that is of particular relevance to robustness is simultaneous masking, whereby dominant frequencies suppress adjacent weaker frequencies. In this paper, we present a companding-based model that mimics simultaneous masking in the front end of a speech recognizer. In an automotive digits recognition task, the front end improves word error rate by 4.0% (25% relative to Mel cepstra) at -5 dB SNR at the cost of a 1.7% increase at 15 dB SNR.
- B. Raj, Rita Singh. 2005. Feature compensation with secondary sensor measurements for robust speech recognition. Abstract: This paper investigates the use of secondary sensor measurements to augment feature compensation methods for robust speech recognition. Secondary sensors measure secondary phenomena associated with human speech production. While such measurements do not provide sufficient information for speech recognition per-se, they do not degrade with the noise that corrupts the acoustic signal and can be used to guide algorithms that attempt to estimate noise compensation algorithms by restricting the region of the acoustic space within which the recorded speech must lie. In this paper we specifically, we investigate the use of measurements obtained from a Glottal ElectroMagnetic Sensor (GEMS) to improve the noise estimation performance of the Vector Taylor Series algorithm. We and show that this can result in significant improvement in performance of the VTS algorithm, and, consequently, recognition performance.
- Dhananjay Bansal, B. Raj, P. Smaragdis. 2005. Bandwidth expansion of narrowband speech using non-negative matrix factorization. Abstract: In this paper, we present a novel technique for the estimation of the high frequency components (4-8kHz) of speech signals from narrow-band (0-4 kHz) signals using convolutive Non-Negative Matrix Factorisation (NMF). The proposed technique utilizes a brief recording of simultaneous broad band and narrow band signals from a target speaker to learn a set of broad-band non-negative bases for the speaker. The low-frequency components of these bases are used to determine how the high-frequency components must be combined in order to reconstruct the high-frequency components of new narrow-band signals from the speaker. Experiments reveal that the technique is able to reconstruct broadband sppech that is perceptually virtually indistinguishable from true broadband recordings. Eurospeech 2005
- B. Raj, Rita Singh, P. Smaragdis. 2005. Recognizing speech from simultaneous speakers. Abstract: In this paper we present and evaluate factored methods for recognition of simultaneous speech from multiple speakers in single-channel recordings. Factored methods decompose the problem of jointly recognizing the speech from each of the speakers by separately recognizing the speech from each speaker. In order to achieve this, the signal components of the target speaker in each case must be enhanced in some manner. We do this in two ways: using an NMF-based speaker separation algorithm that generates separated spectra for each speaker, and a mask estimation method that generates spectral masks for each speaker that must be used in conjunction with a missing-feature method that can recognize speech from partial spectral data. Experiments on synthetic mixtures of signals from the Wall Street Journal corpus show that both approaches can greatly improve the recognition of the individual signals in the mixture.
- B. Raj, R. Stern. 2005. Missing-feature approaches in speech recognition. Abstract: In this article we have reviewed a wide variety of techniques based on the identification of missing spectral features that have proved effective in reducing the error rates of automatic speech recognition systems. These approaches have been conspicuously effective in ameliorating the effects of transient maskers such as impulsive noise or background music. We described two broad classes of missing feature algorithms: feature-vector imputation algorithms (which restore unreliable components of incoming feature vectors) and classifier-modification algorithms (which dynamically reconfigure the classifier itself to cope with the effects of unreliable feature components). We reviewed the mathematics of four major missing feature techniques: the feature-imputation techniques of cluster-based reconstruction and covariance-based reconstruction, and the classifier-modification methods of class-conditional imputation and marginalization. We also discussed the ways in which the common feature extraction procedures of cepstral analysis, temporal-difference features, and mean subtraction can be handled by speech recognition systems that make use of missing feature techniques. We concluded with a discussion of a small number of selected experimental results. These results confirm the effectiveness of all types of missing feature approaches discussed in ameliorating the effects of both stationary and transient noise, as well as the particular effectiveness of both soft masks and fragment decoding.
- Vijay Divi, C. Forlines, J. V. Gemert, B. Raj, B. Schmidt-Nielsen, K. Wittenburg, Joseph Woelfel, Fang-Fang Zhang. 2004. A Speech-in List-out Approach to Spoken User Interfaces. Abstract: Spoken user interfaces are conventionally either dialogue-based or menu-based. In this paper we propose a third approach, in which the task of invoking responses from the system is treated as one of retrieval from the set of all possible responses. Unlike conventional spoken user interfaces that return a unique response to the user, the proposed interface returns a shortlist of possible responses, from which the user must make the final selection. We refer to such interfaces as Speech-In List-Out or SILO interfaces. Experiments show that SILO interfaces can be very effective, are highly robust to degraded speech recognition performance, and can impose significantly lower cognitive load on the user as compared to menu-based interfaces.
- A. Reddy, B. Raj. 2004. Soft mask estimation for single channel speaker separation. Abstract: 1. Abstract The problem of single channel speaker separation, attempts to extract a speech signal uttered by the speaker of interest from a signal containing a mixture of auditory signals. Most algorithms that deal with this problem, are based on masking, where reliable components from the mixed signal spectrogram are inversed to obtain the speech signal from speaker of interest. As of now, most techniques, estimate this mask in a binary fashion, resulting in a hard mask. We present a technique to estimate a soft mask that weights the frequency sub-bands of the mixed signal. The speech signal can then be reconstructed from the estimated power spectrum of the speaker of interest. Experimental results shown in this paper, prove that the results are better than those obtained by estimating the hard mask.
- William Walker, Paul Lamere, Philip Kwok, B. Raj, Rita Singh, E. Gouvêa, Peter Wolf, Joseph Woelfel. 2004. Sphinx-4: a flexible open source framework for speech recognition. Abstract: Sphinx-4 is a flexible, modular and pluggable framework to help foster new innovations in the core research of hidden Markov model (HMM) speech recognition systems. The design of Sphinx-4 is based on patterns that have emerged from the design of past systems as well as new requirements based on areas that researchers currently want to explore. To exercise this framework, and to provide researchers with a "researchready" system, Sphinx-4 also includes several implementations of both simple and state-of-the-art techniques. The framework and the implementations are all freely available via open source.
- Rita Singh, B. Raj. 2004. Classification in Likelihood Spaces. Abstract: In classification methods that explicitly model class-conditional probability distributions, the true distributions are often not known. These are estimated from the data available, to approximate the true distributions. Errors in classification that arise due to this approximation can be reduced to some extent if the estimated distributions are used merely to project data into a space of likelihoods and classification is performed in that space using discriminant functions. In this article, we discuss the rationale behind this, and also the general properties of likelihood projections. We demonstrate the utility of likelihood projections in improving classification performance through experiments carried out on a standard image database and a standard speech database.
- A. Reddy, B. Raj. 2004. A minimum mean squared error estimator for single channel speaker separation. Abstract: The problem of separating out the signals for multiple speakers from a single mixed recording has received considerable atten- tio ni n recent times. Most current techniques are based on the principle of masking :i n order the separate out the signal for any speaker, frequency components that are not believed to be- long to that speaker are suppressed. The signals for the speaker is reconstructed fro mt hepartial spectral information that re- mains. In this paper we present a different kind of technique - one that attempts to estimate all spectral components for the desired speaker. Separated signals are derived from the com- plete spectral descriptions so obtained. Experiments show that this method results in superior reconstruction to masking based methods. form representations of the various speakers by hidden Markov models (HMMs). The parameters of the HMM for any speaker are learnt from training data recorded from the speaker. In addi- tion, Roweis assumes that the log energy in any frequency band of the mixed signal at any time can be attributed to only one of the speakers. This "log-max" assumption is justified by two observations. First, when two or more speakers speak simulta- neously, at any time, any given frequency band is usually domi- nated by a single speaker. Second, in any given frequency band the disparity in the energy levels of the dominant speaker and the other speakers is such that the logarithm of the sum of the energies of the individual speakers can be well approximated by the logarithm of the energy of the dominant speaker. In or- der to reconstruct the signal for any speaker, Roweis estimates the mask for that speaker, i.e. the identity of the time-frequency locations where the speaker dominates. The entire signal is re- constructed entirely from the masked spectrum for the speaker, i.e. fro mt he spectral components identified by the mask. The results achieved with this method are remarkably good. Hershey et. al. (6) augment audio recordings with visual features, such as lip and facial movement, in order to enhance the separation. Additionally, the ys eparate the signal into mul- tiple frequency bands, which are then processed independently. As in Roweis' algorithm, the signals for the individual speakers are reconstructed from masked spectra.
- B. Raj, Rita Singh, R. Stern. 2004. On tracking noise with linear dynamical system models. Abstract: This paper investigates the use of higher-order autoregressive vector predictors for tracking the noise in noisy speech signals. The autoregressive predictors form the state equation of a linear dynamical system that models the spectral dynamics of the noise process. Experiments show that the use of such models to track noise can lead to large gains in recognition performance on speech compensated for the estimated noise. However, predictors of order greater than 1 are not observed to improve the performance beyond that obtained with a first-order predictor. We analyze and explain why this is so.
- M. Seltzer, B. Raj, R. Stern. 2004. Likelihood-maximizing beamforming for robust hands-free speech recognition. Abstract: Speech recognition performance degrades significantly in distant-talking environments, where the speech signals can be severely distorted by additive noise and reverberation. In such environments, the use of microphone arrays has been proposed as a means of improving the quality of captured speech signals. Currently, microphone-array-based speech recognition is performed in two independent stages: array processing and then recognition. Array processing algorithms, designed for signal enhancement, are applied in order to reduce the distortion in the speech waveform prior to feature extraction and recognition. This approach assumes that improving the quality of the speech waveform will necessarily result in improved recognition performance and ignores the manner in which speech recognition systems operate. In this paper a new approach to microphone-array processing is proposed in which the goal of the array processing is not to generate an enhanced output waveform but rather to generate a sequence of features which maximizes the likelihood of generating the correct hypothesis. In this approach, called likelihood-maximizing beamforming, information from the speech recognition system itself is used to optimize a filter-and-sum beamformer. Speech recognition experiments performed in a real distant-talking environment confirm the efficacy of the proposed approach.
- M. Reyes-Gomez, B. Raj, Daniel P. W. Ellis. 2003. Multi-channel source separation by beamforming trained with factorial HMMs. Abstract: Speaker separation has conventionally been treated as a problem of blind source separation (BSS). This approach does not utilize any knowledge of the statistical characteristics of the signals to be separated, relying mainly on the independence between the various signals to separate them. Maximum-likelihood techniques, on the other hand, utilize knowledge of the a priori probability distributions of the signals from the speakers, in order to effect separation. Previously (Reyes-Gomez, M.J. et al., Proc. ICASSP, 2003), we presented a maximum-likelihood speaker separation technique that utilizes detailed statistical information about the signals to be separated, represented in the form of hidden Markov models (HMMs), to estimate the parameters of a filter-and-sum processor for signal separation. We show that the filters that are estimated for a particular utterance by a speaker generalize well to other utterances by the same speaker, provided the location of the various speakers remains constant. Thus, filters that have been estimated using a "training" utterance of a known transcript can be used to separate all future signals by the speaker from mixtures of speech signals in an unsupervised manner. On the other hand, the filters are ineffective for other speakers, even at the same locations, indicating that they capture the spatio-frequency characteristics of the speaker.
- B. Raj, E. Whittaker. 2003. Lossless compression of language model structure and word identifiers. Abstract: Very large reductions in language model memory requirements have recently been reported for large vocabulary continuous speech recognition applications through the pruning and quantization of the floating-point components of the language model: the probabilities and back-off weights. In this paper that work is extended through the compression of the integer components: the word identifiers and storage structures. A novel algorithm is presented for converting ordered lists of monotonically increasing integer values (such as are commonly found in language models) into variable-bit width tree structures such that the most memory efficient configuration is obtained for each original list. By applying this new technique together with the techniques reported previously we obtain an 86% reduction in language model size to 10Mb for no increase in word error rate on the DARPA Hub4 1998 task and a 0.5% absolute increase on the Hub4 1997 task.
- P. Moreno, Uday Jain, B. Raj, R. Stern. 2003. APPROACHES TO MICROPHONE INDEPENDENCE IN AUTOMATIC SPEECH RECOGNITION. Abstract: This paper describes a series of cepstral-based compensation procedures that render the SPHINX-II system more robust with respect to acoustical changes in the environment. The first algorithm, RATZ (MultivaRiate gAussian based cepsTral normaliZation) requires stereo-data for computing compensation terms, and is similar in philosophy to MFCDCN [ref] (in fact MFCDCN can be thought of as a discrete case of RATZ). We also describe a second algorithm, an improved version of CDCN, that does not require stereo training data and yet achieves performance levels comparable to the RATZ and other stereo algorithms. Use of the various compensation algorithms in consort produces a reduction of error rates for SPHINX-II by as much as 20.0% percent relative to the rate achieved with cepstral mean normalization alone, in both development test sets and in the context of the 1994 ARPA CSR evaluations.
- R. Radhakrishan, Z. Xiong, Ajay Divakaran, B. Raj. 2003. Investigation on effectiveness of mid-level feature representation for semantic boundary detection in news video. Abstract: In our past work, we have attempted to use a mid-level feature namely the state population histogram obtained from the Hidden Markov Model (HMM) of a general sound class, for speaker change detection so as to extract semantic boundaries in broadcast news. In this paper, we compare the performance of our previous approach with another approach based on video shot detection and speaker change detection using the Bayesian Information Criterion (BIC). Our experiments show that the latter approach performs significantly better than the former. This motivated us to examine the mid-level feature closely. We found that the component population histogram enabled discovery of broad phonetic categories such as vowels, nasals, fricatives etc, regardless of the number of distinct speakers in the test utterance. In order for it to be useful for speaker change detection, the individual components should model the phonetic sounds of each speaker separately. From our experiments, we conclude that state/component population histograms can only be useful for further clustering or semantic class discovery if the features are chosen carefully so that the individual states represent the semantic categories of interest.
- Rita Singh, Manfred K. Warmuth, B. Raj, Paul Lamere. 2003. Classification with free energy at raised temperatures. Abstract: In this paper we describe a generalized classification method for HMM-based speech recognition systems, that uses free energy as a discriminant function rather than conventional probabilities. The discriminant function incorporates a single adjustable temperature parameter T. The computation of free energy can be motivated using an entropy regularization, where the entropy grows monotonically with the temperature. In the resulting generalized classification scheme, the values of and give the conventional Viterbi and forward algorithms, respectively, as special cases. We show experimentally that if the test data are mismatched with the classifier, classification at temperatures higher than one can lead to significant improvements in recognition performance. The temperature parameter is far more effective in improving performance on mismatched data than a variance scaling factor, which is another apparent single adjustable parameter that has a very similar analytical form.
- M. Seltzer, B. Raj. 2003. Speech-recognizer-based filter optimization for microphone array processing. Abstract: Conventional microphone array processing schemes used for speech recognition enhance the output waveform using optimization criteria that are independent of the recognition system. We present a new filter-and-sum array processing algorithm in which the filter parameters are calibrated to maximize recognizer likelihoods. The proposed method provides significant improvement in recognition accuracy over conventional methods.
- Rita Singh, B. Raj. 2003. Tracking noise via dynamical systems with a continuum of states. Abstract: We model noise as a sequence of states of a dynamical system with a continuum of states. Observations generated by such a system are assumed to be related to the state of the system by a functional relation which models clean speech as the corrupting influence on noise. We show how the closed-form representation of such a dynamical system can be rendered tractable and solved iteratively by dynamically sampling the state space, resulting in an estimated noise sequence (sequence of states), which can then be removed from the noisy speech signal by standard methods. Experiments on speech corrupted by various noises show that the proposed algorithm performs better than our best previous algorithm, VTS, which assumes that the noise is stationary.
- M. Reyes-Gomez, B. Raj, D. Ellis. 2003. Multi-channel source separation by factorial HMMs. Abstract: We present a new speaker-separation algorithm for separating signals with known statistical characteristics from mixed multi-channel recordings. Speaker separation has conventionally been treated as a problem of blind source separation (BSS). This approach does not utilize any knowledge of the statistical characteristics of the signals to be separated, relying mainly on the independence between the various signals to separate them. We present an algorithm that utilizes detailed statistical information about the signals to be separated, represented in the form of hidden Markov models (HMM). We treat the signal separation problem as one of beamforming, where each signal is extracted using a filter-and-sum array. The filters are estimated to maximize the likelihood of the summed output, measured on the HMM for the desired signal. This is done by iteratively estimating the best state sequence through the HMM from a factorial HMM (FHMM) that is the cross-product of the HMMs for the multiple signals, using the current output of the array, and estimating the filters to maximize the likelihood of that state sequence. Experiments show that the proposed method can cleanly extract a background speaker who is 20 dB below the foreground speaker in a two-speaker mixture, when the HMMs for the signals are constructed from knowledge of the utterance transcriptions.
- Peter Wolf, B. Raj. 2002. The MERL SpokenQuery information retrieval system a system for retrieving pertinent documents from a spoken query. Abstract: This paper describes some key concepts developed and used in the design of a spoken-query based information retrieval system developed at the Mitsubishi Electric Research Labs (MERL). Innovations in the system include automatic inclusion of signature terms of documents in the recognizer's vocabulary, the use of uncertainty vectors to represent spoken queries, and a method of indexing that accommodates the usage of uncertainty vectors. This paper describes these techniques and includes experimental results that demonstrate their effectiveness.
- Rita Singh, B. Raj, R. Stern. 2002. Automatic generation of subword units for speech recognition systems. Abstract: Large vocabulary continuous speech recognition (LVCSR) systems traditionally represent words in terms of smaller subword units. Both during training and during recognition, they require a mapping table, called the dictionary, which maps words into sequences of these subword units. The performance of the LVCSR system depends critically on the definition of the subword units and the accuracy of the dictionary. In current LVCSR systems, both these components are manually designed. While manually designed subword units generalize well, they may not be the optimal units of classification for the specific task or environment for which an LVCSR system is trained. Moreover, when human expertise is not available, it may not be possible to design good subword units manually. There is clearly a need for data-driven design of these LVCSR components. In this paper, we present a complete probabilistic formulation for the automatic design of subword units and dictionary, given only the acoustic data and their transcriptions. The proposed framework permits easy incorporation of external sources of information, such as the spellings of words in terms of a nonideographic script.
- M. Seltzer, B. Raj, R. Stern. 2002. Speech recognizer-based microphone array processing for robust hands-free speech recognition. Abstract: We present a new array processing algorithm for microphone array speech recognition. Conventionally, the goal of array processing is to take distorted signals captured by the array and generate a cleaner output waveform. However, speech recognition systems operate on a set of features derived from the waveform, rather than the waveform itself. The goal of an array processor used in conjunction with a recognition system is to generate a waveform which produces a set of recognition features which maximize die likelihood for the words that are spoken, rather than to minimize the waveform distortion. We propose a new array processing algorithm which maximizes the likelihood of the recognition features. This is accomplished through the use of a new objective function which utilizes information from the recognition system itself, obtained in an unsupervised manner, to optimize the parameters of a filter-and-sum array processor. Using the proposed method, improvements in word error rate of up to 36% over conventional methods are achieved on real microphone array tasks in a wide range of environments.
- Rita Singh, R. Stern, B. Raj. 2002. Signal and Feature Compensa-tion Methods for Robust Speech Recognition. Abstract: A process is provided for dimerizing 1-olefins which comprises contacting such olefins in a reaction zone with a minor proportion of at least one catalyst selected from the group consisting of phosphoric acid-promoted and water-promoted boron trifluoride catalyst in a mole ratio of catalyst to olefins of from about 0.005:1 to about 0.1:1 and at a temperature from about 100 DEG C. to about 150 DEG C.
- Rita Singh, M. Seltzer, B. Raj, R. Stern. 2001. Speech in Noisy Environments: robust automatic segmentation, feature extraction, and hypothesis combination. Abstract: The first evaluation for Speech in Noisy Environments (SPINE1) was conducted by the Naval Research Labs (NRL) in August, 2000. The purpose of the evaluation was to test existing core speech recognition technologies for speech in the presence of varying types and levels of noise. In this case the noises were taken from military settings. Among the strategies used by Carnegie Mellon University's successful systems designed for this task were session-adaptive segmentation, robust mel-scale filtering for the computation of cepstra, the use of parallel front-end features and noise-compensation algorithms, and parallel hypotheses combination through word-graphs. This paper describes the motivations behind the design decisions taken for these components, supported by observations and experiments.
- E. Whittaker, B. Raj. 2001. Comparison of width-wise and length-wise language model compression. Abstract: In this paper we investigate the extent to which Katz back-off language models can be compressed through a combination of parameter quantization (width-wise compression) and parameter pruning (length-wise compression) methods while preserving performance. We compare the compression and performance that is achieved using entropy-based pruning against that achieved using only parameter quantization. We then compare combinations of both methods. It is shown that a broadcast news language model can be compressed by up to 83% to only 12.6Mb with no loss in performance on a broadcast news task. Compressing the language model further by quantization to 10.3Mb resulted in only a 0.4% degradation in word error rate which is better than can be achieved through entropy-based pruning alone.
- B. Raj, E. Whittaker. 2001. Quantization-basedLanguageModel Compression. Abstract: This paper describes two techniques for reducing the sizeof statistical back-off gramlanguagemodelsin computermemory. Languagemodelcompresionis achieved throughacombinationof quantizing languagemodelprobabilitiesandback-off weights andthepruning of parametersthataredeterminedto beunnecessary afterquantization. Therecognition performanceof theoriginal andcompressedlanguagemodelsis evaluated acrossthreedifferent languagemodelsandtwo different recognition tasks. The results showthatthelanguagemodelscanbecompresedby upto 60%of theiroriginal sizewith no significantlossin recognition performance.Moreover, thetechniquesthat aredescribed provide a principled methodwith which to compres languagemodels furtherwhile minimising degradationin recognition performance.
- M. Seltzer, B. Raj. 2001. Calibration of microphone arrays for improved speech recognition. Abstract: We present a new microphone array calibration algorithm specifica lly designed for speech recognition. Currently, microphone-array-based speech recognition is performed in two independent stages: array processing, and then recognition. Array processing algorithms designed for speech enhancement are used to process the waveforms before recognition. These systems make the assumption that the best array processing methods will result in the best recognition performance. However, recognition systems interpret a set of features extracted from the speech waveform, not the waveform itself. In our calibration method, the filter parameters of a filter -and-sum array processing scheme are optimized to maximize the likelihood of the recognition features extracted from the resulting output signal. By incorporating the speech recognition system into the design of the array processing algorithm we are able to achieve improvements in word error rate of up to 37% over conventional array processing methods on both simulated and actual microphone array data.
- Paul Lamere, Philip Kwok, E. Gouvêa, B. Raj, Rita Singh, William Walker, Manfred K. Warmuth, Peter Wolf. 2001. THE CMU SPHINX-4 SPEECH RECOGNITION SYSTEM. Abstract: The Sphinx-4 speech recognition system is the latest addition to Carnegie Mellon University's repository of Sphinx speech recognition systems. It has been jointly designed by Carnegie Mellon University, Sun Microsystems Laboratories and Mitsubishi Electric Research Laboratories. It is differently designed from the earlier Sphinx systems in terms of modularity, flexibility and algorithmic aspects. It uses newer search strategies, is universal in its acceptance of various kinds of grammars and language models, types of acoustic models and feature streams. Algorithmic innovations included in the system design enable it to incorporate multiple information sources in an elegant manner. The system is entirely developed on the JavaTM platform and is highly portable, flexible, and easier to use with multithreading. This paper describes the salient features of the Sphinx-4 decoder and includes preliminary performance measures relating to speed and accuracy.
- E. Whittaker, B. Raj. 2001. Quantization-based language model compression. Abstract: This paper describes two techniques for reducing the size of statistical back-off gram language models in computer memory. Language model compression is achieved through a combination of quantizing language model probabilities and back-off weights and the pruning of parameters that are determined to be unnecessary after quantization. The recognition performance of the original and compressed language models is evaluated across three different language models and two different recognition tasks. The results show that the language models can be compressed by up to 60% of their original size with no significant loss in recognition performance. Moreover, the techniques that are described provide a principled method with which to compress language models further while minimising degradation in recognition performance.
- B. Raj, M. Seltzer, R. Stern. 2001. Robust Speech Recognition: The case for restoring missing features. Abstract: Speech recognition systems perform poorly in the presence of corrupting noise. Missing feature methods attempt to compensate for the noise by removing unreliable noise corrupted components of a spectrographic representation of the noisy speech and performing recognition with the remaining reliable components. Conventional classifier compensation methods modify the recognition system to work with the incomplete representation so obtained. This constrains them to perform recognition using spectrographic features which are known to be suboptimal to cepstra. In previous work we have proposed an alternative feature-compensation approach whereby the unreliable components are replaced by estimates derived from the reliable components and the known statistics of clean speech. In this paper we perform a detailed comparison of various aspects of classifier -based and feature-based compensation methods. We show that although the classifier -based compensation methods are superior when recognition is performed with spectrographic features, feature-based compensation methods provide better recognition performance overall, since cepstra derived from the reconstructed spectrogram can now be used for recognition. In addition, they have the added advantages of being computationally less expensive and not requiring modificati on of the recognizer.
- P. Moreno, B. Logan, B. Raj. 2001. A boosting approach for confidence scoring. Abstract: In this paper we present the application of a boosting classification algorithm to confidence scoring. We derive feature vectors from speech recognition lattices and feed them into a boosting classifier . This classifier combines hundreds of very simple ‘weak learners’ and derives classification rules that can reduce the confidence error rate by up to 34%. We compare our results to those obtained using two other standard classification techniques, Support Vector Machines (SVMs) and Classification and Regression Trees (CART), and show significant improvements. Furthermore, the nature of the boosting algorithm allows us to combine the best single classifier and improve its performance. We present experimental results on real world corpora derived from the Compaq SpeechBot Web index and from the HUB4 DARPA evaluation sets. We believe these results have wide applicability to audio indexing and to acoustic and language modeling adaptation where word confide nce scores can be used in iterative adaptation schemes. In Eurospeech’2001
- B. Raj, E. Gouvêa. 2000. CEPSTRAL COMPENSATION USING STATISTICAL LINEARIZATION. Abstract: Speech recognition systems perform poorly on speech degraded by even simple effects such as linear filtering and additive noise. One solution to this problem is to modify the probability density function (PDF) of clean speech to account for the effects of the degradation. However, even for the case of linear filtering and additive noise, it is extremely difficult to do this analytically. Previously-attempted analytical solutions for the problem of noisy speech recognition have either used an overly-simplified mathematical description of the effects of noise on the statistics of speech, or they have relied on the availability of large environment-specific adaptation sets. In this paper we present the Vector Polynomial approximationS (VPS) method to compensate for the effects of linear filtering and additive noise on the PDF of clean speech. VPS also estimates the parameters of the environment, namely the noise and the channel, by using statistically linearized approximations of these effects. We evaluate the performance of this method (VPS) using the CMU SPHINX-II system on the alphanumeric CENSUS database corrupted with artificial white Gaussian noise. VPS provides improvements of up to 15 percent in relative recognition accuracy over our previous best algorithm, VTS, while being up to 20 percent more computationally efficient.
- B. Raj, M. Seltzer, R. Stern. 2000. Reconstruction of damaged spectrographic features for robust speech recognition. Abstract: We present two missing-feature based algorithms that recover noise-corrupted regions of spectrographic representations of speech for noise-robust speech recognition. These algorithms modify the incoming feature vector without any changes to the speech recognition system, in contrast to previously-described approaches. The ﬁrst approach clusters the feature vectors representing clean speech. Missing data are recovered by estimating the spectral cluster in each analysis frame based on the uncorrupted feature values. The second approach uses MAP procedures to estimate the values of missing data elements based on their correlations with the features that are present. Both methods take into account bounds on the clean spectrogram implied by the noisy spectrogram. Large improvements in recognition accuracy are observed when these methods are used on speech corrupted by non-stationary noise when the locations of the corrupt regions of the spectrogram are known. We also present a new method of estimating the locations of corrupt regions in spectrograms that treats the problem of identifying these regions as one of Bayesian classiﬁcation. This method, when used along with the best method to reconstruct them, results in recognition accuracies comparable with the best previous data compensation algorithm on speech corrupted by white noise. It also provides signiﬁcant improvement on speech corrupted by music when the global SNR of the corrupted signal is known a priori .
- Rita Singh, B. Raj, R. Stern. 2000. Automatic generation of phone sets and lexical transcriptions. Abstract: Large vocabulary automatic speech recognition systems model words as sequences of a small set of basic sub-word units (the phoneset), which the systems are trained to classify. All words in the system's vocabulary are transcribed in terms of this set in a dictionary. The phoneset and dictionary are specific to a language and are typically designed manually. The system's performance is critically dependent on the quality of the phoneset and the accuracy of the dictionary. The authors attempt to generate the phoneset and dictionary automatically, using only the training data and their transcriptions. We treat this as a joint optimization problem with a maximum a posteriori solution for the dictionary and a maximum likelihood solution for the phoneset and its acoustic models. Experiments with the DARPA Resource Management corpus show that the automatically generated phoneset and dictionary result in recognition accuracies close to those obtained using manually designed ones.
- Rita Singh, B. Raj, R. Stern. 2000. Structured redefinition of sound units by merging and splitting for improved speech recognition. Abstract: The performance of speech recognition systems degrades when the basic sound units used are poorly defined or incon-sistently used. Several attempts have been made to improve dictionaries automatically, either by redefining pronuncia-tions of words in terms of existing sound units, or by redefining the sound units themselves completely. The problem with these approaches is that, while the former is limited by the sound units used, the latter discards all human information that has been incorporated into an expert-designed recognition dictionary. In this paper we propose a new merging-and-splitting algorithm that attempts to redefine the basic sound units used in the dictionary, while maintaining the expert knowledge built into a manually designed dictionary. Sound units from an existing dictionary are merged based on their inherent confusability, as measured by a Monte-Carlo based metric, and subsequently split to maximize the likelihood of the training data. Experiments with the Resource Management database indicate that this approach results in an improvement in recognition accuracy when context-indepen-dent models are used for recognition. When context-dependent models are used, the improvement observed is reduced.
- M. Seltzer, B. Raj, R. Stern. 2000. Classifier-based mask estimation for missing feature methods of robust speech recognition. Abstract: Missing feature methods of noise compensation for speech recognition operate by removing components of a spectrographic representation of speech that are considered to be corrupt, as indicated by a low signal-to-noise ratio. Recognition is either performed directly on the incomplete spectrograms or the missing components are reconstructed prior to recognition. These methods require a spectrographic mask which accurately labels the reliable and corrupt regions of the spectrogram. Current methods of mask estimation rely on assumptions about the corrupting noise such as stationarity. This is a significant drawback since the missing feature methods themselves have no such restrictions. We present a new mask estimation technique that uses a Bayesian classifier to determine the reliability of spectrographic elements. Features were designed that make no assumptions about the corrupting noise signal, but rather exploit characteristics of the speech signal itself. Missing feature compensation experiments were performed on speech corrupted by a variety of noises. In all cases, classifier-based mask estimation resulted in significantly better recognition accuracy than conventional mask estimation methods.
- M. Ravishankar, Rita Singh, B. Raj, R. Stern. 1999. THE 1999 CMU 10X REAL TIME BROADCAST NEWS TRANSCRIPTION SYSTEM. Abstract: CMU's 10X real time system is the HMM-based SPHINX-III system with a newly developed fast decoder. The fast decoder uses a subvector clustered version of the acoustic models for Gaussian computation and a lexical tree search structure. It was developed in September, 1999, and is currently a first-pass decoder, capable of generating word lattices. It was designed to optimize speed, recognition accuracy as well as memory requirements. For the 1999 Hub 4 evaluation task, the system used two sets of acoustic models - full-bandwidth and narrow-bandwidth. The acoustic models were 6000 senone, 32 Gaussians per state, 3-state HMMs with no skips permitted across states. The system used a single 39 dimensional feature stream consisting of cepstra and cepstral differences. The lattices generated were rescored using a DAG algorithm. The DAG-rescored hypotheses were designated as those of the primary system. The contrastive system consisted of the output of the first pass Viterbi search, with no DAG rescoring of lattices. A trigram language model consisting of 57,000 unigrams, 10 million bigrams and 14.9 million trigrams was used. No adaptation passes were done. In this paper we describe the various components of the primary system. The first-pass word error rate on the 1998 Hub 4 evaluation set was 20.4% with this system. The overall word error rate scored by NIST for the 1999 Hub 4 evaluation set was 27.6%.
- Rita Singh, B. Raj, R. Stern. 1999. Domain adduced state tying for cross-domain acoustic modelling. Abstract: In situations when automatic speech recognition (ASR) systems are rapidly deployed for a new task, the availability of within-domain training data may be limited. In such cases one needs to build the ASR system from other, possibly out-of-domain databases. We refer to the process of building ASR systems for one task domain using data from other domains as cross-domain modelling or CDM. Conventional CDM-based systems perform poorly because the disparity between the triphonetic distributions of the training and test domains is not well accounted for. In this paper we describe two techniques to impose the acousticphonetic structure of the task domain on acoustic models built from out-of-domain data. The first technique, called Extrinsic CDM, combines decision tree structures obtained from a database close in domain to the task domain with acoustic models that are trained from a third less domain-relevant database. In the second technique, called Intrinsic CDM, the task domain data is used to impose the triphonetic distribution of the task domain on the decision trees built from an out-of-domain large database. Both these techniques result in acoustic models which perform better than conventional CDM models.
- R. Stern, B. Raj, P. Moreno. 1999. COMPENSATION FOR ENVIRONMENTAL DEGRADATION IN AUTOMATIC SPEECH RECOGNITION. Abstract: The accuracy of speech recognition systems degrades when operated in adverse acoustical environments. This paper reviews various methods by which more detailed mathematical descriptions of the effects of environmental degradation can improve speech recognition accuracy using both “data-driven” and “model-based” compensation strategies. Data-driven methods learn environmental characteristics through direct comparisons of speech recorded in the noisy environment with the same speech recorded under optimal conditions. Model-based methods use a mathematical model of the environment and attempt to use samples of the degraded speech to estimate model parameters. These general approaches to environmental compensation are discussed in terms of recent research in environmental robustness at CMU, and in terms of similar efforts at other sites. These compensation algorithms are evaluated in a series of experiments measuring recognition accuracy for speech from the ARPA Wall Street Journal database that is corrupted by artificially-added noise at various signal-to-noise ratios (SNRs), and in more natural speech recognition tasks.
- Rita Singh, B. Raj, R. Stern. 1999. Automatic clustering and generation of contextual questions for tied states in hidden Markov models. Abstract: Most current automatic speech recognition systems based on HMMs cluster or tie together subsets of the subword units with which speech is represented. This tying improves the recognition accuracy when systems are trained with limited data, and is performed by classifying the sub-phonetic units using a series of binary tests based on speech production, called "linguistic questions". This paper describes a new method for automatically determining the best combinations of subword units to form these questions. The hybrid algorithm proposed clusters state distributions of context-independent phones to obtain questions for triphonetic contexts. Experiments confirm that the questions thus generated can replace manually generated questions and can provide improved recognition accuracy. Automatic generation of questions has the additional important advantage of extensibility to languages for which the phonetic structure is not well understood by the system designer, and can be effectively used in situations where the subword units are not phonetically motivated.
- B. Raj, Rita Singh, R. Stern. 1998. Inference of missing spectrographic features for robust speech recognition. Abstract: Two types of algorithms are introduced that recover missing time-frequency regions of log-spectral representations of speech. These compensation algorithms modify the incoming feature vector without any changes to the speech recognition system, in contrast to previously-described approaches. The first approach clusters the log-spectral vectors representing clean speech. Missing data are recovered by estimating the spectral cluster in each analysis frame on the basis of the feature values that are present. The second approach uses MAP procedures to estimate the values of missing data elements based on their correlation with the features that are present. Greatest recognition accuracy was obtained using the correlation-based approach, presumably because of its ability to exploit the temporal as well as spectral structure of speech. The recognition accuracy provided by these algorithms approaches but does not exceed that obtained by traditional marginalization. Nevertheless, it is believed that these algorithms provide greater computational efficiency and enable greater flexibility in recognition system structure.
- M. Siegler, Uday Jain, B. Raj, R. Stern. 1997. Automatic Segmentation, Classification and Clustering of Broadcast News Audio. Abstract: Automatic recognition of broadcast feeds from radio and television sources has been gaining importance recently, especially with the success of systems such as the CMU Informedia system [1]. In this work we describe the problems faced in adapting a system built to recognize one utterance at a time to a task that requires recognition of an entire half hour show. We break the problem into three components: segmentation, classification, and clustering. We show that a priori knowledge of acoustic conditions and speakers in the broadcast data is not required for segmentation. The system is able to detect changes in acoustics, recognize previously observed conditions, and use this to pool adaptation data. We also describe a novel application of the Symmetric Kullback-Leibler distance metric that is used as a single solution to both the segmentation and clustering problems. The three components are evaluated through comparisons between the Partitioned and Unpartitioned components of the 1996 ARPA Hub 4 evaluation test set.
- B. Raj, V. Parikh, R. Stern. 1997. The effects of background music on speech recognition accuracy. Abstract: Recognition of broadcast data, such as TV and radio programs is a topic of great interest. One of the problems with such data is the frequent presence of background music that degrades the performance of speech recognition systems. In this paper we examine the effects of different kinds of music on automatic speech recognition systems by comparing the effects of music with the relatively well-known effects of white noise on these systems. We also examine the extent to which compensation algorithms that have been successfully applied to noisy speech are also helpful in improving recognition accuracy for speech that is corrupted by music. It is hoped that these experimental comparisons will lead to a better understanding of how to compensate for the effects of background music.
- P. Placeway, Scotte Chen, M. Eskénazi, Uday Jain, V. Parikh, B. Raj, M. Ravishankar, R. Rosenfeld, K. Seymore, M. Siegler, R. Stern, Eric H. Thayer. 1997. The 1996 Hub-4 Sphinx-3 System. Abstract: This paper describes the CMU Sphinx-3 system, and the configuration we used for the 1996 DARPA (Hub-4) evaluation. The model structure, acoustic modeling, language modeling, lexical modeling, and system structure are summarized. We also discuss the experimental results obtained with this system on the most recent DARPA evaluation, and some subsequent results are also discussed.
- K. Seymore, Stanley F. Chen, Sam-joo Doh, M. Eskénazi, B. Raj, M. Ravishankar, Roni Rosenfeld, M. Siegler, R. Stern, Eric H. Thayer. 1997. The 1997 CMU Sphinx-3 English Broadcast News Transcription System. Abstract: This paper describes the 1997 Hub-4 Broadcast News Sphinx3 speech recognition system. This year’s system includes fullbandwidth acoustic models trained on Broadcast News and Wall Street Journal acoustic training data, an expanded vocabulary, and a 4-gram language model for N-best list rescoring. The system structure, acoustic and language models, and adaptation components are described in detail, and results are presented to establish the contributions of multiple recognition passes. Additionally, experimental results are presented for several different acoustic and language model configurations.
- E. Gouvêa, P. Moreno, B. Raj, T. M. Sullivan, R. Stern. 1996. ADAPTATION AND COMPENSATION : APPROACHES TO MICR OPHONE AND SPEAKER INDEPENDENCE IN AUTOMATIC SPEECH RECOGNITION. Abstract: This paper describes recent efforts by the CMU speech group to address the important problems of robustness to changes in environment and speaker. Results are presented in the context of the 1995 ARPA common Hub 3 evaluation of speech recorded through different microphones at different signal-to-noise ratios (SNRs). For speech that is considered to be of high quality we addressed the problem of speaker variability through a speaker normalization technique. For speech recorded at lower SNRs, we used a combination of environmental compensation techniques previously developed in our group. Speaker normalization reduced the relative error rate for clean speech by 3.5 percent, and the combination of environmental compensation with the use of noise-corrupted speech in the training process reduced the relative error rate for noisy speech by 54.9 percent.
- B. Raj, E. Gouvêa, P. Moreno, R. Stern. 1996. Cepstral compensation by polynomial approximation for environment-independent speech recognition. Abstract: Speech recognition systems perform poorly on speech degraded by even simple effects such as linear filtering and additive noise. One possible solution to this problem is to modify the probability density function (PDF) of clean speech to account for the effects of the degradation. However, even for the case of linear filtering and additive noise, it is extremely difficult to do this analytically. Previously attempted analytical solutions to the problem of noisy speech recognition have either used an overly simplified mathematical description of the effects of noise on the statistics of speech, or they have relied on the availability of large environment specific adaptation sets. Some of the previous methods required the use of adaptation data that consists of simultaneously recorded or "stereo" recordings of clean and degraded speech. We introduce an approximation based method to compute the effects of the environment on the parameters of the PDF of clean speech. We perform compensation by vector polynomial approximations (VPS) for the effects of linear filtering and additive noise on the clean speech. We also estimate the parameters of the environment, namely the noise and the channel, by using piecewise linear approximations of these effects. We evaluate the performance of this method (VPS) using the CMU SPHINX-II system and the 100 word alphanumeric CENSUS database. Performance is evaluated at several SNRs, with artificial white Gaussian noise added to the database. VPS provides improvements of up to 15 percent in relative recognition accuracy.
- P. Moreno, B. Raj, R. Stern. 1996. A vector Taylor series approach for environment-independent speech recognition. Abstract: In this paper we introduce a new analytical approach to environment compensation for speech recognition. Previous attempts at solving analytically the problem of noisy speech recognition have either used an overly-simplified mathematical description of the effects of noise on the statistics of speech or they have relied on the availability of large environment-specific adaptation sets. Some of the previous methods required the use of adaptation data that consists of simultaneously-recorded or "stereo" recordings of clean and degraded speech. In this work we introduce the use of a vector Taylor series (VTS) expansion to characterize efficiently and accurately the effects on speech statistics of unknown additive noise and unknown linear filtering in a transmission channel. The VTS approach is computationally efficient. It can be applied either to the incoming speech feature vectors, or to the statistics representing these vectors. In the first case the speech is compensated and then recognized; in the second case HMM statistics are modified using the VTS formulation. Both approaches use only the actual speech segment being recognized to compute the parameters required for environmental compensation. We evaluate the performance of two implementations of VTS algorithms using the CMU SPHINX-II system on the 100-word alphanumeric CENSUS database and on the 1993 5000-word ARPA Wall Street Journal database. Artificial white Gaussian noise is added to both databases. The VTS approaches provide significant improvements in recognition accuracy compared to previous algorithms.
- R. Stern, P. Moreno, B. Raj. 1996. Compensation for speech recognition in degraded acoustical environments. Abstract: The accuracy of speech recognition systems degrades when operated in adverse acoustical environments. This paper discusses two ways in which more detailed mathematical descriptions of the effects of environmental degradation can improve speech recognition accuracy using both ‘‘data‐driven’’ and ‘‘model‐based’’ compensation strategies. Data‐driven methods learn environmental characteristics through direct comparisons of speech recorded in the noisy environment with the same speech recorded under optimal conditions. Model‐based methods use a mathematical model of the environment and attempt to use samples of the degraded speech to estimate model parameters. Two approaches to data‐driven compensation, RATZ and STAR, are described, as well as a new approach to model‐based compensation, referred to as the vector Taylor series (VTS) algorithm. Compensation algorithms are evaluated in a series of experiments measuring recognition accuracy for speech from the ARPA Wall Street Journal database that is corrupted by...
- Uday Jain, M. Siegler, Sam-joo Doh, E. Gouvêa, J. Huerta, P. Moreno, B. Raj, R. Stern. 1995. RECOGNITION OF CONTINUOUS BROADCAST NEWS WITH MULTIPLE UNKNOWN SPEAKERS AND ENVIRONMENTS. Abstract: Practical applications of continuous speech recognition in realistic environments place increasing demands for speaker and environment independence. Until recently, this robustness has been measured using evaluation procedures where speaker and environment boundaries are known, with utterances containing complete or nearly complete sentences. This paper describes recent efforts by the CMU speech group to improve the recognition of speech found in long sections of the broadcast news show Marketplace. Most of our effort was concentrated in two areas: the automatic segmentation and classification of environments, and the construction of a suitable lexicon and language model. We review the extensions to SPHINX-II that were necessary to enable it to process continuous broadcast news and we compare the recognition accuracy of the SPHINX-II system for different environmental and speaker conditions.
- P. Moreno, B. Raj, E. Gouvêa, R. Stern. 1995. Multivariate-Gaussian-based cepstral normalization for robust speech recognition. Abstract: We introduce a new family of environmental compensation algorithms called multivariate gaussian based cepstral normalization (RATZ). RATZ assumes that the effects of unknown noise and filtering on speech features can be compensated by corrections to the mean and variance of components of Gaussian mixtures, and an efficient procedure for estimating the correction factors is provided. The RATZ algorithm can be implemented to work with or without the use of "stereo" development data that had been simultaneously recorded in the training and testing environments. "Blind" RATZ partially overcomes the loss of information that would have been provided by stereo training through the use of a more accurate description of how noisy environments affect clean speech. We evaluate the performance of the two RATZ algorithms using the CMU SPHINX-II system on the alphanumeric census database and compare their performance with that of previous environmental-robustness developed at CMU.
- P. Moreno, B. Raj, R. Stern. 1995. APPROACHES TO ENVIRONMENT COMPENSATION IN AUTOMATIC SPEECH RECOGNITION. Abstract: This paper describes a series of cepstral-based compensation procedures that render the SPHINX-II continuous speech recognition system more robust with respect to acoustical changes in the environment. The first two algorithms, SNR based MultivaRiate gAussian based cepsTral normaliZation (SNR-based RATZ) and STAtistical Reestimation of HMMs (STAR), compensate for environmental degradation based on comparisons of simultaneouslyrecorded data in the training and testing environments (“stereo data”). They differ in that RATZ modifies the incoming feature vectors to a recognition system while STAR modifies the internal representation of speech by the system. We also describe N-CDCN, an improved version of codeword-dependent cepstral normalization (CDCN) which does not require stereo training data but nevertheless achieves performance levels comparable to RATZ and other algorithms that require stereo training. Use of these compensation algorithms significantly reduces the error rates for SPHINX-II. The algorithms are tested in a variety of databases and environmental conditions. INTRODUCTION Robustness with respect to environmental variability remains a continuing problem for speech recognition technology (e.g. [3]). For example, the use of microphones other than the ARPA standard Sennheiser HM-414 headset (CLSTLK) severely degrades the performance of speech recognition systems like the SPHINX-II, even in relatively quiet environments [1, 4]. Traditional algorithms to compensate for environmental variation have either relied on the availability of simultaneously-recorded data in the training and testing environments (“stereo data”), or have utilized structural models to define the degradation. For example, multiple fixed codeword-dependent cepstral normalization (MFCDCN) [4] uses stereo data to compute correction vectors to compensate for the effects of the environment. Dual-channel codebook adaptation (DCCA) [4], on the other hand, modifies the statistical representation used in the HMMs for speech on the basis of comparisons obtained from stereo data. The other approach to environmental compensation is through the use of structural models of degradation. On the other hand, codeword-dependant cepstral normalization (CDCN) [1] assumes that speech is degraded by unknown additive noise and unknown linear filtering. It makes use of expectationmaximization (EM) techniques to determine the parameters characterizing these distortions. In this paper we describe three new cepstral-domain compensation strategies, SNR based MultivaRiate gAussian based cepsTral normaliZation (SNR-based RATZ), STAtistical Reestimation of HMMs (STAR), and new CDCN (NCDCN). SNR-based RATZ and STAR both make use of stereo data. They differ in that RATZ modifies the incoming feature vectors to a recognition system while STAR modifies the internal representation of speech by the system. RATZ and STAR are similar in philosophy to MFCDCN and DCCA, respectively [4], but they achieve improved performance through the use of better mathematical models which introduce strong structural constraints into the assumed distribution for speech. N-CDCN is a modification and improvement of the original CDCN algorithm, which is based on a structural model of degradation. EFFECT OF THE ENVIRONMENT ON SPEECH STATISTICS In this section we describe how even well-behaved environments, such as those that can be modeled by unknown linear filtering and additive stationary noise, modify the statistics of “clean” speech in very unpredictable ways. Even though we can formulate equations that analytically describe how the pdfs of clean speech change, the solutions for these equations are mathematically intractable. For analytical purposes, we adopt the simple model of degradation proposed by Acero [1]. In this model, degraded speech is characterized by passing high-quality clean speech through a linear filter and contaminating the filtered output by additive stationary noise. For simplicity, we will also assume that the feature vector is unidimensional, APPROACHES TO ENVIRONMENT COMPENSATION IN AUTOMATIC SPEECH RECOGNITION Pedro J. Moreno, Bhiksha Raj, Richard M. Stern Department of Electrical and Computer Engineering and School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, U.S.A. although all conclusions developed can be easily extended to an arbitrary N-dimensional space such as the log spectral domain. The degraded speech can be characterized as: (1) where represents the power spectrum of the degraded speech, is the power spectrum of the clean speech, is the transfer function of the linear filter, and is the power spectrum of the additive noise. In the logspectral domain this relation can be expressed as:
- P. Moreno, B. Raj, R. Stern. 1995. A unified approach for robust speech recognition. Abstract: There are two major structural approaches to robust speech recognition. In the first approach to the problem, compensation is performed by modifying the incoming cepstral stream using ML or MMSE methods to estimate parameters characterizing environmental degradation, from direct frame-by-frame comparisons between speech recorded in high-quality and degraded acoustical environments, or by signal processing techniques such as spectral subtraction. The second approach tackles the problem by modifying the statistics of the internal representation of speech cepstra in the classifier to make them more closely resemble the statistics of degraded speech. This paper attempts to unify these approaches to robust speech recognition by presenting three techniques that share the same basic assumptions and internal structure but differ in whether they modify the incoming speech cepstra or whether they modify the classifier statistics. We present SNR-dependent multivaRiate gAussian-based cepsTral normaliZation (SNR-RATZ) and SNR-based Blind RATZ (SNR-BRATZ), which modify incoming cepstra, along with STAR (STAtistical Re-estimation), which modifies the internal statistics of the classifier. The algorithms were tested using the SPHINX-II speech recognition system on the CENSUS database, a database of strings of letters and numbers to which unknown added and unknown linear filtering was introduced artificially. While all the algorithms showed good performance, STAR was observed to provide lower error rates as SNR decreases than any of the algorithms that modify incoming cepstra.
- Dhananjay Bansal, B. Raj, P. Smaragdis. None. EXPANSION OF NARROWBAND S TRIX FACTORIZA. Abstract: In this paper, we present a novel technique for the estimation of the high frequency components (4-8kHz) of speech signals from narrow-band (0-4 kHz) signals using convolutive Non-Negative Matrix Factorisation (NMF). The proposed technique utilizes a brief recording of simultaneous broad band and narrow band signals from a target speaker to learn a set of broad-band non-negative "bases" for the speaker. The low-frequency components of these bases are used to determine how the high-frequency components must be combined in order to reconstruct the high-frequency components of new narrow-band signals from the speaker. Experiments reveal that the technique is able to reconstruct broadband speech that is perceptually virtually indistinguishable from true broadband recordings.
- K. Kumatani, J. McDonough, B. Raj. None. Fundamental Technologies in Modern Speech Recognition Distant Speech Recognition Holds the Promise of the Most Natural Human Computer Interface Because It Enables Man-machine Interactions through Speech. Abstract: D istant speech recognition (DSR) holds the promise of the most natural human computer interface because it enables man-machine interactions through speech, without the necessity of donning intrusive body-or head-mounted microphones. Recognizing distant speech robustly, however, remains a challenge. This contribution provides a tutorial overview of DSR systems based on microphone arrays. In particular, we present recent work on acoustic beamforming for DSR, along with experimental results verifying the effectiveness of the various algorithms described here; beginning from a word error rate (WER) of 14.3% with a single microphone of a linear array, our state-of-the-art DSR system achieved a WER of 5.3%, which was comparable to that of 4.2% obtained with a lapel microphone. Moreover, we present an emerging technology in the area of far-field audio and speech processing based on spherical microphone arrays. Performance comparisons of spherical and linear arrays reveal that a spherical array with a diameter of 8.4 cm can provide recognition accuracy comparable or better than that obtained with a large linear array with an aperture length of 126 cm. INTRODUCTION When the signals from the individual sensors of a microphone array with a known geometry are suitably combined, the array [ From close-talking microphones to far-field sensors ] functions as a spatial filter capable of suppressing noise, reverberation, and competing speech. Such beamforming techniques have received a great deal of attention within the acoustic array processing community in the recent past [1]– [7]. Despite this effort, however, such techniques have often been ignored within the mainstream community working on DSR. As pointed out in [6] and [7], this could be due to the fact that the disparate research communities for acoustic array processing and automatic speech recognition (ASR) have failed to adopt each other's best practices. For instance, the array processing community tends to ignore speaker adaptation techniques, which can compensate for mismatches between acoustic conditions during training and testing. Moreover, this community has largely preferred to work on controlled, synthetic recordings, obtained by convolving noise-and reverberation-free speech with measured, static room impulse responses, with subsequent artificial addition of which featured actual array recordings of real speakers; this task, however , has fallen out of favor, to the extent that it is currently not even mentioned on the PASCAL CHiME Challenge Web site, nor in any of the concomitant publications. This is unfortunate because improvements obtained with novel speech enhancement techniques tend to diminish, or …
- B. Raj, E. Gouvêa, R. Stern. None. Ble Mixture Gaussian Distributions with the following Means and Variances: 4.2. Performance of Ratz on Speech Recogni- Tion in Noise 5. Summary Acknowledgements -0.2 0.2 0.4 0.6 0.8 1.0 1.2. Abstract: The data were contaminated by artificial noise which had a two-dimensional Gaussian distribution with a mean value of 0.50 and a variance of 0.001 for both dimensions. The covariance matrix was diagonal. Figure 3 compares the effectiveness of both algorithms in estimating clean speech vectors in the presence of this noise. The filled circles in the two panels of Figure 3 indicate the locations of the clean speech vectors, and the filled squares indicate the locations of the noisy speech before compensation was applied. The dashed arrows and continuous arrows indicate the corrections provided by the stereo RATZ and blind RATZ algorithms, respectively. It can be seen that blind RATZ provides a compensation that is almost as complete as that provided by stereo RATZ. Similar results have been observed on real speech statistics. The effectiveness of the RATZ algorithms was evaluated using the CMU census database [1], a continuous speaker-independent database consisting of strings of letters, numbers, and a few control words with a total vocabulary size of 107 words. The training set consisted of 1018 sentences stereo recorded over a noise-canceling close-talking microphone (CLSTK) and the desktop Crown-PZM6FS microphone (CRPZM). The testing set consisted of 140 stereo-recorded sentences. The adaptation set used by all compensation algorithms had a size of 400 stereo sentences randomly chosen from the training set. The SPHINX-II continuous speech recognition system was used. In Table 1 we compare the recognition error rate of several versions of RATZ to the error rates of previous algorithms developed at CMU [1,2]. The system was trained on clean speech and the adaptation set was used to learn the noisy speech distributions. Compensation algorithms were applied to the noisy data before recognition. A recognition error rate of 12.4% was achieved using the CLSTK microphone with no compensation. It can be seen that the RATZ algorithms perform better than virtually all of the previous CMU algorithms. We also note that accounting for the change in the variance improves performance. Finally, blind RATZ, which does not make explicit use of the stereo training data performs almost as well as stereo RATZ, and performs better that some of the previous algorithms that use stereo training, such as SDCN. In this paper we introduce a new family of algorithms to deal with the problem of speech recognition in noisy environments. We analyze the effects of noise on the statistics of common speech features , …
- B. Schmidt-Nielsen, B. Harsham, B. Raj. None. Speech-Based UI Design for the Automobile. Abstract: In this chapter we discuss a variety of topics relating to speech-based user interfaces for use in an automotive environment. We begin by presenting a number of design principles for the design of such interfaces, derived from several decades of combined experience in the development and evaluation of spoken user interfaces (UI) for automobiles, along with three case studies of current automotive navigation interfaces. Finally, we present a new model for speech-based user interfaces in automotive environments that recasts the goal of the UI from supporting the navigation among and selection from multiple states to that of selecting the desired command from a short list. We also present experimental evidence that UIs based on this approach can impose significantly lower cognitive load on a driver than conventional UIs. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c © Mitsubishi Electric Research Laboratories, Inc., 2008 201 Broadway, Cambridge, Massachusetts 02139 Handbook of Research on User Interface Design and Evaluation for Mobile Technology Hershey • New York InformatIon scIence reference Volume I Joanna Lumsden National Research Council of Canada Institute for Information Technology – e-Business, Canada Acquisitions Editor: Kristin Klinger Development Editor: Kristin Roth Senior Managing Editor: Jennifer Neidig Managing Editor: Sara Reed Copy Editor: Joy Langel, Katie Smalley, and Angela Thor Typesetter: Jeff Ash Cover Design: Lisa Tosheff Printed at: Yurchak Printing Inc. Published in the United States of America by Information Science Reference (an imprint of IGI Global) 701 E. Chocolate Avenue, Suite 200
