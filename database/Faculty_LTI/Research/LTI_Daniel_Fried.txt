Daniel Fried
Paper count: 50
- Jiefu Ou, Benno Krojer, Daniel Fried. 2023. Pragmatic Inference with a CLIP Listener for Contrastive Captioning. Abstract: We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity - outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations
- Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas Muennighoff, Mayank Mishra, A. Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier, Hailey Schoelkopf, S. Troshin, Dmitry Abulkhanov, M. Romero, M. Lappert, F. Toni, Bernardo Garc'ia del R'io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, I. Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, D. Lansky, Huu Nguyen, Danish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, S. Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra. 2023. SantaCoder: don't reach for the stars!. Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.
- Jing Yu Koh, R. Salakhutdinov, Daniel Fried. 2023. Grounding Language Models to Images for Multimodal Generation. Abstract: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an ef-fective, general solution for leveraging pretrained language models in visually grounded settings.
- Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, J. Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, J. Stillerman, S. Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, N. Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, M. Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, A. Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries. 2023. StarCoder: may the source be with you!. Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.
- Jing Yu Koh, R. Salakhutdinov, Daniel Fried. 2023. Grounding Language Models to Images for Multimodal Inputs and Outputs. Abstract: We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
- Jing Yu Koh, Daniel Fried, R. Salakhutdinov. 2023. Generating Images with Multimodal Language Models. Abstract: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.
- Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig. 2023. WebArena: A Realistic Web Environment for Building Autonomous Agents. Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
- Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig. 2022. Execution-Based Evaluation for Open-Domain Code Generation. Abstract: To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling -- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.
- Daniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Patel, Aida Nematzadeh. 2022. Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches. Abstract: People rely heavily on context to enrich meaning beyond what is literally said, enabling concise but effective communication. To interact successfully and naturally with people, user-facing artificial intelligence systems will require similar skills in pragmatics: relying on various types of context -- from shared linguistic goals and conventions, to the visual and embodied world -- to use language effectively. We survey existing grounded settings and pragmatic modeling approaches and analyze how the task goals, environmental contexts, and communicative affordances in each work enrich linguistic meaning. We present recommendations for future grounded task design to naturally elicit pragmatic phenomena, and suggest directions that focus on a broader range of communicative contexts and affordances.
- Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, M. Lewis, Wen-tau Yih, Daniel Fried, Sida I. Wang. 2022. Coder Reviewer Reranking for Code Generation. Abstract: Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.
- Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Choi. 2022. Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. Abstract: Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models’ ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.
- Weiyan Shi, Emily Dinan, Adithya Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, M. Lewis. 2022. AutoReply: Detecting Nonsense in Dialogue Introspectively with Discriminative Replies. Abstract: Existing approaches built separate classifiers to detect nonsense in dialogues. In this paper, we show that without external classifiers, dialogue models can detect errors in their own messages introspectively, by calculating the likelihood of replies that are indicative of poor messages. For example, if an agent believes its partner is likely to respond"I don't understand"to a candidate message, that message may not make sense, so an alternative message should be chosen. We evaluate our approach on a dataset from the game Diplomacy, which contains long dialogues richly grounded in the game state, on which existing models make many errors. We first show that hand-crafted replies can be effective for the task of detecting nonsense in applications as complex as Diplomacy. We then design AutoReply, an algorithm to search for such discriminative replies automatically, given a small number of annotated dialogue examples. We find that AutoReply-generated replies outperform handcrafted replies and perform on par with carefully fine-tuned large supervised models. Results also show that one single reply without much computation overheads can also detect dialogue nonsense reasonably well.
- Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, Sida I. Wang. 2022. Natural Language to Code Translation with Execution. Abstract: Generative models of code, pretrained on large corpora of programs, have shown great success in translating natural language to code (Chen et al., 2021; Austin et al., 2021; Li et al., 2022, inter alia). While these models do not explicitly incorporate program semantics (i.e., execution results) during training, they are able to generate correct solutions for many problems. However, choosing a single correct program from a generated set for each problem remains challenging. In this work, we introduce execution result–based minimum Bayes risk decoding (MBR-EXEC) for program selection and show that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks. We select output programs from a generated candidate set by marginalizing over program implementations that share the same semantics. Because exact equivalence is intractable, we execute each program on a small number of test inputs to approximate semantic equivalence. Across datasets, execution or simulated execution significantly outperforms the methods that do not involve program semantics. We find that MBR-EXEC consistently improves over all execution-unaware selection methods, suggesting it as an effective approach for natural language to code translation.
- Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, M. Lewis. 2022. InCoder: A Generative Model for Code Infilling and Synthesis. Abstract: Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models
- Grace Luo, Giscard Biamby, Trevor Darrell, Daniel Fried, Anna Rohrbach. 2022. G^3: Geolocation via Guidebook Grounding. Abstract: We demonstrate how language can improve geolocation: the task of predicting the location where an image was taken. Here we study explicit knowledge from human-written guidebooks that describe the salient and class-discriminative visual features humans use for geolocation. We propose the task of Geolocation via Guidebook Grounding that uses a dataset of StreetView images from a diverse set of locations and an associated textual guidebook for GeoGuessr, a popular interactive geolocation game. Our approach predicts a country for each image by attending over the clues automatically extracted from the guidebook. Supervising attention with country-level pseudo labels achieves the best performance. Our approach substantially outperforms a state-of-the-art image-only geolocation method, with an improvement of over 5% in Top-1 accuracy. Our dataset and code can be found at https://github.com/g-luo/geolocation_via_guidebook_grounding.
- Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, M. Lewis. 2022. Contrastive Decoding: Open-ended Text Generation as Optimization. Abstract: Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.
- Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, S. Yih, Daniel Fried, Si-yi Wang, Tao Yu. 2022. DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation. Abstract: We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.
- Jessy Lin, Daniel Fried, D. Klein, A. Dragan. 2022. Inferring Rewards from Language in Context. Abstract: In classic instruction following, language like “I’d like the JetBlue flight” maps to actions (e.g., selecting that flight). However, language also conveys information about a user’s underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences. On a new interactive flight–booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that first maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning).
- Daniel Fried, Nicholas Tomlin, Jennifer Hu, Roma Patel, Aida Nematzadeh. 2022. Pragmatics in Grounded Language Learning: Phenomena, Tasks, and Modeling Approaches. Abstract: People rely heavily on context to enrich meaning beyond what is literally said, enabling concise but effective communication. To interact successfully and naturally with people, user-facing artiﬁcial intelligence systems will require similar skills in pragmatics : relying on various types of context—from shared linguistic goals and conventions, to the visual and embodied world—to use language effectively. We survey existing grounded settings and pragmatic modeling approaches and analyze how the task goals, environmental contexts, and communicative affordances in each work enrich linguistic meaning. We present recom-mendations for future grounded task design to naturally elicit pragmatic phenomena, and suggest directions that focus on a broader range of communicative contexts and affordances.
- A. Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, S. Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David J. Wu, Hugh Zhang, Markus Zijlstra. 2022. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Abstract: Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game. Description AI masters Diplomacy The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players’ motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. —YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.
- David Gaddy, Daniel Fried, Nikita Kitaev, Mitchell Stern, Rodolfo Corona, John DeNero, D. Klein. 2021. Interactive Assignments for Teaching Structured Neural NLP. Abstract: We present a set of assignments for a graduate-level NLP course. Assignments are designed to be interactive, easily gradable, and to give students hands-on experience with several key types of structure (sequences, tags, parse trees, and logical forms), modern neural architectures (LSTMs and Transformers), inference algorithms (dynamic programs and approximate search) and training methods (full and weak supervision). We designed assignments to build incrementally both within each assignment and across assignments, with the goal of enabling students to undertake graduate-level research in NLP by the end of the course.
- Rodolfo Corona, Daniel Fried, Coline Devin, D. Klein, Trevor Darrell. 2021. Modular Networks for Compositional Instruction Following. Abstract: Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, non-modular sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.
- Daniel Fried, Justin T Chiu, D. Klein. 2021. Reference-Centric Models for Grounded Collaborative Dialogue. Abstract: We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner’s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous state of the art for the task, obtaining a 20% relative improvement in successful task completion in self-play evaluations and a 50% relative improvement in success in human evaluations.
- Daniel Fried. 2021. Learning Grounded Pragmatic Communication. Abstract: Learning Grounded Pragmatic Communication
- Sheng Shen, Daniel Fried, Jacob Andreas, D. Klein. 2019. Pragmatically Informative Text Generation. Abstract: We improve the informativeness of models for conditional text generation using techniques from computational pragmatics. These techniques formulate language production as a game between speakers and listeners, in which a speaker should generate output text that a listener can use to correctly identify the original input that the text describes. While such approaches are widely used in cognitive science and grounded language learning, they have received less attention for more standard language generation tasks. We consider two pragmatic modeling methods for text generation: one where pragmatics is imposed by information preservation, and another where pragmatics is imposed by explicit modeling of distractors. We find that these methods improve the performance of strong existing systems for abstractive summarization and generation from structured meaning representations.
- Daniel Fried, Nikita Kitaev, D. Klein. 2019. Cross-Domain Generalization of Neural Constituency Parsers. Abstract: Neural parsers obtain state-of-the-art results on benchmark treebanks for constituency parsing—but to what degree do they generalize to other domains? We present three results about the generalization of neural parsers in a zero-shot setting: training on trees from one corpus and evaluating on out-of-domain corpora. First, neural and non-neural parsers generalize comparably to new domains. Second, incorporating pre-trained encoder representations into neural parsers substantially improves their performance across all domains, but does not give a larger relative improvement for out-of-domain treebanks. Finally, despite the rich input representations they learn, neural parsers still benefit from structured output prediction of output trees, yielding higher exact match accuracy and stronger generalization both to larger text spans and to out-of-domain corpora. We analyze generalization on English and Chinese corpora, and in the process obtain state-of-the-art parsing results for the Brown, Genia, and English Web treebanks.
- Ronghang Hu, Daniel Fried, Anna Rohrbach, D. Klein, Trevor Darrell, Kate Saenko. 2019. Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation. Abstract: Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.
- Daniel Fried, D. Klein. 2018. Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing. Abstract: Dynamic oracles provide strong supervision for training constituency parsers with exploration, but must be custom defined for a given parser’s transition system. We explore using a policy gradient method as a parser-agnostic alternative. In addition to directly optimizing for a tree-level metric such as F1, policy gradient has the potential to reduce exposure bias by allowing exploration during training; moreover, it does not require a dynamic oracle for supervision. On four constituency parsers in three languages, the method substantially outperforms static oracle likelihood training in almost all settings. For parsers where a dynamic oracle is available (including a novel oracle which we define for the transition system of Dyer et al., 2016), policy gradient typically recaptures a substantial fraction of the performance gain afforded by the dynamic oracle.
- Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, D. Klein, Trevor Darrell. 2018. Speaker-Follower Models for Vision-and-Language Navigation. Abstract: Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.
- Mitchell Stern, Daniel Fried, D. Klein. 2017. Effective Inference for Generative Neural Parsing. Abstract: Generative neural models have recently achieved state-of-the-art results for constituency parsing. However, without a feasible search procedure, their use has so far been limited to reranking the output of external parsers in which decoding is more tractable. We describe an alternative to the conventional action-level beam search used for discriminative neural models that enables us to decode directly in these generative models. We then show that by improving our basic candidate selection strategy and using a coarse pruning function, we can improve accuracy while exploring significantly less of the search space. Applied to the model of Choe and Charniak (2016), our inference procedure obtains 92.56 F1 on section 23 of the Penn Treebank, surpassing prior state-of-the-art results for single-model systems.
- Daniel Fried, Mitchell Stern, D. Klein. 2017. Improving Neural Parsing by Disentangling Model Combination and Reranking Effects. Abstract: Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.
- Daniel Fried, Jacob Andreas, D. Klein. 2017. Unified Pragmatic Models for Generating and Following Instructions. Abstract: We show that explicit pragmatic inference aids in correctly generating and following natural language instructions for complex, sequential tasks. Our pragmatics-enabled models reason about why speakers produce certain instructions, and about how listeners will react upon hearing them. Like previous pragmatic models, we use learned base listener and speaker models to build a pragmatic speaker that uses the base listener to simulate the interpretation of candidate descriptions, and a pragmatic listener that reasons counterfactually about alternative descriptions. We extend these models to tasks with sequential structure. Evaluation of language generation and interpretation shows that pragmatic inference improves state-of-the-art listener models (at correctly interpreting human instructions) and speaker models (at producing instructions correctly interpreted by humans) in diverse settings.
- Dane Bell, Daniel Fried, Luwen Huangfu, M. Surdeanu, S. Kobourov. 2016. Challenges for using social media for early detection of T2DM. Abstract: Twitter and other social media data are utilized for a wide variety of applications such as marketing and stock market prediction. Each application and appropriate domain of social media text presents its own challenges and benefits. We discuss methods for detecting obesity, a risk factor for Type II Diabetes Mellitus (T2DM), from the language of food on Twitter on community data, the peculiarities of this data, and the development of individual-level data for this task.
- Dane Bell, Daniel Fried, Luwen Huangfu, M. Surdeanu, S. Kobourov. 2016. Towards Using Social Media to Identify Individuals at Risk for Preventable Chronic Illness. Abstract: We describe a strategy for the acquisition of training data necessary to build a social-media-driven early detection system for individuals at risk for (preventable) type 2 diabetes mellitus (T2DM). The strategy uses a game-like quiz with data and questions acquired semi-automatically from Twitter. The questions are designed to inspire participant engagement and collect relevant data to train a public-health model applied to individuals. Prior systems designed to use social media such as Twitter to predict obesity (a risk factor for T2DM) operate on entire communities such as states, counties, or cities, based on statistics gathered by government agencies. Because there is considerable variation among individuals within these groups, training data on the individual level would be more effective, but this data is difficult to acquire. The approach proposed here aims to address this issue. Our strategy has two steps. First, we trained a random forest classifier on data gathered from (public) Twitter statuses and state-level statistics with state-of-the-art accuracy. We then converted this classifier into a 20-questions-style quiz and made it available online. In doing so, we achieved high engagement with individuals that took the quiz, while also building a training set of voluntarily supplied individual-level data for future classification.
- Daniel Fried, Churchill. 2015. Low-rank tensor approximations for compositional distributional semantics. Abstract: This thesis explores compositional distributional semantics: methods for mapping words to feature vectors representing their meaning, and composing these word vectors to produce representations of the meanings of longer expressions such as phrases and sentences. Several compositional distributional semantic methods use matrices and their generalization, higher-order tensors, to model multi-way interactions between vectors. Unfortunately, the size of these higher-order tensors has been one obstacle to large-scale implementations of the compositional frameworks that would be able to produce representations for full-length sentences with a diverse vocabulary. In this work, we investigate whether we can match the performance of full matrices and tensors with low-rank approximations that use a fraction of the original number of parameters. We compare low-rank matrices and tensors to full, unconstrained-rank matrices and tensors on standard semantic similarity tasks for two syntactic constructions: adjectives represented by matrices, and transitive verbs represented by third-order tensors. Using lowrank approximations allows us to reduce the number of the parameters in the models by about 40% for matrices, and by 99% (two orders of magnitude) for the third-order tensors. Despite this reduction in the size of the models, the low-rank matrices and tensors achieve performance comparable to, and occasionally surpassing, the full models. The parameters of these low-rank representations can be optimized directly using standard gradient-based methods, allowing them to be incorporated into existing machine learning models for compositional distributional semantics.
- Daniel Fried, Peter Alexander Jansen, Gus Hahn-Powell, M. Surdeanu, Peter Clark. 2015. Higher-order Lexical Semantic Models for Non-factoid Answer Reranking. Abstract: Lexical semantic models provide robust performance for question answering, but, in general, can only capitalize on direct evidence seen during training. For example, monolingual alignment models acquire term alignment probabilities from semi-structured data such as question-answer pairs; neural network language models learn term embeddings from unstructured text. All this knowledge is then used to estimate the semantic similarity between question and answer candidates. We introduce a higher-order formalism that allows all these lexical semantic models to chain direct evidence to construct indirect associations between question and answer texts, by casting the task as the traversal of graphs that encode direct term associations. Using a corpus of 10,000 questions from Yahoo! Answers, we experimentally demonstrate that higher-order methods are broadly applicable to alignment and language models, across both word and syntactic representations. We show that an important criterion for success is controlling for the semantic drift that accumulates during graph traversal. All in all, the proposed higher-order approach improves five out of the six lexical semantic models investigated, with relative gains of up to +13% over their first-order variants.
- Daniel Fried, T. Polajnar, S. Clark. 2015. Low-Rank Tensors for Verbs in Compositional Distributional Semantics. Abstract: Several compositional distributional semantic methods use tensors to model multi-way interactions between vectors. Unfortunately, the size of the tensors can make their use impractical in large-scale implementations. In this paper, we investigate whether we can match the performance of full tensors with low-rank approximations that use a fraction of the original number of parameters. We investigate the effect of low-rank tensors on the transitive verb construction where the verb is a third-order tensor. The results show that, while the low-rank tensors require about two orders of magnitude fewer parameters per verb, they achieve performance comparable to, and occasionally surpassing, the unconstrained-rank tensors on sentence similarity and verb disam-
- Daniel Fried, M. Surdeanu, S. Kobourov, M. Hingle, Dane Bell. 2014. Analyzing the language of food on social media. Abstract: We investigate the predictive power behind the language of food on social media. We collect a corpus of over three million food-related posts from Twitter and demonstrate that many latent population characteristics can be directly predicted from this data: overweight rate, diabetes rate, political leaning, and home geographical location of authors. For all tasks, our language-based models significantly outperform the majority-class baselines. Performance is further improved with more complex natural language processing, such as topic modeling. We analyze which textual features have greatest predictive power for these datasets, providing insight into the connections between the language of food, geographic locale, and community characteristics. Lastly, we design and implement an online system for real-time query and visualization of the dataset. Visualization tools, such as geo-referenced heatmaps and temporal histograms, allow us to discover more complex, global patterns mirrored in the language of food.
- Daniel Fried, Kevin Duh. 2014. Incorporating Both Distributional and Relational Semantics in Word Representations. Abstract: We investigate the hypothesis that word representations ought to incorporate both distributional and relational semantics. To this end, we employ the Alternating Direction Method of Multipliers (ADMM), which flexibly optimizes a distributional objective on raw text and a relational objective on WordNet. Preliminary results on knowledge base completion, analogy tests, and parsing show that word representations trained on both objectives can give improvements in some cases.
- Daniel Fried, Zhen Li, A. Jannesari, F. Wolf. 2013. Predicting Parallelization of Sequential Programs Using Supervised Learning. Abstract: We investigate an automatic method for classifying which regions of sequential programs could be parallelized, using dynamic features of the code collected at runtime. We train a supervised learning algorithm on versions of the NAS Parallel Benchmark (NPB) code hand-annotated with OpenMP parallelization directives in order to approximate the parallelization that might be produced by a human expert. A model comparison shows that support vector machines and decision trees have comparable performance on this classification problem, but boosting using AdaBoost is able to increase the performance of the decision trees. We further analyze the relative importance of the collected program features and demonstrate that within-loop instruction counts provide the greatest contribution to decision tree error reduction, with dependency graph features of secondary importance.
- C. Dawson, Jeremy B. Wright, Antons Rebguns, M. A. Valenzuela-Escarcega, Daniel Fried, P. Cohen. 2013. A generative probabilistic framework for learning spatial language. Abstract: The language of space and spatial relations is a rich source of abstract semantic structure. We develop a probabilistic model that learns to understand utterances that describe spatial configurations of objects in a tabletop scene by seeking the meaning that best explains the sentence chosen. The inference problem is simplified by assuming that sentences express symbolic representations of (latent) semantic relations between referents and landmarks in space, and that given these symbolic representations, utterances and physical locations are conditionally independent. As such, the inference problem factors into a symbol-grounding component (linking propositions to physical locations) and a symbol-translation component (linking propositions to parse trees). We evaluate the model by eliciting production and comprehension data from human English speakers and find that our system recovers the referent of spatial utterances at a level of proficiency approaching human performance.
- Daniel Fried, S. Kobourov. 2013. Maps of Computer Science. Abstract: We describe a practical approach for visual exploration of research papers. Specifically, we use the titles of papers from the DBLP database to create what we call maps of computer science (MoCS). Words and phrases from the paper titles are the cities in the map, and countries are created based on word and phrase similarity, calculated using co-occurence. With the help of heatmaps, we can visualize the profile of a particular conference or journal over the base map. Similarly, heatmap profiles can be made of individual researchers or groups such as a department. The visualization system also makes it possible to change the data used to generate the base map. For example, a specific journal or conference can be used to generate the base map and then the heatmap overlays can be used to show the evolution of research topics in the field over the years. As before, individual researchers or research group profiles can be visualized using heatmap overlays over a specific journal or conference base map. We outline a modular and extensible system for term extraction using natural language processing techniques, and show the applicability of methods of information retrieval to calculation of term similarity and creation of a topic map. The system is available at mocs.cs.arizona.edu.
- Luca Del Pero, Joshua Bowdish, Daniel Fried, Bonnie Kermgard, Emily Hartley, Kobus Barnard. 2012. Bayesian geometric modeling of indoor scenes. Abstract: We propose a method for understanding the 3D geometry of indoor environments (e.g. bedrooms, kitchens) while simultaneously identifying objects in the scene (e.g. beds, couches, doors). We focus on how modeling the geometry and location of specific objects is helpful for indoor scene understanding. For example, beds are shorter than they are wide, and are more likely to be in the center of the room than cabinets, which are tall and narrow. We use a generative statistical model that integrates a camera model, an enclosing room “box”, frames (windows, doors, pictures), and objects (beds, tables, couches, cabinets), each with their own prior on size, relative dimensions, and locations. We fit the parameters of this complex, multi-dimensional statistical model using an MCMC sampling approach that combines discrete changes (e.g, adding a bed), and continuous parameter changes (e.g., making the bed larger). We find that introducing object category leads to state-of-the-art performance on room layout estimation, while also enabling recognition based only on geometry.
- Daniel Fried. 2011. Crowdsourcing in the Software Development Industry. Abstract: The term crowdsourcing was coined by journalist Jeff Howe, who defines it as “outsourcing a task to a large group of people in the form of an open call”. Crowdsourcing has been used increasingly by the software industry to both lower opportunity costs and increase quality of output by utilizing capital from outside the company, in the form of the experience, labor, or creativity of outside programmers worldwide. Some platforms for crowdsourcing applications, such as Amazon’s Mechanical Turk, provide a means for the acquisition of large amounts of human knowledge in an inexpensive manner. Other platforms, such as TopCoder, use crowdsourcing methods to drive software coding and development, creating contests where programmers compete for a monetary prize by designing algorithms that meet the company’s specifications. A third type of platform, exemplified by MathWorks’s programming competitions, utilizes a unique form of “competitive collaboration” to produce highly efficient software with almost no financial cost to the project coordinators. This paper will investigate competitive and collaborative software frameworks for online crowdsourcing. Other topics investigated will be participants in the collaboration process, external and intrinsic incentives to ensure crowd participation, and parallel versus iterative design and development in crowdsourced applications. Introduction – Open Source Software The growth of the free and open source software (FOSS) movement in the 1980s built a foundation for the distributed development of software and the incorporation of design contributions from a diverse and geographically non-localized community of programmers (von Krogh and von Hippel 2003). The open-source community used the growing capabilities of the Internet to share software and code, coordinating the development of sophisticated open source projects such as the Apache Web Server and the Linux operating system through “user innovation networks,” giving anyone with Web access the power to “download, use, modify, and further develop” the community’s software (von Hippel 2008). Open source’s economic model was a hybrid of private investment and collective action – programmers “used their own resources to privately invest in creating novel software code... then freely revealed it as a public good” (von Krogh and von Hippel 2003). By releasing the source code for their programs, the development communities lost their competitive edge with vendors of proprietary code such as Microsoft, but gained widespread adoption of their code and appreciation of its robust and easily modifiable qualities. FOSS demonstrated the capability of a distributed group to develop successful software, even when most of the contributors did not receive financial compensation for their labor. In these ways, the open source movement laid the foundation for the software development crowdsourcing platforms that will be discussed in this paper. Micro-Crowdsourcing and Amazon's Mechanical Turk Micro-crowdsourcing refers to the distribution of small tasks requiring little skill and time to complete. Monetary compensation for micro-crowdsourcing, if any is given, is typically small, in accordance with the undemanding nature of the work. While larger scale crowdsourcing projects may involve design or a creative process, micro-crowdsourcing views workers as human processors to complete massive amounts of simple jobs in parallel. This analysis will focus on Amazon's Mechanical Turk micro-crowdsourcing platform, one of the largest publicly available platforms with more than 100,000 workers. Amazon CEO Jeff Bezos describes Mechanical Turk as “artificial artificial intelligence” (Pontin), referring to its use as a tool in the implementation of artificial intelligence applications such as audio transcription, image tagging, and object tracking in computer vision systems (Corney). Computers have difficulty distinguishing the features in sample input needed to make these applications function, and Mechanical Turk provides an inexpensive and quick method to have humans perform these identification tasks. Mechanical Turk is centered on the completion of Human Intelligence Tasks, or HITs. A HIT is a “single, self-contained task that a Worker can work on, submit an answer, and collect a reward for completing” (mTurk). These tasks are set up by Requesters, and completed by Workers. Requesters use an API provided by Amazon to set up simple HITs which workers can complete using any web browser, logged into their worker account. These submissions are reviewed upon completion by the Requester, and the Worker is potentially paid, with Amazon receiving a small commission of 10%. (Figure 1) HITs are characterized by the small amount of time required for their completion and the small amount paid to the worker, frequently as low as $0.01. The Mechanical Turk model places few obligations on either party involved. Workers are able to choose HITs based on a description of the HIT, reward per HIT, number available, and time allotted for completion. A worker can choose to work on any HIT provided he or she meets qualifications set by the requester. However, workers are not obligated to complete tasks, and can stop at any time and work on other tasks. Requesters can set minimum standards for the workers, including experience on the website, number of submissions, and whether these submissions were accepted or rejected by other requesters. Additionally, the requester can choose to make potential workers undergo simple tests to determine their competence in the area of the task. Once the HIT is completed, the requester can review the task and choose to either accept or reject the submission, affecting the worker's public record on the site. Requesters also have the option of giving a bonus to individual workers upon completion of the HIT, in addition to the stated reward for completion. Figure 1: Mechanical Turk Framework Mechanical Turk has been used as a knowledge-acquisition backend to generate large datasets for research and software development projects. The accuracy of artificial intelligence systems such as natural language processing and visual recognition systems is improved by training with datasets that correctly match an input with the desired output. Construction of these datasets involves tasks such as matching an image with a set of descriptive words, or matching an audio file with a transcription of that file – tasks that are best performed by humans. Services such as Mechanical Turk provide the ability to divide construction of these datasets up into nearly identical subtasks and distribute them to a large group of workers who are able to work on them independently and in parallel. In certain cases, such as the construction of MIT's LabelMe image annotation dataset, this has allowed the construction of a dataset more quickly and less expensively than traditional methods would have allowed. LabelMe has accumulated over 400,000 images and associated descriptive words since 2005 through the use of Mechanical Turk and a similar but non-paying interface (Torralba 2010). The ease of participation in services such as Mechanical Turk enables widespread use, but also raises concerns about the quality of the task submissions. The anonymous nature of the workers in the crowd makes it theoretically easy to submit erroneous or poor quality work with no lasting effect on the worker's ability to complete future tasks. While Mechanical Turk has methods to try to maintain high quality submissions, such as keeping track of each user's success in completing previous tasks, it is very easy for a worker to disassociate himself from a poor record by creating a new account, or artificially boost his rating by creating, completing, and approving his own HITs (Ipeirotis). However, experimental generation of natural language processing data sets using Mechanical Turk have produced results that compare favorably to those produced by experts, which are typically “extremely expensive in both annotator-hours and financial cost” (Snow 2008). Raw translation and annotation data collected from Mechanical Turk is more abundant but of poorer quality than data produced by linguistic experts for the same tasks. However, through statistical pooling and exclusion of outliers, the non-expert data can be normalized to achieve accuracy very similar to expert produced data. A study that used Mechanical Turk to generate a range of linguistic data sets found that, on average, only 4 non-expert annotations per example were required to achieve the same accuracy as an expert evaluation. The parallel evaluation of the tasks allowed them to be completed quickly, at a rate of 1724 annotations / hour, and at a low cost, of 875 annotations / dollar (Snow 2008). These results show a vast improvement over the costs of data produced by experts in the field, which have the potential to cost thousands of dollars if conducted in an academic setting using linguists or graduate students. An additional model that has been used to generate high quality results using Mechanical Turk involves iterative, rather than parallel, labor. In this model, instead of having different problems solved by different workers in parallel, successive workers do tasks that build each on each other, with one worker's outputs used as the input for the next worker. The TurkIt software developed by Greg Little of MIT's CSAIL builds on the Mechanical Turk interface functions, and automatically generates new HITs based on the results of previous HITs (Little 2009). TurkIt's framework allows for iterative cycles based on two types of tasks: improvement tasks and voting tasks. In Little's experiment, this cycle was applied to an image description task. A worker was presented with an image and a brief paragraph that described it, and asked to improve the description. After the worker submitted his improvements, another task was generated that presented
- A. Hanukoglu, G. Gewurtz, L. Zaidel, M. Krispin, Daniel Fried. 1992. Benign cystic mesothelioma of the peritoneum: the occurrence of an adult entity in a child.. Abstract: Benign cystic mesothelioma of the peritoneum is considered a disease of adulthood occurring predominantly in women. We report a case of benign cystic mesothelioma of the peritoneum in an 11-year-old boy. The innumerable cystic lesions that varied in size occupied the entire peritoneal cavity. He underwent two laparotomies in a period of 15 months during which a partial removal of the tumor was performed. Four years after the second operation the child is well and active, despite significant tumor tissue left behind after the first two operations. The management of this disease is discussed, and the importance of a conservative approach minimizing the number of laparotomies and avoiding radical surgical intervention especially in childhood is emphasized.
- M. Lévy, E. Wang, Daniel Fried. 1990. Diseases That Mimic Meningitis. Abstract: A retrospective review of charts for 650 children who had lumbar puncture for suspected meningitis was undertaken to determine the characteristics of patients with and without meningitis, identify other conditions suggesting meningitis, and evaluate the predictive value of signs and symptoms of meningitis.The incidence of positive lumbar punctures increased with patient age. Younger infants did not present with classical features of meningitis. Bulging fontanel, lethargy, and irritability were nonspecific symptoms. Vomiting and headache, although not specific, proved to be more sensitive indicators of meningeal infection. Most patients with meningitis (75%) had at least one sign of meningeal irritation, but so did 25% of patients without meningitis. Brudzinski's sign was not specific. In contrast, nuchal rigidity and Kernig's sign had high predictive value. Up to age five, the diseases most often suggesting meningitis were right-sided pneumonia, gastroenteritis, otitis, tonsillitis, exanthema subitum, and...
- Uzi Brook, L. Singer, Daniel Fried. 1989. Development of Severe Stevens‐Johnson Syndrome After Administration of Slow‐Release Theophylline. Abstract: Abstract: A 9‐year‐old boy developed Stevens‐Johnson syndrome after taking slow‐release theophylline for bronchial asthma. Multisystem symptoms were severe, especially in the skin and eyes. This is the first reported case of Stevens‐Johnson syndrome atter administration of the ophylline (Theodur).
- A. Hanukoglu, Daniel Fried, E. Somekh. 1986. Inheritance of Familial Primary Endocardial Fibroelastosis. Abstract: Two related families with 15 children, seven of whom developed endocardial fibroelastosis (EFE) are described. Three of the children died during infancy, and the disease was confirmed in one of them at autopsy. The survivors, two sisters age 5 years and 15 months (Family A) and two sisters age 17 and 14 years (Family B), are now symptomless and show a decrease in left ventricular hypertrophy. The mode of inheritance of EFE in our two families appears to be either autosomal or X-linked dominant with reduced penetrance.
