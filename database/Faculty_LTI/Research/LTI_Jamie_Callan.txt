Jamie Callan
Paper count: 331
- Gustavo Gonçalves, João Magalhães, Jamie Callan. 2023. Conversational Search with Random Walks over Entity Graphs. Abstract: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.
- Luís Borges, Bruno Martins, Jamie Callan. 2023. KALE: Using a K-Sparse Projector for Lexical Expansion. Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.
- Zhen Fan, Luyu Gao, Jamie Callan. 2023. CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms. Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a "bag-of-CSFs", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.
- Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig. 2023. Active Retrieval Augmented Generation. Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
- Jamie Callan, J. Petke. 2023. Multi-Objective Improvement of Android Applications. Abstract: Non-functional properties, such as runtime or memory use, are important to mobile app users and developers, as they affect user experience. Previous work on automated improvement of non-functional properties in mobile apps failed to address the inherent trade-offs between such properties. We propose a practical approach and the first open-source tool, GIDroid (2023), for multi-objective automated improvement of Android apps. In particular, we use Genetic improvement, a search-based technique that navigates the space of software variants to find improved software. We use a simulation-based testing framework to greatly improve the speed of search. GIDroid contains three state-of-the-art multi-objective algorithms, and two new mutation operators, which cache the results of method calls. Genetic improvement relies on testing to validate patches. Previous work showed that tests in open-source Android applications are scarce. We thus wrote tests for 21 versions of 7 Android apps, creating a new benchmark for performance improvements. We used GIDroid to improve versions of mobile apps where developers had previously found improvements to runtime, memory, and bandwidth use. Our technique automatically re-discovers 64% of existing improvements. We then applied our approach to current versions of software in which there were no known improvements. We were able to improve execution time by up to 35%, and memory use by up to 33% in these apps.
- Luyu Gao, Xueguang Ma, Jimmy J. Lin, Jamie Callan. 2022. Precise Zero-Shot Dense Retrieval without Relevance Labels. Abstract: While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder’s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn).
- Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig. 2022. PAL: Program-aided Language Models. Abstract: Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .
- Arnold Overwijk, Chenyan Xiong, X. Liu, Cameron VandenBerg, Jamie Callan. 2022. ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information. Abstract: ClueWeb22, the newest iteration of the ClueWeb line of datasets, provides 10 billion web pages affiliated with rich information. Its design was influenced by the need for a high quality, large scale web corpus to support a range of academic and industry research, for example, in information systems, retrieval-augmented AI systems, and model pretraining. Compared with earlier ClueWeb corpora, the ClueWeb22 corpus is larger, more varied, of higher-quality, and aligned with the document distributions in commercial web search. Besides raw HTML, ClueWeb22 includes rich information about the web pages provided by industry-standard document understanding systems, including the visual representation of pages rendered by a web browser, parsed HTML structure information from a neural network parser, and pre-processed cleaned document text to lower the barrier to entry. Many of these signals have been widely used in industry but are available to the research community for the first time at this scale.
- Zhengbao Jiang, Luyu Gao, J. Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, Graham Neubig. 2022. Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer. Abstract: Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents. This is usually done through two separate models, a retriever that encodes the query and finds nearest neighbors, and a reader based on Transformers. These two components are usually modeled separately, which necessitates a cumbersome implementation and is awkward to optimize in an end-to-end fashion. In this paper, we revisit this design and eschew the separate architecture and training in favor of a single Transformer that performs retrieval as attention (RAA), and end-to-end training solely based on supervision from the end QA task. We demonstrate for the first time that an end-to-end trained single Transformer can achieve both competitive retrieval and QA performance on in-domain datasets, matching or even slightly outperforming state-of-the-art dense retrievers and readers. Moreover, end-to-end adaptation of our model significantly boosts its performance on out-of-domain datasets in both supervised and unsupervised settings, making our model a simple and adaptable end-to-end solution for knowledge-intensive tasks.
- Arnold Overwijk, Chenyan Xiong, Jamie Callan. 2022. ClueWeb22: 10 Billion Web Documents with Rich Information. Abstract: ClueWeb22, the newest iteration of the ClueWeb line of datasets, is the result of more than a year of collaboration between industry and academia. Its design is influenced by the research needs of the academic community and the real-world needs of large-scale industry systems. Compared with earlier ClueWeb datasets, the ClueWeb22 corpus is larger, more varied, and has higher-quality documents. Its core is raw HTML, but it includes clean text versions of documents to lower the barrier to entry. Several aspects of ClueWeb22 are available to the research community for the first time at this scale, for example, visual representations of rendered web pages, parsed structured information from the HTML document, and the alignment of document distributions (domains, languages, and topics) to commercial web search. This talk shares the design and construction of ClueWeb22, and discusses its new features. We believe this newer, larger, and richer ClueWeb corpus will enable and support a broad range of research in IR, NLP, and deep learning.
- Luyu Gao, Jamie Callan. 2022. Long Document Re-ranking with Modular Re-ranker. Abstract: Long document re-ranking has been a challenging problem for neural re-rankers based on deep language models like BERT. Early work breaks the documents into short passage-like chunks. These chunks are independently mapped to scalar scores or latent vectors, which are then pooled into a final relevance score. These encode-and-pool methods however inevitably introduce an information bottleneck: the low dimension representations. In this paper, we propose instead to model full query-to-document interaction, leveraging the attention operation and modular Transformer re-ranker framework. First, document chunks are encoded independently with an encoder module. An interaction module then encodes the query and performs joint attention from the query to all document chunk representations. We demonstrate that the model can use this new degree of freedom to aggregate important information from the entire document. Our experiments show that this design produces effective re-ranking on two classical IR collections Robust04 and ClueWeb09, and a large-scale supervised collection MS-MARCO document ranking.
- Jamie Callan, J. Petke. 2022. Improving Responsiveness of Android Activity Navigation via Genetic Improvement. Abstract: Responsiveness issues are one of the key reasons why mobile phone users abandon an app or leave bad reviews. In this work, we explore the use of Genetic Improvement to automatically refactor applications to reduce the time taken to move between and within Android activities, without affecting their functionality. This particular Android responsiveness issue has not previously been tackled before. With its application directly to source code, our approach can be used to complement previous work, which modifies the operating system, or focuses on detection of specific coding patterns. We present a fully automated technique for finding improvements to this responsiveness, which does not require the use of an Android device or emulator. We apply our approach to 7 real-world open source applications and find improvements of up to 30% in navigation response time.
- Luyu Gao, Xueguang Ma, Jimmy J. Lin, Jamie Callan. 2022. Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval. Abstract: Recent rapid advancements in deep pre-trained language models and the introductions of large datasets have powered research in embedding-based dense retrieval. While several good research papers have emerged, many of them come with their own software stacks. These stacks are typically optimized for some particular research goals instead of efficiency or code structure. In this paper, we present Tevatron, a dense retrieval toolkit optimized for efficiency, flexibility, and code simplicity. Tevatron provides a standardized pipeline for dense retrieval including text processing, model training, corpus/query encoding, and search. This paper presents an overview of Tevatron and demonstrates its effectiveness and efficiency across several IR and QA data sets. We also show how Tevatron's flexible design enables easy generalization across datasets, model architectures, and accelerator platforms(GPU/TPU). We believe Tevatron can serve as an effective software foundation for dense retrieval system research including design, modeling, and optimization.
- Luyu Gao, Jamie Callan. 2021. Is Your Language Model Ready for Dense Representation Fine-tuning?. Abstract: Pre-trained language models (LM) have become go-to text representation encoders. Prior research used deep LMs to encode text sequences such as sentences and passages into single dense vector representations. These dense representations have been used in efficient text comparison and embedding-based retrieval. However, dense encoders suffer in low resource situations. Many techniques have been developed to solve this problem. Despite their success, not much is known about why this happens. This paper shows that one cause lies in the readiness of the LM to expose its knowledge through dense representation in fine-tuning, which we term Optimization Readiness. To validate the theory, we present Condenser, a general pre-training architecture based on Transformer LMs, to improve dense optimization readiness. We show that fine-tuning from Condenser significantly improves performance for small and/or noisy training sets.1
- HongChien Yu, Chenyan Xiong, Jamie Callan. 2021. Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback. Abstract: Dense retrieval systems conduct first-stage retrieval using embedded representations and simple similarity metrics to match a query to documents. Its effectiveness depends on encoded embeddings to capture the semantics of queries and documents, a challenging task due to the shortness and ambiguity of search queries. This paper proposes ANCE-PRF, a new query encoder that uses pseudo relevance feedback (PRF) to improve query representations for dense retrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top retrieved documents from a dense retrieval model, ANCE, and it learns to produce better query embeddings directly from relevance labels. It also keeps the document index unchanged to reduce overhead. ANCE-PRF significantly outperforms ANCE and other recent dense retrieval systems on several datasets. Analysis shows that the PRF encoder effectively captures the relevant and complementary information from PRF documents, while ignoring the noise with its learned attention mechanism.
- Luyu Gao, Jamie Callan. 2021. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. Abstract: Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval. However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential. In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space. We use the recently proposed Condenser pre-training architecture, which learns to condense information into the dense vector through LM pre-training. On top of it, we propose coCondenser, which adds an unsupervised corpus-level contrastive loss to warm up the passage embedding space. Experiments on MS-MARCO, Natural Question, and Trivia QA datasets show that coCondenser removes the need for heavy data engineering such as augmentation, synthesis, or filtering, and the need for large batch training. It shows comparable performance to RocketQA, a state-of-the-art, heavily engineered system, using simple small batch fine-tuning.
- Luyu Gao, Zhuyun Dai, Jamie Callan. 2021. COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List. Abstract: Classical information retrieval systems such as BM25 rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens’ contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of exact match and the representation power of deep language models. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency.
- Luyu Gao, Yunyi Zhang, Jiawei Han, Jamie Callan. 2021. Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup. Abstract: Contrastive learning has been applied successfully to learn vector representations of text. Previous research demonstrated that learning high-quality representations benefits from batch-wise contrastive loss with a large number of negatives. In practice, the technique of in-batch negative is used, where for each example in a batch, other batch examples’ positives will be taken as its negatives, avoiding encoding extra negatives. This, however, still conditions each example’s loss on all batch examples and requires fitting the entire large batch into GPU memory. This paper introduces a gradient caching technique that decouples backpropagation between contrastive loss and the encoder, removing encoder backward pass data dependency along the batch dimension. As a result, gradients can be computed for one subset of the batch at a time, leading to almost constant memory usage.
- Luyu Gao, Jamie Callan. 2021. Condenser: a Pre-training Architecture for Dense Retrieval. Abstract: Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs’ internal attention structure is not ready-to-use for dense encoders, which needs to aggregate text information into the dense representation. We propose to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation. Our experiments show Condenser improves over standard LM by large margins on various text retrieval and similarity tasks.
- Nour Jedidi, Gustavo Gonçalves, Jamie Callan. 2021. Query Rewriting with Expansion and Multi-Turn Entity Graphs for Answer Selection. Abstract: Conversational search is challenging in part because often the meaning of the current question cannot be fully understood without contextual information from previous questions and/or answers. This paper describes research on using query reformulation and lightweight reranking based on a multi-turn entity graph to represent and make use of contextual information in the CAsT 2021 track.
- Jamie Callan, J. Petke. 2021. Optimising SQL Queries Using Genetic Improvement. Abstract: Structured Query Language (SQL) queries are ubiquitous in modern software engineering. These queries can be costly when run on large databases with many entries and tables to consider. We propose using Genetic Improvement (GI) to explore patches for these queries, with the aim of optimising their execution time, whilst maintaining the functionality of the program in which they are utilised. Specifically, we propose three ways in which SQL JOIN statements can be mutated in order to improve performance. We also discuss the requirements of software being improved in this manner and the potential challenges of our approach.
- Zhuyun Dai, Jamie Callan. 2020. Context-Aware Document Term Weighting for Ad-Hoc Search. Abstract: Bag-of-words document representations play a fundamental role in modern search engines, but their power is limited by the shallow frequency-based term weighting scheme. This paper proposes HDCT, a context-aware document term weighting framework for document indexing and retrieval. It first estimates the semantic importance of a term in the context of each passage. These fine-grained term weights are then aggregated into a document-level bag-of-words representation, which can be stored into a standard inverted index for efficient retrieval. This paper also proposes two approaches that enable training HDCT without relevance labels. Experiments show that an index using HDCT weights significantly improved the retrieval accuracy compared to typical term-frequency and state-of-the-art embedding-based indexes.
- Luyu Gao, Zhuyun Dai, Jamie Callan. 2020. Understanding BERT Rankers Under Distillation. Abstract: Deep language models, such as BERT pre-trained on large corpora, have given a huge performance boost to state-of-the-art information retrieval ranking systems. Knowledge embedded in such models allows them to pick up complex matching signals between passages and queries. However, the high computation cost during inference limits their deployment in real-world search scenarios. In this paper, we study if and how the knowledge for search within BERT can be transferred to a smaller ranker through distillation. Our experiments demonstrate that it is crucial to use a proper distillation procedure, which produces up to nine times speedup while preserving the state-of-the-art performance.
- Zhuyun Dai, Jamie Callan. 2020. Context-Aware Passage TermWeighting For First Stage Retrieval. Abstract: Term frequency is a commonmethod for identifying the importance of a term in a document. But term frequency ignores how a term interacts with its text context, which is key to estimating documentspecific term weights. This paper proposes a Deep Contextualized Term Weighting framework (DeepCT) that maps the contextualized term representations fromBERT to into context-aware termweights for passage retrieval. The new, deep term weights can be stored in an ordinary inverted index for efficient retrieval. Experiments on two datasets demonstrate that DeepCT greatly improves the accuracy of first-stage passage retrieval algorithms.
- Shuo Zhang, K. Balog, Jamie Callan. 2020. Generating Categories for Sets of Entities. Abstract: Category systems are central components of knowledge bases, as they provide a hierarchical grouping of semantically related concepts and entities. They are a unique and valuable resource that is utilized in a broad range of information access tasks. To aid knowledge editors in the manual process of expanding a category system, this paper presents a method of generating categories for sets of entities. First, we employ neural abstractive summarization models to generate candidate categories. Next, the location within the hierarchy is identified for each candidate. Finally, structure-, content-, and hierarchy-based features are used to rank candidates to identify by the most promising ones (measured in terms of specificity, hierarchy, and importance). We develop a test collection based on Wikipedia categories and demonstrate the effectiveness of the proposed approach.
- Jeffrey Dalton, Chenyan Xiong, Vaibhav Kumar, Jamie Callan. 2020. CAsT-19: A Dataset for Conversational Information Seeking. Abstract: CAsT-19 is a new dataset that supports research on conversational information seeking. The corpus is 38,426,252 passages from the TREC Complex Answer Retrieval (CAR) and Microsoft MAchine Reading COmprehension (MARCO) datasets. Eighty information seeking dialogues (30 train, 50 test) are an average of 9 to 10 questions long. A dialogue may explore a topic broadly or drill down into subtopics. Questions contain ellipsis, implied context, mild topic shifts, and other characteristics of human conversation that may prevent them from being understood in isolation. Relevance assessments are provided for 30 training topics and 20 test topics. CAsT-19 promotes research on conversational information seeking by defining it as a task in which effective passage selection requires understanding a question's context (the dialogue history). It focuses attention on user modeling, analysis of prior retrieval results, transformation of questions into effective queries, and other topics that have been difficult to study with existing datasets.
- Vaibhav Kumar, Vikas Raunak, Jamie Callan. 2020. Ranking Clarification Questions via Natural Language Inference. Abstract: Given a natural language query, teaching machines to ask clarifying questions is of immense utility in practical natural language processing systems. Such interactions could help in filling information gaps for better machine comprehension of the query. For the task of ranking clarification questions, we hypothesize that determining whether a clarification question pertains to a missing entry in a given post (on QA forums such as StackExchange) could be considered as a special case of Natural Language Inference (NLI), where both the post and the most relevant clarification question point to a shared latent piece of information or context. We validate this hypothesis by incorporating representations from a Siamese BERT model fine-tuned on NLI and Multi-NLI datasets into our models and demonstrate that our best performing model obtains a relative performance improvement of 40 percent and 60 percent respectively (on the key metric of Precision@1), over the state-of-the-art baseline(s) on the two evaluation sets of the StackExchange dataset, thereby, significantly surpassing the state-of-the-art.
- Vaibhav Kumar, Jamie Callan. 2020. Making Information Seeking Easier: An Improved Pipeline for Conversational Search. Abstract: This paper presents a highly effective pipeline for passage retrieval in a conversational search setting. The pipeline comprises of two components: Conversational Term Selection (CTS) and Multi-View Reranking (MVR). CTS is responsible for performing the first-stage of passage retrieval. Given an input question, it uses a BERT-based classifier (trained with weak supervision) to de-contextualize the input by selecting relevant terms from the dialog history. Using the question and the selected terms, it issues a query to a search engine to perform the first-stage of passage retrieval. On the other hand, MVR is responsible for contextualized passage reranking. It first constructs multiple views of the information need embedded within an input question. The views are based on the dialog history and the top documents obtained in the first-stage of retrieval. It then uses each view to rerank passages using BERT (fine-tuned for passage ranking). Finally, MVR performs a fusion over the rankings produced by the individual views. Experiments show that the above combination improves first-state retrieval as well as the overall accuracy in a reranking pipeline. On the key metric of NDCG@3, the proposed combination achieves a relative performance improvement of 14.8% over the state-of-the-art baseline and is also able to surpass the Oracle.
- Zhuyun Dai, Jamie Callan. 2020. Context-Aware Term Weighting For First Stage Passage Retrieval. Abstract: Term frequency is a common method for identifying the importance of a term in a document. But term frequency ignores how a term interacts with its text context, which is key to estimating document-specific term weights. This paper proposes a Deep Contextualized Term Weighting framework (DeepCT) that maps the contextualized term representations from BERT to into context-aware term weights for passage retrieval. The new, deep term weights can be stored in an ordinary inverted index for efficient retrieval. Experiments on two datasets demonstrate that DeepCT greatly improves the accuracy of first-stage passage retrieval algorithms.
- Luyu Gao, Zhuyun Dai, Zhenhua Fan, Jamie Callan. 2020. Complementing Lexical Retrieval with Semantic Residual Embedding. Abstract: Information retrieval traditionally has relied on lexical matching signals, but lexical matching cannot handle vocabulary mismatch or topic-level matching. Neural embedding based retrieval models can match queries and documents in a latent semantic space, but they lose token-level matching information that is critical to IR. This paper presents CLEAR, a deep retrieval model that seeks to complement lexical retrieval with semantic embedding retrieval. Importantly, CLEAR uses a residual-based embedding learning framework, which focuses the embedding on the deep language structures and semantics that the lexical retrieval fails to capture. Empirical evaluation demonstrates the advantages of CLEAR over classic bag-of-words retrieval models, recent BERT-enhanced lexical retrieval models, as well as a BERT-based embedding retrieval. A full-collection retrieval with CLEAR can be as effective as a BERT-based reranking system, substantially narrowing the gap between full-collection retrieval and cost-prohibitive reranking systems
- Luyu Gao, Zhuyun Dai, Jamie Callan. 2020. Modularized Transfomer-based Ranking Framework. Abstract: Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.
- Shuo Zhang, Zhuyun Dai, K. Balog, Jamie Callan. 2020. Summarizing and Exploring Tabular Data in Conversational Search. Abstract: Tabular data provide answers to a significant portion of search queries. However, reciting an entire result table is impractical in conversational search systems. We propose to generate natural language summaries as answers to describe the complex information contained in a table. Through crowdsourcing experiments, we build a new conversation-oriented, open-domain table summarization dataset. It includes annotated table summaries, which not only answer questions but also help people explore other information in the table. We utilize this dataset to develop automatic table summarization systems as SOTA baselines. Based on the experimental results, we identify challenges and point out future research directions that this resource will support.
- Jeffrey Dalton, Chenyan Xiong, Jamie Callan. 2020. TREC CAsT 2019: The Conversational Assistance Track Overview. Abstract: The Conversational Assistance Track (CAsT) is a new track for TREC 2019 to facilitate Conversational Information Seeking (CIS) research and to create a large-scale reusable test collection for conversational search systems. The document corpus is 38,426,252 passages from the TREC Complex Answer Retrieval (CAR) and Microsoft MAchine Reading COmprehension (MARCO) datasets. Eighty information seeking dialogues (30 train, 50 test) are an average of 9 to 10 questions long. Relevance assessments are provided for 30 training topics and 20 test topics. This year 21 groups submitted a total of 65 runs using varying methods for conversational query understanding and ranking. Methods include traditional retrieval based methods, feature based learning-to-rank, neural models, and knowledge enhanced methods. A common theme through the runs is the use of BERT-based neural reranking methods. Leading methods also employed document expansion, conversational query expansion, and generative language models for conversational query rewriting (GPT-2). The results show a gap between automatic systems and those using the manually resolved utterances, with a 35% relative improvement of manual rewrites over the best automatic system.
- Luyu Gao, Zhuyun Dai, Jamie Callan. 2020. EARL: Speedup Transformer-based Rankers with Pre-computed Representation. Abstract: Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, their performance gains come at a steep computational cost. This paper presents a novel Embed Ahead Rank Later (EARL) framework, which speeds-up Transformer-based rankers by pre-computing representations and keeping online computation shallow. EARL dis-entangles the attention in a typical Transformer-based ranker into three asynchronous tasks and assign each to a dedicated Transformer: query understanding, document understanding, and relevance judging. With such a ranking framework, query and document token representations can be offline computed and reused. We also propose a new judger transformer block that keeps online relevance judging light and shallow. Our experiments demonstrate that EARL can be as effective as previous state-of-the-art BERT rankers in accuracy while substantially faster in evaluation time.
- J. Mackenzie, Zhuyun Dai, L. Gallagher, Jamie Callan. 2020. Efficiency Implications of Term Weighting for Passage Retrieval. Abstract: Language model pre-training has spurred a great deal of attention for tasks involving natural language understanding, and has been successfully applied to many downstream tasks with impressive results. Within information retrieval, many of these solutions are too costly to stand on their own, requiring multi-stage ranking architectures. Recent work has begun to consider how to "backport" salient aspects of these computationally expensive models to previous stages of the retrieval pipeline. One such instance is DeepCT, which uses BERT to re-weight term importance in a given context at the passage level. This process, which is computed offline, results in an augmented inverted index with re-weighted term frequency values. In this work, we conduct an investigation of query processing efficiency over DeepCT indexes. Using a number of candidate generation algorithms, we reveal how term re-weighting can impact query processing latency, and explore how DeepCT can be used as a static index pruning technique to accelerate query processing without harming search effectiveness.
- Vaibhav Kumar, Jamie Callan. 2019. A Step towards Context Identification for Conversational Search. Abstract: e system comprises of three dierent components. e rst component makes a decision whether to incorporate contextual information for the current query in ongoing conversation. e decision is based on the KL-divergence between the retrieved documents for the original query and whether the query consists of pronouns. e second component identies the contextual information (if required) for the answering the current query. is identication is performed using an SVM classier which uses BERT aention weights along with other linguistic features. Finally, the third component utilises Indri for document retrieval.
- Zhuyun Dai, Zhen Fan, Hafeezul Rahman, Jamie Callan. 2019. Local Matching Networks for Engineering Diagram Search. Abstract: Finding diagrams that contain a specific part or a similar part is important in many engineering tasks. In this search task, the query part is expected to match only a small region in a complex image. This paper investigates several local matching networks that explicitly model local region-to-region similarities. Deep convolutional neural networks extract local features and model local matching patterns. Spatial convolution is employed to cross-match local regions at different scale levels, addressing cases where the target part appears at a different scale, position, and/or angle. A gating network automatically learns region importance, removing noise from sparse areas and visual metadata in engineering diagrams. Experimental results show that local matching approaches are more effective for engineering diagram search than global matching approaches. Suppressing unimportant regions via the gating network enhances accuracy. Matching across different scales via spatial convolution substantially improves robustness to scale and rotation changes. A pipelined architecture efficiently searches a large collection of diagrams by using a simple local matching network to identify a small set of candidate images and a more sophisticated network with convolutional cross-scale matching to re-rank candidates.
- Zhuyun Dai, Jamie Callan. 2019. Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval. Abstract: Term frequency is a common method for identifying the importance of a term in a query or document. But it is a weak signal, especially when the frequency distribution is flat, such as in long queries or short documents where the text is of sentence/passage-length. This paper proposes a Deep Contextualized Term Weighting framework that learns to map BERT's contextualized text representations to context-aware term weights for sentences and passages. When applied to passages, DeepCT-Index produces term weights that can be stored in an ordinary inverted index for passage retrieval. When applied to query text, DeepCT-Query generates a weighted bag-of-words query. Both types of term weight can be used directly by typical first-stage retrieval algorithms. This is novel because most deep neural network based ranking models have higher computational costs, and thus are restricted to later-stage rankers. Experiments on four datasets demonstrate that DeepCT's deep contextualized text understanding greatly improves the accuracy of first-stage retrieval algorithms.
- Zhuyun Dai, Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. Abstract: Neural networks provide new possibilities to automatically learn complex language patterns and query-document relations. Neural IR models have achieved promising results in learning query-document relevance patterns, but few explorations have been done on understanding the text content of a query or a document. This paper studies leveraging a recently-proposed contextual neural language model, BERT, to provide deeper text understanding for IR. Experimental results demonstrate that the contextual text representations from BERT are more effective than traditional word embeddings. Compared to bag-of-words retrieval models, the contextual language model can better leverage language structures, bringing large improvements on queries written in natural languages. Combining the text understanding ability with search knowledge leads to an enhanced pre-trained BERT model that can benefit related search tasks where training data are limited.
- Gustavo Gonçalves, João Magalhães, Jamie Callan. 2019. Proximity-Based Entity Ranking. Abstract: This work explores the value of a combination of features, including entity proximity, for improving a learning-to-rank entity ranking system.
- Zhuyun Dai, Jamie Callan. 2019. An Evaluation of Weakly-Supervised DeepCT in the TREC 2019 Deep Learning Track. Abstract: This paper describes our participation in the TREC 2019 Deep Learning Track document ranking task. We developed a deep learning based document term weighting approach based on our previous work of DeepCT. It used the contextualized token embeddings generated by BERT to estimate a term’s importance in passages, and combines passage term weights into document-level term weights. The weighted document is stored in an ordinary inverted index and searched using a multi-ﬁeld BM25, which is eﬃcient. We tested two ways of training DeepCT: a query-based method using sparse relevant query-document pairs, and a weakly-supervised method using document title-body pairs.
- Keyang Xu, K. Y. Gao, Jamie Callan. 2018. A Structure-Oriented Unsupervised Crawling Strategy for Social Media Sites. Abstract: Existing techniques for efficiently crawling social media sites rely on URL patterns, query logs, and human supervision. This paper describes SOUrCe, a structure-oriented unsupervised crawler that uses page structures to learn how to crawl a social media site efficiently. SOUrCe consists of two stages. During its unsupervised learning phase, SOUrCe constructs a sitemap that clusters pages based on their structural similarity and generates a navigation table that describes how the different types of pages in the site are linked together. During its harvesting phase, it uses the navigation table and a crawling policy to guide the choice of which links to crawl next. Experiments show that this architecture supports different styles of crawling efficiently, and does a better job of staying focused on user-created contents than baseline methods.
- Yubin Kim, Jamie Callan. 2018. Measuring the Effectiveness of Selective Search Index Partitions without Supervision. Abstract: Selective search architectures partition a document collection into topic-oriented index shards, usually using algorithms that have random components. Different mappings of documents into index shards (shard maps) produce different search accuracy and consistency, however identifying which shard maps will deliver the highest average effectiveness is an open problem. This paper presents a new metric, Area Under Recall Curve (AUReC), to evaluate and compare shard maps. AUReC is the first such metric that is independent of resource selection and shard cut-off estimation. It does not require an end-to-end evaluation or manual gold-standard judgements. Experiments show that its predictions are highly-correlated with evaluating end-to-end systems of various configurations, while being easier to implement and computationally inexpensive.
- Hafeezul Rahman Mohammad, Keyang Xu, Jamie Callan, J. Culpepper. 2018. Dynamic Shard Cutoff Prediction for Selective Search. Abstract: Selective search architectures use resource selection algorithms such as Rank-S or Taily to rank index shards and determine how many to search for a given query. Most prior research evaluated solutions by their ability to improve efficiency without significantly reducing early-precision metrics such as P@5 and NDCG@10. This paper recasts selective search as an early stage of a multi-stage retrieval architecture, which makes recall-oriented metrics more appropriate. A new algorithm is presented that predicts the number of shards that must be searched for a given query in order to meet recall-oriented goals. Decoupling shard ranking from deciding how many shards to search clarifies efficiency vs. effectiveness trade-offs, and enables them to be optimized independently. Experiments on two corpora demonstrate the value of this approach.
- Mary Arpita Pyreddy, Varshini Ramaseshan, N. Joshi, Zhuyun Dai, Chenyan Xiong, Jamie Callan, Zhiyuan Liu. 2018. Consistency and Variation in Kernel Neural Ranking Model. Abstract: This paper studies the consistency of the kernel-based neural ranking model K-NRM, a recent state-of-the-art neural IR model, which is important for reproducible research and deployment in the industry. We find that K-NRM has low variance on relevance-based metrics across experimental trials. In spite of this low variance in overall performance, different trials produce different document rankings for individual queries. The main source of variance in our experiments was found to be different latent matching patterns captured by K-NRM. In the IR-customized word embeddings learned by K-NRM, the query-document word pairs follow two different matching patterns that are equally effective, but align word pairs differently in the embedding space. The different latent matching patterns enable a simple yet effective approach to construct ensemble rankers, which improve K-NRM's effectiveness and generalization abilities.
- Flávio Martins, João Magalhães, Jamie Callan. 2018. Modeling Temporal Evidence from External Collections. Abstract: Newsworthy events are broadcast through multiple mediums and prompt the crowds to produce comments on social media. In this paper, we propose to leverage on this behavioral dynamics to estimate the most relevant time periods for an event (i.e., query). Recent advances have shown how to improve the estimation of the temporal relevance of such topics. In this approach, we build on two major novelties. First, we mine temporal evidences from hundreds of external sources into topic-based external collections to improve the robustness of the detection of relevant time periods. Second, we propose a formal retrieval model that generalizes the use of the temporal dimension across different aspects of the retrieval process. In particular, we show that temporal evidence of external collections can be used to (i) infer a topic's temporal relevance, (ii) select the query expansion terms, and (iii) re-rank the final results for improved precision. Experiments with TREC Microblog collections show that the proposed time-aware retrieval model makes an effective and extensive use of the temporal dimension to improve search results over the most recent temporal models. Interestingly, we observe a strong correlation between precision and the temporal distribution of retrieved and relevant documents.
- Flávio Martins, João Magalhães, Jamie Callan. 2018. A Vertical PRF Architecture for Microblog Search. Abstract: In microblog retrieval, query expansion can be essential to obtain good search results due to the short size of queries and posts. Since information in microblogs is highly dynamic, an up-to-date index coupled with pseudo-relevance feedback (PRF) with an external corpus has a higher chance of retrieving more relevant documents and improving ranking. In this paper, we focus on the research question:how can we reduce the query expansion computational cost while maintaining the same retrieval precision as standard PRF? Therefore, we propose to accelerate the query expansion step of pseudo-relevance feedback. The hypothesis is that using an expansion corpus organized into verticals for expanding the query, will lead to a more efficient query expansion process and improved retrieval effectiveness. Thus, the proposed query expansion method uses a distributed search architecture and resource selection algorithms to provide an efficient query expansion process. Experiments on the TREC Microblog datasets show that the proposed approach can match or outperform standard PRF in MAP and NDCG@30, with a computational cost that is three orders of magnitude lower.
- Gustavo Gonçalves, João Magalhães, Chenyan Xiong, Jamie Callan. 2018. Improving Ad Hoc Retrieval With Bag Of Entities. Abstract: This work explores the value of entity information for improving a feature-based learning-to-rank search engine.
- Cameron VandenBerg, Jamie Callan. 2018. Sifaka: Text Mining Above a Search API. Abstract: Text mining and analytics software has become popular, but little attention has been paid to the software architectures of such systems. Often they are built from scratch using special-purpose software and data structures, which increases their cost and complexity. This demo paper describes Sifaka, a new open-source text mining application constructed above a standard search engine index using existing application programmer interface (API) calls. Indexing integrates popular annotation software libraries to augment the full-text index with noun phrase and named-entities; n-grams are also provided. Sifaka enables a person to quickly explore and analyze large text collections using search, frequency analysis, and co-occurrence analysis; and import existing document labels or interactively construct document sets that are positive or negative examples of new concepts, perform feature selection, and export feature vectors compatible with popular machine learning software. Sifaka demonstrates that search engines are good platforms for text mining applications while also making common IR text mining capabilities accessible to researchers in disciplines where programming skills are less common.
- Chenyan Xiong, Zhengzhong Liu, Jamie Callan, Tie-Yan Liu. 2018. Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling. Abstract: This paper presents a Kernel Entity Salience Model (KESM) that improves text understanding and retrieval by better estimating entity salience (importance) in documents. KESM represents entities by knowledge enriched distributed representations, models the interactions between entities and words by kernels, and combines the kernel scores to estimate entity salience. The whole model is learned end-to-end using entity salience labels. The salience model also improves ad hoc search accuracy, providing effective ranking features by modeling the salience of query entities in candidate documents. Our experiments on two entity salience corpora and two TREC ad hoc search datasets demonstrate the effectiveness of KESM over frequency-based and feature-based methods. We also provide examples showing how KESM conveys its text understanding ability learned from entity salience to search.
- Zhuyun Dai, Chenyan Xiong, Jamie Callan, Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search. Abstract: This paper presents \textttConv-KNRM, a Convolutional Kernel-based Neural Ranking Model that models n-gram soft matches for ad-hoc search. Instead of exact matching query and document n-grams, \textttConv-KNRM uses Convolutional Neural Networks to represent n-grams of various lengths and soft matches them in a unified embedding space. The n-gram soft matches are then utilized by the kernel pooling and learning-to-rank layers to generate the final ranking score. \textttConv-KNRM can be learned end-to-end and fully optimized from user feedback. The learned model»s generalizability is investigated by testing how well it performs in a related domain with small amounts of training data. Experiments on English search logs, Chinese search logs, and TREC Web track tasks demonstrated consistent advantages of \textttConv-KNRM over prior neural IR methods and feature-based methods.
- Keyang Xu, Zhengzhong Liu, Jamie Callan. 2017. De-duping URLs with Sequence-to-Sequence Neural Networks. Abstract: Many URLs on the Internet point to identical contents, which increase the burden of web crawlers. Techniques that detect such URLs (known as URL de-duping) can greatly save resources such as bandwidth and storage for crawlers. Traditional de-duping methods are usually limited to heavily engineered rule matching strategies.In this work, we propose a novel URL de-duping framework based on sequence-to-sequence (Seq2Seq) neural networks. A single concise translation model can take the place of thousands of explicit rules. Experiments indicate that a vanilla Seq2Seq architecture yields robust and accurate results in detecting duplicate URLs. Furthermore, we demonstrate the efficiency of this framework in the real large-scale web environment.
- Zhuyun Dai, Chenyan Xiong, Jamie Callan. 2017. An Evaluation of the Kernel Based Neural Ranking Model in NTCIR-13 WWW. Abstract: This paper describes CMUIR’s participation in the NTCIR13 We Want Web (WWW) task. In the context of the Chinese subtask, we experimented with a neural network approach using the kernel based neural ranking model (KNRM). The model learns a word embedding that encodes IRcustomized soft match patterns from a Chinese search log. The learned model is then directly applied to re-rank the baseline run result lists of the Chinese subtask. We extend K-NRM to incorporate multiple document fields for richer text presentation. We also experimented with different reranking cutoffs to reduce the effect of the gap between training and testing domains. Evaluation results confirmed the effectiveness of K-NRM.
- Chenyan Xiong, Jamie Callan, Tie-Yan Liu. 2017. Word-Entity Duet Representations for Document Ranking. Abstract: This paper presents a word-entity duet framework for utilizing knowledge bases in ad-hoc retrieval. In this work, the query and documents are modeled by word-based representations and entity-based representations. Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space connections through the knowledge graph. To handle the uncertainties from the automatically constructed entity representations, an attention-based ranking model AttR-Duet is developed. With back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents with the word-entity duet. Evaluation results on TREC Web Track ad-hoc task demonstrate that all of the four-way interactions in the duet are useful, the attention mechanism successfully steers the model away from noisy entities, and together they significantly outperform both word-based and entity-based learning to rank systems.
- Zhuyun Dai, Yubin Kim, Jamie Callan. 2017. Learning To Rank Resources. Abstract: We present a learning-to-rank approach for resource selection. We develop features for resource ranking and present a training approach that does not require human judgments. Our method is well-suited to environments with a large number of resources such as selective search, is an improvement over the state-of-the-art in resource selection for selective search, and is statistically equivalent to exhaustive search even for recall-oriented metrics such as MAP@1000, an area in which selective search was lacking.
- Chenyan Xiong, Jamie Callan, Zhiyuan Liu. 2017. Convolutional Neural Networks for So-Matching N-Grams in Ad-hoc Search Zhuyun Dai. Abstract: is paper presents Conv-KNRM, a Convolutional Kernel-based Neural Ranking Model that models n-gram so matches for ad-hoc search. Instead of exact matching query and document n-grams, Conv-KNRM uses Convolutional Neural Networks to represent ngrams of various lengths and so matches them in a unied embedding space. e n-gram so matches are then utilized by the kernel pooling and learning-to-rank layers to generate the nal ranking score. Conv-KNRM can be learned end-to-end and fully optimized from user feedback. e learned model’s generalizability is investigated by testing how well it performs in a related domain with small amounts of training data. Experiments on English search logs, Chinese search logs, and TREC Web track tasks demonstrated consistent advantages of Conv-KNRM over prior neural IR methods and feature-based methods.
- K. Y. Gao, Jamie Callan. 2017. Scientific Table Search Using Keyword Queries. Abstract: Tables are common and important in scientific documents, yet most text-based document search systems do not capture structures and semantics specific to tables. How to bridge different types of mismatch between keywords queries and scientific tables and what influences ranking quality needs to be carefully investigated. This paper considers the structure of tables and gives different emphasis to table components. On the query side, thanks to external knowledge such as knowledge bases and ontologies, key concepts are extracted and used to build structured queries, and target quantity types are identified and used to expand original queries. A probabilistic framework is proposed to incorporate structural and semantic information from both query and table sides. We also construct and release TableArXiv, a high quality dataset with 105 queries and corresponding relevance judgements for scientific table search. Experiments demonstrate significantly higher accuracy overall and at the top of the rankings than several baseline methods.
- Chenyan Xiong, Russell Power, Jamie Callan. 2017. Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding. Abstract: This paper introduces Explicit Semantic Ranking (ESR), a new ranking technique that leverages knowledge graph embedding. Analysis of the query log from our academic search engine, SemanticScholar.org, reveals that a major error source is its inability to understand the meaning of research concepts in queries. To addresses this challenge, ESR represents queries and documents in the entity space and ranks them based on their semantic connections from their knowledge graph embedding. Experiments demonstrate ESR's ability in improving Semantic Scholar's online production system, especially on hard queries where word-based ranking fails.
- Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, K. Balog, Svein Erik Bratsberg, Alexander Kotov, Jamie Callan. 2017. DBpedia-Entity v2: A Test Collection for Entity Search. Abstract: The DBpedia-entity collection has been used as a standard test collection for entity search in recent years. We develop and release a new version of this test collection, DBpedia-Entity v2, which uses a more recent DBpedia dump and a unified candidate result pool from the same set of retrieval models. Relevance judgments are also collected in a uniform way, using the same group of crowdsourcing workers, following the same assessment guidelines. The result is an up-to-date and consistent test collection.To facilitate further research, we also provide details about the pre-processing and indexing steps, and include baseline results from both classical and recently developed entity search methods.
- Hongyu Li, Chenyan Xiong, Jamie Callan. 2017. Natural Language Supported Relation Matching for Question Answering with Knowledge Graphs. Abstract: This work focuses on the relation matching problem in knowledge based question answering systems. Finding the right relation a natural question asks is a key step in current knowledge based question answering systems, while also being the most difficult one, because of the mismatch between natural language question and formal relation type definitions. In this paper, we present two approaches to tackle this problem. The first approach tries to directly learn the soft match between the question and the relations from the training data using neural networks. The second approach enriches the relation name with natural language support sentences generated from Wikipedia, which provide additional matches with the question. Experiments on the WebQuestions dataset demonstrate that both of our approaches improve the relation matching accuracy of a prior state-of-the-art. Our further analysis reveals the high quality of support sentences and suggests the rich potential of support sentences in question answering and semantic parsing tasks.
- Chenyan Xiong, Zhengzhong Liu, Jamie Callan, E. Hovy. 2017. JointSem: Combining Query Entity Linking and Entity based Document Ranking. Abstract: Entity-based ranking systems often employ entity linking systems to align entities to query and documents. Previously, entity linking systems were not designed specifically for search engines and were mostly used as a preprocessing step. This work presents JointSem, a joint semantic ranking system that combines query entity linking and entity-based document ranking. In JointSem, the spotting and linking signals are used to describe the importance of candidate entities in the query, and the linked entities are utilized to provide additional ranking features for the documents. The linking signals and the ranking signals are combined by a joint learning-to-rank model, and the whole system is fully optimized towards end-to-end ranking performance. Experiments on TREC Web Track datasets demonstrate the effectiveness of joint learning of entity linking and entity-based ranking.
- Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. Abstract: This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.
- Jing Chen, Chenyan Xiong, Jamie Callan. 2016. An Empirical Study of Learning to Rank for Entity Search. Abstract: This work investigates the effectiveness of learning to rank methods for entity search. Entities are represented by multi-field documents constructed from their RDF triples, and field-based text similarity features are extracted for query-entity pairs. State-of-the-art learning to rank methods learn models for ad-hoc entity search. Our experiments on an entity search test collection based on DBpedia confirm that learning to rank methods are as powerful for ranking entities as for ranking documents, and establish a new state-of-the-art for accuracy on this benchmark dataset.
- Zhuyun Dai, Chenyan Xiong, Jamie Callan. 2016. Query-Biased Partitioning for Selective Search. Abstract: Selective search is a cluster-based distributed retrieval architecture that reduces computational costs by partitioning a corpus into topical shards, and selectively searching them. Prior research formed topical shards by clustering the corpus based on the documents' contents. This content-based partitioning strategy reveals common topics in a corpus. However, the topic distribution produced by clustering may not match the distribution of topics in search traffic, which may reduce the effectiveness of selective search. This paper presents a query-biased partitioning strategy that aligns document partitions with topics from query logs. It focuses on two parts of the partitioning process: clustering initialization and document similarity calculation. A query-driven clustering initialization algorithm uses topics from query logs to form cluster seeds. A query-biased similarity metric favors terms that are important in query logs. Both methods boost retrieval effectiveness, reduce variance, and produce a more balanced distribution of shard sizes.
- Yubin Kim, Jamie Callan, J. Culpepper, Alistair Moffat. 2016. Load-Balancing in Distributed Selective Search. Abstract: Simulation and analysis have shown that selective search can reduce the cost of large-scale distributed information retrieval. By partitioning the collection into small topical shards, and then using a resource ranking algorithm to choose a subset of shards to search for each query, fewer postings are evaluated. Here we extend the study of selective search using a fine-grained simulation investigating: selective search efficiency in a parallel query processing environment; the difference in efficiency when term-based and sample-based resource selection algorithms are used; and the effect of two policies for assigning index shards to machines. Results obtained for two large datasets and four large query logs confirm that selective search is significantly more efficient than conventional distributed search. In particular, we show that selective search is capable of both higher throughput and lower latency in a parallel environment than is exhaustive search.
- Flávio Martins, João Magalhães, Jamie Callan. 2016. Barbara Made the News: Mining the Behavior of Crowds for Time-Aware Learning to Rank. Abstract: In Twitter, and other microblogging services, the generation of new content by the crowd is often biased towards immediacy: what is happening now. Prompted by the propagation of commentary and information through multiple mediums, users on the Web interact with and produce new posts about newsworthy topics and give rise to trending topics. This paper proposes to leverage on the behavioral dynamics of users to estimate the most relevant time periods for a topic. Our hypothesis stems from the fact that when a real-world event occurs it usually has peak times on the Web: a higher volume of tweets, new visits and edits to related Wikipedia articles, and news published about the event. In this paper, we propose a novel time-aware ranking model that leverages on multiple sources of crowd signals. Our approach builds on two major novelties. First, a unifying approach that given query q, mines and represents temporal evidence from multiple sources of crowd signals. This allows us to predict the temporal relevance of documents for query q. Second, a principled retrieval model that integrates temporal signals in a learning to rank framework, to rank results according to the predicted temporal relevance. Evaluation on the TREC 2013 and 2014 Microblog track datasets demonstrates that the proposed model achieves a relative improvement of 13.2% over lexical retrieval models and 6.2% over a learning to rank baseline.
- Allan, Y. Saad, C. Buckley, C. Zhai, M. Zhong, E. Voorhees, E. J. Zobel, M. Huang, S. Robertson, J. Xu, W. Ma, S. Dumais, U. Deppisch, C. Zhai, C. Clarke, J. Zobel, D. D. Jaco, James Allan, G. Garbolino, D. Lewis, A. Singhal, Jamie Callan, M. W. Davis, N. Belkin. 2016. A social model for Literature Access : Towards a weighted social network of authors. Abstract: This paper presents a novel retrieval approach for literature access based on social network analysis. In fact, we investigate a social model where authors represent the main entities and relationships are extracted from co-author and citation links. Moreover, we define a weighting model for social relationships which takes into account the authors positions in the social network and their mutual collaborations. Assigned weights express influence, knowledge transfer and shared interest between authors. Furthermore, we estimate document relevance by combing the document-query similarity and the document social importance derived from corresponding authors. To evaluate the effectiveness of our model, we conduct a series of experiments on a scientific document dataset that includes textual content and social data extracted from the academic social network CiteULike. Final results show that the proposed model improves the retrieval effectiveness and outperforms traditional and social information retrieval baselines.
- Chenyan Xiong, Jamie Callan, Tie-Yan Liu. 2016. Bag-of-Entities Representation for Ranking. Abstract: This paper presents a new bag-of-entities representation for document ranking, with the help of modern knowledge bases and automatic entity linking. Our system represents query and documents by bag-of-entities vectors constructed from their entity annotations, and ranks documents by their matches with the query in the entity space. Our experiments with Freebase on TREC Web Track datasets demonstrate that current entity linking systems can provide sufficient coverage of the general domain search task, and that bag-of-entities representations outperform bag-of-words by as much as 18% in standard document ranking tasks.
- Anagha Kulkarni, Jamie Callan. 2015. Selective Search. Abstract: The traditional search solution for large collections divides the collection into subsets (shards), and processes the query against all shards in parallel (exhaustive search). The search cost and the computational requirements of this approach are often prohibitively high for organizations with few computational resources. This article investigates and extends an alternative: selective search, an approach that partitions the dataset based on document similarity to obtain topic-based shards, and searches only a few shards that are estimated to contain relevant documents for the query. We propose shard creation techniques that are scalable, efficient, self-reliant, and create topic-based shards with low variance in size, and high density of relevant documents. The experimental results demonstrate that the effectiveness of selective search is on par with that of exhaustive search, and the corresponding search costs are substantially lower with the former. Also, the majority of the queries perform as well or better with selective search. An oracle experiment that uses optimal shard ranking for a query indicates that selective search can outperform the effectiveness of exhaustive search. Comparison with a query optimization technique shows higher improvements in efficiency with selective search. The overall best efficiency is achieved when the two techniques are combined in an optimized selective search approach.
- Yazhe Wang, Jamie Callan, Baihua Zheng. 2015. Should We Use the Sample? Analyzing Datasets Sampled from Twitter’s Stream API. Abstract: Researchers have begun studying content obtained from microblogging services such as Twitter to address a variety of technological, social, and commercial research questions. The large number of Twitter users and even larger volume of tweets often make it impractical to collect and maintain a complete record of activity; therefore, most research and some commercial software applications rely on samples, often relatively small samples, of Twitter data. For the most part, sample sizes have been based on availability and practical considerations. Relatively little attention has been paid to how well these samples represent the underlying stream of Twitter data. To fill this gap, this article performs a comparative analysis on samples obtained from two of Twitter’s streaming APIs with a more complete Twitter dataset to gain an in-depth understanding of the nature of Twitter data samples and their potential for use in various data mining tasks.
- Chenyan Xiong, Jamie Callan. 2015. Query Expansion with Freebase. Abstract: Large knowledge bases are being developed to describe entities, their attributes, and their relationships to other entities. Prior research mostly focuses on the construction of knowledge bases, while how to use them in information retrieval is still an open problem. This paper presents a simple and effective method of using one such knowledge base, Freebase, to improve query expansion, a classic and widely studied information retrieval task. It investigates two methods of identifying the entities associated with a query, and two methods of using those entities to perform query expansion. A supervised model combines information derived from Freebase descriptions and categories to select terms that are effective for query expansion. Experiments on the ClueWeb09 dataset with TREC Web Track queries demonstrate that these methods are almost 30% more effective than strong, state-of-the-art query expansion algorithms. In addition to improving average performance, some of these methods have better win/loss ratios than baseline algorithms, with 50% fewer queries damaged.
- Guoqing Zheng, Jamie Callan. 2015. Learning to Reweight Terms with Distributed Representations. Abstract: Term weighting is a fundamental problem in IR research and numerous weighting models have been proposed. Proper term weighting can greatly improve retrieval accuracies, which essentially involves two types of query understanding: interpreting the query and judging the relative contribution of the terms to the query. These two steps are often dealt with separately, and complicated yet not so effective weighting strategies are proposed. In this paper, we propose to address query interpretation and term weighting in a unified framework built upon distributed representations of words from recent advances in neural network language modeling. Specifically, we represent term and query as vectors in the same latent space, construct features for terms using their word vectors and learn a model to map the features onto the defined target term weights. The proposed method is simple yet effective. Experiments using four collections and two retrieval models demonstrates significantly higher retrieval accuracies than baseline models.
- Bhavana Dalvi, Jamie Callan, William W. Cohen. 2015. Research Showcase @ CMU. Abstract: The multiple Barnes function, de(cid:12)ned as a generalization of the Euler gamma function, is used in many applications of pure and applied mathematics and theoretical physics. This paper presents new integral representations as well as special values of the Barnes function. Moreover, the Barnes function is expressed in a closed form by means of the Hurwitz zeta function. These results can be used for numeric and symbolic computations of the Barnes function
- Reyyan Yeniterzi, Jamie Callan. 2015. Moving from Static to Dynamic Modeling of Expertise for Question Routing in CQA Sites. Abstract: 
 
 CQA sites are dynamic environments where new users join constantly, or the activity levels or interest of existing users change over time. Classic expertise estimation approaches which were mostly developed for static datasets, cannot effectively model changing expertise and interest levels in these sites. This paper proposes how available temporal information in CQA sites can be used to make these existing approaches more effective for expertise related applications like question routing. Adapting two widely used expert finding approaches for question routing returned consistent and statistically significant improvements over the original approaches, which shows the effectiveness of the proposed temporal modeling.
 

- Chenyan Xiong, Jamie Callan. 2015. EsdRank: Connecting Query and Documents through External Semi-Structured Data. Abstract: This paper presents EsdRank, a new technique for improving ranking using external semi-structured data such as controlled vocabularies and knowledge bases. EsdRank treats vocabularies, terms and entities from external data, as objects connecting query and documents. Evidence used to link query to objects, and to rank documents are incorporated as features between query-object and object-document correspondingly. A latent listwise learning to rank algorithm, Latent-ListMLE, models the objects as latent space between query and documents, and learns how to handle all evidence in a unified procedure from document relevance judgments. EsdRank is tested in two scenarios: Using a knowledge base for web search, and using a controlled vocabulary for medical search. Experiments on TREC Web Track and OHSUMED data show significant improvements over state-of-the-art baselines.
- J. Araki, Jamie Callan. 2014. An annotation similarity model in passage ranking for historical fact validation. Abstract: State-of-the-art question answering (QA) systems employ passage retrieval based on bag-of-words similarity models with respect to a query and a passage. We propose a combination of a traditional bag-of-words similarity model and an annotation similarity model to improve passage ranking. The proposed annotation similarity model is generic enough to process annotations of arbitrary types. Historical fact validation is a subtask to determine whether a given sentence tells us historically correct information, which is important for a QA task on world history. Experimental results show that the combined model gains up to 7.7% and 4.2% improvements in historical fact validation in terms of precision at rank 1 and mean reciprocal rank, respectively.
- Reyyan Yeniterzi, Jamie Callan. 2014. Analyzing bias in CQA-based expert finding test sets. Abstract: Data retrieved from community question answering (CQA) sites, such as content and users' assessments of content, is commonly used for expertise estimation related tasks. One such task, in which the received votes are directly used as graded relevance assessment values, is ranking replies of a question. Even though these available assessments values are very practical for evaluation purposes, they may not always reflect the correct assessment value of the content, due to the possible temporal or presentation bias introduced by the CQA system during voting process. This paper analyzes a very commonly used CQA data collection in terms of these introduced biases and their effects on the experimental evaluation of approaches. A more bias free test set construction approach, which has correlated results with the manual assessments, is also proposed in this paper.
- Shriphani Palakodety, Jamie Callan. 2014. Query Transformations for Result Merging. Abstract: Abstract : This paper describes Carnegie Mellon University's entry at the TREC 2014 Federated Web Search track (FedWeb14). Federated search pipelines typically have two components: (i) resource-selection, and (ii) result-merging. This work documents experiments to modify queries to merge results in the federated-search pipeline. Approaches from previous attempts at solving this problem involved custom query- document similarity scores or rank-combination methods. In this document, we explore how term-dependence models and query expansion strategies influence result-merging.
- Reyyan Yeniterzi, Jamie Callan. 2014. Constructing effective and efficient topic-specific authority networks for expert finding in social media. Abstract: Authority-based approaches are widely used in expert retrieval from social media. However, most of these approaches are applied to either topic-independent networks, or more topic-dependent networks which still contain topic-irrelevant users as nodes and interactions as edges. Therefore, authority estimation over these graphs is still not topic-specific enough. This paper proposes a more topic-focused authority network construction approach which provides more effective topic-specific authority modeling of users. Focusing the computational effort to more topic-specific authority networks also leads to significant gains in running time for authority estimation.
- Di Xu, Jamie Callan. 2014. Modelling Psychological Needs for User-dependent Contextual Suggestion. Abstract: This paper presents our approach for the Contextual Suggestion track of 2014 Text REtrieval Conference (TREC). The task aims to provide recommendations on points of interests (POI) for various kinds of users under different contexts. This becomes challenging due to the limited amount of training data provided by TREC and the demanding constraints for a suggestion to be judged as relevant. Our approach does not deviate from existing Machine Learning based methods in principle, but sticks closely to the defined relevance judgement criteria, by focusing primarily on modelling users’ preferences on POI categories, and investigating upon their psychological expectations on the textual descriptions of the POIs. The latter is considered as our novelty in this work. Support Vector Regression was used for suggestion ranking, an ad-hoc web information extractor was used to collect POI descriptions, and a description evaluation mechanism was engaged to select proper POI descriptions subject to the nature of the POIs. Our results suggest that our methods are effective in obtaining satisfying user-specific POI rankings and generating descriptions that meet users’ psychological expectations.
- Di Xu, Jamie Callan. 2014. Towards a Simple and Efficient Web Search Framework. Abstract: Abstract : The Web Track of 2014 Text REtrieval Conference (TREC) addresses the most fundamental problem of Information Retrieval. We did not intend to craft a system that beats the state-of-the-art search engines, but to design a light weight and cost-effective system with comparable performances. We introduce a twopass retrieval framework, with the first pass consisting of a simple and efficient retrieval model that focuses on recall, and the second pass a wave of feature extraction algorithms run on the set of top ranked documents, followed by Learning to Rank (LETOR) algorithms that provide different precision oriented rankings, and their outputs are combined using data fusion. We have focused on using statistical Language Models with novel and well-known smoothing techniques, different LETOR methods and various data fusion techniques. In addition, we have also tried using topic modelling with Hierarchical Dirichlet Allocation for query expansion in the hope of improving diversity of our results. However, the topic modelling approach has turned out to be unsuccessful, and we have not been able to spot the problem and benefit from it in this work. In addition we also present some further analyses demonstrating that our approach is robust against overfitting, and some general studies on overfitting in the context of LETOR.
- Bhavana Dalvi, Chenyan Xiong, Jamie Callan. 2014. A language modeling approach to entity recognition and disambiguation for search queries. Abstract: The Entity Recognition and Disambiguation (ERD) problem refers to the task of recognizing mentions of entities in a given query string, disambiguating them, and mapping them to entities in a given Knowledge Base(KB). If there are multiple ways to interpret the query, then an ERD system is supposed to group candidate entity annotations into consistent interpretations.
 In this paper, we propose a four step solution to this problem. First, we generate candidate entity strings by segmenting queries in different ways. Second, we retrieve candidate entities by searching for these candidate entity stringsin Freebase. Third, we rank the candidate entities using language model based query likelihood scores. Finally, we group the entity annotations into interpretations. We also present both quantitative and qualitative evaluation of our methods based on 91 training, 500 validation and 1000 test queries. Our system achieved an F1 score of 0.42 on the set of validation queries, whereas the NULL baseline which returns no annotations for any query achieved an F1 score of 0.3. Similarly, on the test queries, our method achieved an F1 score of 0.36 and outperformed the NULL baseline which achieved an F1 score of 0.2.
- Jamie Callan. 2013. Selective Search of Large Text Collections. Abstract: Information retrieval research has been hampered by the difficulty of conducting research on web datasets of realistic size due to the computational resources required for such datasets. When the text collection is too large for a single machine, its index is divided into ‘shards’ that are distributed across a computer cluster and searched in parallel, which is effective but expensive. This talk describes an alternative architecture for large-scale text search in which the corpus is decomposed into index shards that are expected to have skewed utility distributions, thus enabling most shards to be ignored for most queries. This selective search architecture is equally effective, but has lower computational costs, which makes it an attractive architecture for organizations that have modest computational resources, modest query traffic, and elastic response time requirements. Selective search is a new application of ideas developed originally for distributed, federated, and aggregated search, however this new application of those ideas challenges some of the assumptions and design criteria that motivated prior research. The partitioning process creates text collections, thus inviting research on what characteristics are desired or to be avoided in a text collection to enable accurate search. Most resource selection algorithms are designed to search a static number of resources; however, when the distribution of content across a set of index shards is skewed intentionally, it is important to dynamically estimate the search effort required for each query. Selective search is a cooperative search architecture, which simplifies the problem of merging results from different shards for unstructured queries; however, better methods of merging results for structured queries are still desirable. This talk provides an introduction to the selective search architectures, and how it relates to prior research on distributed and federated search. It considers the types of problems and environments that might, or might not, benefit from the selective search architecture. It concludes by discussing open research problems and interesting research directions.
- Bhavana Dalvi, William W. Cohen, Jamie Callan. 2013. Classifying entities into an incomplete ontology. Abstract: Exponential growth of unlabeled web-scale datasets, and class hierarchies to represent them, has given rise to new challenges for hierarchical classification. It is costly and time consuming to create a complete ontology of classes to represent entities on the Web. Hence, there is a need for techniques that can do hierarchical classification of entities into incomplete ontologies. In this paper we present Hierarchical Exploratory EM algorithm (an extension of the Exploratory EM algorithm [7]) that takes a seed class hierarchy and seed class instances as input. Our method classifies relevant entities into some of the classes from the seed hierarchy and on its way adds newly discovered classes into the hierarchy. Experiments with subsets of the NELL ontology and text datasets derived from the ClueWeb09 corpus show that our Hierarchical Exploratory EM approach improves seed class F1 by up to 21% when compared to its semi-supervised counterpart.
- Bhavana Dalvi, William W. Cohen, Jamie Callan. 2012. WebSets: extracting sets of entities from the web using unsupervised information extraction. Abstract: We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs.
- Jamie Callan, Alistair Moffat. 2012. Panel on use of proprietary data. Abstract: A panel discussion on the use of proprietary data was held at SIGIR 2012 in Portland. This report summarizes the positions put forward by the six panelists and the points that arose during the wider discussion that followed.
- Le Zhao, Jamie Callan. 2012. Automatic term mismatch diagnosis for selective query expansion. Abstract: People are seldom aware that their search queries frequently mismatch a majority of the relevant documents. This may not be a big problem for topics with a large and diverse set of relevant documents, but would largely increase the chance of search failure for less popular search needs. We aim to address the mismatch problem by developing accurate and simple queries that require minimal effort to construct. This is achieved by targeting retrieval interventions at the query terms that are likely to mismatch relevant documents. For a given topic, the proportion of relevant documents that do not contain a term measures the probability for the term to mismatch relevant documents, or the term mismatch probability. Recent research demonstrates that this probability can be estimated reliably prior to retrieval. Typically, it is used in probabilistic retrieval models to provide query dependent term weights. This paper develops a new use: Automatic diagnosis of term mismatch. A search engine can use the diagnosis to suggest manual query reformulation, guide interactive query expansion, guide automatic query expansion, or motivate other responses. The research described here uses the diagnosis to guide interactive query expansion, and create Boolean conjunctive normal form (CNF) structured queries that selectively expand 'problem' query terms while leaving the rest of the query untouched. Experiments with TREC Ad-hoc and Legal Track datasets demonstrate that with high quality manual expansion, this diagnostic approach can reduce user effort by 33%, and produce simple and effective structured queries that surpass their bag of word counterparts.
- Anagha Kulkarni, Almer S. Tigelaar, D. Hiemstra, Jamie Callan. 2012. Shard ranking and cutoff estimation for topically partitioned collections. Abstract: Large document collections can be partitioned into 'topical shards' to facilitate distributed search. In a low-resource search environment only a few of the shards can be searched in parallel. Such a search environment faces two intertwined challenges. First, determining which shards to consult for a given query: shard ranking. Second, how many shards to consult from the ranking: cutoff estimation. In this paper we present a family of three algorithms that address both of these problems. As a basis we employ a commonly used data structure, the central sample index (CSI), to represent the shard contents. Running a query against the CSI yields a flat document ranking that each of our algorithms transforms into a tree structure. A bottom up traversal of the tree is used to infer a ranking of shards and also to estimate a stopping point in this ranking that yields cost-effective selective distributed search. As compared to a state-of-the-art shard ranking approach the proposed algorithms provide substantially higher search efficiency while providing comparable search effectiveness.
- Yubin Kim, Reyyan Yeniterzi, Jamie Callan. 2012. Overcoming Vocabulary Limitations in Twitter Microblogs. Abstract: Abstract : One major di culty in performing ad-hoc search on microblogs such as Twitter is the limited vocabulary of each document due their short length. In this paper, two approaches to addressing this issue are presented. The rst is query expansion through pseudo-relevance feedback and the other is document expansion of tweets using web documents linked from the body of the tweet. Tweets are expanded by concatenating the contents of the title tag and the meta descriptor tags of the document to the tweet itself. These two approaches gave additive gains in MAP and Precision at 30.
- Nachiketa Sahoo, R. Krishnan, G. Duncan, Jamie Callan. 2012. Research Note - The Halo Effect in Multicomponent Ratings and Its Implications for Recommender Systems: The Case of Yahoo! Movies. Abstract: Collaborative filtering algorithms learn from the ratings of a group of users on a set of items to find personalized recommendations for each user. Traditionally they have been designed to work with one-dimensional ratings. With interest growing in recommendations based on multiple aspects of items, we present an algorithm for using multicomponent rating data. The presented mixture model-based algorithm uses the component rating dependency structure discovered by a structure learning algorithm. The structure is supported by the psychometric literature on the halo effect. This algorithm is compared with a set of model-based and instance-based algorithms for single-component ratings and their variations for multicomponent ratings. We evaluate the algorithms using data from Yahoo! Movies. Use of multiple components leads to significant improvements in recommendations. However, we find that the choice of algorithm depends on the sparsity of the training data. It also depends on whether the task of the algorithm is to accurately predict ratings or to retrieve relevant items. In our experiments a model-based multicomponent rating algorithm is able to better retrieve items when training data are sparse. However, if the training data are not sparse, or if we are trying to predict the rating values accurately, then the instance-based multicomponent rating collaborative filtering algorithms perform better. Beyond generating recommendations we show that the proposed model can fill in missing rating components. Theories in psychometric literature and the empirical evidence suggest that rating specific aspects of a subject is difficult. Hence, filling in the missing component values leads to the possibility of a rater support system to facilitate gathering of multicomponent ratings.
- Jamie Callan. 2012. TOIS Reviewers: October 2009 To September 2012. Abstract: The articles published by ACM Transactions on Information Systems (TOIS) are selected through a peer-review process. Associate Editors on the editorial board recruit reviewers and manage the review process for each article. Reviewers read the article and write reviews that identify the problem that the article addresses, the new methods that are proposed, the quality of the experimental evaluation, and the novel intellectual contributions. Associate Editors contribute an additional review that summarizes the decision process. The quality of a scientific journal depends entirely on the quality of the reviewers and editorial board. It is essential that reviewers and editors be highly-skilled, rigorous, and thorough. However, the TOIS review process is also characterized by substantial mentoring and guidance. The typical TOIS article is revised twice prior to publication. Typically reviewers offer many suggestions about how an article can be strengthened, for example, broadening coverage of related work, adding experiments that address important related questions, narrowing claims, or improving writing quality. Perhaps the most important duty of an Associate Editor is to organize and synthesize the many and varied suggestions into a clear set of instructions about how to improve the article. The mentoring and guidance that our reviewers and editors provide is an important difference between the review processes of journals and conferences. We believe that it produces much stronger articles. The scholars listed below provided reviews to TOIS during the last three years. (Associate Editors are not included, although they occasionally provide reviews, too.) On behalf of the journal and the scientific communities that it serves, I thank them for their service to the journal. The journal would not exist without their effort and dedication. Reviewers with asterisks next to their names (e.g., “John Doe ***”) are noted for their reviewing excellence; the quality of their reviews was in the top 10% of all reviews received by the journal. Their contributions to the journal and the scientific community that it serves are especially noteworthy.
- Bhavana Dalvi, William W. Cohen, Jamie Callan. 2012. Collectively Representing Semi-Structured Data from the Web. Abstract: In this paper, we propose a single low-dimensional representation of a large collection of table and hyponym data, and show that with a small number of primitive operations, this representation can be used effectively for many purposes. Specifically we consider queries like set expansion, class prediction etc. We evaluate our methods on publicly available semi-structured datasets from the Web.
- Le Zhao, Xiaozhong Liu, Jamie Callan. 2012. WikiQuery - An Interactive Collaboration Interface for Creating, Storing and Sharing Effective CNF Queries. Abstract: Conjunctive Normal Form (CNF) expansion can effec- tively address the vocabulary mismatch problem, a problem that current retrieval techniques have very limited ability to solve. Meanwhile, expert searchers are found to spend large amounts of time carefully creating manual CNF queries. These CNF queries are highly effective, and can outperform bag of word queries by a large margin. However, not many effective tools exist that can facilitate the efficient manual creation of effective CNF queries. We describe such a publicly available search tool, WikiQuery, which can efficiently assist the users to create CNF queries through easy query editing and immediate access to search results. Experiments show that ordinary search users, with limited prior knowledge of Boolean queries, can use this intuitive tool to create effective CNF queries. We argue that tools like WikiQuery can attract and retain certain users from the commercial Web search engines, and may be a good starting point to build a research Web search engine.
- W. Hersh, Jamie Callan, Y. Maarek, M. Sanderson. 2012. Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. Abstract: We are delighted to welcome you to the 35th edition of SIGIR, the ACM International Conference on Research and Development in Information Retrieval. The conference continues its tradition of being the premier forum for research and development information retrieval, the computer science discipline behind what many call "search". The high number of submitted papers, this year again, demonstrates both the breadth and depth of the research being done in this vibrant field, both in academia and industry. We have done our best to ensure that these papers meet high standards of quality in terms of technical contribution, innovation, presentation, reference to previous work, and methodology. At the same time, we have tried to be flexible in the application of these criteria in order to consider papers describing novel and innovative work that may be somewhat unconventional. 
 
The conference received 483 full paper submissions this year. Examining the country code of the paper's contact author, we found that 185 (38%) come from the Americas; 158 (33%), Asia and Pacific region; and 140 (29%) from Europe, the Middle East and Africa. Of these, 98 (20%) were accepted, essentially the same as last year's acceptance rate and up from the 16.7% rate of the year before. There was almost no difference in the acceptance rates across the three broad regions. The top five countries in terms of accepted papers were the U.S.A. (36), China (14), the U.K. & Spain (both 7), and the Netherlands (6). In addition, 208 short papers were submitted to the poster track, of which 76 (36.5%) were accepted. In the other categories, there were 17 (47.2%) demonstrations, 4 workshops, and 16 tutorials accepted. The top five technical areas (as inferred from the primary keyword assigned by the authors) covered by the accepted papers, were queries and query analysis (18%), retrieval models and ranking (14%), web IR & social media search (13%), document representation and content analysis (11%), and users and interactive IR (9%). This was a small re-ordering of the topics from last year. 
 
SIGIR this year again used a two-tier double-blind reviewing approach. In a first stage, at least three reviewers read every paper and provided ratings and comments. Then, in a second stage, the primary and secondary Area Chairs ensured the quality of the reviewing process by studying, validating, and summarizing these reviews, and adding their own feedback and ratings. When required, Area Chairs initiated a discussion among the reviewers to resolve any controversial issues or significant differences of opinion. Once the discussion stage was completed, the two Area Chairs made the final decisions for nearly all submitted papers. At the program committee meeting held in Haifa, Israel, the Program Chairs and the attending Area Chairs went over the reviews, verified the process, gathered additional input, and made decisions in the few cases for which assistance had been requested.
- Le Zhao, Jamie Callan. 2011. How to Make Manual Conjunctive Normal Form Queries Work in Patents Search. Abstract: This year we focused on the Technology Survey (TS) task: Given a natural language description of the topic, look for related patents about that topic. The task is close to an ad hoc retrieval task, except for the additional information of the specific chemicals or chemical reactions that the user cares about. Since there are only 6 topics for the TS task, this notebook paper is more of a case study report, than the ordinary TREC report with significance tests. We found that on average, with the infAP measure, manually created conjunctive normal form queries performed similarly as automatic keyword search with some tuning of term weights. Manual queries do not seem to always help, especially when initial keyword performance is high, but can give large improvements on difficult queries. We also used the same querying strategy in the Patent Olympics 2011 ChemAthlon task, and also include some of the ChemAthlon cases in this report. Since CNF queries are strictly more expressive than keyword queries, we try to identify problems that may have caused the manual CNF queries to be seen sometimes performing worse than the automatic keyword queries.
- Bhavana Dalvi, William W. Cohen, Jamie Callan. 2011. WebSets : Unsupervised Information Extraction approach to Extract Sets of Entities from the Web [ Extended Abstract ]. Abstract: We propose an unsupervised information extraction system, which exploits the structured information in the form of HTML tables to build meaningful sets of entities belonging to certain categories. Due to redundancy on the Web, we believe that entities belonging to important categories will frequently co-occur in table columns. We present a clustering algorithm to cluster such frequently occurring entities into meaningful sets. Then we find candidate category names for each set, using Hearst patterns like “X such as Y”, “X including Y”. Experimental results on four different datasets show that our method can extract meaningful sets of entities (with avg. cluster precision of 97-99%). It also proposes reasonable category names for them. We present an application of this method to enhance an existing knowledge base. Experiments show that our method improves the coverage of existing categories with 80-90% accuracy. It also suggests new categories that can be added to the knowledge base.
- Thesis, Le Zhao, Jamie Callan, Yiming Yang, J. Carbonell. 2011. Modeling and Predicting Term Mismatch for Full-Text Retrieval. Abstract: The probability that a term appears in a relevant document is a fundamental quantity in the theory of probabilistic information retrieval, however prior research provided few clues about how to estimate it reliably. Since this probability measures how likely it is that a term has to appear in a document in order for the document to be relevant, in this thesis, it is called term necessity. Equivalently, it is also the proportion of relevant documents that contain the term, thus measures term recall, or the complement of term mismatch. This thesis uses exploratory data analysis to identify common reasons that user-specified query terms fail to match relevant documents, develops features correlated with each reason, and integrates them into a model that can be trained from data. The resulting term necessity predictions can be used as term weights in state-of-the-art retrieval models to improve retrieval accuracy substantially. Feature-based necessity prediction also supports diagnosis and improvement of query components. The thesis research will develop several forms of diagnosis and intervention. The simplest form is interactive feedback in which potential problems with query components are identified for a person to fix. More nuanced approaches to automatic formulations of structured queries are based on interventions that address different causes of term mismatch. For example, removing unnecessary terms, expanding the terms that are likely to mismatch, and weighting term disjunctions after query expansion. Improved weighting of structured query components also provides a new approach to addressing the field-length biases that persist in state-of-the-art retrieval models for structured documents. Collectively, these interventions leverage term necessity predictions to address a variety of common problems related to formation of effective queries.
- Jaime Arguello, Fernando Diaz, Jamie Callan. 2011. Learning to aggregate vertical results into web search results. Abstract: Aggregated search is the task of integrating results from potentially multiple specialized search services, or verticals, into the Web search results. The task requires predicting not only which verticals to present (the focus of most prior research), but also predicting where in the Web results to present them (i.e., above or below the Web results, or somewhere in between). Learning models to aggregate results from multiple verticals is associated with two major challenges. First, because verticals retrieve different types of results and address different search tasks, results from different verticals are associated with different types of predictive evidence (or features). Second, even when a feature is common across verticals, its predictiveness may be vertical-specific. Therefore, approaches to aggregating vertical results require handling an inconsistent feature representation across verticals, and, potentially, a vertical-specific relationship between features and relevance. We present 3 general approaches that address these challenges in different ways and compare their results across a set of 13 verticals and 1070 queries. We show that the best approaches are those that allow the learning algorithm to learn a vertical-specific relationship between features and relevance.
- Jamie Callan, Paul Ogilvie. 2010. Retrieval using document structure and annotations. Abstract: Successful retrieval of information from text collections requires effective use of the information present in a collection. The structure of documents in the collection and the relationships between elements within a document and other documents contain important information about the meaning of these elements. For example, the words present in the title of a web page may contain important clues about that page's content. The text of a link to the web page may also be an important indicator of the page's content. 
Researchers have long recognized that structure can be an important indicator of relevance. Yet the majority of prior work is limited to experiments on small test collections and evaluated on a single retrieval task. These limitations hamper the generality of the conclusions. The recent construction of large and diverse test collections provides us the opportunity to reconsider the general task of retrieval in collections with structure. 
This dissertation draws on three retrieval tasks to identify important properties of retrieval systems supporting the use of structure and annotations. We investigate known-item finding of web pages, retrieving elements from XML articles, and the retrieval of answer-bearing sentences as a component of a question-answering system. The retrieval model, an adaptation of the Inference Network model, clarifies the query language and simplifies the process of smoothing using multiple representations. The experiments in this dissertation show state-of-the-art results for these tasks and also provide novel insights to the shape of the parameter space when using mixtures of language models. Our experiments with question-answering further show how semantic predicates automatically annotated on a collection can be used to improve a system's ability to retrieve answer-bearing sentences.
- Jamie Callan. 2010. Search engine support for software applications. Abstract: Question-answering, computer-assisted language learning, text mining, and other software applications that use a full-search engine to find information in a large text corpus are becoming common. A software application may use metadata and text annotations to reduce the mismatch between the concept-based representations convenient for inference and the word-based representations typically used for text retrieval. Software applications may also be able to specify detailed requirements that retrieved passages must satisfy. This use of text search is very different than the ad-hoc, interactive search that information retrieval research typically studies. Search engine developers are beginning to respond by extending indexing and retrieval models developed for structured (e.g., XML) documents to support multiple representations of document content, text annotations, metadata, and relationships. These new requirements force developers to reconsider basic assumptions about index data structures and ranked retrieval models. How best to use these new capabilities is an open problem. Straightforward transformation of a detailed information need into a complex structured query can produce a query that is effective for exact-match retrieval, but a challenge for the retrieval model to use effectively for best-match retrieval. Bag-of-words retrieval is often disparaged, but its advantage is that it is robust: It works well even when desired documents do not exactly meet expectations. This talk discusses some of the problems encountered when extending a search engine to support queries posed by other software applications and structured documents with derived annotations
- Anagha Kulkarni, Jamie Callan. 2010. Topic-based Index Partitions for Efficient and Effective Selective Search. Abstract: Indexes for large collections are often divided into shards that are distributed across multiple computers and searched in parallel to provide rapid interactive search. Typically, all index shards are searched for each query. This paper investigates document allocation policies that permit searching only a few shards for each query (selective search) without sacrificing search quality. Three types of allocation policies (random, source-based and topic-based) are studied. Kmeans clustering is used to create topic-based shards. We manage the computational cost of applying these techniques to large datasets by defining topics on a subset of the collection. Experiments with three large collections demonstrate that selective search using topic-based shards reduces search costs by at least an order of magnitude without reducing search accuracy.
- Nachiketa Sahoo, Jamie Callan. 2010. The Halo Effect in Multi-component Ratings and its Implications for Recommender Systems : The Case of Yahoo ! Movies. Abstract: Collaborative filtering algorithms learn from the ratings of a group of users on a set of items to find personalized recommendations for each user. Traditionally they have been designed to work with one dimensional ratings. With interest growing in recommending based on multiple aspects of items (Adomavicius and Kwon 2007, Adomavicius and Tuzhilin 2005) we present an algorithm for using multi-component rating data. The presented mixture model based algorithm uses the component rating dependency structure discovered by a structure learning algorithm. The structure is supported by the psychometric literature on the halo effect. This algorithm is compared with a set of model based and instance based algorithms for single-component ratings and their variations for multi-component ratings. We evaluate the algorithms using data from Yahoo! Movies. Use of multiple components leads to significant improvements in recommendations. However, we find that the choice of algorithm depends on the sparsity of the training data. It also depends on whether the task of the algorithm is to accurately predict ratings or retrieve relevant items. In our experiments a model based multi-component rating algorithm is able to better retrieve items when training data is sparse. However, if the training data is not sparse, or if we are trying to predict the rating values accurately then the instance based multi-component rating collaborative filtering algorithms perform better. Beyond generating recommendations we show that the proposed model can fill-in missing rating components. Theories in psychometric literature and the empirical evidence suggest that rating specific aspects of a subject is difficult. Hence, filling in the missing component values leads to the possibility of a rater support system to facilitate gathering of multi-component ratings.
- Le Zhao, Jamie Callan. 2010. Term necessity prediction. Abstract: The probability that a term appears in relevant documents (P(t | R)) is a fundamental quantity in several probabilistic retrieval models, however it is difficult to estimate without relevance judgments or a relevance model. We call this value term necessity because it measures the percentage of relevant documents retrieved by the term - how necessary a term's occurrence is to document relevance. Prior research typically either set this probability to a constant, or estimated it based on the term's inverse document frequency, neither of which was very effective. This paper identifies several factors that affect term necessity, for example, a term's topic centrality, synonymy and abstractness. It develops term- and query-dependent features for each factor that enable supervised learning of a predictive model of term necessity from training data. Experiments with two popular retrieval models and 6 standard datasets demonstrate that using predicted term necessity estimates as user term weights of the original query terms leads to significant improvements in retrieval accuracy.
- Michael Heilman, Kevyn Collins-Thompson, Jamie Callan, M. Eskénazi, Alan Juffs, L. Wilson. 2010. Personalization of Reading Passages Improves Vocabulary Acquisition. Abstract: The REAP tutoring system provides individualized and adaptive English as a Second Language vocabulary practice. REAP can automatically personalize instruction by providing practice readings about topics that match interests as well as domain-based, cognitive objectives. While most previous research on motivation in intelligent tutoring environments has focused on increasing extrinsic motivation, this study focused on increasing personal interest. Students were randomly split into control and treatment groups. The control-condition tutor chose texts to maximize domain-based goals such as the density of practice opportunities for target words. The treatment-condition tutor also preferred texts that matched personal interests. The results show positive effects of personalization, and also demonstrate the importance of negotiating between motivational and domain-based goals.
- D. Feng, Jamie Callan, E. Hovy, Marius Pasca. 2010. Proceedings of the NAACL HLT 2010 Workshop on Semantic Search. Abstract: Welcome to the NAACL HLT Workshop on Semantic Search! 
 
Information retrieval (IR) research has been actively driven by the challenging information overload problem and many successful general-purpose commercial search engines. While the popularity of the largest search engines is a confirmation of the success and utility of IR, the identification, representation, and use of the often-complex semantics behind user queries has not yet been fully explored. 
 
In this workshop we target methods that exploit semantics in search-related tasks. One of the major obstacles in bridging the gap between IR and Natural Language Processing (NLP) is how to retain the flexibility and precision of working with text at the lexical level while gaining the greater descriptive precision that NLP provides. We have solicited contributions on automatic analysis of queries and documents in order to encode and exploit information beyond surface-level keywords: named entities, relations, semantic roles, etc. 
 
This workshop is meant to accelerate the pace of progress in semantic search techniques by connecting IR and NLP, bridging semantic analysis and search methodologies, and exploring the potentials of search utilizing semantics. We also focus on forming an interest group from different areas of research, exploring collaboration opportunities, providing deeper insight into bringing semantics into search, and provoking or encouraging discussions on all of its potential.
- Bhavana Dalvi, Jamie Callan, William W. Cohen. 2010. Entity List Completion Using Set Expansion Techniques. Abstract: Abstract : Set expansion refers to expanding a partial set of "seed" objects into a more complete set. In this paper, we focus on relation and list extraction techniques to perform Entity List Completion task through a two stage retrieval process. First stage takes given query entity and target entity examples as seeds and does set expansion. In second stage, only those candidates who have valid URI in Billion Triple dataset are ranked according to type match with given types. First stage of this system focuses on the recall while second stage tries to improve precision of the outputted list. We submitted the results on the Web as well as ClueWeb09 corpus.
- Anagha Kulkarni, Jamie Callan. 2010. Document allocation policies for selective searching of distributed indexes. Abstract: Indexes for large collections are often divided into shards that are distributed across multiple computers and searched in parallel to provide rapid interactive search. Typically, all index shards are searched for each query. For organizations with modest computational resources the high query processing cost incurred in this exhaustive search setup can be a deterrent to working with large collections. This paper investigates document allocation policies that permit searching only a few shards for each query (selective search) without sacrificing search accuracy. Random, source-based and topic-based document-to-shard allocation policies are studied in the context of selective search. A thorough study of the tradeoff between search cost and search accuracy in a sharded index environment is performed using three large TREC collections. The experimental results demonstrate that selective search using topic-based shards cuts the search cost to less than 1/5th of that of the exhaustive search without reducing search accuracy across all the three datasets. Stability analysis shows that 90% of the queries do as well or improve with selective search. An overlap-based evaluation with an additional 1000 queries for each dataset tests and confirms the conclusions drawn using the smaller TREC query sets.
- Dong Nguyen, Jamie Callan. 2010. Combination of Evidence for Effective Web Search. Abstract: Abstract : In this paper we describe Carnegie Mellon University's sub-mission to the TREC 2010 Web Track. Our baseline run combines different methods, of which in particular the spam prior and mixture model were found the most effective. We also experimented with expansion over the Wikipedia corpus and found that picking the right Wikipedia articles for expansion can improve performance substantially. Furthermore, we did preliminary experiments with combining expansion over the Wikipedia corpus with expansion over the top ranked web pages.
- Hui Yang, Jamie Callan. 2009. OntoCop: Constructing Ontologies for Public Comments. Abstract: U.S. law defines a process known as Notice and Comment Rulemaking that requires regulatory agencies to seek comment from the public before establishing new regulations. Regulatory agencies are also expected to demonstrate that the final regulation addresses all substantive issues raised by the public. Most proposed regulations attract few comments from the public, but each year a few regulations attract tens of thousands or hundreds of thousands of comments. When comment volume is high, most comments are form letters and modified form letters, which are not difficult to process [1], but there are still tens of thousands of unique comments that address a variety of issues. There may also be political or public pressure for the agency to issue regulations quickly. In these cases, regulatory agencies and other interested parties need tools that help them to quickly make sense of public comments.
- G. Yang, Jamie Callan. 2009. Feature selection for automatic taxonomy induction. Abstract: Most existing automatic taxonomy induction systems exploit one or more features to induce a taxonomy; nevertheless there is no systematic study examining which are the best features for the task under various conditions. This paper studies the impact of using different features on taxonomy induction for different types of relations and for terms at different abstraction levels. The evaluation shows that different conditions need different technologies or different combination of the technologies. In particular, co-occurrence and lexico-syntactic patterns are good features for is-a, sibling and part-of relations; contextual, co-occurrence, patterns, and syntactic features work well for concrete terms; co-occurrence works well for abstract terms.
- Jonathan L. Elsas, Pinar E. Donmez, Jamie Callan, J. Carbonell. 2009. Pairwise Document Classification for Relevance Feedback. Abstract: In this paper we present Carnegie Mellon University’s submission to the TREC 2009 Relevance Feedback Track. In this submission we take a classication approach on document pairs to using relevance feedback information. We explore using textual and non-textual document-pair features to classify unjudged documents as relevant or non-relevant, and use this prediction to re-rank a baseline document retrieval. These features include co-citation measures, URL similarities, as well as features often used in machine learning systems for document ranking such as the dierence in scores assigned by the baseline retrieval system.
- Le Zhao, Jamie Callan. 2009. Effective and efficient structured retrieval. Abstract: Search engines that support structured documents typically support structure created by the author (e.g., title, section), and may also support structure added by an annotation process (e.g., part of speech, named entity, semantic role). Exploiting such structure can be difficult. Query structure may fail to match structure in a relevant document for a variety of reasons, thus structured queries, although containing more information than keyword queries, are often less effective than unstructured queries. This paper studies retrieval of sentences with annotations for a question answering task. Three problems of structured retrieval are identified and solutions proposed. Structural mismatch is addressed by query structure expansion of predicted relevant structures. Lack of presence of all key aspects of a question is solved by Boolean filtering of result sentences. The score variations of the annotator generated fields with all the different lengths are accounted for by using field specific smoothing. Experiments show that each solution incrementally improves structured retrieval, and a combination of Boolean filtering, structural expansion, and keyword queries outperforms keyword and simple structured retrieval baselines.
- Jaime Arguello, Fernando Diaz, Jamie Callan, Jean-François Crespo. 2009. Sources of evidence for vertical selection. Abstract: Web search providers often include search services for domain-specific subcollections, called verticals, such as news, images, videos, job postings, company summaries, and artist profiles. We address the problem of vertical selection, predicting relevant verticals (if any) for queries issued to the search engine's main web search page. In contrast to prior query classification and resource selection tasks, vertical selection is associated with unique resources that can inform the classification decision. We focus on three sources of evidence: (1) the query string, from which features are derived independent of external resources, (2) logs of queries previously issued directly to the vertical, and (3) corpora representative of vertical content. We focus on 18 different verticals, which differ in terms of semantics, media type, size, and level of query traffic. We compare our method to prior work in federated search and retrieval effectiveness prediction. An in-depth error analysis reveals unique challenges across different verticals and provides insight into vertical selection for future work.
- Jaime Arguello, Jamie Callan, Fernando Diaz. 2009. Classification-based resource selection. Abstract: In some retrieval situations, a system must search across multiple collections. This task, referred to as federated search, occurs for example when searching a distributed index or aggregating content for web search. Resource selection refers to the subtask of deciding, given a query, which collections to search. Most existing resource selection methods rely on evidence found in collection content. We present an approach to resource selection that combines multiple sources of evidence to inform the selection decision. We derive evidence from three different sources: collection documents, the topic of the query, and query click-through data. We combine this evidence by treating resource selection as a multiclass machine learning problem. Although machine learned approaches often require large amounts of manually generated training data, we present a method for using automatically generated training data. We make use of and compare against prior resource selection work and evaluate across three experimental testbeds.
- Le Zhao, Jamie Callan. 2009. Formulating Simple Structured Queries Using Temporal and Distributional Cues in Patents. Abstract: Abstract : Patent prior art retrieval aims to find related publications, especially patents, which may invalidate the patent. The task exhibits its own characteristic because of the possible use of a whole patent as a query. This work focuses on the use of date fields and content fields of the query patent to formulate effective structured queries. Retrieval is performed on the collection of patents which also share the same structure as the query patent, mainly priority dates, application date, publication date and content fields. Unsurprisingly, results show that filtering using date information improves retrieval significantly. However, results also show that a careful choice of the date filter is important, given the multiple date fields existent in a patent. The actual ranking query is constructed based on word distributions of title, claims and content fields of the query patent. The overall MAP of this citation finding task is still in the lower 0.1 range. An error analysis focusing on the lower performing topics finds that the citation finding task (given publication recommend citations, which is a very similar setup as this year's prior art evaluation) can be very different from the prior art task (finding patents that invalidates the query patent). It raises the concern that just the citations included in query patents can be a biased and incomplete set of relevance judgements for the prior art task.
- G. Yang, Jamie Callan. 2009. A Metric-based Framework for Automatic Taxonomy Induction. Abstract: This paper presents a novel metric-based framework for the task of automatic taxonomy induction. The framework incrementally clusters terms based on ontology metric, a score indicating semantic distance; and transforms the task into a multi-criteria optimization based on minimization of taxonomy structures and modeling of term abstractness. It combines the strengths of both lexico-syntactic patterns and clustering through incorporating heterogeneous features. The flexible design of the framework allows a further study on which features are the best for the task under various conditions. The experiments not only show that our system achieves higher F1-measure than other state-of-the-art systems, but also reveal the interaction between features and various types of relations, as well as the interaction between features and term abstractness.
- Nachiketa Sahoo, R. Krishnan, G. Duncan, Jamie Callan. 2008. On Multi-component Rating and Collaborative Filtering for Recommender Systems : The Case of Yahoo ! Movies. Abstract: Collaborative filtering algorithms learn from the ratings of a group of users on a set of items to find recommendations for each user. Traditionally they have been designed to work with one dimensional ratings. With interest growing in recommending based on multiple aspects of items (Adomavicius and Kwon 2007, Adomavicius and Tuzhilin 2005) we present an algorithm for using multi-component rating data. This mixture model based algorithm uses the dependency structure between the rating components discovered by a structure learning algorithm and validated by the psychometric literature on the halo effect. We evaluate the algorithm using data from Yahoo! Movies. Use of multiple components leads to significant improvements in recommendations when we have small amount of training data–a case very common in real life implementations. However, when there is more training data we gain little from using additional components of the ratings. Beyond generating recommendations we show that the proposed model can fill-in missing rating components. Theories in psychometric literature and the empirical evidence suggest that rating specific aspects of a subject is difficult. Hence, filling in the missing component values leads to the possibility of a rater support system to facilitate gathering of multi-component ratings. We also show that this allows recommendations to be generated for more users.
- G. Yang, Jamie Callan. 2008. Metric-based ontology learning. Abstract: Ontology learning is an important task in Artificial Intelligence, Semantic Web and Text Mining. This paper presents a novel framework for, and solutions to, three practical problems in ontology learning. An incremental clustering approach is used to solve the problem of unknown group names. Learned models at each level of an ontology address the problem of no control over concept abstractness. A metric learning module moves beyond the limitation of traditional use of features and incorporates heterogeneous semantic evidence into the learning process. The metric-based learning framework integrates these separate components into a single, unified solution. An extensive evaluation with WordNet and Open Directory Project data demonstrates that the method is more effective than a state-of-the-art baseline algorithm.
- Jaime Arguello, Jamie Callan, Stuart W. Shulman. 2008. Recognizing Citations in Public Comments. Abstract: ABSTRACT Notice and comment rulemaking is central to how U.S. federal agencies craft new regulation. E-rulemaking, the process of soliciting and considering public comments that are submitted electronically, poses a challenge for agencies. The large volume of comments received makes it difficult to distill and address the most substantive concerns of the public. This work attempts to alleviate this burden by applying existing machine learning techniques to the problem of recognizing citation sentences. A citation in this context is defined as a statement in which the author of the public comment references an external source of factual information that is associated with a specific person or organization. The problem is formulated as a binary classification problem: Is a specific person or organization mentioned in a sentence being referenced as an external source of information? We show that our definition of a citation is reproducible by human judges and that citations can be detected using machine learning techniques with some success. Casting this as a machine learning problem requires selecting an appropriate representation of the sentence. Several feature sets are evaluated individually and in combination. Superior results are obtained by combining feature sets. Syntactic features, which characterize the structure of the sentence rather than its content, significantly improve accuracy when combined with other features, but not when used in isolation. Although prediction error rate is adequate, coverage could be improved. An error analysis enumerates short-term and long-term challenges that must be overcome to improve recall.
- Jaime Arguello, Jonathan L. Elsas, Changkuk Yoo, Jamie Callan, J. Carbonell. 2008. Document and Query Expansion Models for Blog Distillation. Abstract: This paper presents the CMU submission to the 2008 TREC blog distillation track. Similar to last year’s experiments, we evaluate dierent retrieval models and apply a query expansion method that leverages the link structure in Wikipedia. We also explore using a corpus that combines several dierent representations of the documents, using both the feed XML and permalink HTML, and apply initial experiments with spam filtering.
- G. Yang, Jamie Callan. 2008. Ontology generation for large email collections. Abstract: This paper presents a new approach to identifying concepts expressed in a collection of email messages, and organizing them into an ontology or taxonomy for browsing. It incorporates techniques from text mining, information retrieval, natural language processing and machine learning to generate a concept ontology. Nominal N-gram mining is used to identify candidate concepts. Wordnet and surface text pattern matching are used to identify relationships among the concepts. A supervised clustering algorithm is then used to further cluster the concepts. The experiments show that the approach is effective.
- Jaime Arguello, Jonathan L. Elsas, Jamie Callan, J. Carbonell. 2008. Document Representation and Query Expansion Models for Blog Recommendation. Abstract: We explore several different document representation models and two query expansion models for the task of recommending blogs to a user in response to a query. Blog relevance ranking differs from traditional document ranking in ad-hocinformation retrieval in several ways: (1) the unit of output (the blog) is composed of a collection of documents (the blog posts) rather than a single document, (2) the query represents an ongoing and typically multifaceted interest in the topic rather than a passing ad-hoc information need and (3) due to the propensity of spam, splogs, and tangential comments, the blogosphere is particularly challenging to use as a source for high-quality query expansion terms. We address these differences at the document representation level, by comparing retrieval models that view either the blog or its constituent posts as the atomic units of retrieval, and at the query expansion level, by making novel use of the links and anchor text in Wikipedia1 to expand a user's initial query. We develop two complementary models of blog retrieval that perform at comparable levels of precision and recall. We also show consistent and significant improvement across all models using our Wikipedia expansion strategy.
- Anagha Kulkarni, Jamie Callan. 2008. Dictionary Definitions based Homograph Identification using a Generative Hierarchical Model. Abstract: A solution to the problem of homograph (words with multiple distinct meanings) identification is proposed and evaluated in this paper. It is demonstrated that a mixture model based framework is better suited for this task than the standard classification algorithms - relative improvement of 7% in F1 measure and 14% in Cohen's kappa score is observed.
- M. Bilotti, Le Zhao, Jamie Callan, Eric Nyberg. 2008. Focused Retrieval over Richly-Annotated Collections. Abstract: This paper introduces a theoretical framework for focused retrieval, based on a formalism called the annotation graph. Annotation graph-based retrieval provides a rich retrieval representation that directly supports query-time constraintchecking of arbitrary relations. This representation can support focused retrieval tasks, such as Question Answering systems, which often have information needs containing constraint types that can not be queried easily under many retrieval models. The problem of annotation graph-based retrieval is mapped onto existing XML element retrieval functionality in the Indri search engine. The remainder of the paper serves to identify and discuss the issues that emerged and illustrate by example what in our opinion constitutes the upcoming research challenges facing the focused retrieval community.
- E. Efthimiadis, Jamie Callan, R. Larson. 2008. Approaches to teaching & learning information retrieval. Abstract: Summary 
 
 
 
The explosion of the web has made search an integral part of our daily lives. We search for almost any conceivable topic. Web search engines have made search easily approachable to almost everyone. Yet, for information professionals it is more important than ever before to know “how search works” in order to be more effective in their work. Search Engines or Information Retrieval systems often appear to searchers as “black boxes.” There is some sort of magic that happens between typing some keywords in a query box and getting back results. This approach contributes to the development of inadequate conceptual models of search. 
 
 
 
The panel brings LIS and CS educators involved in teaching “information retrieval” to discuss experiential learning approaches to teaching IR. Efthimiadis (UW) will be presenting the IR-Toolbox, an interactive system developed for teaching IR processes to Information School students. Ray Larson (UCB) will be discussing his approach of using open source IR engines to create a mini-TREC competition environment in class. Jamie Callan (CMU) will be talking about the Lemur Toolkit system and its use in teaching undergraduate and graduate students. 
 
 
 
Following the panel presentation of the experiential teaching methods, an interactive session with the audience will follow. The discussion session will focus on the audience's needs and experiences while learning or teaching information retrieval.
- Le Zhao, Chenmin Liang, Jamie Callan. 2008. Extending Relevance Model for Relevance Feedback. Abstract: Abstract : Relevance feedback is the retrieval task where the system is given not only an information need, but also some relevance judgement information, usually from users' feedback for an initial result list by the system. With different amount of feedback information available, the optimal feedback strategy might be very different. In TREC Relevance Feedback task, the system is given different sets of feedback information from 1 relevant document to over 40 judgements with at least 3 relevant. Thus, in this work, we try to develop a feedback algorithm that works well on all levels of feedback by extending the relevance model for pseudo relevance feedback to include judged relevant documents when scoring feedback terms. Within these different levels of feedback, it is more difficult for the feedback algorithm to perform well when given minimal amount of feedback. Experiments show that our algorithm performs robustly in those difficult cases.
- Yangbo Zhu, Jamie Callan, J. Carbonell. 2008. The impact of history length on personalized search. Abstract: Personalized search is a promising way to better serve different users' information needs. Search history is one of the major information sources for search personalization. We investigated the impact of history length on the effectiveness of personalized ranking. We carried out task-based user study for Web search, and obtained ranked relevance judgments for all queries. Query contexts derived from previous queries in the same task are used to re-rank results for the current query. Experimental results show that the performance of personalization generally improves as more queries are accumulated, but most of the benefits come from a few immediately preceding queries.
- Le Zhao, Jamie Callan. 2008. A generative retrieval model for structured documents. Abstract: Structured documents contain elements defined by the author(s) and annotations assigned by other people or processes. Structured documents pose challenges for probabilistic retrieval models when there are mismatches between the structured query and the actual structure in a relevant document or erroneous structure introduced by an annotator. This paper makes three contributions. First, a new generative retrieval model is proposed to deal with the mismatch problem. This new model extends the basic keyword language model by treating structure as hidden variable during the generation process. Second, variations of the model are compared. Third, term-level and structure-level smoothing strategies are studied. Evaluation was conducted with INEX XML retrieval and question-answering retrieval tasks. Experimental results indicate that the optimal structured retrieval model is task dependent, two-level Dirichlet smoothing significantly outperforms two-level Jelinek-Mercer smoothing, and with accurate structured queries, the proposed structured retrieval model outperforms keyword retrieval significantly, on both QA and INEX datasets.
- Jamie Callan, Kevyn Collins-Thompson. 2008. Robust model estimation methods for information retrieval. Abstract: Information retrieval algorithms attempt to match a user's description of their information need with relevant information in a collection of documents or other data. Applications include Web search engines, filtering and recommendation systems, computer-assisted language tutors, and many others. A key challenge of retrieval algorithms is to perform effective matching when many factors, such as the user's true information need, may be highly uncertain and can only be partially observed via a small number of keywords. This dissertation develops broadly applicable algorithms for measuring and exploiting such uncertainty in retrieval algorithms to make them more effective and reliable. Our contributions include new theoretical models, statistical methods, evaluation techniques, and retrieval algorithms. 
As an application, we focus on a long-studied approach to improving retrieval matching that adds related terms to a query — a process known as query expansion. Query expansion works well on average, but even state-of-the-art methods are still highly unreliable and can greatly hurt results for individual queries. We show how sensitivity information for an expansion algorithm can be obtained and used to improve its reliability without reducing overall effectiveness. 
Our approach proceeds in two steps. First, treating the base expansion method as a 'black box', we gather information about how the algorithm's output — a set of expansion terms — changes with perturbations of the initial query and top-ranked documents. This step also results in a set of plausible expansion model candidates. We then introduce a novel risk framework based on convex optimization that prunes and combines these candidates to produce a much more reliable version of the original baseline expansion algorithm. Highlights of our results include: • A new algorithmic framework for estimating more precise query and document models, based on treating queries and document sets as random variables instead of single observations. • The first significant application and analysis of convex optimization methods to query expansion problems in information retrieval. • A new family of statistical similarity measures we call perturbation kernels that are efficient to compute and give context-sensitive word clustering. • The introduction of risk-reward analysis to information retrieval, including tradeoff curves, analysis, and risk measures. • A new general form of query difficulty measure that reflects clustering in the collection as well as the relation between a query and the collection.
- Jonathan L. Elsas, Jaime Arguello, Jamie Callan, J. Carbonell. 2008. Retrieval and feedback models for blog feed search. Abstract: Blog feed search poses different and interesting challenges from traditional ad hoc document retrieval. The units of retrieval, the blogs, are collections of documents, the blog posts. In this work we adapt a state-of-the-art federated search model to the feed retrieval task, showing a significant improvement over algorithms based on the best performing submissions in the TREC 2007 Blog Distillation task[12]. We also show that typical query expansion techniques such as pseudo-relevance feedback using the blog corpus do not provide any significant performance improvement and in many cases dramatically hurt performance. We perform an in-depth analysis of the behavior of pseudo-relevance feedback for this task and develop a novel query expansion technique using the link structure in Wikipedia. This query expansion technique provides significant and consistent performance improvements for this task, yielding a 22% and 14% improvement in MAP over the unexpanded query for our baseline and federated algorithms respectively.
- G. Yang, Jamie Callan. 2008. Learning the distance metric in a personal ontology. Abstract: Personal ontology construction is the task of sorting through relevant materials, identifying the main topics and concepts, and organizing them to suit personal needs. Automatic construction of personal ontologies is difficult in part because measuring the semantic distance between two concepts is difficult. Knowledge-based approaches use either knowledge bases, such as WordNet, or lexico-syntactic patterns to induce the differences between concepts. However, these techniques are only applicable for a subset of concepts and leave the majority unmeasurable. On the other hand, statistical approaches are able to induce the differences between any concept pair but lack of human knowledge involvement and hence suffer from low precision.
 In the context of personal ontology construction, semantic distances between concepts need to reflect personal preferences. Based on that, this paper presents a supervised hierarchical clustering framework to incorporate personal preferences for distance metric learning in personal ontology construction. In this framework, periodic manual guidance provides training data for learning a distance metric and the learned metric is used during automatic activities to further construct the ontology. A detailed user study demonstrates that the approach is effective and accelerates the construction of personal ontologies.
- Jonathan L. Elsas, Jaime Arguello, Jamie Callan, J. Carbonell. 2007. Retrieval and Feedback Models for Blog Distillation. Abstract: This paper presents our system and results for the Feed Distillation task in the Blog track at TREC 2007. Our experiments focus on two dimensions of the task: (1) a large-document model (feed retrieval) vs. a small-document model (entry or post retrieval) and (2) a novel query expansion method using the link structure and link text found within Wikipedia.
- Yangbo Zhu, Le Zhao, Jamie Callan. 2007. Structured Queries for Legal Documents Search. Abstract: This paper reports the experiments of using Indri for the main and routing (relevance feedback) tasks in the TREC 2007 Legal Track. For the main task, we analyze ranking algorithms using different fields, boolean constraints, phrase and synonym operators. Evaluation results show that structured queries outperform bag-of-words ones. For the routing task, we train a linear SVM classifier for each topic. Terms with the largest weights are selected to form new queries. Both keywords and simple structured features (term.field) have been investigated. Named-Entity tags, LingPipe sentence breaker and metadata fields of the original documents have been used to generate the field information. Results show that structured features and weighted queries improves retrieval, but only marginally. We also show which structures are more useful. It turns out metadata fields are not as important as we thought.
- Michael Heilman, Kevyn Collins-Thompson, Jamie Callan, M. Eskénazi. 2007. Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts. Abstract: This work evaluates a system that uses interpolated predictions of reading difficulty that are based on both vocabulary and grammatical features. The combined approach is compared to individual grammar- and language modeling-based approaches. While the vocabulary-based language modeling approach outperformed the grammar-based approach, grammar-based predictions can be combined using confidence scores with the vocabulary-based predictions to produce more accurate predictions of reading difficulty for both first and second language texts. The results also indicate that grammatical features may play a more important role in second language readability than in first language readability.
- Jamie Callan, James Allan, Charles L. A. Clarke, S. Dumais, David A. Evans, Mark Sanderson, ChengXiang Zhai. 2007. Meeting of the MINDS. Abstract: This invention relates to a process for the synthesis of ortho-sulfobenzoic acid anhydride which comprises heating a 2-sulfohalide benzoate in the presence of a catalytic amount of a Friedel-Crafts catalyst. Especially preferred in the practice of this invention is the reaction of ortho-carbomethoxybenzenesulfonyl chloride in the presence of a zinc or iron halide to produce ortho-sulfobenzoic acid anhydride.
- Kevyn Collins-Thompson, Jamie Callan. 2007. Estimation and use of uncertainty in pseudo-relevance feedback. Abstract: Existing pseudo-relevance feedback methods typically perform averaging over the top-retrieved documents, but ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feed-back model by resampling a given query's top-retrieved documents, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by emphasizing terms related to multiple query aspects. The result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.
- Anagha Kulkarni, Jamie Callan, M. Eskénazi. 2007. Dictionary definitions: the likes and the unlikes. Abstract: The task of grouping word definitions from ESL (English as a Second Language) dictionaries based on the similarity of their meanings is the focus of this work. It is demonstrated that lexical features and unsupervised machine learning algorithms can be effectively used to approach this problem. Analysis of the efficacy of this methodology for this task and the involved data which consists of very short and very few definitions per group is provided.
- Jaime Arguello, Jamie Callan. 2007. A bootstrapping approach for identifying stakeholders in public-comment corpora. Abstract: A stakeholder is an individual, group, organization, or community that has an interest or stake in a consensus-building process. The goal of stakeholder identification is identifying stakeholder mentions in natural language text. We present novel work in using a bootstrapping approach for the identification of stakeholders in public comment corpora. We refine the definition of a stakeholder by categorizing stakeholders into 2 distinct stakeholder types and experiment with automatically identifying one of these two types: instances where the author identifies him/herself as a member of a particular group. An existing bootstrapping information extraction algorithm is combined individually with 3 distinct extraction pattern templates. Results show that this stakeholder group can be learned in a minimally supervised bootstrapping framework using 2 of the 3 extraction pattern templates. An experimental analysis explores the challenges in applying the third extraction pattern template to this problem. Results on all 3 extraction pattern templates provide insight on the unique and novel challenge of identifying stakeholders.
- D. Hillard, Stephen Purpura, J. Wilkerson, B. Jones, F. Baumgartner, R. Zeckhauser, Jesse Shapiro, Claire Cardie, E. Hovy, D. Lazer, Michael Neblo, K. Esterling, Aleks Jakulin, M. Baum, Jamie Callan, Micah Altman, David King, James E. Purpura, A. Gibson, J. Rigg, Wilkerson Hillard. 2007. An Active Learning Framework for Classifying Political Text. Abstract: We develop a framework and tools for applying a computer‐assisted context analysis system and find that it achieves levels of accuracy comparable to humans for about 80% less effort when starting from scratch (no labeled examples). The system is presented using a case study of Congressional bill titles as a proxy for the full text of Congressional bills. We also demonstrate that the system can use information learned from previous experiments to reduce the labeling requirements still further to over 90% savings of the current human effort. This study assumes that social scientists have a need to locate individual documents in a subject area. To support this need, "Topic classification," where documents are coded according to some organizing framework, is used to facilitate search and summarization. Our proposed framework for effectively employing machine learning methods mitigates the high costs of the standard method of topic classification ‐ human labeling. We scientifically evaluate the efficacy and accuracy of the automated approach using a large corpus of 380,000 human‐labeled events and a classification system that includes 20 major policy topics, 226 subtopics, and a demonstrably strong level of human inter‐ coder agreement.
- Yangbo Zhu, Le Zhao, Jamie Callan, J. Carbonell. 2007. Stuctured Queries for Legal Search. Abstract: This paper reports the experiments of using Indri for the main and routing (relevance feedback) tasks in the TREC 2007 Legal Track. For the main task, we analyze ranking algorithms using different fields, boolean constraints and structured operators. Evaluation results show that structured queries outperform bag-of-words ones. Boolean constraints improve both precision and recall. For the routing task, we train a linear SVM classifier for each topic. Terms with the largest weights are selected to form new queries. Both keywords and simple structured features (term.field) have been investigated. Named-Entity tags, LingPipe sentence breaker and metadata fields of the original documents are used to generate the field information. Results show that structured features and weighted queries improves retrieval, but only marginally. We also show which structures are more useful. It turns out metadata fields are not as important as we thought.
- Kevyn Collins-Thompson, Jamie Callan. 2007. Automatic and Human Scoring of Word Definition Responses. Abstract: Assessing learning progress is a critical step in language learning applications and experiments. In word learning, for example, one important type of assessment is a definition production test, in which subjects are asked to produce a short definition of the word being learned. In current practice, each free response is manually scored according to how well its meaning matches the target definition. Manual scoring is not only time-consuming, but also limited in its flexibility and ability to detect partial learning effects. This study describes an effective automatic method for scoring free responses to definition production tests. The algorithm compares the text of the free response to the text of a reference definition using a statistical model of text semantic similarity that uses Markov chains on a graph of individual word relations. The model can take advantage of both corpusand knowledge-based resources. Evaluated on a new corpus of human-judged free responses, our method achieved significant improvements over random and cosine baselines in both rank correlation and label error.
- Jamie Callan, James Allan, C. Clarke, S. Dumais, David A. Evans, M. Sanderson, ChengXiang Zhai. 2007. Meeting of the MINDS: an information retrieval research agenda. Abstract: Since its inception in the late 1950s, the field of Information Retrieval (IR) has developed tools that help people find, organize, and analyze information. The key early influences on the field are well-known. Among them are H. P. Luhn's pioneering work, the development of the vector space retrieval model by Salton and his students, Cleverdon's development of the Cranfield experimental methodology, Spärck Jones' development of idf, and a series of probabilistic retrieval models by Robertson and Croft. Until the development of the WorldWideWeb (Web), IR was of greatest interest to professional information analysts such as librarians, intelligence analysts, the legal community, and the pharmaceutical industry.
- M. Bilotti, Paul Ogilvie, Jamie Callan, Eric Nyberg. 2007. Structured retrieval for question answering. Abstract: Bag-of-words retrieval is popular among Question Answering (QA) system developers, but it does not support constraint checking and ranking on the linguistic and semantic information of interest to the QA system. We present anapproach to retrieval for QA, applying structured retrieval techniques to the types of text annotations that QA systems use. We demonstrate that the structured approach can retrieve more relevant results, more highly ranked, compared with bag-of-words, on a sentence retrieval task. We also characterize the extent to which structured retrieval effectiveness depends on the quality of the annotations.
- Jie Lu, Jamie Callan. 2007. Content-Based Peer-to-Peer Network Overlay for Full-Text Federated Search. Abstract: Peer-to-peer network overlays have mostly been designed to support search over document names, identifiers, or keywords from a small or controlled vocabulary. In this paper we propose a content-based P2P network overlay for full-text federated search over heterogeneous, open-domain contents. Local algorithms are developed to dynamically construct a network overlay with content-based locality and content-based small-world properties. Experimental results using P2P testbeds of real documents demonstrate the effectiveness of our approach.
- Hui Yang, Luo Si, Jamie Callan. 2006. Knowledge Transfer and Opinion Detection in the TREC2006 Blog Track. Abstract: The paper describes the opinion detection system developed in Carnegie Mellon University for TREC 2006 Blog track. The system performed a two-stage process: passage retrieval and opinion detection. Due to lack of training data for the TREC Blog corpus, online opinion reviews provided in other domains, such as movie review and product review, were used as the training data. Knowledge transfer was performed to make the cross-domain learning possible. Logistic regression ranked the sentence-level opinions vs. objective statements. The evaluation shows that the algorithm is effective in the task.
- Stuart W. Shulman, E. Hovy, Jamie Callan, S. Zavestoski. 2006. Progress in language processing technology for electronic rulemaking. Abstract: In this project, we are developing new text processing tools that help people perform advanced analysis of large collections of text commentary. This problem is increasingly faced by the U.S. federal government's regulation writers who formulate the rules and regulations that define the details of laws enacted by Congress. Our research focuses on text clustering, text searching, near-duplicate detection, opinion identification, stakeholder characterization, and extractive summarization, as well as the impact of such tools on the process of rulemaking itself. Versions of a Rule-Writer's Workbench are being built by researchers at ISI and CMU, made available for experimental use by our government partners at the DOT and EPA, and evaluated by researchers at the Library and Information Science and Sociology departments at the universities of Pittsburgh and San Francisco, respectively. This project started in October 2004 and is funded for 3 years under the NSF's Digital Government Program.
- G. Yang, Jamie Callan, Stuart W. Shulman. 2006. Next steps in near-duplicate detection for eRulemaking. Abstract: Large volume public comment campaigns and web portals that encourage the public to customize form letters produce many near-duplicate documents, which increases processing and storage costs, but is rarely a serious problem. A more serious concern is that form letter customizations can include substantive issues that agencies are likely to overlook. The identification of exact- and near-duplicate texts, and recognition of unique text within near-duplicate documents, is an important component of data cleaning and integration processes for eRulemaking.This paper presents DURIAN (DUplicate Removal In lArge collectioN), a refinement of a prior near-duplicate detection algorithm DURIAN uses a traditional bag-of-words document representation, document attributes ("metadata"), and document content structure to identify form letters and their edited copies in public comment collections. Experimental results demonstrate that DURIAN is about as effective as human assessors. The paper concludes by discussing challenges to moving near-duplicate detection into operational rulemaking environments.
- Michael Heilman, Kevyn Collins-Thompson, Jamie Callan. 2006. Classroom Success of an Intelligent Tutorin Reading Compre. Abstract: We present an intelligent tutoring system called REAP that provides reader-specific lexical practice for improved reading comprehension. REAP offers individualized practice to students by presenting authentic and appropriate reading materials selected automatically from the web. We encountered a number of challenges that must be met in order for the system to be effective in a classroom setting. These include general challenges for a system that uses authentic materials, as well as more specific challenges that arise from integrating the system with pre-existing classroom curricula. We discuss how these challenges were met, and present evidence that REAP has gained acceptance into the classroom at the English Language Institute at the University of Pittsburgh.
- Michael Heilman, Kevyn Collins-Thompson, Jamie Callan, M. Eskénazi. 2006. Classroom success of an intelligent tutoring system for lexical practice and reading comprehension. Abstract: We present an intelligent tutoring system called REAP that provides reader-specific lexical practice for improved reading comprehension. REAP offers individualized practice to students by presenting authentic and appropriate reading materials selected automatically from the web. We encountered a number of challenges that must be met in order for the system to be effective in a classroom setting. These include general challenges for a system that uses authentic materials, as well as more specific challenges that arise from integrating the system with pre-existing classroom curricula. We discuss how these challenges were met, and present evidence that REAP has gained acceptance into the classroom at the English Language Institute at the University of Pittsburgh. 1. System Description We begin with brief descriptions of the REAP intelligent tutoring system and its primary users. For a more detailed description of the REAP project, please see [1] and [2]. The REAP project’s goal is to provide appropriate, authentic reading materials to students learning to read. It gathers and selects documents automatically from the web, which raises a number of concerns that will be discussed in this paper. The system has focused on English language so far, but future developments could extend the scope of the project to other languages. REAP incorporates a variety of statistical language modeling and information retrieval methods in order to model students’ knowledge and find useful reading passages for them. Recent work on the REAP system includes creating a system for the University of Pittsburgh’s English Language Institute (ELI) Reading 4 course, an upper-level course for English as a Second Language (ESL) that focuses on reading skills. A study on usability of REAP is currently in progress at the ELI. In this study, which we will refer to as the Spring ’06 ELI Study, thirty-three students use the system once a week for forty minutes over the course of the semester, reading documents containing target unknown vocabulary identified from a pre-test. REAP gathers documents from the Web in order to find useful, authentic reading material for these students. The documents are analyzed according to syntactic features, readability, length, and the occurrence of target vocabulary. The tutor uses an extended version of the Lemur Toolkit for Language Modeling and Information Retrieval [3] to annotate the documents and create an index for language-model based retrieval. When a student uses REAP, the system searches among this set of documents for those that satisfy a number of constraints, including the student’s target vocabulary list, document length, his or her user model, and the target reading level for the course, which is sixth to eighth grade. After reading a document, usually from one to two pages in length, the student works through a series of automatically generated exercises based on the target vocabulary found in the reading. The student model is updated after every reading so that the optimal document can be retrieved for the next reading passage. By using authentic reading materials the REAP system offers realistic training and individualized curricula to students. Reading textbooks and hand-selected materials are usually wellcontrolled, appropriate, and contain high-quality input, yet such materials are also static, difficult to produce, and very limited in quantity. In a classroom setting, it is typical that all students see the same material from a textbook, regardless of the state of their lexical or grammatical development. Also, reading materials for use in most classrooms must meet a wide variety of syntactic and lexical constraints in order for students of a given reading proficiency to be able to read them without confusion. Teachers or textbook authors often have to heavily edit or even produce the reading materials themselves in order to meet these constraints, introducing some amount of artificiality into the materials. Intelligent tutoring systems such as REAP can examine large corpora such as the Web in order to automatically select materials that meet these various criteria. Students using REAP work toward their ultimate goal of reading real text by actually reading real text. Intelligent tutoring systems also provide students with individualized practice rather than static sets of exercises. Students go through readings at very different rates, and so faster students need a greater number of more difficult reading passages than do slower students. In a current study ten students using the REAP system had completed fewer than ten reading passages, while twelve students had completed twenty or more, despite having the same time on task. The average number completed was just under seventeen. REAP selects as many documents as are necessary for each student, and these documents satisfy certain lexical, syntactic, and readability constraints based on a model of the current student’s knowledge. Finding a large number of appropriate documents for an entire classroom of students can in many cases only be accomplished by an intelligent tutoring system. Such a system is therefore very valuable to language teachers. The value of the system is demonstrated in results from an exit survey taken toward the end of a recent study, shown in Figure 1. The students (N=33) were asked to respond on a Likert scale from 1 to 5 indicating the degree to which they agree to given statements about the system. The results indicate that students feel that the REAP system is easy to use, valuable for learning both target and non-target vocabulary, and worth using in future classes. Students wanted more personalization and choice of the reading topics, however, to make the system more engaging.
- Nachiketa Sahoo, Jamie Callan, R. Krishnan, G. Duncan, R. Padman. 2006. Incremental hierarchical clustering of text documents. Abstract: Incremental hierarchical text document clustering algorithms are important in organizing documents generated from streaming on-line sources, such as, Newswire and Blogs. However, this is a relatively unexplored area in the text document clustering literature. Popular incremental hierarchical clustering algorithms, namely Cobweb and Classit, have not been widely used with text document data. We discuss why, in the current form, these algorithms are not suitable for text clustering and propose an alternative formulation that includes changes to the underlying distributional assumption of the algorithm in order to conform with the data. Both the original Classit algorithm and our proposed algorithm are evaluated using Reuters newswire articles and Ohsumed dataset.
- T. Avrahami, Lawrence Yau, Luo Si, Jamie Callan. 2006. The FedLemur project: Federated search in the real world. Abstract: Federated search and distributed information retrieval systems provide a single user interface for searching multiple full-text search engines. They have been an active area of research for more than a decade, but in spite of their success as a research topic, they are still rare in operational environments. This article discusses a prototype federated search system developed for the U.S. government's FedStats Web portal, and the issues addressed in adapting research solutions to this operational environment. A series of experiments explore how well prior research results, parameter settings, and heuristics apply in the FedStats environment. The article concludes with a set of lessons learned from this technology transfer effort, including observations about search engine quality in the “real world.” © 2006 Wiley Periodicals, Inc.
- G. Yang, Jamie Callan. 2006. Near-duplicate detection by instance-level constrained clustering. Abstract: For the task of near-duplicated document detection, both traditional fingerprinting techniques used in database community and bag-of-word comparison approaches used in information retrieval community are not sufficiently accurate. This is due to the fact that the characteristics of near-duplicated documents are different from that of both "almost-identical" documents in the data cleaning task and "relevant" documents in the search task. This paper presents an instance-level constrained clustering approach for near-duplicate detection. The framework incorporates information such as document attributes and content structure into the clustering process to form near-duplicate clusters. Gathered from several collections of public comments sent to U.S. government agencies on proposed new regulations, the experimental results demonstrate that our approach outperforms other near-duplicate detection algorithms and as about as effective as human assessors.
- G. Yang, Jamie Callan, Stuart W. Shulman. 2006. DURIAN: a demo for near-duplicate detection. Abstract: Recently, the move from paper to electronic public comments makes it much easier for individuals to customize form letters while harder for agencies to identify substantive information since there are many near-duplicate comments that express the same viewpoint in slightly different language. The identification of exact- and near-duplicate texts, and recognition of unique text within near-duplicate documents, is an important component of data cleaning and integration processes for eRulemaking.This brief paper describes a demonstration of a near-duplicate detection system, DURIAN (DUplicate Removal In lArge collectioN), that identifies and organizes the near-duplicates for eRulemaking applications.
- Jie Lu, Jamie Callan. 2006. User modeling for full-text federated search in peer-to-peer networks. Abstract: User modeling for information retrieval has mostly been studied to improve the effectiveness of information access in centralized repositories. In this paper we explore user modeling in the context of full-text federated search in peer-to-peer networks. Our approach models a user's persistent, long-term interests based on past queries, and uses the model to improve search efficiency for future queries that represent interests similar to past queries. Our approach also enables queries representing a user's transient, ad-hoc interests to be automatically recognized so that search for these queries can rely on a relatively large search radius to avoid sacrificing effectiveness for efficiency. Experimental results demonstrate that our approach can significantly improve the efficiency of full-text federated search without degrading its accuracy. Furthermore, the proposed approach does not require a large amount of training data, and is robust to a range of parameter values.
- Lu Jie, Jielu, Jamie Callan, J Callan. 2006. Federated Search of Text-Based Digital Libraries in Peer-to-Peer Networks. Abstract: Peer-to-peer (P2P) networks integrate autonomous computing resources without requiring a central coordinating authority, which makes them a potentially robust and scalable model for providing federated search capability to large-scale networks of text-based digital libraries. However, peer-to-peer networks have so far provided very limited support for full-text federated search with relevance-based document ranking. This paper provides solutions to full-text federated search of text-based digital libraries in hierarchical peer-to-peer networks. Existing approaches to full-text search are adapted and new methods are developed for the problems of resource representation, resource selection, and result merging according to the unique characteristics of hierarchical peer-to-peer networks. Experimental results demonstrate that the proposed approaches offer a better combination of accuracy and efficiency than more common alternatives for federated search of text-based digital libraries in peer-to-peer networks.
- Nachiketa Sahoo, R. Krishnan, G. Duncan, Jamie Callan. 2006. Collaborative Filtering with Multi-component Rating for Recommender Systems. Abstract: The dependency structure among the component ratings is discovered and incorporated into a mixture model. The parameters of the model were estimated using the Expectation Maximization algorithm. The algorithm was evaluated using data collected from Yahoo Movies. It was found that using multiple components leads to improved recommendations over using only one component when small amount of training data is used. However, when we have enough training data no gain from using additional components of the ratings was observed.
- Henrik Nottelmann, K. Aberer, Jamie Callan, W. Nejdl. 2006. The CIKM 2005 workshop on information retrieval in peer-to-peer networks. Abstract: Peer-to-peer (P2P) networks have emerged as a popular way to build large scale information systems by using the principle of resource sharing. The P2P paradigm holds many promises, e.g. scalability, failure resilience and increased autonomy of nodes. For these reasons P2P seems also to be an interesting architectural paradigm for realizing large scale information retrieval systems. However, search methods in P2P networks are still mostly limited to simple keyword queries and the use of advanced retrieval models is in its infancy.
- Luo Si, Jie Lu, Jamie Callan. 2006. Combining Multiple Resources, Evidences and Criteria for Genomic Information Retrieval. Abstract: We participated in the passage retrieval and aspect retrieval subtasks of the TREC 2006 Genomics Track. This paper describes the methods developed for these two subtasks. For passage retrieval, our query expansion method utilizes multiple external biomedical resources to extract acronyms, aliases, and synonyms, and we propose a post-processing step which combines the evidence from multiple scoring methods to improve relevance-based passage rankings. For aspect retrieval, our method estimates the topical aspects of the retrieved passages and generates passage rankings by considering both topical relevance and topical novelty. Empirical results demonstrate the effectiveness of these methods.
- G. Yang, Jamie Callan, Luo Si. 2006. Knowledge Transfer and Opinion Detection in the TREC 2006 Blog Track. Abstract: The paper describes the opinion detection system developed in Carnegie Mellon University for TREC 2006 Blog track. The system performed a two-stage process: passage retrieval and opinion detection. Due to lack of training data for the TREC Blog corpus, online opinion reviews provided in other domains, such as movie review and product review, were used as the training data. Knowledge transfer was performed to make the cross-domain learning possible. Logistic regression ranked the sentence-level opinions vs. objective statements. The evaluation shows that the algorithm is effective in the task. Introduction The Blog track is a new task in the TREC 2006 evaluation. The main task of the track is “opinion detection” in the domain of the online blogs posted during the period of Dec 2005 to Feb 2006. The posts and the comments are from Technorati, Bloglines, Blogpulse and other web hosts. The system developed in Carnegie Mellon University for the opinion detection task consists of the modules described below. Data Preprocessing The data from NIST are mainly xml files with tags similar to previous TREC web collections, such as WT10G or GOV2. Three types of files are provided by NIST: permalinks (html documents containing the posts), RSS feeds, and blog homepages (html documents containing the homepages of the feeds). Permalinks contains the actual content of the corpus, and are the main target of this task. Indexing, retrieval and opinion detection are all performed based on permalink documents. Further study should involve RSS feeds since they reveal the structure and network of multiple blog posts. Due to messy nature of online html, data cleaning is an important preprocessing step. Two approaches were tried. The first utilized the built-in html file cleaning functions of the latest Indri 2.3.1 toolkit [1]. Additional preprocessing was done to handle stylesheets and javascript, which were not handled by the current version of Indri. The index was then built based on the “trecweb” format supported by Indri. The unit for indexing and retrieval is one permalink document, i.e., one blog post with its following comments. However, it was soon realized that taking the raw html files and throwing them into Indri to index limits the flexibility of gathering more information from the raw text, for example, sentence structure, paragraph information, part-of-speech tagging, etc, which could be important for opinion detection in later stages. These could all be done by creating more functions in Indri, however, due to the amount of programming effort and time constraints, the first approach was discarded. The second approach was to transform the html files as closely as possible into regular text files. This was done by several steps. Removing HTML tags, scripts, stylesheets: A wrapper was created on top of a tool called “striptags” from the REAP project [2]. The text documents looked much neater; however, there were still advertisements, text from side bars and menus floating around. These are all noise in the main text. To remove them another module with machine learned patterns could possibly remove them. However, such noise would also be filtered out automatically by the retrieval and opinion detection in the later stages, hence leaving them in caused little harm to the final results. 1 This work was done while the author was at the Language Technologies Institute at Carnegie Mellon University. Removing Non-English characters: The TREC 2006 Blog corpus contains non-English posts. Characters with ASCII code less than 32 and greater than 126 were removed from the corpus. Sentence Splitting: A modified version of UIUC’s sentence splitter [3] was used to annotate the corpus with <s> and </s> tags that identified the beginning and end of each sentence. Creating Artificial Paragraphs: Original line breaks from the text were reserved as segmentations of paragraphs. Moreover, for long original paragraphs, an around-100-word paragraph break was introduced with no crossing of the sentence boundaries. Removing Dummy Sentences: If a sentence contained only punctuation, or just a single number, it was removed. This step filtered out form counters largely available on the Web documents. Moreover, if within a sentence, any word had an occurrence of more than N times (N=20 in these tests), that sentence was removed. This step filtered out advertisement and web category anchor text, which were not important content of the Web blog and hence are irrelevant to any potential query <DOC> <TEXT> <PARAGRAPH> <s>blah blah blah</s> <s>blah blah blah</s> ... </PARAGRAPH> <PARAGRAPH> ... </TEXT> </DOC> . Query Formulation The topic file that NIST provided contains topics each with the title, description and narrative parts. Based on the title field, the topics could be classified into 6 categories [Figure 1]: blog topic category distribution
- Pucktada Treeratpituk, Jamie Callan. 2006. Automatically labeling hierarchical clusters. Abstract: Government agencies must often quickly organize and analyze large amounts of textual information, for example comments received as part of notice and comment rulemaking. Hierarchical organization is popular because it represents information at different levels of detail and is convenient for interactive browsing. Good hierarchical clustering algorithms are available, but there are few good solutions for automatically labeling the nodes in a cluster hierarchy.This paper presents a simple algorithm that automatically assigns labels to hierarchical clusters. The algorithm evaluates candidate labels using information from the cluster, the parent cluster, and corpus statistics. A trainable threshold enables the algorithm to assign just a few high-quality labels to each cluster. Experiments with Open Directory Project (ODP) hierarchies indicate that the algorithm creates cluster labels that are similar to labels created by ODP editors.
- Kevyn Collins-Thompson, Jamie Callan. 2005. Query expansion using random walk models. Abstract: It has long been recognized that capturing term relationships is an important aspect of information retrieval. Even with large amounts of data, we usually only have significant evidence for a fraction of all potential term pairs. It is therefore important to consider whether multiple sources of evidence may be combined to predict term relations more accurately. This is particularly important when trying to predict the probability of relevance of a set of terms given a query, which may involve both lexical and semantic relations between the terms.We describe a Markov chain framework that combines multiple sources of knowledge on term associations. The stationary distribution of the model is used to obtain probability estimates that a potential expansion term reflects aspects of the original query. We use this model for query expansion and evaluate the effectiveness of the model by examining the accuracy and robustness of the expansion methods, and investigate the relative effectiveness of various sources of term evidence. Statistically significant differences in accuracy were observed depending on the weighting of evidence in the random walk. For example, using co-occurrence data later in the walk was generally better than using it early, suggesting further improvements in effectiveness may be possible by learning walk behaviors.
- Kevyn Collins-Thompson, Jamie Callan. 2005. Predicting reading difficulty with statistical language models. Abstract: A potentially useful feature of information retrieval systems for students is the ability to identify documents that not only are relevant to the query but also match the student's reading level. Manually obtaining an estimate of reading difficulty for each document is not feasible for very large collections, so we require an automated technique. Traditional readability measures, such as the widely used Flesch-Kincaid measure, are simple to apply but perform poorly on Web pages and other non-traditional documents. This work focuses on building a broadly applicable statistical model of text for different reading levels that works for a wide range of documents. To do this, we recast the well-studied problem of readability in terms of text categorization and use straightforward techniques from statistical language modeling. We show that with a modified form of text categorization, it is possible to build generally applicable classifiers with relatively little training data. We apply this method to the problem of classifying Web pages according to their reading difficulty level and show that by using a mixture model to interpolate evidence of a word's frequency across grades, it is possible to build a classifier that achieves an average root mean squared error of between one and two grade levels for 9 of 12 grades. Such classifiers have very efficient implementations and can be applied in many different scenarios. The models can be varied to focus on smaller or larger grade ranges or easily retrained for a variety of tasks or populations.
- Henrik Nottelmann, K. Aberer, Jamie Callan, W. Nejdl. 2005. Proceedings of the 2005 ACM workshop on Information retrieval in peer-to-peer networks, P2PIR '05, Bremen, Germany, November 4, 2005. Abstract: It is our great pleasure to welcome you to the 2th Workshop on Information Retrieval in Peer-to-Peer Networks -- P2PIR 2005. This year's workshop aims at bringing together young researchers from Information Retrieval and Database Systems working on peer-to-peer information systems. Both communities have their own strategies at solving the problem of efficient and effective query routing in peer-to-peer networks, and a closer collaboration could have a large impact on future P2PIR research. As such, this proposed workshop continues the efforts from an SIGIR workshop last year on the same topic, and the primary goal is to foster the collaboration process started there.The call for papers attracted 15 submissions from Asia, Canada, the United States, Australia and Europe. The program committee accepted 6 papers that cover a variety of topics, including query routing, clustering and browsing, queries over RDF data and weighting schemes for peer-to-peer networks. In addition, the program includes two discussion sessions in order to benefit from different views in the two research communities. One discussion session will deal with problems and potential solutions in evaluating large-scale peer-to-peer networks, the other one about algorithmic and methodological problems.
- Yi Zhang, Jamie Callan. 2005. Combining Multiple Forms of Evidence While Filtering. Abstract: This paper studies how to go beyond relevance and enable a filtering system to learn more interesting and detailed data driven user models from multiple forms of evidence. We carry out a user study using a real time web based personal news filtering system, and collect extensive multiple forms of evidence, including explicit and implicit user feedback. We explore the graphical modeling approach to combine these forms of evidence. To test whether the approach can help us understand the domain better, we use graph structure learning algorithm to derive the causal relationships between different forms of evidence. To test whether the approach can help the system improve the performance, we use the graphical inference algorithms to predict whether a user likes a document based on multiple forms of evidence. The results show that combining multiple forms of evidence using graphical models can help us better understand the filtering problem, improve filtering system performance, and handle various data missing situations naturally.
- Luo Si, Jamie Callan. 2005. Modeling search engine effectiveness for federated search. Abstract: Federated search links multiple search engines into a single, virtual search system. Most prior research of federated search focused on selecting search engines that have the most relevant contents, but ignored the retrieval effectiveness of individual search engines. This omission can cause serious problems when federating search engines of different qualities.This paper proposes a federated search technique that uses utility maximization to model the retrieval effectiveness of each search engine in a federated search environment. The new algorithm ranks the available resources by explicitly estimating the amount of relevant material that each resource can return, instead of the amount of relevant material that each resource contains. An extensive set of experiments demonstrates the effectiveness of the new algorithm.
- Paul Ogilvie, Jamie Callan. 2005. Experiments with Language Models for Known-Item Finding of E-mail Messages. Abstract: We present experiments using language models to rank e-mail messages for the Known-Item Finding task of the Enterprise track. We combine evidence from the text of the message, its subject, the text of the thread the in which the message occurs, and the text of messages that are in reply to the message. We find that the only statistically significant differences suggest that in addition to the text of the message, the subject is a very important piece of evidence. We also explore the use of a depth based prior, where emphasis is place on messages near the root of the thread structure, which has mixed results.
- Stuart W. Shulman, E. Hovy, Jamie Callan, S. Zavestoski. 2005. Language processing technologies for electronic rulemaking: a project highlight. Abstract: In this project, we are developing new text processing tools that help people perform advanced analysis of large collections of text commentary. This problem is increasingly faced by the United States federal government's regulation writers who formulate the rules and regulations that define the details of laws enacted by Congress. Our research focuses on text clustering, text searching using information retrieval, near-duplicate detection, opinion identification, stakeholder characterization, and extractive summarization, as well as the impact of such tools on the process of rulemaking itself. Versions of a Rule-Writer's Workbench will be built by Computer Science researchers at ISI and CMU, deployed annually for experimental use by our government partners, and evaluated by social science researchers from the Library and Information Science and Sociology departments at the Universities of Pittsburgh and San Francisco respectively. This three-year project started in October 2004 and is funded under the National Science Foundation's Digital Government program.
- G. Yang, Jamie Callan. 2005. Near-duplicate detection for eRulemaking. Abstract: U.S. regulatory agencies are required to solicit, consider, and respond to public comments before issuing regulations. In recent years, agencies have begun to accept comments via both email and Web forms. The transition from paper to electronic comments makes it much easier for individuals to customize "form" letters, which they do, creating "near-duplicate" comments that express the same viewpoint in slightly different languages. This paper explores the use of simple text clustering and retrieval algorithms for identifying near-duplicate public comments. Experiments with public comments about a recent regulation proposed by the Environmental Protection Agency (EPA) demonstrate the effectiveness of the algorithms.
- Kevyn Collins-Thompson, Jamie Callan. 2004. A Language Modeling Approach to Predicting Reading Difficulty. Abstract: We demonstrate a new research approach to the problem of predicting the reading difficulty of a text passage, by recasting readability in terms of statistical language modeling. We derive a measure based on an extension of multinomial naïve Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage. The resulting classifier is not specific to any particular subject and can be trained with relatively little labeled data. We perform predictions for individual Web pages in English and compare our performance to widely-used semantic variables from traditional readability measures. We show that with minimal changes, the classifier may be retrained for use with French Web documents. For both English and French, the classifier maintains consistently good correlation with labeled grade level (0.63 to 0.79) across all test sets. Some traditional semantic variables such as type-token ratio gave the best performance on commercial calibrated test passages, while our language modeling approach gave better accuracy for Web documents and very short passages (less than 10 words).
- Kevyn Collins-Thompson, Jamie Callan. 2004. Information retrieval for language tutoring: an overview of the REAP project. Abstract: Typical Web search engines are designed to run short queries against a huge collection of hyperlinked documents quickly and cheaply, and are often tuned for the types of queries people submit most often [2]. Many other types of applications exist for which large, open collections like the Web would be a valuable resource. However, these applications may require much more advanced support from information retrieval technology than is currently available. In particular, an application may have to describe more complex information needs, with a varied set of properties and data models, including aspects of the user’s context and goals.
- Jamie Callan, F. Crestani, M. Sanderson. 2004. Distributed multimedia information retrieval : SIGIR 2003 Workshop on Distributed Information Retrieval, Toronto, Canada, August 1, 2003 : revised selected and invited papers. Abstract: Resource Discovery.- Harvesting: Broadening the Field of Distributed Information Retrieval.- Using Query Probing to Identify Query Language Features on the Web.- Resource Selection.- The Effect of Database Size Distribution on Resource Selection Algorithms.- Decision-Theoretic Resource Selection for Different Data Types in MIND.- Distributed Web Search as a Stochastic Game.- Data Fusion.- Collection Fusion for Distributed Image Retrieval.- New Methods of Results Merging for Distributed Information Retrieval.- Recent Results on Fusion of Effective Retrieval Strategies in the Same Information Retrieval System.- Architectures.- The MIND Architecture for Heterogeneous Multimedia Federated Digital Libraries.- Apoidea: A Decentralized Peer-to-Peer Architecture for Crawling the World Wide Web.- Towards Virtual Knowledge Communities in Peer-to-Peer Networks.- The Personalized, Collaborative Digital Library Environment CYCLADES and Its Collections Management.
- Jamie Callan, N. Fuhr. 2004. The SIGIR peer-to-peer information retrieval workshop. Abstract: Peer-to-peer (P2P) systems have emerged as a popular way to share huge volumes of data. The P2P paradigm holds many promises: it fits naturally with the Internet, the universal knowledge and service exchange medium; it favors scalability, by allowing the seamless plugging of data, services and computational resources into the global system; it increases system resilience, by avoiding unique points of failures; and it can speed up global access by distributing the indexing and query processing tasks to multiple computing nodes. However, retrieval methods for P2P systems are still in their infancy. P2P networks are prone to congestion when messages are not routed intelligently. Many of the most effective routing or data placement methods developed recently rely on relatively simple retrieval methods and homogeneous network environments.
- Stuart W. Shulman, E. Hovy, S. Zavestoski, Jamie Callan. 2004. SGER Collaborative. Abstract: Abstract This short Project Highlight presents a year-end summary of a collaborative small grant for exploratory research. It relates the background and scope of current research plans by the eRulemaking Research Group. Finally, this Project Highlight notes the challenges for collaboration between social and information scientists.
- W. Bruce Croft, Jamie Callan. 2004. A Language Modeling Approach to Metadata for Cross-Database Linkage and Search. Abstract: This research demonstrates that language models are a sound and effective foundation on which to build large-scale, distributed information systems for government applications. It contributes to providing an alternative to human-generated metadata for locating information resources. Manual indexing is expensive, and studies show that people are inconsistent and inaccurate when doing indexing, which leads to poor retrieval effectiveness. Generating content descriptions automatically from the markup and structure of documents is less expensive and, when coupled with good search techniques, can be used to locate relevant information more consistently. The evaluation testbeds for our research have been government databases such as those found in FedStats and GPO.
- Kevyn Collins-Thompson, Paul Ogilvie, Jamie Callan. 2004. Initial Results with Structured Queries and Language Models on Half a Terabyte of Text. Abstract: 3. INDEXING ENVIRONMENT We indexed all of the GOV2 corpus documents, submitting complete documents to the indexer. We did no special processing for titles or other document structure. After running each document through Indri’s HTML parser and stripping out tags, terms were normalized by removing punctuation, converting to lowercase, and performing Krovetz stemming [6]. Indri has a utility to extract anchor text, but a working version of this was not available at the time of our experiments, so no anchor text (or other linkderived features) was included. We did not use an indextime stop list or acronym list. We partitioned the GOV2 corpus into six roughly equalsized pieces and built a separate index for each subset. There were two reasons for doing this. First, it was our intent to distribute retrieval for the corpus across multiple machines: Indri has a facility for transparently federating retrieval results across the network. Second, it helped reduce the time to find and fix problems with interim Indri releases (typically, pathological HTML cases that would cause the parser to crash).
- James Allan, Jamie Callan, M. Sanderson, Jinxi Xu, S. Wegmann. 2004. Inquery and Trec-7 (draft|workshop Notebook Version). Abstract: ipated in only four of the tracks that were part of the TREC-7 workshop. We worked on ad-hoc retrieval, ltering, VLC, and the SDR track. This report covers the work done on each track successively. We start with a discussion of IR tools that were broadly applied in our work. Although UMass used a wide range of tools, from Unix shell scripts, to PC spreadsheets, three major tools were applied across almost all tracks: the Inquery search engine, the InRoute ltering engine, and a a query expansion technique known as LCA. This section provides a brief overview of each of those so that the discussion does not have to repeated for each track. 1.1 Inquery All tracks other than the ltering track used Inqueryy6] as the search engine, sometimes for training, and always for generating the nal ranked lists for the test. We used Inquery V3.2, an in-house development version of the Inquery system made available by the CIIR (V3.1). The diierences between the two are not consequential for this study. The current belief function used by Inquery to calculate the belief in term t within document d is: w t;d = 0:4 + 0:6 tf t;d tf t;d + 0:5 + 1:5 length(d) avg len log N+0:5 nt log N + 1 where n t is the number of documents containing term t, N is the number of documents in the collection, \avg len" is the average length (in words) of documents in the collection, length(d) is the length (in words) of document d, and tf t;d is the number of times term t occurs in document d.
- M. Renda, Jamie Callan. 2004. The robustness of content-based search in hierarchical peer to peer networks. Abstract: Hierarchical <i>peer to peer</i> networks with multiple directory services are an important architecture for large-scale file sharing due to their effectiveness and efficiency. Recent research argues that they are also an effective method of providing large-scale content-based federated search of text-based digital libraries. In both cases the directory services are critical resources that are subject to attack or failure, but the latter architecture may be particularly vulnerable because content is less likely to be replicated throughout the network.
 This paper studies the robustness, effectiveness and efficiency of content-based federated search in hierarchical <i>peer to peer</i> networks when directory services fail unexpectedly. Several recovery methods are studied using simulations with varying failure rates. Experimental results show that quality of service and efficiency degrade gracefully as the number of directory service failures increases. Furthermore, they show that content-based search mechanisms are more resilient to failures than the match-based search techniques.
- Luo Si, Jamie Callan. 2004. Unified utility maximization framework for resource selection. Abstract: This paper presents a unified utility framework for resource selection of distributed text information retrieval. This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases. With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications. Specifically, when used for database recommendation, the selection is optimized for the goal of high-recall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents). This new model provides a more solid framework for distributed information retrieval. Empirical studies show that it is at least as effective as other state-of-the-art algorithms.
- Jamie Callan, N. Fuhr, W. Nejdl. 2004. Proceedings of the SIGIR Workshop on Peer-to-Peer Information Retrieval, 27th Annual International ACM SIGIR Conference, July 29, 2004, Sheffield, UK. Abstract: We study the problem of distributed resource sharing in wide-area networks such as the Internet and the Web. The architecture that we envision supports both query and publish/subscribe functionality using data models and languages from Information Retrieval. We propose to approach this problem using ideas from self-organized overlay networks and especially distributed hash tables like Chord. This paper concentrates only on how to offer the envisaged publish/subscribe functionality and discusses our on-going work.
- Jie Lu, Jamie Callan. 2003. Content-based retrieval in hybrid peer-to-peer networks. Abstract: Hybrid peer-to-peer architectures use special nodes to provide directory services for regions of the network ("regional directory services"). Hybrid peer-to-peer architectures are a potentially powerful model for developing large-scale networks of complex digital libraries, but peer-to-peer networks have so far tended to use very simple methods of resource selection and document retrieval. In this paper, we study the application of content-based resource selection and document retrieval to hybrid peer-to-peer networks. The directory nodes that provide regional directory services construct and use the content models of neighboring nodes to determine how to route query messages through the network. The leaf nodes that provide information use content-based retrieval to decide which documents to retrieve for queries. The experimental results demonstrate that using content-based retrieval in hybrid peer-to-peer networks is both more accurate and more efficient for some digital library environments than more common alternatives such as Gnutella 0.6.
- Paul Ogilvie, Jamie Callan. 2003. Combining Structural Information and the Use of Priors in Mixed Named-Page and Homepage Finding. Abstract: This paper presents Carnegie Mellon University’s experiments on the mixed named-page and homepage finding task of the TREC 12 Web Track. Our results were strong; we achieved the success using language models estimated from combining information from document text, in-link text, and information present in the structure of the documents. We also present experiments using expectations about posterior distributions to create class-based prior probabilities. We find that priors do provide a large gain for our official runs, but we do further experiments that show the priors do not always help. Some preliminary analysis shows that the prior probabilities are not providing the desired posterior distributions. In cases where applying the priors harm performance, the observed posterior distributions in the rankings are far off of the desired posterior distributions.
- James Allan, N. Belkin, Paul N. Bennett, Jamie Callan, C. Clarke, Fernando Diaz, S. Dumais, N. Ferro, D. Harman, D. Hiemstra, I. Ruthven, T. Sakai, Mark D. Smucker, J. Zobel. 2003. An overview of the special issue. Abstract: This special issue is devoted to the concept of Industrial Ecology, which has engendered both excitement and criticism in its claim of providing a new unifying principle for operationalizing sustainable development. This volume explores the history and evolution of the concept; its various meanings; methodologies for its operationalization; its relationship to pollution control, pollution prevention, and accident prevention; and finally, its relationship with biological ecosystems. Suren Erkman provides a useful and brief history of industrial ecology, tracing its multi-nation origins as far back as 1970 or even 1955. He argues that it is a more expansive concept than either pollution control or pollution prevention-and, in fact, encompasses and integrates both into new management practices-a point others would disagree with. Erkman sees industrial ecology as correcting the ‘disconnect’ of industrial systems with the ecosystem, and with each other. He cites the important work of Robert Ayres on tracking materials Aows and promoting increased recovery/recovery/se, and credits Robert Frosch and Nicholas Gallopoulos with reviving and propelling the concept into prominence with their classic 1989 article in Scientific American, emphasizing ‘industrial metabolism’ while acknowledging the imperfectness of industrial ecosystems to completely mimic the biological ecosystem and eliminate all adverse environmental effects. The author acknowledges Chihiro Watanabe’s work in Japan which is ‘the only country where ideas on industrial ecology were ever taken seriously and put into practice on a large scale. . .’ Finally, Erkman sees the evolution of industrial ecology into two main directions: (1) eco-industrial parks and islands of sustainability and (2) dematerialization, decarbonization,
- Jie Lu, Jamie Callan. 2003. Reducing Storage Costs for Federated Search of Text Databases. Abstract: In environments containing many text search engines a federated search system provides people with a single point of access. When search engines are managed by independent organizations two key problems are discovering and representing the contents of each text database. Query-based sampling is a recent technique for discovering the contents of uncooperative databases so as to create database resource descriptions that support a variety of necessary capabilities. However, when the documents obtained by query-based sampling are very long, as is common in some government environments, disk storage costs can be surprisingly large. This paper investigates methods of pruning sampled documents to reduce storage costs. The experimental results demonstrate that disk storage costs can be reduced by 54-93% while causing only minor losses in federated search accuracy.
- Rong Jin, Luo Si, ChengXiang Zhai, Jamie Callan. 2003. Collaborative filtering with decoupled models for preferences and ratings. Abstract: In this paper, we describe a new model for collaborative filtering. The motivation of this work comes from the fact that two users with very similar preferences on items may have very different rating schemes. For example, one user may tend to assign a higher rating to all items than another user. Unlike previous models of collaborative filtering, which determine the similarity between two users only based on their rating performance, our model treats the user's preferences on items separately from the user's rating scheme. More specifically, for each user, we build two separate models: a preference model capturing which items are favored by the user and a rating model capturing how the user would rate an item given the preference information. The similarity of two users is computed based on the underlying preference model, instead of the surface ratings. We compare the new model with several representative previous approaches on two data sets. Experiment results show that the new model outperforms all the previous approaches that are tested consistently on both data sets.
- Jamie Callan, F. Crestani, Henrik Nottelmann, P. Pala, X. Shou. 2003. Resource selection and data fusion in multimedia distributed digital libraries. Abstract: Jamie Callan, Fabio Crestani, Henrik Nottelmann, Pietro Pala, Xiao Mang Shou School of Computer Studies, Carnegie Mellon University, Pittsburgh, PA, USA Dept. Computer and Information Sciences, University of Strathclyde, Glasgow, UK Institute of Informatics and Interactive Systems, University of Duisburg-Essen, Duisburg, Germany 4 Dip. Sistemi e Informatica, Universita degli Studi di Firenze, Firenze, Italy Dept. of Information Studies, University of Sheffield, Sheffield, UK callan@cs.cmu.edu, f.crestani@cis.strath.ac.uk, nottelmann@uni-duisburg.de, pala@dsi.unifi.it, x.m.shou@shef.ac.uk
- Yi Zhang, W. Xu, Jamie Callan. 2003. Exploration and Exploitation in Adaptive Filtering Based on Bayesian Active Learning. Abstract: In the task of adaptive information filtering, a system receives a stream of documents but delivers only those that match a person's information need. As the system filters it also refines its knowledge about the user's information needs based on relevance feedback from the user. Delivering a document thus has two effects: i) it satisfies the user's information need immediately, and ii) it helps the system better satisfy the user in the future by improving its model of the user's information need. The traditional approach to adaptive information filtering fails to recognize and model this second effect. 
 
This paper proposes utility divergence as the measure of model quality. Unlike the model quality measures used in most active learning methods, utility divergence is represented on the same scale as the filtering system's target utility function. Thus it is meaningful to combine the expected immediate utility with the model quality, and to quantitatively manage the trade-off between exploitation and exploration. The proposed algorithm is implemented for setting the filtering system's dissemination threshold, a major problem for adaptive filtering systems. Experiments with TREC-9 and TREC-10 filtering data demonstrate that the proposed method is effective.
- Luo Si, Jamie Callan. 2003. Relevant document distribution estimation method for resource selection. Abstract: Prior research under a variety of conditions has shown the CORI algorithm to be one of the most effective resource selection algorithms, but the range of database sizes studied was not large. This paper shows that the CORI algorithm does not do well in environments with a mix of "small" and "very large" databases. A new resource selection algorithm is proposed that uses information about database sizes as well as database contents. We also show how to acquire database size estimates in uncooperative environments as an extension of the query-based sampling used to acquire resource descriptions. Experiments demonstrate that the database size estimates are more accurate for large databases than estimates produced by a competing method; the new resource ranking algorithm is always at least as effective as the CORI algorithm; and the new algorithm results in better document rankings than the CORI algorithm.
- Eric Nyberg, T. Mitamura, Jamie Callan, J. Carbonell, R. Frederking, Kevyn Collins-Thompson, L. Hiyakumoto, Yifen Huang, C. Huttenhower, S. Judy, Jeongwoo Ko, Anna Kupsc, L. Lita, V. Pedro, David Svoboda, Benjamin Van Durme. 2003. The JAVELIN Question-Answering System at TREC 2003: A Multi-Strategh Approach with Dynamic Planning. Abstract: The JAVELIN system evaluated at TREC 2003 is an integrated architecture for open-domain question answering. JAVELIN employs a modular approach that addresses individual aspects of the QA task in an abstract manner. The System implements a planner that controls the execution and information o w, as well as a multiple answer seeking strategies used differently depending on the type of question.
- Paul Ogilvie, Jamie Callan. 2003. INEX 2003 Workshop Proceedings, Schloss Dagstuhl, Germany, December 15-17, 2003. Abstract: This paper presents a language modeling system for ranking flat text queries against a collection of structured documents. The retrieval system, built using the Lemur toolkit, produces probability estimates that arbitrary document components generated the query. This paper describes storage mechanisms and retrieval algorithms for the evaluation of unstructured queries over XML documents. The paper includes retrieval experiments using a generative language model on the content only topics of the INEX testbed, demonstrating the strengths and flexibility of language modeling to a variety of problems. We also describe index characteristics, running times, and the effectiveness of the retrieval algorithm.
- Charles L. A. Clarke, G. Cormack, Jamie Callan, D. Hawking, A. Smeaton. 2003. Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. Abstract: Welcome to Toronto! We are pleased to host the Annual International ACM SIGIR Conference on its second visit to Canada. The tutorials, keynote speech, papers, posters, demos and workshops to be given over the next five days represent current techniques, challenges, and advances in information retrieval.Since the 8th SIGIR Conference in Montreal, 1985, information retrieval applications have become ubiquitous. It is difficult to imagine using a personal computer, a library, the web, or a peer-to-peer file sharing system without relying on the results of information retrieval research. At the same time it is easy to observe limitations in the tools we use and to imagine how they might be improved. These observations provide the impetus for current and future research.Toronto, Canada's largest city with a population of 2.5 million, is home to virtually all of the world's cultural groups, boasting safe and clean streets, first class entertainment, fine dining, major league sports, parks, and recreation facilities. It may surprise you that Toronto is also a major centre for television and movie production, third in North America after Los Angeles and New York. "Chicago" -winner of six Academy Awards including Best Picture - was filmed here.
- Luo Si, Jamie Callan. 2003. A semisupervised learning method to merge search engine results. Abstract: The proliferation of searchable text databases on local area networks and the Internet causes the problem of finding information that may be distributed among many disjoint text databases (distributed information retrieval). How to merge the results returned by selected databases is an important subproblem of the distributed information retrieval task. Previous research assumed that either resource providers cooperate to provide normalizing statistics or search clients download all retrieved documents and compute normalized scores without cooperation from resource providers.This article presents a semisupervised learning solution to the result merging problem. The key contribution is the observation that information used to create resource descriptions for resource selection can also be used to create a centralized sample database to guide the normalization of document scores returned by different databases. At retrieval time, the query is sent to the selected databases, which return database-specific document scores, and to a centralized sample database, which returns database-independent document scores. Documents that have both a database-specific score and a database-independent score serve as training data for learning to normalize the scores of other documents. An extensive set of experiments demonstrates that this method is more effective than the well-known CORI result-merging algorithm under a variety of conditions.
- Paul Ogilvie, Jamie Callan. 2003. Combining document representations for known-item search. Abstract: This paper investigates the pre-conditions for successful combination of document representations formed from structural markup for the task of known-item search. As this task is very similar to work in meta-search and data fusion, we adapt several hypotheses from those research areas and investigate them in this context. To investigate these hypotheses, we present a mixture-based language model and also examine many of the current meta-search algorithms. We find that compatible output from systems is important for successful combination of document representations. We also demonstrate that combining low performing document representations can improve performance, but not consistently. We find that the techniques best suited for this task are robust to the inclusion of poorly performing document representations. We also explore the role of variance of results across systems and its impact on the performance of fusion, with the surprising result that the correct documents have higher variance across document representations than highly ranking incorrect documents.
- S. Berretti, Jamie Callan, Henrik Nottelmann, X. Shou, Shengli Wu. 2003. MIND: resource selection and data fusion in multimedia distributed digital libraries. Abstract: Stefano Berretti, Jamie Callan, Henrik Nottelmann, Xiao Mang Shou, Shengli Wu 1 Dip. Sistemi e Informatica, Università degli Studi di Firenze, Firenze, Italy School of Computer Studies, Carnegie Mellon University, Pittsburgh, PA, USA Institute of Informatics and Interactive Systems, University of Duisburg-Essen, Duisburg, Germany Dept. of Information Studies, University of Sheffield, Sheffield, UK Dept. Computer and Information Sciences, University of Strathclyde, Glasgow, UK berretti@dsi.unifi.it, callan@cs.cmu.edu, nottelmann@uni-duisburg.de, x.m.shou@shef.ac.uk, shengli.wu@cis.strath.ac.uk
- Luo Si, Jie Lu, Jamie Callan. 2003. Distributed information retrieval with skewed database size distributions. Abstract: The proliferation of government information on local area networks and the Internet creates the problem of finding information that may be distributed among many disjoint text databases (distributed information retrieval or federated search). A distributed information retrieval system is composed of three components: Resource representation, resource selection and result merging. Previous research suggested that the CORI algorithm is one of the most effective resource selection algorithms, but its effectiveness in environments containing a wide range of database sizes was not studied thoroughly. This paper shows that the CORI algorithm does not work well in environments with a skewed distribution of database sizes. We present a new resource selection algorithm based on estimating the distribution of relevant documents among the online databases. This new algorithm selects resources more accurately than the CORI algorithm, which can lead to improved document rankings.
- Paul Ogilvie, Jamie Callan. 2003. Using Language Models for Flat Text Queries in XML Retrieval. Abstract: This paper presents a language modeling system for ranking flat text queries against a collection of structured documents. The system, built using Lemur, produces probability estimates that arbitrary document components generated the query. This paper describes storage mechanisms and retrieval algorithms for the evaluation of unstructured queries over XML documents. The paper includes retrieval experiments using a generative language model on the content only topics of the INEX testbed, demonstrating the strengths and flexibility of language modeling to a variety of problems. We also describe index characteristics, running times, and the effectiveness of the retrieval algorithm.
- Jamie Callan, F. Crestani, M. Sanderson. 2003. SIGIR 2003 workshop on distributed information retrieval. Abstract: During the last decade companies, governments, and research groups worldwide have directed significant effort towards the creation of sophisticated digital libraries across a variety of disciplines. As digital libraries proliferate, in a variety of media, and from a variety of sources, problems of resource selection and data fusion become major obstacles. Traditional search engines, even very large systems such as Google, are unable to provide access to the “Hidden Web” of information that is only available via digital library search interfaces. Effective, reliable information retrieval also requires the ability to pose multimedia queries across many digital libraries. The answer to a query about the lyrics to a folk song might be text or an audio recording, but few systems today could deliver both data types in response to a single, simple query. Distributed Information Retrieval addresses issues that arise when people have routine access to thousands of multimedia digital libraries.
- James Allan, J. Aslam, N. Belkin, C. Buckley, Jamie Callan, W. Bruce Croft, S. Dumais, N. Fuhr, D. Harman, David J. Harper, D. Hiemstra, Thomas Hofmann, E. Hovy, Wessel Kraaij, J. Lafferty, V. Lavrenko, D. Lewis, E. Liddy, R. Manmatha, A. McCallum, J. Ponte, J. Prager, Dragomir R. Radev, P. Resnik, S. Robertson, R. Rosenfeld, S. Roukos, M. Sanderson, R. Schwartz, A. Singhal, A. Smeaton, Howard R. Turtle, E. Voorhees, R. Weischedel, Jinxi Xu, ChengXiang Zhai. 2003. Challenges in information retrieval and language modeling: report of a workshop held at the center for intelligent information retrieval, University of Massachusetts Amherst, September 2002. Abstract: Information retrieval (IR) research has reached a
point where it is appropriate to assess progress and
to define a research agenda for the next five to ten
years. This report summarizes a discussion of IR
research challenges that took place at a recent
workshop.
The attendees of the workshop considered
information retrieval research in a range of areas
chosen to give broad coverage of topic areas that
engage information retrieval researchers. Those
areas are retrieval models, cross-lingual retrieval,
Web search, user modeling, filtering, topic detection
and tracking, classification, summarization, question
answering, metasearch, distributed retrieval,
multimedia retrieval, information extraction, as well
as testbed requirements for future work. The
potential use of language modeling techniques in
these areas was also discussed.
The workshop identified major challenges within
each of those areas. The following are recurring
themes that ran throughout:
• User and context sensitive retrieval
• Multi-lingual and multi-media issues
• Better target tasks
• Improved objective evaluations
• Substantially more labeled data
• Greater variety of data sources
• Improved formal models
Contextual retrieval and global information access
were identified as particularly important long-term
challenges.
- Eric Nyberg, T. Mitamura, J. Carbonell, Jamie Callan, Kevyn Collins-Thompson, Krzysztof Czuba, M. Duggan, L. Hiyakumoto, N. Hu, Yifen Huang, Jeongwoo Ko, L. Lita, S. Murtagh, V. Pedro, David Svoboda. 2002. The JAVELIN Question-Answering System at TREC 2002. Abstract: This Conference Proceeding is brought to you for free and open access by the School of Computer Science at Research Showcase. It has been acceptedfor inclusion in Computer Science Department by an authorized administrator of Research Showcase. For more information, please contactkbehrman@andrew.cmu.edu.
- Jamie Callan. 2002. Getting What You Want: Accurate Document Filtering in a Terabyte World. Abstract: Abstract : This document describes information retrieval research techniques to users who are interested in receiving filtered information for specific topics or domains. The research includes techniques for setting document filtering thresholds, for adaptive learning and for filtering based upon user feedback in order to refine the document filtering process.
- Yi Zhang, Jamie Callan, T. Minka. 2002. Novelty and redundancy detection in adaptive filtering. Abstract: This paper addresses the problem of extending an adaptive information filtering system to make decisions about the novelty and redundancy of relevant documents. It argues that relevance and redundance should each be modelled explicitly and separately. A set of five redundancy measures are proposed and evaluated in experiments with and without redundancy thresholds. The experimental results demonstrate that the cosine similarity metric and a redundancy measure based on a mixture of language models are both effective for identifying redundant documents.
- J. Aslam, Jamie Callan, R. Manmatha, M. Sanderson, E. Voorhees. 2002. MetaSearch : Data Fusion and Distributed Retrieval. Abstract: MetaSearch is the process of retrieving and combining information from multiple sources, and it is typically studied in one of two forms: (1) data fusion, the combination of information from multiple sources that index an effectively common data set and (2) collection fusion or distributed retrieval, the combination of information from multiple sources that index effectively disjoint data sets. As more and more retrieval mechanisms become available over common data sets (e.g., the World Wide Web) and specialized data sets (e.g., medical and law libraries), the process of identifying likely sources of relevant information, retrieving information from those sources, and effectively combining the information thus gathered will only grow in importance. A future wherein ubiquitous mobile wireless devices exist, capable of forming ad hoc peer-topeer networks and submitting and fielding requests for information, gives rise to a new host of challenges and potential rewards.
- Jamie Callan, P. Kantor, D. Grossman. 2002. Information retrieval and OCR: from converting content to grasping meaning. Abstract: IR and OCR have largely developed independent standards and metrics, with OCR focused on literal accuracy, and IR focused on essential "content/meaning". With more and more media not only paper, but in multiple image formats, the opportunities and challenges for OCR on new formats -- video and still images -- are enormous. While OCR is assessed in metrics that emphasize words and characters, IR has learned to apply end-to-end metrics that ask whether the needs of the users can be met by existing systems. The same considerations apply also to the problem of providing permanent worldwide access to millions of pages of legacy print documents, representing the shared human record as it existed until just a few years ago.The International Society for Optical Engineering (SPIE) has held a series of Document Recognition and Retrieval (DRR) conferences. The tenth, DRR X will be held in January 2003, in Santa Clara California. In 2001, Dan LoPresti of Bell Labs decided that the area would benefit from more intense collaboration between those who specialize in finding the words on a page image, and those researchers who know how to find the right documents, given the words. He invited Paul Kantor (Rutgers) to join the DRR Chairs, and together they invited Dave Lewis (Consultant) to give a keynote address at DRR VIII. Dan then stepped down. Paul chaired DRR IX (2002) and then handed the reins to Tapas Kanungo (IBM, Almaden) and together they invited Jamie Callan (CMU), David Grossman (IIT) and Alex Hauptmann (CMU) to join the conference committee for DRR X.To improve communication between SIGIR and DRR, this group proposed a SIGIR workshop on this area. The workshop on "Information Retrieval and OCR: From Converting Content to Grasping Meaning" was intended to stimulate cross-fertilization between OCR and IR, in hopes that better use of IR will enable the OCR community to avoid expensive hand processing, and to demonstrate that the combination of present static and dynamic image processing and present state-of-the-art robust information retrieval can generate substantial advances in both extraction of messages from image streams and conversion of existing paper variants. It solicited papers dealing with future applications, such as the indexing and retrieval of text embedded in static or video graphic images, with problems of skew, distortion, and obscuration, as well as state-of-the-art discussions of the storage and retrieval of handwritten or print legacy materials.The workshop was held on August 15, 2002 in Tampere, Finland, immediately following the SIGIR 2002 conference. Although the workshop was intended to appeal to a wide range of IR and OCR researchers (and indeed was proposed at the request of colleagues from the OCR community), it primarily drew people with a background in IR. About a dozen people participated. The small size allowed a very interactive, seminar-style format and very vigorous discussion between and during presentations. Most presentations ran 30% to 50% longer than planned, and our impression is that most of the participants found it very productive.
- Luo Si, Jamie Callan. 2002. Using sampled data and regression to merge search engine results. Abstract: This paper addresses the problem of merging results obtained from different databases and search engines in a distributed information retrieval environment. The prior research on this problem either assumed the exchange of statistics necessary for normalizing scores (cooperative solutions) or is heuristic. Both approaches have disadvantages. We show that the problem in uncooperative environments is simpler when viewed as a component of a distributed IR system that uses query-based sampling to create resource descriptions. Documents sampled for creating resource descriptions can also be used to create a sample centralized index, and this index is a source of training data for adaptive results merging algorithms. A variety of experiments demonstrate that this new approach is more effective than a well-known alternative, and that it allows query-by-query tuning of the results merging function.
- Jamie Callan, T. Mitamura. 2002. Knowledge-based extraction of named entities. Abstract: The usual approach to named-entity detection is to learn extraction rules that rely on linguistic, syntactic, or document format patterns that are consistent across a set of documents. However, when there is no consistency among documents, it may be more effective to learn document-specific extraction rules.This paper presents a knowledge-based approach to learning rules for named-entity extraction. Document-specific extraction rules are created using a generate-and-test paradigm and a database of known named-entities. Experimental results show that this approach is effective on Web documents that are difficult for the usual methods.
- Rong Jin, Luo Si, Alexander Hauptmann, Jamie Callan. 2002. Language model for IR using collection information. Abstract: Information retrieval using meta data can be traced back to the early age of IR where documents are represented by the controlled vocabulary. In this paper, we explore the usage of meta-data information under the framework of language model. We present a new language model that is able to take advantage of the category information for documents to improve the retrieval accuracy. We compare the new language model with the traditional language model over the TREC4 dataset where the collection information for documents is obtained using the k-means clustering method. The new language model outperforms the traditional language model, which verifies our statement.
- Paul Ogilvie, Jamie Callan. 2002. Language Models and Structured Document Retrieval. Abstract: We discuss possibilities for the use of language models in structured document retrieval. We use a tree-based generative language model for ranking documents and components. Nodes in the tree correspond to document components such as titles, sections, and paragraphs. At each node in the document tree, there is a language model. The language model for a leaf node is estimated directly from the text present in the document component associated with the node. Inner nodes in the tree are estimated using a linear interpolation among the children nodes. This paper also describes how some common structural queries would be satisfied within this model.
- Jie Lu, Jamie Callan. 2002. Pruning long documents for distributed information retrieval. Abstract: Query-based sampling is a method of discovering the contents of a text database by submitting queries to a search engine and observing the documents returned. In prior research sampled documents were used to build resource descriptions for automatic database selection, and to build a centralized sample database for query expansion and result merging. An unstated assumption was that the associated storage costs were acceptable.When sampled documents are long, storage costs can be large. This paper investigates methods of pruning long documents to reduce storage costs. The experimental results demonstrate that building resource descriptions and centralized sample databases from the pruned contents of sampled documents can reduce storage costs by 54-93% while causing only minor losses in the accuracy of distributed information retrieval.
- Kevyn Collins-Thompson, Paul Ogilvie, Yi Zhang, Jamie Callan. 2002. Information Filtering, Novelty Detection, and Named-Page Finding. Abstract: In TREC 11, our group participated in the Novelty track, Filtering track, and the Named-Page Finding task of the Web track. This paper describes our approaches, experiments, and results. As the approach for each task is quite different, the paper contains a section for each of the tasks. The following section describes our experiments in adaptive filtering, Section 3 describes named-page finding, and section 4 discusses the Novelty track.
- Luo Si, Rong Jin, Jamie Callan, Paul Ogilvie. 2002. A language modeling framework for resource selection and results merging. Abstract: Statistical language models have been proposed recently for several information retrieval tasks, including the resource selection task in distributed information retrieval. This paper extends the language modeling approach to integrate resource selection, ad-hoc searching, and merging of results from different text databases into a single probabilistic retrieval model. This new approach is designed primarily for Intranet environments, where it is reasonable to assume that resource providers are relatively homogeneous and can adopt the same kind of search engine. Experiments demonstrate that this new, integrated approach is at least as effective as the prior state-of-the-art in distributed IR.
- Jamie Callan. 2002. Searching for Needles in a World of Haystacks. Abstract: Current Web search engines are based on a single database model of information retrieval. This paper argues that next generation Web search will be based on a multi-database model of information retrieval. The strengths, weaknesses, open research problems, and prospects of several multi-database retrieval models are discussed briefly.
- Jamie Callan, W. Xu, Yi Zhang. 2002. Exact Maximum Likelihood Estimation for Word Mixtures. Abstract: The mixture model for generating document is a generative language model used in information retrieval. While using this model, there are situations that we need to find the maximum likelihood estimation of the density of one multinomial, given fixed mixture weight and the densities of the other multinomial. In this paper, we provide an exact solution and a quick algorithm to solve this problem. The new algorithm is guaranteed to find the exact optimal result, at a fast speed. More over, some interesting properties of the solution are discussed.
- W. Bruce Croft, Jamie Callan, J. Lafferty. 2001. Workshop on language modeling and information retrieval. Abstract: The language modeling approach to information retrieval (IR) is a new framework that has been proposed and developed within the past five years, although its roots in the IR literature go back more than twenty years. Research carried out at a number of sites has confirmed that the language modeling approach is a theoretically attractive and potentially very effective probabilistic framework for building IR systems.
- Yi Zhang, Jamie Callan. 2001. The Bias Problem and Language Models in Adaptive Filtering. Abstract: We used the YFILTER filtering system for experiments on updating profiles and setting thresholds. We developed a new method of using language models for updating profiles that is more focused on picking informative/discriminative words for query. The new method was compared with the well-known Rocchio algorithm. Dissemination thresholds were set based on maximum likelihood estimation that models and compensates for the sampling bias inherent in adaptive filtering. Our experimental results suggest that using what kind of distribution to model the scores of relevant and non- relevant documents is corpus dependant. The experimental results also show the sampling bias problem of training data while filtering makes the final profile learned biased.
- Yi Zhang, Jamie Callan. 2001. Maximum likelihood estimation for filtering thresholds. Abstract: Information filtering systems based on statistical retrieval models usually compute a numeric score indicating how well each document matches each profile. Documents with scores above profile-specificdissemination thresholdsare delivered.
An optimal dissemination threshold is one that maximizes a given utility function based on the distributions of the scores of relevant and non-relevant documents. The parameters of the distribution can be estimated using relevance information, but relevance information obtained while filtering isbiased. This paper presents a new method of adjusting dissemination thresholds that explicitly models and compensates for this bias. The new algorithm, which is based on the Maximum Likelihood principle, jointly estimates the parameters of the density distributions for relevant and non-relevant documents and the ratio of the relevant document in the corpus. Experiments with TREC-8 and TREC-9 Filtering Track data demonstrate the effectiveness of the algorithm.
- Jamie Callan, Margaret E. Connell. 2001. Query-based sampling of text databases. Abstract: The proliferation of searchable text databases on corporate networks and the Internet causes a database selection problem for many people. Algorithms such as gGLOSS and CORI can automatically select which text databases to search for a given information need, but only if given a set of resource descriptions that accurately represent the contents of each database. The existing techniques for a acquiring resource descriptions have significant limitations when used in wide-area networks controlled by many parties. This paper presents query-based sampling, a new technicque for acquiring accurate resource descriptions. Query-based sampling does not require the cooperation of resource providers, nor does it require that resource providers use a particular search engine or representation technique. An extensive set of experimental results demonstrates that accurate resource descriptions are crated, that computation and communication costs are reasonable, and that the resource descriptions do in fact enable accurate automatic dtabase selection.
- Paul Ogilvie, Jamie Callan. 2001. The effectiveness of query expansion for distributed information retrieval. Abstract: Query expansion has been shown effective for both single database retrieval and for distributed information retrieval where complete collection information is available. One might expect that query expansion would then work for distributed information retrieval when complete collection information is not available. However, this does not appear to be the case. When using local context analysis for query expansion in distributed retrieval with partial information, the most significant reason query expansion does not work is that merging scores of documents retrieved by expanded queries is very difficult. However, we have found that using sampled information for query expansion can give boosts in a single database environment, and that when more information is available, query expansion can work in distributed environments. We also show that most of the benefit of query expansion in distributed retrieval comes from finding good documents, and not from selecting good databases.
- A. Smeaton, Jamie Callan. 2001. Joint DELOS-NSF workshop on personalisation and recommender systems in digital libraries. Abstract: One of the important ways for users to feel comfortable with and become productive using information technology is to personalise or tailor systems to individuals or groups of users. This covers both explicit personalisation directly by the user, and implicit tailoring by systems that track users usage patterns and preferences and adapt systems and interfaces accordingly. The concept of personalisation thus is about making systems different for individual people, but the concept of personalization itself can mean different things.
- Paul Ogilvie, Jamie Callan. 2001. Experiments Using the Lemur Toolkit. Abstract: This paper describe experiments with Lemur Toolkit. We participated in the ad-hoc retrieval task of the Web Track
- Jamie Callan, R. Krishnan, A. Montgomery. 2001. Peer-to-Peer Networks for Self-Organizing Virtual Communities. Abstract: 1 Project Summary Locating information on the Internet is a leading usability problem. Often desired information exists but is difficult to find. A method for addressing this problem in the physical world is to create a community. Communities tend to be organized around individuals with similar needs and characteristics. The commonalties within a community provide a context in which to evaluate retrieved information, increasing the chance that information will be relevant to the specific information need. For example, a biomedical researcher interested in locating current genetic research on Parkinson's Disease will have much different requirements than a patient who has been diagnosed with this disease. A search by each of these individuals using a general-purpose search engine would be treated in the same way, since syntactically the searches may be identical. However, if the biomedical researcher presents the request to other biomedical researchers the obtained information is likely to be much more relevant for his or her needs. Communities help preserve the information need context that is missing in general purpose search engines. Virtual communities have developed on the Internet, organized around particular topics, to serve purposes similar to more traditional communities in the physical world. The existing approaches to virtual community formation tend to be top down or ad-hoc. Virtual community hosting sites such as geocities.com do not support community formation based on content or type of information retrieved. Online communities must find an efficient means of locating individuals for which their information is highly valuable, or else only large-scale online communities will be practical. Recent failures of online business communities (e.g., Chemdex.com in the Life Sciences industry) have been attributed to the high costs of member acquisition. We propose a new approach to forming virtual communities that is based on peer-to-peer computer networks. Peer-to-peer computer networks enable members to communicate directly with one another. The communications links between network nodes form a dynamic network topology. Nodes are aware of which other nodes they are connected to, and nodes can disconnect from one part of the network (one set of nodes) and reconnect to another part of the network (another set of nodes) when it is convenient or advantageous to do so. Large-scale peer-to-peer networks offer the possibility of self-organizing communities, in which nodes recognize and create relatively stable connections to other nodes with similar interests/requirements/capabilities. A given node might participate in several such virtual communities at a given …
- Yi Zhang, Jamie Callan. 2001. A Generative Model for Filtering Thresholds. Abstract: This paper presents a generative model of score distribution, focused on the case of information filtering, where sampling of training data is not random. Parameters of the model were estimated using the Maximum Likelihood Principle, conjugate priors, and conjugate gradient descent. Experiments on TREC8 and TREC9 Filtering Track datasets are reported. Our method obtained significant improvements compared to a baseline.
- Luo Si, Jamie Callan. 2001. A statistical model for scientific readability. Abstract: In this paper, we present a new method of using statistical models to estimate readability [1]. Language Model is used to capture the content information. It is combined with linguistic feature model by a linear form. Experiments show that this new method has a better performance than the widely used Flesch-Kincaid readability formula.
- R. Manmatha, James Allan, W. Bruce Croft, Jamie Callan. 2000. Infrastructure of Large Scale Multimedia Information Indexing Retrieval and Organization. Abstract: Abstract : The equipment acquired through DURIP has enabled NSF's National Center for Intelligent Information Retrieval (CIIR) at the University of Massachusetts to further its mission of carrying out leading research in the areas of organizing, classifying and retrieving text and more recently multimedia. CIIR's goal is to provide organizations and individuals tools to organize information, so that relevant information may be easily obtained and new relationships discovered. The center has a number of grants from the defense department (see the list in section D) and the DURIP equipment has aided in the research performed as part of these grants. The Center has trained over 60 graduate students and 50 undergraduate students to do leading edge research and development in information retrieval. Currently, the center has 3 faculty, 10 technical staff (including. one postdoctoral research associate) and 16 graduate and 12 undergraduate students are being trained.
- Jade Goldstein-Stewart, Vibhu Mittal, J. Carbonell, Jamie Callan. 2000. Creating and evaluating multi-document sentence extract summaries. Abstract: This paper discusses passage extraction approaches to multidocument summarization that use available information about the document set as a whole and the relationships between the documents to build on single document summarization methodology. Multi-document summarization di ers from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries, as well as the user's goals in creating the summary. Our approach addresses these issues by using domain-independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for di erent genres, corpora characteristics and user requirements. We examined how humans create multi-document summaries as well as the characteristics of such summaries and use these summaries to evaluate the performance of various multidocument summarization algorithms.
- Jamie Callan. 2000. SIGIR Announces Member Plus Program. Abstract: SIGIR is offering a new membership class beginning July 1, 2000. The new "Member Plus" membership package includes conference proceedings from three conferences sponsored by ACM SIGIR: The Member Plus package also includes the benefits of "Basic" membership, such as SIGIR Forum and reduced registration fees at the SIGIR conference. Member Plus membership is an inexpensive method of staying abreast of research at three leading IR conferences.
- Allison L. Powell, J. French, Jamie Callan, Margaret E. Connell, C. Viles. 2000. The impact of database selection on distributed searching. Abstract: The proliferation of online information resources increases the importance of effective and efficient distributed searching. Distributed searching is cast in three parts — database selection, query processing, and results merging. In this paper we examine the effect of database selection on retrieval performance. We look at retrieval performance in three different distributed retrieval testbeds and distill some general results. First we find that good database selection can result in better retrieval effectiveness than can be achieved in a centralized database. Second we find that good performance can be achieved when only a few sites are selected and that the performance generally increases as more sites are selected. Finally we find that when database selection is employed, it is not necessary to maintain collection wide information (CWI), e.g. global idf. Local information can be used to achieve superior performance. This means that distributed systems can be engineered with more autonomy and less cooperation. This work suggests that improvements in database selection can lead to broader improvements in retrieval performance, even in centralized (i.e. single database) systems. Given a centralized database and a good selection mechanism, retrieval performance can be improved by decomposing that database conceptually and employing a selection step.
- L. Larkey, Margaret E. Connell, Jamie Callan. 2000. Collection selection and results merging with topically organized U.S. patents and TREC data. Abstract: We investigate three issues in d istributed information retrieval, considering both TREC data and U.S. Patents: (1) topical organization o f large text collections, (2) collection ranking and selection with topically organized collections (3) results merging, particularly document score normalization, with topically organized collections. We find that it is better to organize collections topically, and that topical collections can be well ranked using either INQUERY’s CORI algorithm, or the Kullback-Leibler divergence (KL), but KL is far worse than CORI for non-topically organized collections. For r esults merging, collections organized b y topic require global idfs for the best performance. Contrary to results found elsewhere, normalized scores are not as good as global idfs for merging when the collections are topically organized.
- Yi Zhang, Jamie Callan. 2000. YFilter at TREC-9. Abstract: Abstract : We built a filtering system YFILTER this year, which we used for experiments on profile updating and thresholds setting. Our focus is using incremental Rocchio for introducing new query terms and term weighting. Although 1, 0.5, 0.25 is a widely used Rocchio ratio for query expansion based on relevance feedback, we found that the optimal setting for information filtering is corpus and profile dependent. In addition to a new Rocchio ratio, we tested a modified idf measure for term weighting (ydf) that is biased towards words with middle range term frequency.
- Jamie Callan. 2000. Next Generation Web Search: Setting Our Sites.. Abstract: The current state of web search is most successful at directing users to appropriate web sites. Once at the site, the user has a choice of following hyperlinks or using site search, but the latter is notoriously problematic. One solution is to develop specialized search interfaces that explicitly support the types of tasks users perform using the information specific to the site. A new way to support task-based site search is to dynamically present appropriate metadata that organizes the search results and suggests what to look at next, as a personalized intermixing of search and hypertext.
- Jamie Callan, Margaret E. Connell, Aiqun Du. 1999. Automatic discovery of language models for text databases. Abstract: The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database selection algorithm such as GIOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative approach has important limitations.
This paper demonstrates that cooperation is not required. Instead, the database selection service can construct its own language models by sampling database contents via the normal process of running queries and retrieving documents. Although random sampling is not possible, it can be approximated with carefully selected queries. This sampling approach avoids the limitations that characterize the cooperative approach, and also enables additional capabilities. Experimental results demonstrate that accurate language models can be learned from a relatively small number of queries and documents.
- A. Berger, John La erty chair, J. Carbonell, D. Sleator, Jamie Callan. 1999. Information Retrieval and Information Theory. Abstract: Information retrieval is concerned with how to classify information and how to judge the similarity between two objects, such aswritten documents. As the amount of information available in digital formhas grown, so too has theneed for accurate and scalable algorithms for handling this information. Information theory is concerned with the production and transmission of information. Using a framework known as the source-channel model of communication, information theory has established theoretical bounds on the limits of data compression and communication in the presence of noise and has contributed to practical technologies as varied as cellular communication and statistical translation. In this work we establish a set of connections between information retrieval and information theory. In particular, we focus on two central and well-studied problems in retrieval: classifying documents and ranking documents by relevance to a query. Viewing these problems from a source-channel perspective, we develop new statistical algorithms for each: to the classification problem we apply methods from errorcorrecting coding theory, and to the ranking problem we apply methods from statistical translation. In both cases, we report on the architecture and empirical behavior of prototype systems, whose performance improves on that of state of the art techniques. 1 Two Problems in Information Retrieval
- James Allan, Jamie Callan, M. Sanderson, Jinxi Xu, S. Wegmann. 1999. INQUERY and TREC-8. Abstract: Abstract : This year the Center for Intelligent Information Retrieval (CIIR) at the University of Massachusetts participated in three of the tracks: the cross-language, question answering, and query tracks. We used approaches that were similar to those used in past years. Although UMass used a wide range of tools, from Unix shell scripts, to PC spreadsheets, three major tools and techniques were applied across almost all tracks: the Inquery search engine, query processing, and a query expansion technique known as LCA. All three tracks used Inquery as the search engine, sometimes for training, and always for generating the final ranked lists for the test. In the cross language track, we experimented some techniques for crossing the character encoding boundaries. Our efforts were moderately successful, but we do not believe that our approach worked well in comparison to other techniques. In the question answering track, we focused on bringing answer-containing documents to the top of the ranked list. This is an important sub-task for most methods of tackling Q&A, and we are pleased with our results. We are now looking at alternate ways of thinking about that task that leverage the differences between retrieval for Q&A and for IR. Finally, we continued to participate in the query track, providing large numbers of query variants, and running our system on the huge number of resulting queries. Our analysis showed how query expansion compensates for some of the problems that can occurs in query formulation.
- J. French, Allison L. Powell, Jamie Callan. 1999. Effective and Efficient Automatic Database Selection. Abstract: We examine a class of database selection algorithms that require only document frequency information. The CORI algorithm is an instance of this class of algorithms. In previous work, we showed that CORI is more effective than gGlOSS when evaluated against a relevance-based standard. In this paper, we introduce a family of other algorithms in this class and examine components of these algorithms and of the CORI algorithm to begin identifying the factors responsible for their performance. We establish that the class of algorithms studied here is more effective and efficient than gGlOSS and is applicable to a wider variety of operational environments. In particular, this methodology is completely decoupled from the database indexing technology so is as useful in heterogeneous environments as in homogeneous environments.
- J. French, Allison L. Powell, Jamie Callan, C. Viles, T. Emmitt, K. Prey, Y. Mou. 1999. Comparing the performance of database selection algorithms. Abstract: We compare the performance of two database selection algorithms reported in the literature. Their performance is compared using a common testbed designed specifically for database selection techniques. The testbed is a decomposition of the TREC/TIPSTER data into 236 subcollections. We present results of a recent investigation of the performance of the CORI algorithm and compare the performance with earlier work that examined the performance of gGlOSS. The databases from our testbed were ranked using both the gGlOSS and CORI techniques and compared to the RBR baseline, a baseline derived from TREC relevance judgements. We examined the degree to which CORI and gGlOSS approximate this baseline. Our results confirm our earlier observation that the gGlOSS Ideal(l) ranks do not estimate relevance-based ranks well. We also find that CORI is a uniformly better estimator of relevance-based ranks than gGlOSS for the test environment used in this study. Part of the advantage of the CORI algorithm can be explained by a strong correlation between gGlOSS and a size-based baseline (SBR). We also find that CORI produces consistently accurate rankings on testbeds ranging from 100--921 sites. However for a given level of recall, search effort appears to scale linearly with the number of databases.
- James Allan, Jamie Callan, M. Sanderson, Jinxi Xu, S. Wegmann. 1998. Inquery and Trec-7 1.2 Inroute. Abstract: ipated in only four of the tracks that were part of the TREC-7 workshop. We w orked on ad-hoc retrieval, ltering, VLC, and the SDR track. This report covers the work done on each track successively. We start with a discussion of IR tools that were broadly applied in our work. Although UMass used a wide range of tools, from Unix shell scripts, to PC spreadsheets, three major tools were applied across almost all tracks: the Inquery search engine, the InRoute ltering engine, and a a query expansion technique known as LCA. This section provides a brief overview of each of those so that the discussion does not have to repeated for each track. 1.1 Inquery All tracks other than the ltering track used Inqueryy6 as the search engine, sometimes for training, and always for generating the nal ranked lists for the test. We used Inquery V3.2, an in-house development version of the Inquery system made available by the CIIR V3.1. The diierences between the two are not consequential for this study. The current belief function used by Inquery to calculate the belief in term t within document d is: w t;d = 0 :4 + 0 :6 tf t;d tf t;d + 0 :5 + 1 :5 lengthd avg len log N+0:5 nt log N + 1 where n t is the number of documents containing term t, N is the number of documents in the collection, avg len" is the average length in words of documents in the collection, lengthd is the length in words of document d, and tf t;d is the number of times term t occurs in document d. The InRoute ltering system is based on the same Bayesian inference network model as InQuery. It is designed to operate eeciently in high-volume ltering environments, where incoming documents must be processed rapidly, one at a time. It uses the same document indexing techniques, query language, and scoring 1
- Jamie Callan. 1998. Learning while filtering documents. Abstract: This paper examines the problems of learning queries and dissemination thresholds from relevance feedback in a dynamic information filtering environment. It revisits the EG algorithm for learning queries, identifying several problems in using it reliably for information filtering, and providing solutions. It also presents a new algorithm for learning dissemination thresholds automatically, from the same relevance feedback information used to learn queries.
- Aiqun Du, Jamie Callan. 1998. Probing a Collection to Discover Its Language Model. Abstract: Abstract : Most solutions to distributed IR rely on access to a language model for each text collection, but it has been unclear how the model can be obtained reliably in real-world distributed environments. This paper proposes a solution based upon probing the collection and demonstrates its effectiveness on four databases.
- Jinxi Xu, Jamie Callan. 1998. Effective retrieval with distributed collections. Abstract: Abstract : This paper evaluates the retrieval effectiveness of distributed information retrieval systems in realistic environments. We find that when a large number of collections are available, the retrieval effectiveness is significantly worse than that of centralized systems, mainly because typical queries are not adequate for the purpose of choosing the right collections. We propose two techniques to address the problem. One is to use phrase information in the collection selection index and the other is query expansion. Both techniques enhance the discriminatory power of typical queries for choosing the right collections and hence significantly improve retrieval results. Query expansion, in particular, brings the effectiveness of searching a large set of distributed collections close to that of searching a centralized collection.
- Aiqun Du, Jamie Callan. 1998. Improving Efficiency of Indexing by Using a Hierarchical Merge Approach. Abstract: Abstract : As electronic collections become larger and more numerous, systems capable of working with very large collections will be more and more in demand. On the other hand, computer main memory is relatively limited and constrained compared to the amount of data in a large collection. The requirement for larger memory while building big databases can sometimes make this resource a bottleneck for an information indexing system. INQUERY is a state-of-the-art, widely used, full-text information retrieval system. The INQUERY document indexing system consists of two main operations: parsing and merging. The subsystem responsible for parsing is called the Parser. It creates partial inverted lists by scanning, lexically analyzing, and inverting documents. A partial inverted list contains document entries for a subset of the documents in the collection. It must be combined with other partial inverted lists for the same term to create a final inverted list for the document collection. The Parser buffers partial inverted lists in main memory and flushes them to intermediate files when the buffer is full. The subsystem responsible for merging is called the Merger. After all of the documents have been parsed, the Merger combines the intermediate files to produce the final inverted lists for the collection. The aim of this project is to solve the efficiency and scalability problem of the Merger for the INQUERY indexing system. The speed performance of the old Merger (merge_btl) degrades significantly when used in building big databases under tight memory space limitations. The authors have found a better solution by using a hierarchical merge approach. Timing tests on the new Merger indicates that merge time can be significantly reduced. Hierarchical merge nicely solves the problem of scalability and greatly improves the efficiency of building very large databases for information retrieval systems.
- James Allan, Jamie Callan, W. Bruce Croft, Lisa Ballesteros, Donald Byrd, R. Swan, Jinxi Xu. 1997. INQUERY Does Battle With TREC-6. Abstract: Participation du CIIR (Center for Intelligent Information Retrieval) de l'Universite du Massachusetts au congres TREC 6
- James Allan, Jamie Callan, B. Croft, Lisa Ballesteros, J. Broglio, Jinxi Xu, Hongming Shu. 1997. Inquery at Trec-5 1 Ad-hoc Experiments. Abstract: The University of Massachusetts participated in ve tracks in TREC-5: Ad-hoc, Routing, Filtering , Chinese, and Spanish. Our results are generally positive, continuing to indicate that the techniques we have applied perform well in a variety of settings. Signiicant changes in our approaches include emphasis on identifying key concepts/terms in the query topics, expansion of the query using a variant of automatic feedback called \Local Context Analysis", and application of these techniques to a non-European language. The results show the broad applicability of Local Context Analysis, demonstrate successful identiication and use of key concepts, raise interesting questions about how key concepts aaect precision, support the belief that many IR techniques can be applied across languages, present an intriguing lack of tradeoo between recall and precision when ltering, and connrm once again several known results about query formulation and combination. Regrettably, three of our oocial submissions were marred by errors in the processing (an unde-tected syntax error in some queries, and an incomplete data set in an another case). The following discussion analyzes corrected runs as well as those (not particularly meaningful) submitted runs. Our experiments were conducted with version 3.1 of the INQUERY information retrieval system. INQUERY is based on the Bayesian inference network retrieval model. It is described elsewhere 5, 4, 12, 11], so this paper focuses on relevant diierences to the previously published algorithms. The emphasis in this year's Ad-Hoc experiments was on how to create very eeective queries from short natural language topics. In particular, how to create several rather diierent queries from the Description eld of the topic. The underlying assumption, based on past experience, was that a combination of several \pretty good" queries would be more eeective (or, at least easier to create) than one \excellent" query. Three techniques were explored. The rst, which we call basic query processing in this paper, is an extension of the query processing techniques we used in previous TREC evaluations. The second, which we call core query processing, attempted to identify and emphasize the most critical query terms. The third, local context analysis, is a new approach to query expansion 14]. Each technique is discussed in more detail in the subsections that follow. Two Ad-Hoc runs were submitted. Each query in INQ301 was a combination of three queries created from the Description (<desc>) elds of topics 251-300. Each query in INQ302 was a
- R. Papka, Jamie Callan, A. Barto. 1996. Text-Based Information Retrieval Using Exponentiated Gradient Descent. Abstract: The following investigates the use of single-neuron learning algorithms to improve the performance of text-retrieval systems that accept natural-language queries. A retrieval process is explained that transforms the natural-language query into the query syntax of a real retrieval system: the initial query is expanded using statistical and learning techniques and is then used for document ranking and binary classification. The results of experiments suggest that Kivinen and Warmuth's Exponentiated Gradient Descent learning algorithm works significantly better than previous approaches.
- D. Lewis, R. Schapire, Jamie Callan, R. Papka. 1996. Training algorithms for linear text classifiers. Abstract: Systems for text retrieval, routing, categorization and other IR tasks rely heavily on linear classifiers. We propose that two machine learning algorithms, the Widrow-Hoff and EG algorithms, be used in training linear text classifiers. In contrast to most IR methods, theoretical analysis provides performance guarantees and guidance on parameter settings for these algorithms. Experimental data is presented showing Widrow-Hoff and EG to be more effective than the widely used Rocchio algorithm on several categorization and routing tasks.
- Jamie Callan. 1996. Document filtering with inference networks. Abstract: Although statistical retrieval models are now accepted widely, there has been little research on how to adapt them tothedemands ofhighspeed documeut filtering. The problems of document retrieval and document filtering are similar at an abstract level, but the architectures required, the optirnizations that are possible, andthecluality of the infermation available, are all different. This paper describes a new statistical document filtering system called InRoute, the problems of flteringe ffectiveness and efficiency that arise with such a system, and experiments with various solutions.
- S. Boisen, Michael Crystal, Erik Peterson, R. Weischedel, J. Broglio, Jamie Callan, W. Bruce Croft, Therese Firmin Hand, T. P. Keenan, M. E. Okurowski. 1996. Chinese Information Extraction and Retrieval. Abstract: This paper provides a summary of the following topics:1. what was learned from porting the INQUERY information retrieval engine and the INFINDER term finder to Chinese2. experiments at the University of Massachusetts evaluating INQUERY performance on Chinese newswire (Xinhua),3. what was learned from porting selected components of PLUM to Chinese4. experiments evaluating the POST part of speech tagger and named entity recognition on Chinese.5. program issues in technology development.
- S. Vasanthakumar, Jamie Callan, W. Bruce Croft. 1996. Integrating INQUERY with an RDBMS to Support Text Retrieval.. Abstract: Information is a combination of structured data and unstructured data. Traditionally, relational database management systems (RDBMS) have been designed to handle structured data. IR systems can handle text (unstructured data) very well but are not designed to handle structured data. With present day information being a combination of structured and unstructured data, there is an increasing demand for an IR-DBMS system that incorporates features of both IR and DBMSs. We discuss a framework that incorporates powerful text retrieval in relational database management systems. An extended SQL with probabilistic operators for text retrieval is deened. This paper also discusses an implementation of the probabilistic operators in SQL.
- James Allan, Jamie Callan, W. Bruce Croft, Lisa Ballesteros, J. Broglio, Jinxi Xu, Hongming Shu. 1996. INQUERY at TREC-5. Abstract: L'equipe de l'universite du Massachusetts a explore trois techniques de traitement de question : le traitement de la question de base, le traitement de la question centrale (concept cle), l'analyse du contexte local. Les AA. presentent les resultats obtenus lors de leurs experiences sur l'espagnol et le chinois de routing et de filtrage a l'aide du systeme INQUIRY
- W. Bruce Croft, Jamie Callan, D. Aronow. 1995. Effective access to distributed heterogeneous medical text databases.. Abstract: INQUERY is an advanced text information retrieval system developed by the Information Retrieval Laboratory of the University of Massachusetts in Amherst. It is based on Bayesian inference networks, which are probabilistic models for reasoning with multiple sources of uncertain evidence. The evidence, in this case, is the presence or absence of words and/or phrases in a document. Evidence is combined into belief that a document is relevant. The INQUERY retrieval engine has been developed with the support of ARPA, NSF, and industrial funding. It has a number of unique features and has achieved excellent results in the TIPSTER and TREC evaluations. Informatics research and application development using INQUERY has recently begun in the medical domain, including a new ARPA initiative concerned with clinical text. The features that we will focus on in this demonstration are: Automatic processing of natural language queries, including the extraction of phrases and specific medical concepts such as drug doses; Document selection through automatic relevance feedback and routing techniques, including the construction of complex queries using the INQUERY query language; The integration of conventional database techniques with text analysis and retrieval; Automatic thesaurus generation and query expansion using the PhraseFinder system; Distributed database access, including automatic database selection and merging of local searches; this will be demonstrated using a collection of medical databases; Retrieval based on passages, rather than whole documents.
- Jamie Callan, Zhihong Lu, W. Bruce Croft. 1995. Searching Distributed Collections With Inference Networks. Abstract: The use of information retrieval systems in networked environments raises a new set of issues that have received little attention. These issues include ranking document collections for relevance to a query, selecting the best set of collections from a ranked list, and merging the document rankings that are returned from a set of collections. This paper describes methods of addressing each issue in the inference network model, discusses their implementation in the INQUERY system, and presents experimental results demonstrating their effectiveness.
- Jamie Callan, Zhihong Lu, W. Bruce Croft. 1995. Searching distributed collections with inference networks. Abstract: The use of information retrieval systems in networked environments raises a new set of issues that have received little attention. These issues include ranking document collections for relevance to a query, selecting the best set of collections from a ranked list, and merging the document rankings that are returned from a set of collections. This paper describes methods of addressing each issue in the inference network model, dkcusses their implementation in the INQUERY system, and presents experimental results demonstrating their effectiveness.
- Samuel DeFazio, Amjad M. Daoud, Lisa Ann Smith, Jagannathan Srinivasan, W. Bruce Croft, Jamie Callan. 1995. Integrating IR and RDBMS using cooperative indexing. Abstract: The full integration of information retrieval (IR) features into a database management system (DBMS) has long been recognized as both a significant goal and a challenging undertaking. By full integration we mean: i) support for document storage, indexing, retrieval, and update, ii) transaction semantics, thus all database operations on documents have the ACID properties of atomicity, consistency, isolation, and durability, iii) concurrent addition, update, and retrieval of documents, and iv) database query language extensions to provide ranking for document retrieval operations. It is also necessary for the integrated offering to exhibit scaleable performance for document indexing and retrieval processes, To identify the implementation requirements imposed by the desired level of integration, we layered a representative IR application on Oracle Rdb and then conducted a number of database load and document retrieval experiments. The results of these experiments suggest that infrastructural extensions are necessary to obtain both the desired level of IR integration and scaleable performance. With the insight gained from our initial experiments, we developed an approach, called cooperative indexing, that provides a framework to achieve both scalability and full integration of IR and RDBMS technology. Prototype implementations of system-level extensions to support cooperative indexing were evaluated with a modified version of Oracle Rdb. Our experimental findings validate the cooperative indexing scheme and suggest alternatives to further improve performance.
- James Allan, Lisa Ballesteros, Jamie Callan, W. Bruce Croft, Zhihong Lu. 1995. Recent Experiments with INQUERY. Abstract: Abstract : Past TREC experiments by the University of Massachusetts have focused primarily on ad hoc query creation. Substantial effort was directed towards automatically translating TREC topics into queries using a set of simple heuristics and query expansion. Less emphasis was placed on the routing task although results were generally good. The Spanish experiments in TREC-3 concentrated on simple indexing sophisticated stemming and simple methods of creating queries. The TREC-4 experiments were a departure from the past. The ad hoc experiments involved "fine tuning" existing approaches and modifications to the INQUERY term weighting algorithm. However, much of the research focus in TREC-4 was on the routing, Spanish, and collection merging experiments. These tracks more closely match our broader research interests in document routing document filtering distributed IR, and multilingual retrieval. The University of Massachusetts experiments were conducted with version 3.0 of the INQUERY information retrieval system. INQUERY is based on the Bayesian inference network retrieval model. It is described elsewhere [7, 5, 12, 11], so this paper focuses on relevant differences to the previously published algorithms.
- E. Brown, Jamie Callan, W. Bruce Croft. 1994. Fast Incremental Indexing for Full-Text Information Retrieval. Abstract: Full-text information retrieval systems have traditionally been designed for archival environments. They often provide little or no support for adding new documents to an existing document collection, requiring instead that the entire collection be re-indexed. Modern applications, such as information filtering, operate in dynamic environments that require frequent additions to document collections. We provide this ability using a traditional inverted file index built on top of a persistent object store. The data management facilities of the persistent object store are used to produce efficient incremental update of the inverted lists. We describe our system and present experimental results showing superior incremental indexing and competitive query processing performance.
- J. Broglio, Jamie Callan, W. Bruce Croft, Daniel W. Nachbar. 1994. Document Retrieval and Routing Using the INQUERY System. Abstract: The INQUERY retrieval and routing system, which is based on the Bayesian inference net retrieval model, has been described in a number of papers (5,4,10,11). In the TREC experiments this year, a number of new techniques were introduced for both the ad-hoc retrieval and routing runs. In addition, experiments with Spanish retrieval were carried out.
- J. Broglio, Jamie Callan, W. Bruce Croft. 1994. Inquery System Overview 1. Description of Final System 1.1. Approach 1.2. Processing Flow Country Recognizer: for Each Mention of a Country Company Name Recognizer: for Each Citation of A. Abstract: (which includes MCC as a subcontractor), has focused on the following goals: Improving the eeectiveness of information retrieval techniques for large, full-text databases, Improving the eeectiveness of routing techniques appropriate for long-term information needs, and Demonstrating the eeectiveness of these retrieval and routing techniques for Japanese full text databases 5]. Our general approach to achieving these goals has been to use improved representations of text and information needs in the framework of a new model of retrieval. This model uses Bayesian networks to describe how text and queries should be used to identify relevant documents 7, 4, 8]. Retrieval (and routing) is viewed as a proba-bilistic inference process which compares text representations based on diierent forms of linguistic and statistical evidence to representations of information needs based on similar evidence from natural language queries and user interaction. Learning techniques are used to modify the initial queries both for short-term and long-term information needs (relevance feedback and routing, respectively). This approach (generally known as the inference net model and implemented in the INQUERY system 2]) emphasizes retrieval based on combination of evidence. Diierent text representations (such as words, phrases, paragraphs, or manually assigned keywords) and diier-ent versions of the query (such as natural language and Boolean) can be combined in a consistent probabilistic framework. This type of \data fusion" has been known to be eeective in the information retrieval context for a number of years, and was one of the primary motivations for developing the inference net approach. Another feature of the inference net approach is the ability to capture complex structure in the network representing the information need (i.e. the query). A practical consequence of this is that complex Boolean queries can be evaluated as easily as natural language queries and produce ranked output. It is also possible to represent \rule-based" or \concept-based" queries in the same probabilistic framework. This has led to us concentrating on automatic analysis of queries and techniques for enhancing queries rather than on in-depth analysis of the documents in the database. In general, it is more eeec-tive (as well as eecient) to analyze short query texts than millions of document texts. The results of the query analysis are represented in the INQUERY query language which contains a number of operators, such as #SUM, #AND, #OR, #NOT, #PHRASE, and #SYN. These operators implement diierent methods of combining evidence. Some of the speciic research issues we are …
- J. Broglio, Jamie Callan, W. Bruce Croft. 1993. An Overview of the INQUERY System as Used for the TIPSTER Project. Abstract: TR #93-85 An Overview of the INQUERY System as Used fo J. Broglio, J. P. Callan and W. Bruce Croft Computer Science Department, LGRC Box 34610 University of Massachusetts Amherst, MA 01003-4601 November 1, 1993. ASTRACT Research in the Information Retrieval laboratory was funded for two years by the TIPSTER program, a large-scale project in information retrieval and fact extraction. This report was written at the end of Phase I (the first two years). It reviews the INQUERY information retrieval system and provides a broad overview of the research results obtained during the TIPSTER project. The results include scaling up to a two gigabtye document collection, significant improvements in processing natural language queries, the WordFinder system that finds new words and phrases related to a query, document routing techniques, and a Japanese version of the system.
- J. Broglio, Jamie Callan, W. Bruce Croft. 1993. Technical Issues in Building an Information Retrieval System for Chinese. Abstract: ment of Commerce under cooperative agreement number EEC-9209623. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.
- Jamie Callan, W. Bruce Croft. 1993. An evaluation of query processing strategies using the TIPSTER collection. Abstract: The TIPSTER collection is unusual because of both its size and detail. In particular, it describes a set of information needs, as opposed to traditional queries. These detailed representations of information need are an opportunity for research on different methods of formulating queries. This paper describes several methods of constructing queries for the INQUERY information retrieval system, and then evaluates those methods on the TIPSTER document collection. Both AdHoc and Routing query processing methods are evaluated.
- N. Belkin, Colleen Cool, W. Bruce Croft, Jamie Callan. 1993. The effect multiple query representations on information retrieval system performance. Abstract: Five independently generated Boolean query formulations for ten different TREC topics were produced by ten different expert online searchers. These different formulations were grouped, and the groups, and combinations of them, were used as searches against the TREC test collection, using the INQUERY probabilistic inference network retrieval engine, Results show that progressive combination of query formulations leads to progressively improving retrieval performance, Results were compared against the performance of INQUERY natural language based queries, and in combination with them. The issue of recall as a performance measure in large databases was raised, since overlap between the searches conducted in this study, and the TREC-1 searches, was smaller than expected.
- J. Broglio, Jamie Callan, W. Bruce Croft. 1993. INQUERY System Overview. Abstract: The TIPSTER project in the Information Retrieval Laboratory of the Computer Science Department, University of Massachusetts, Amherst (which includes MCC as a subcontractor), has focused on the following goals:• Improving the effectiveness of information retrieval techniques for large, full-text databases,• Improving the effectiveness of routing techniques appropriate for long-term information needs, and• Demonstrating the effectiveness of these retrieval and routing techniques for Japanese full text databases [5].
- W. Bruce Croft, Jamie Callan, J. Broglio. 1993. TREC-2 Routing and Ad-Hoc Retrieval Evaluation using the INQUERY System. Abstract: The ARPA TIPSTER project which is the source of the data and funding for TREC, has involved four sites in the area of text retrieval and routing. The TIPSTER project in the information Retrieval of the Computer Science Department, University of Massachussetts, Amherst (which includes MCC as a subcontractor), has focused on the following goals: improving the effectiveness of information retrieval techniques for large, full-text databases; 2: improving the effectiveness of routing techiques appropriate for long-term information needs; 3- Demonstrating the effectiveness of these retrieval and routinq techniques for Japanese full text database. Our general approach to achieve these goals has been to use improved representations of text and information needs in the framework of a new model of retrieval. This model uses Bayesian netwoks to describe how text and queries should be uses to identify relevant document. Retrieval (and routing) is viewed as a probabilistic inference process whic compares text representations based on different forms of linguistic and statistical evidence to representations of information needs. Learnin techniques are uses to modify the initial query both for short-terme and long-term information needs (relevance feedback and routing, respectively)
- Jamie Callan, W. Bruce Croft. 1993. An Approach to Incorporating CBR Concepts in IR Systems. Abstract: Information retrieval (IR) is concerned with techniques that can provide effective access to large databases of objects that contain primarily text. It is generally assumed that there is a wide variety of users and a large number of potential queries. Although these characteristics are different than those of a typical case-based reasoning (CBR) application, there are opportunities for integration of the two approaches. To demonstrate that this is possible, we first describe IR techniques in more detail. After a brief overview of CBR, we then outline an approach to integration that takes advantage of the strengths of both approaches. We also give a simple example of how an integrated CBR/IR system could be used to address a real information need.
- Jamie Callan. 1993. Knowledge-based feature generation for inductive learning. Abstract: Inductive learning is an approach to machine learning in which concepts are learned from examples and counterexamples. One requirement for inductive learning is an explicit representation of the characteristics, or features, that determine whether an object is an example or counterexample. Obvious or easily available representations do not reliably satisfy this requirement, so constructive induction algorithms have been developed to satisfy it automatically. However, there are some features, known to be useful, that have been beyond the capabilities of most constructive induction algorithms. 
This dissertation develops knowledge-based feature generation, a stronger, but more restricted, method of constructive induction than was available previously. Knowledge-based feature generation is a heuristic method of using one general and easily available form of domain knowledge to create functional features for one class of learning problems. The method consists of heuristics for creating features, for pruning useless new features, and for estimating feature cost. It has been tested empirically on problems ranging from simple to complex, and with inductive learning algorithms of varying power. The results show knowledge-based feature generation to be a general method of creating useful new features for one class of learning problems.
- Jamie Callan. 1991. Constructive Induction on Domain Information Constructive Induction on Domain Information. Abstract: One obstacle to wider use of inductive learning algorithms in problem-solving systems is the sensitivity of the algorithms to the way in which examples of the concept are represented. Humans normally decide how the examples will be represented, so success in incorporating inductive learning algorithms varies from person to person. Constructive induction reduces, but does not eliminate, this sensitivity. An ideal solution would eliminate the need for any human intervention in determining how a problem-solving system and an inductive learning algorithm are integrated. This paper shows how a problem-solver can use its domain knowledge to automatically create a representation of examples that is adequate for learning search control knowledge. The resulting representation describes the examples in terms of how and how well they satisfy the problem-solver's goals. Experimental evidence from two domains is presented to support the claim that this approach is generally useful.
- Jamie Callan, P. Utgoff. 1991. Constructive Induction on Domain Information. Abstract: It is well-known that inductive learning algorithms are sensitive to the way in which examples of a concept are represented. Constructive induction reduces this sensitivity by enabling the inductive algorithm to create new terms with which to describe examples. However, new terms are usually created as functions of existing terms, so an extremely poor initial representation makes the search for new terms intractable. 
 
This work considers inductive learning within a problem-solving environment. It shows that information about the problem-solving task can be used to create terms that are suitable for learning search control knowledge. The resulting terms describe the problem-solver's progress in achieving its goals. Experimental evidence from two domains is presented in support of the approach.
- Jamie Callan, Tom Fawcett, E. Rissland. 1991. CABOT: An Adaptive Approach to Case-Based Search. Abstract: This paper describes CABOT, a case-based system that is able to adjust its retrieval and adaptation metrics, in addition to storing cases. It has been applied to the game of OTHELLO. Experiments show that CABOT saves about half as many cases as similar systems that do not adjust their retrieval and adaptation mechanisms. It also consistently beats these systems. These results suggest that existing case-based systems could save fewer cases without reducing their current levels of performance. They also demonstrate that it is beneficial to distinguish failures due to missing information, faulty retrieval, and faulty adaptation.
- Jamie Callan. 1990. Use of Domain Knowledge in Constructive Induction. Abstract: One of the important problems in integrating inductive learning algorithms with problem-solving systems is determining how they communicate. It is well-known that inductive algorithms are sensitive to the vocabulary with which examples of a concept are described. It is also known that a vocabulary can be acceptable for problem-solving but cause the inductive algorithm to learn slowly or inaccurately. It remains an open question how best to choose a language with which a problem-solving system and an inductive algorithm communicate. This thesis proposal considers the representation problem for a class of architectures in which the problem-solver is a search procedure and the inductive learning algorithm is a source of heuristic guidance. It presents a solution in which the problem-solver uses its domain knowledge to generate automatically a set of features suitable for learning search control knowledge. Each new feature describes the problem-solver's progress in achieving one of its goals. Experimental evidence from two domains supports the claim that this solution results in faster and more accurate learning. A program of additional research and experiments is described that is expected to provide further support for the solution.
- P. Utgoff, S. Saxena, Jamie Callan, Tom Fawcett. 1989. Representation Problems in Machine Learnin: A Proposal. Abstract: THE PROJECT INVESTIGATES METHODS FOR AUTOMATICALLY CONSTRUCTING OR SHIFTING REPRESENTATIONS FOR MACHINE LEARNING. THE REPRESENTATION IN WHICH GENERALIZATIONS MUST BE EXPRESSED IS A FUNDAMENTAL BIAS THAT STRONGLY INFLUENCES THE INFERENCES THAT A LEARNING PROGRAM MAKES. FOR EFFECTIVE LEARNING, A PROGRAM NEEDS AN APPROPRIATE REPRESENTATION. CURRENTLY, LEARN- ING PROGRAMS ARE NOT ABLE TO FIND AN APPROPRIATE REPRESENTATION. INSTEAD, A PERSON MUST HANDCRAFT A REPRESENTATION AND THEN ASSESS THE EFFECTIVENESS OF THE LEARNING FOR THAT. AN INTELLIGENT LEARNING PROGRAM SHOULD BE ABLE TO PERFORM THIS ACTIVITY AUTONOMOUSLY. ISSUES ADDRESSED INCLUDE USE OF MULTIPLE REPRESENTATIONS, CONCEPT FORMA- TION, FEATURE GENERATION, FEATURE DISCOVER, REPRESENTATION EVALUATION, AND EVALUATION FUNCTION LEARNING. EXPECTED RESULTS INCLUDE IMPROVED ABILITY TO CONSTRUCT HYBRID REPRESENTATIONS, TO CONSTRUCT NONLINEAR EVALUATION FUNC- TIONS FROM QUALITATIVE CRITICISM, TO EVALUATE AND SELECT REPRESENTATIONS WITH DOMAIN-INDEPENDENT METRICS, TO GENERATE AND SELECT PLAUSIBLE FEATURES, AND TO DISCOVER FEATURES VIA INTERESTINGNESS HEURISTICS DURING PROBLEM SOLVING.
- Edward J. DeJesus, Jamie Callan, Curtis R. Whitehead. 1986. PEARL: An Expert System for Power Supply Layout. Abstract: The use of artificial intelligence (AI) expert systems technology has demonstrated its advantages with many new tools in the computer aided design (CAD) field. This paper describes how domain specific knowledge was integrated with a conventional CAD architecture to develop an expert system. The combination resulted in a tool that provides intelligent assistance to printed wiring board (PWB) layout designers. This CAD tool focuses entirely on the layout requirements of power supply circuits for PWBs. The system is called PEARL for Power-supply Expert Assisted Rule-based Layout. PEARL acts as an advisor to the layout designer, providing expert assistance with the placement of components on PWBs. It is presently being enhanced to provide etch routing assistance as well.
- Jinxi Xu, Jamie Callan. None. *-uH Effective Retrieval with Distributed Collections. Abstract: This paper evaluates the retrieval effectiveness of distributed information retrieval systems in realistic environments. We find that when a large number of collections are available, the retrieval effectiveness is significantly worse than that of centralized systems, mainly because typical queries are not adequate for the purpose of choosing the right collections. We propose two techniques to address the problem. One is to use phrase information in the collection selection index and the other is query expansion. Both techniques enhance the discriminatory power of typical queries for choosing the right collections and hence significantly improve retrieval results. Query expansion, in particular, brings the effectiveness of searching a large set of distributed collections close to that of searching a centralized collection.
