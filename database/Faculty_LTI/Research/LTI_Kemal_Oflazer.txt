Kemal Oflazer
Paper count: 163
- Figen Beken Fikri, Kemal Oflazer, B. Yanikoglu. 2021. Semantic Similarity Based Evaluation for Abstractive News Summarization. Abstract: ROUGE is a widely used evaluation metric in text summarization. However, it is not suitable for the evaluation of abstractive summarization systems as it relies on lexical overlap between the gold standard and the generated summaries. This limitation becomes more apparent for agglutinative languages with very large vocabularies and high type/token ratios. In this paper, we present semantic similarity models for Turkish and apply them as evaluation metrics for an abstractive summarization task. To achieve this, we translated the English STSb dataset into Turkish and presented the first semantic textual similarity dataset for Turkish as well. We showed that our best similarity models have better alignment with average human judgments compared to ROUGE in both Pearson and Spearman correlations.
- Figen Beken Fikri, Kemal Oflazer, B. Yanikoglu. 2021. Turkish Dataset for Semantic Textual Similarity. Abstract: Semantic textual similarity is the task of determining how similar two texts are. In this study, we present the first Turkish evaluation benchmark dataset for semantic textual similarity. We created the dataset by translating the English STS benchmark (STSb) dataset via Google Cloud Translation API and provided various benchmark results. We used Language- Agnostic SEntence Representations (LASER), Language-agnostic BERT Sentence Embedding (LaBSE), Multilingual Universal Sentence Encoder (MUSE) and pre-trained BERT/RoBERTa models to compute sentence embeddings. We also fine-tuned pretrained BERT/RoBERTa models to compute similarity scores. We further fine tuned pre-trained BERT/RoBERTa models with SBERT architecture. In our experimental designs, we used the Turkish natural language inference (NLI-TR) dataset as well. The model performances were computed by Pearson and Spearman's correlation coefficient between the predicted similarity scores and the gold labels. The best results were obtained by fine-tuning the BERTurk model first on NLI-TR dataset, then on the STSb-TR dataset.
- A. Pandian, Lamana Mulaffer, Kemal Oflazer, Amna AlZeyara. 2020. Precision Event Coreference Resolution Using Neural Network Classifiers. Abstract: This paper presents a neural network classifier approach to detecting precise within-document (WD) and cross-document (CD) event coreference clusters effectively using only event mention based features. Our approach does not rely on any event argument features such as semantic roles or spatio-temporal arguments and uses no sophisticated clustering approach. Experimental results on the ECB+dataset show that our simple approach out performsstate-of-the-art methods for both within-document and cross-document event coreference resolution while producing clusters of high precision, which is useful for several downstream tasks.
- Houda Bouamor, Nizar Habash, Mohammad Salameh, W. Zaghouani, Owen Rambow, Dana Abdulrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani, Alexander Erdmann, Kemal Oflazer. 2018. The MADAR Arabic Dialect Corpus and Lexicon. Abstract: In this paper, we present two resources that were created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project. The ﬁrst is a large parallel corpus of 25 Arabic city dialects in the travel domain. The second is a lexicon of 1,045 concepts with an average of 45 words from 25 cities per concept. These resources are the ﬁrst of their kind in terms of the breadth of their coverage and the ﬁne location granularity. The focus on cities, as opposed to regions in studying Arabic dialects, opens new avenues to many areas of research from dialectology to dialect identiﬁcation and machine translation.
- A. Pandian, Lamana Mulaffer, Kemal Oflazer, Amna AlZeyara. 2018. Event Coreference Resolution Using Neural Network Classifiers. Abstract: This paper presents a neural network classifier approach to detecting both within- and cross- document event coreference effectively using only event mention based features. Our approach does not (yet) rely on any event argument features such as semantic roles or spatiotemporal arguments. Experimental results on the ECB+ dataset show that our approach produces F1 scores that significantly outperform the state-of-the-art methods for both within-document and cross-document event coreference resolution when we use B3 and CEAFe evaluation measures, but gets worse F1 score with the MUC measure. However, when we use the CoNLL measure, which is the average of these three scores, our approach has slightly better F1 for within- document event coreference resolution but is significantly better for cross-document event coreference resolution.
- J. Araki, Lamana Mulaffer, A. Pandian, Yukari Yamakawa, Kemal Oflazer, T. Mitamura. 2018. Interoperable Annotation of Events and Event Relations across Domains. Abstract: This paper presents methodologies for interoperable annotation of events and event relations across different domains, based on notions proposed in prior work. In addition to the interoperability, our annotation scheme supports a wide coverage of events and event relations. We employ the methodologies to annotate events and event relations on Simple Wikipedia articles in 10 different domains. Our analysis demonstrates that the methodologies can allow us to annotate events and event relations in a principled manner against the wide variety of domains. Despite our relatively wide and flexible annotation of events, we achieve high inter-annotator agreement on event annotation. As for event relations, we obtain reasonable inter-annotator agreement. We also provide an analysis of issues on annotation of events and event relations that could lead to annotators’ disagreement.
- Ossama Obeid, Salam Khalifa, Nizar Habash, Houda Bouamor, W. Zaghouani, Kemal Oflazer. 2018. MADARi: A Web Interface for Joint Arabic Morphological Annotation and Spelling Correction. Abstract: In this paper, we introduce MADARi, a joint morphological annotation and spelling correction system for texts in Standard and Dialectal Arabic. The MADARi framework provides intuitive interfaces for annotating text and managing the annotation process of a large number of sizable documents. Morphological annotation includes indicating, for a word, in context, its baseword, clitics, part-of-speech, lemma, gloss, and dialect identification. MADARi has a suite of utilities to help with annotator productivity. For example, annotators are provided with pre-computed analyses to assist them in their task and reduce the amount of work needed to complete it. MADARi also allows annotators to query a morphological analyzer for a list of possible analyses in multiple dialects or look up previously submitted analyses. The MADARi management interface enables a lead annotator to easily manage and organize the whole annotation process remotely and concurrently. We describe the motivation, design and implementation of this interface; and we present details from a user study working with this system.
- Houda Bouamor, W. Zaghouani, Mona T. Diab, Ossama Obeid, Kemal Oflazer, Mahmoud A. Ghoneim, A. Hawwari. 2016. On Arabic Multi-Genre Corpus Diacritization. Abstract: One of the characteristics of writing in Modern Standard Arabic (MSA) is that the commonly used orthography is mostly consonantal and does not provide full vocalization of the text. It sometimes includes optional diacritical marks (henceforth, diacritics or vowels). Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants. Diacritics on the other hand comprise short vowels, gemination markers, nunation markers, as well as other markers (such as hamza, the glottal stop which appears in conjunction with a small number of letters, dots on letters, elongation and emphatic markers) which in all, if present, render a more or less exact precise reading of a word. In this study, we are mostly addressing three types of diacritical marks: short vowels, nunation, and shadda (gemination). Diacritics are extremely useful for text readability and understanding. Their absence in Arabic text adds another layer of lexical and morphological ambiguity. Naturally occurring Arabic text has some percentage of these diacritics present depending on genre and domain. For instance, religious text such as the Quran is fully diacritized to minimize chances of reciting it incorrectly. So are children's educational texts. Classical poetry tends to be diacritized as well. However, news text and other genre are sparsely diacritized (e.g., around 1.5% of tokens in the United Nations Arabic corpus bear at least one diacritic (Diab et al., 2007)). In general, building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available diacritized MSA corpora are generally limited to the newswire genres (those distributed by the LDC) or religion related texts such as Quran or the Tashkeela corpus. In this paper we present a pilot study where we annotate a sample of non-diacritized text extracted from five different text genres. We explore different annotation strategies where we present the data to the annotator in three modes: basic (only forms with no diacritics), intermediate (basic forms–POS tags), and advanced (a list of forms that is automatically diacritized). We show the impact of the annotation strategy on the annotation quality. It has been noted in the literature that complete diacritization is not necessary for readability Hermena et al. (2015) as well as for NLP applications, in fact, (Diab et al., 2007) show that full diacritization has a detrimental effect on SMT. Hence, we are interested in discovering the optimal level of diacritization. Accordingly, we explore different levels of diacritization. In this work, we limit our study to two diacritization schemes: FULL and MIN. For FULL, all diacritics are explicitly specified for every word. For MIN, we explore what is a minimum and optimal number of diacritics that needs to be added in order to disambiguate a given word in context and make a sentence easily readable and unambiguous for any NLP application. We conducted several experiments on a set of sentences that we extracted from five corpora covering different genres. We selected three corpora from the currently available Arabic Treebanks from the Linguistic Data Consortium (LDC). These corpora were chosen because they are fully diacritized and had undergone significant quality control, which will allow us to evaluate the anno tation accuracy as well as our annotators understanding of the task. We select a total of 16,770 words from these corpora for annotation. Three native Arabic annotators with good linguistic background annotated the corpora samples. Diab et al. (2007), define six different diacritization schemes that are inspired by the observation of the relevant naturally occurring diacritics in different texts. We adopt the FULL diacritization scheme, in which all the diacritics should be specified in a word. Annotators were asked to fully diacritize each word. The text genres were annotated following the different strategies: - Basic: In this mode, we ask for annotation of words where all diacritics are absent, including the naturally occurring ones. The words are presented in a raw tokenized format to the annotators in context. - Intermediate: In this mode, we provide the annotator with words along with their POS information. The intuition behind adding POS is to help the annotator disambiguate a word by narrowing down on the diacritization possibilities. - Advanced: In this mode, the annotation task is formulated as a selection task instead of an editng task. Annotators are provided with a list of automatically diacritized candidates and are asked to choose the correct one, if it appears in the list. Otherwise, if they are not satisfied with the given candidates, they can manually edit the word and add the correct diacritics. This technique is designed in order to reduce annotation time and especially reduce annotator workload. For each word, we generate a list of vowelized candidates using MADAMIRA (Pasha et al., 2014). MADAMIRA is able to achieve a lemmatization accuracy 99.2% and a diacritization accuracy of 86.3%. We present the annotator with the top three candidates suggested by MADAMIRA, when possible. Otherwise, only the available candidates are provided. We also provided annotators with detailed guidelines, describing our diacritization scheme and specifying how to add diacritics for each annotation strategy. We described the annotation procedure and specified how to deal with borderline cases. We also provided in the guidelines many annotated examples to illustrate the various rules and exceptions. In order to determine the most optimized annotation setup for the annotators, in terms of speed and efficiency, we test the results obtained following the three annotation strategies. These annotations are all conducted for the FULL scheme. We first calculated the number of words annotated per hour, for each annotator and in each mode. As expected, following the Advanced mode, our three annotators could annotate an average of 618.93 words per hour which is double those annotated in the Basic mode (only 302.14 words). Adding POS tags to the Basic forms, as in the Intermediate mode, does not accelerate the process much. Only − 90 more words are diacritized per hour compared to the basic mode. Then, we evaluated the Inter-Annotator Agreement (IAA) to quantify the extent to which independent annotators agree on the diacritics chosen for each word. For every text genre, two annotators were asked to annotate independently a sample of 100 words. We measured the IAA between two annotators by averaging WER (Word Error Rate) over all pairs of words. The higher the WER between two annotations, the lower their agreement. The results obtained show clearly that the Advanced mode is the best strategy to adopt for this diacritization task. It is the less confusing method on all text genres (with WER between 1.56 and 5.58). We also conducted a preliminary study for a minimum diacritization scheme. This is a diacritization scheme that encodes the most relevant differentiating diacritics to reduce confusability among words that look the same (homographs) when undiacritized but have different readings. Our hypothesis in MIN is that there is an optimal level of diacritization to render a text unambiguous for processing and enhance its readability. We showed the difficulty in defining such a scheme and how subjective this task can be. Acknowledgement This publication was made possible by grant NPRP-6-1020-1-199 from the Qatar National Research Fund (a member of the Qatar Foundation).
- W. Zaghouani, Houda Bouamor, A. Hawwari, Mona T. Diab, Ossama Obeid, Mahmoud A. Ghoneim, Sawsan Alqahtani, Kemal Oflazer. 2016. Guidelines and Framework for a Large Scale Arabic Diacritized Corpus. Abstract: This paper presents the annotation guidelines developed as part of an effort to create a large scale manually diacritized corpus for various Arabic text genres. The target size of the annotated corpus is 2 million words. We summarize the guidelines and describe issues encountered during the training of the annotators. We also discuss the challenges posed by the complexity of the Arabic language and how they are addressed. Finally, we present the diacritization annotation procedure and detail the quality of the resulting annotations.
- W. Zaghouani, Nizar Habash, Ossama Obeid, Behrang Mohit, Houda Bouamor, Kemal Oflazer. 2016. Building an Arabic Machine Translation Post-Edited Corpus: Guidelines and Annotation. Abstract: We present our guidelines and annotation procedure to create a human corrected machine translated post-edited corpus for the Modern Standard Arabic. Our overarching goal is to use the annotated corpus to develop automatic machine translation post-editing systems for Arabic that can be used to help accelerate the human revision process of translated texts. The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we created comprehensive and simplified annotation guidelines which were used by a team of five annotators and one lead annotator. In order to ensure a high annotation agreement between the annotators, multiple training sessions were held and regular inter-annotator agreement measures were performed to check the annotation quality. The created corpus of manual post-edited translations of English to Arabic articles is the largest to date for this language pair.
- Rahim Dehkharghani, B. Yanikoglu, Y. Saygin, Kemal Oflazer. 2016. Sentiment analysis in Turkish at different granularity levels. Abstract: Abstract Sentiment analysis has attracted a lot of research interest in recent years, especially in the context of social media. While most of this research has focused on English, there is ample data and interest in the topic for many other languages, as well. In this article, we propose a comprehensive sentiment analysis system for Turkish. We cover different levels of sentiment analysis such as aspect, sentence, and document levels as well as some linguistic issues such as conjunction and intensification in Turkish sentiment analysis. Our system is evaluated on Turkish movie reviews and the obtained accuracies range from sixty per cent to seventy-nine per cent in ternary and binary classification tasks at different levels of analysis.
- W. Zaghouani, A. Hawwari, Sawsan Alqahtani, Houda Bouamor, Mahmoud A. Ghoneim, Mona T. Diab, Kemal Oflazer. 2016. Using Ambiguity Detection to Streamline Linguistic Annotation. Abstract: Arabic writing is typically underspecified for short vowels and other markups, referred to as diacritics. In addition to the lexical ambiguity exhibited in most languages, the lack of diacritics in written Arabic adds another layer of ambiguity which is an artifact of the orthography. In this paper, we present the details of three annotation experimental conditions designed to study the impact of automatic ambiguity detection, on annotation speed and quality in a large scale annotation project.
- W. Zaghouani, Nizar Habash, Ossama Obeid, Behrang Mohit, Houda Bouamor, Kemal Oflazer. 2016. Annotation Guidelines and Framework for Arabic Machine Translation Post-Edited Corpus. Abstract: 1. Introduction Machine translation (MT) became widely used by translation companies to reduce their costs and improve their speed. Therefore, the demand for quick and accurate machine translations is growing. Machine translation (MT) systems often produce incorrect output with many grammatical and lexical choice errors. Correcting machine-produced translation errors, or MT Post-Editing (PE) can be done automatically or manually. The availability of annotated resources is required for such approaches. When it comes to the Arabic language, to the best of our knowledge, there is no MT manually post-edited corpora available to build such systems. Therefore, there is a clear need to build such valuable resources for the Arabic language. In this abstract, we present our guidelines and annotation procedure to create a human corrected MT corpus for the Modern Standard Arabic (MSA). The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we created a comprehensive and simplified annotation guidelines which were used by a team of five annotators and one lead annotator. In order to ensure a high annotation agreement between the annotators, multiple training sessions were held and regular inter annotator agreement (IAA) measures were performed to check the annotation quality 2. Corpus We collected a corpus of 100K of English news article taken from the collaborative journalism Wikinews website. Afterwards, the corpus collected was automatically translated from English to Arabic using the Google Translate API paid service. 3. Guidelines In order to annotate the MT corpus, we use the general annotation correction guidelines we designed previously for L1 described in Zaghouani et al. (2014) and we add specific MT post-editing correction rules. In the general correction guidelines we place the errors to be corrected into seven categories: spelling, word choice, morphology, syntax, proper names, dialectal usage and punctuation. We refer to Zaghouani et al. (2014) for more details about these errors. In the MT post-editing guidelines, we provide the annotators with detailed annotation procedure and explain how to deal with borderline cases. We include many annotated examples to illustrate some specific cases of machine translation correction rules. Since there are equally-accurate alternative ways to edit the machine translation output, all being considered correct, some using fewer edits than others, we explained in the guidelines that the machine translated texts should be corrected with a minimum number of edits necessary to achieve an acceptable translation quality. However, correcting the accuracy errors and producing a semantically coherent text is more important than minimizing the number of edits and therefore, the annotators were asked to pay attention to the following three aspects: accuracy, fluency and style. 4. Annotation Pipeline The annotation team consisted of a lead annotator and six annotators. The lead annotator is also the annotation workflow manager of this project. He frequently evaluate the quality of the annotation, monitor and report on the annotation progress. A clearly defined protocol is set, including a routine for the Post-editing annotation job assignment and the inter-annotator agreement evaluation. The lead annotators is also responsible of the corpus selection and normalization process beside the annotation of the gold standard to be used to compute the Inter-Annotator Agreement (IAA) portion of the corpus. The annotation itself is done using an in house built web annotation framework built originally for the manual correction of errors in L1 and L2 texts (Obeid et al., 2013). This framework includes two major components: 1. The annotation management interface which is used to assist the lead annotator in the general work-flow process, it allows the user to upload, assign, monitor, evaluate and export annotation tasks. 2. The MT post-editing annotation interface is the actual annotation tool, which allows the annotators to do the manual correction of the MT Arabic output. 5. Evaluation The low average WER of 4.92 obtained show a high agreement with the post-editing done in the first round between three annotators. The results obtained with the MT are comparable to those obtained with the L2 corpus, this can be explained by the difficult nature of both corpora and the multiple acceptable corrections for both. 6. Related Work Large scale manually corrected MT corpora are not yet widely available due to the high cost related to building such resources. For the Arabic language, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sentences and 22k rankings. Our corpus is a part of the Qatar Arabic Language Bank (QALB) project, a large scale manually annotated annotation project (Zaghouani et al., 2014; Zaghouani et al., 2015). The project goal was to create an error corrected 2M-word corpus for online user comments on news websites, native speaker essays, non-native speaker essays and machine translation output. 7. Conclusion We have presented in detail the methodology used to create a 100K word English to Arabic MT manually post-edited corpus, including the development of the guidelines as well as the annotation procedure and the quality control procedure using frequent inter-annotator measures. The created guidelines will be made publicly available and we look forward to distribute the post-edited corpus in a planned shared task on automatic error correction and getting feedback from the community on its usefulness as it was in the previous shared tasks we organized for the L1 and L2 corpus (Mohit et al., 2014; Rozovskaya et al., 2015).We believe that this corpus will be valuable to advance research efforts in the machine translation area since manually annotated data is often needed by the MT community. We believe that our methodology for guideline development and annotation consistency checking can be applied in other projects and other languages as well. 8. Acknowledgement This project is supported by the National Priority Research Program (NPRP grant 4-1058-1-168) of the Qatar National Research Fund (a member of the Qatar Foundation). The statements made herein are solely the responsibility of the authors. 9. References Obeid, O., Zaghouani, W., Mohit, B., Habash, N., Oflazer,K., and Tomeh, N. (2013). A Web-based Annotation Framework For Large-Scale Text Correction. In The Companion Volume of the Proceedings of IJCNLP 2013: System Demonstrations, Nagoya, Japan, October. Mohit, B., Rozovskaya, A., Habash, N., Zaghouani, W., and Obeid, O. (2014). The first QALB shared task on automatic text correction for Arabic. ANLP 2014, page 39. Rozovskaya Alla; Houda Bouamor; Nizar Habash; Wajdi Zaghouani; Ossama Obeid; Behrang Mohit. The Second QALB Shared Task on Automatic Text Correction for Arabic. In Proceedings of the ACL 2015 Workshop on Arabic Natural Language Processing (ANLP), Beijing, China, July 2015. Zaghouani, W., Mohit, B., Habash, N., Obeid, O., Tomeh,N., Rozovskaya, A., Farra, N., Alkuhlani, S., and Oflazer, K. (2014). Large scale Arabic error annotation: Guidelines and framework. In International Conference on Language Resources and Evaluation (LREC 2014). Zaghouani, W., Habash, N., Bouamor, H., Rozovskaya, A., Mohit, B., Heider, A., and Oflazer, K. (2015). Correction annotation for non-native Arabic texts: Guidelines and corpus. Proceedings of The 9th Linguistic Annotation Workshop, pages 129-139.
- W. Zaghouani, Nizar Habash, Houda Bouamor, Alla Rozovskaya, Behrang Mohit, Abeer Heider, Kemal Oflazer. 2015. Correction Annotation for Non-Native Arabic Texts: Guidelines and Corpus. Abstract: We present our correction annotation guidelines to create a manually corrected nonnative (L2) Arabic corpus. We develop our approach by extending an L1 large-scale Arabic corpus and its manual corrections, to include manually corrected non-native Arabic learner essays. Our overarching goal is to use the annotated corpus to develop components for automatic detection and correction of language errors that can be used to help Standard Arabic learners (native and non-native) improve the quality of the Arabic text they produce. The created corpus of L2 text manual corrections is the largest to date. We evaluate our guidelines using inter-annotator agreement and show a high degree of consistency.
- Houda Bouamor, W. Zaghouani, Mona T. Diab, Ossama Obeid, Kemal Oflazer, Mahmoud A. Ghoneim, A. Hawwari. 2015. A Pilot Study on Arabic Multi-Genre Corpus Diacritization. Abstract: Arabic script writing is typically underspecified for short vowels and other mark up, referred to as diacritics. Apart from the lexical ambiguity found in words, similar to that exhibited in other languages, the lack of diacritics in written Arabic script adds another layer of ambiguity which is an artifact of the orthography. Diacritization of written text has a significant impact on Arabic NLP applications. In this paper, we present a pilot study on building a diacritized multi-genre corpus in Arabic. We annotate a sample of nondiacritized words extracted from five text genres. We explore different annotation strategies: Basic where we present only the bare undiacritized forms to the annotators, Intermediate (Basic forms+their POS tags), and Advanced (automatically diacritized words). We present the impact of the annotation strategy on annotation quality. Moreover, we study different diacritization schemes in the process.
- Houda Bouamor, Hassan Sajjad, Nadir Durrani, Kemal Oflazer. 2015. QCMUQ@QALB-2015 Shared Task: Combining Character level MT and Error-tolerant Finite-State Recognition for Arabic Spelling Correction. Abstract: We describe the CMU-Q and QCRI’s joint efforts in building a spelling correction system for Arabic in the QALB 2015 Shared Task. Our system is based on a hybrid pipeline that combines rule-based linguistic techniques with statistical meth-ods using language modeling and machine translation, as well as an error-tolerant ﬁnite-state automata method. We trained and tested our spelling corrector using the dataset provided by the shared task orga-nizers. Our system outperforms the base-line system and yeilds better correction quality with an F-score of 68.12 on L1-test-2015 testset and 38.90 on the L2-test-2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask.
- Serena Jeblee, Wes Feely, Houda Bouamor, A. Lavie, Nizar Habash, Kemal Oflazer. 2014. Domain and Dialect Adaptation for Machine Translation into Egyptian Arabic. Abstract: In this paper, we present a statistical machine translation system for English to Dialectal Arabic (DA), using Modern Standard Arabic (MSA) as a pivot. We create a core system to translate from English to MSA using a large bilingual parallel corpus. Then, we design two separate pathways for translation from MSA into DA: a two-step domain and dialect adaptation system and a one-step simultaneous domain and dialect adaptation system. Both variants of the adaptation systems are trained on a 100k sentence tri-parallel corpus of English, MSA, and Egyptian Arabic generated by a rule-based transformation. We test our systems on a held-out Egyptian Arabic test set from the 100k sentence corpus and we achieve our best performance using the two-step domain and dialect adaptation system with a BLEU score of 42.9.
- Ahmed S. Salama, Houda Bouamor, Behrang Mohit, Kemal Oflazer. 2014. YouDACC: the Youtube Dialectal Arabic Comment Corpus. Abstract: This paper presents YOUDACC, an automatically annotated large-scale multi-dialectal Arabic corpus collected from user comments on Youtube videos. Our corpus covers different groups of dialects: Egyptian (EG), Gulf (GU), Iraqi (IQ), Maghrebi (MG) and Levantine (LV). We perform an empirical analysis on the crawled corpus and demonstrate that our location-based proposed method is effective for the task of dialect labeling.
- Houda Bouamor, Hanan Alshikhabobakr, Behrang Mohit, Kemal Oflazer. 2014. A Human Judgement Corpus and a Metric for Arabic MT Evaluation. Abstract: We present a human judgments datasetand an adapted metric for evaluation ofArabic machine translation. Our mediumscaledataset is the first of its kind for Arabicwith high annotation quality. We usethe dataset to adapt the BLEU score forArabic. Our score (AL-BLEU) providespartial credits for stem and morphologicalmatchings of hypothesis and referencewords. We evaluate BLEU, METEOR andAL-BLEU on our human judgments corpusand show that AL-BLEU has the highestcorrelation with human judgments. Weare releasing the dataset and software tothe research community.
- Mehdi Zaghouani, Nizar Habash, Behrang Mohit, Abeer Heider, Alla Rozovskaya, Kemal Oflazer. 2014. Annotation Guidelines For Non-native Arabic Text In The Qatar Arabic Language Bank. Abstract: Annotation Guidelines for Non-native Arabic Text in the Qatar Arabic Language Bank The Qatar Arabic Language Bank (QALB) is a corpus of naturally written unedited Arabic and its manual edited corrections. QALB has about 1.5 million words of text written and post-edited by native speakers. The corpus was the focus of a shared task on automatic spelling correction in the Arabic Natural Language Processing Workshop that was held in conjunction with 2014 Conference on Empirical Methods for Natural Language Processing (EMNLP) in Doha, with nine research teams from around the world competing. In this poster we discuss some of the challenges of extending QALB to include non-native Arabic text. Our overarching goal is to use QALB data to develop components for automatic detection and correction of language errors that can be used to help Standard Arabic learners (native and non-native) improve the quality of the Arabic text they produce. The QALB annotation guidelines have focused on native speaker text. Learners of Arabic as a second language (L2 speakers) typically have to adapt to a different script and a different vocabulary with new grammatical rules. These factors contribute to the propagation of errors made by L2 speakers that are of different nature than those produced by native speakers (L1 speakers), who are mostly affected by their dialects and levels of education and use of standard Arabic. Our extended L2 guidelines build on our L1 guidelines with a focus on the types of errors usually found in the L2 writing style and how to deal with problematic ambiguous cases. Annotated examples are provided in the guidelines to illustrate the various annotation rules and their exceptions. As with the L1 guidelines, the L2 texts should be corrected with a minimum number of edits that produce semantically coherent (accurate) and grammatically correct (fluent) Arabic. The guidelines also devise a priority order for corrections that prefer less intrusive edits starting with inflection, then cliticization, derivation, preposition correction, word choice correction, and finally word insertion. This project is supported by the National Priority Research Program (NPRP grant 4-1058-1-168) of the Qatar National Research Fund (a member of the Qatar Foundation). The statements made herein are solely the responsibility of the authors.
- Houda Bouamor, Nizar Habash, Kemal Oflazer. 2014. A Multidialectal Parallel Corpus of Arabic. Abstract: The daily spoken variety of Arabic is often termed the colloquial or dialect form of Arabic. There are many Arabic dialects across the Arab World and within other Arabic speaking communities. These dialects vary widely from region to region and to a lesser extent from city to city in each region. The dialects are not standardized, they are not taught, and they do not have official status. However they are the primary vehicles of communication (face-to-face and recently, online) and have a large presence in the arts as well. In this paper, we present the first multidialectal Arabic parallel corpus, a collection of 2,000 sentences in Standard Arabic, Egyptian, Tunisian, Jordanian, Palestinian and Syrian Arabic, in addition to English. Such parallel data does not exist naturally, which makes this corpus a very valuable resource that has many potential applications such as Arabic dialect identification and machine translation.
- W. Zaghouani, Behrang Mohit, Nizar Habash, Ossama Obeid, Nadi Tomeh, Alla Rozovskaya, N. Farra, Sarah Alkuhlani, Kemal Oflazer. 2014. Large Scale Arabic Error Annotation: Guidelines and Framework. Abstract: We present annotation guidelines and a web-based annotation framework developed as part of an effort to create a manually annotated Arabic corpus of errors and corrections for various text types. Such a corpus will be invaluable for developing Arabic error correction tools, both for training models and as a gold standard for evaluating error correction algorithms. We summarize the guidelines we created. We also describe issues encountered during the training of the annotators, as well as problems that are specific to the Arabic language that arose during the annotation process. Finally, we present the annotation tool that was developed as part of this project, the annotation pipeline, and the quality of the resulting annotations.
- Serena Jeblee, Houda Bouamor, W. Zaghouani, Kemal Oflazer. 2014. CMUQ@QALB-2014: An SMT-based System for Automatic Arabic Error Correction. Abstract: In this paper, we describe the CMUQ system we submitted to The ANLP-QALB 2014 Shared Task on Automatic Text Correction for Arabic. Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline and reaches an F-score of 65.42% on the test set of QALB corpus. This ranks us 3rd in the competition.
- Mahmoud Azab, Houda Bouamor, Behrang Mohit, Kemal Oflazer. 2013. Dudley North visits North London: Learning When to Transliterate to Arabic. Abstract: We report the results of our work on automating the transliteration decision of named entities for English to Arabic machine translation. We construct a classification-based framework to automate this decision, evaluate our classifier both in the limited news and the diverse Wikipedia domains, and achieve promising accuracy. Moreover, we demonstrate a reduction of translation error and an improvement in the performance of an English-to-Arabic machine translation system.
- Elif Eyigoz, D. Gildea, Kemal Oflazer. 2013. Simultaneous Word-Morpheme Alignment for Statistical Machine Translation. Abstract: Current word alignment models for statistical machine translation do not address morphology beyond merely splitting words. We present a two-level alignment model that distinguishes between words and morphemes, in which we embed an IBM Model 1 inside an HMM based word alignment model. The model jointly induces word and morpheme alignments using an EM algorithm. We evaluated our model on Turkish-English parallel data. We obtained significant improvement of BLEU scores over IBM Model 4. Our results indicate that utilizing information from morphology improves the quality of word alignments.
- Houda Bouamor, Behrang Mohit, Kemal Oflazer. 2013. SuMT: A Framework of Summarization and MT. Abstract: We present a novel system combination of machine translation and text summarization which provides high quality summary translations superior to the baseline translation of the entire document. We first use supervised learning and build a classifier that predicts if the translation of a sentence has high or low translation quality. This is a reference-free estimation of MT quality which helps us to distinguish the subset of sentences which have better translation quality. We pair this classifier with a stateof-the-art summarization system to build an MT-aware summarization system. To evaluate summarization quality, we build a test set by summarizing a bilingual corpus. We evaluate the performance of our system with respect to both MT and summarization quality and, demonstrate that we can balance between improving MT quality and maintaining a decent summarization quality.
- Elif Eyigoz, D. Gildea, Kemal Oflazer. 2013. Multi-Rate HMMs for Word Alignment. Abstract: We apply multi-rate HMMs, a tree structured HMM model, to the word-alignment problem. Multi-rate HMMs allow us to model reordering at both the morpheme level and the word level in a hierarchical fashion. This approach leads to better machine translation results than a morphemeaware model that does not explicitly model morpheme reordering.
- Nathan Schneider, Behrang Mohit, Chris Dyer, Kemal Oflazer, Noah A. Smith. 2013. Supersense Tagging for Arabic: the MT-in-the-Middle Attack. Abstract: We consider the task of tagging Arabic nouns with WordNet supersenses. Three approaches are evaluated. The first uses an expertcrafted but limited-coverage lexicon, Arabic WordNet, and heuristics. The second uses unsupervised sequence modeling. The third and most successful approach uses machine translation to translate the Arabic into English, which is automatically tagged with English supersenses, the results of which are then projected back into Arabic. Analysis shows gains and remaining obstacles in four Wikipedia topical domains.
- Mahmoud Azab, Ahmed S. Salama, Kemal Oflazer, Hideki Shima, J. Araki, T. Mitamura. 2013. An NLP-based Reading Tool for Aiding Non-native English Readers. Abstract: This paper describes a text-reading tool that makes extensive use of widelyavailable NLP tools and resources to aid non-native English speakers overcome language related hindrances while reading a text. It is a web-based tool, that can be accessed from browsers running on PCs or tablets, and provides the reader with an intelligent e-book functionality.
- Ahmed S. Salama, Kemal Oflazer, Susannah Hagan. 2013. Typesetting for Improved Readability using Lexical and Syntactic Information. Abstract: We present results from our study of which uses syntactically and semantically motivated information to group segments of sentences into unbreakable units for the purpose of typesetting those sentences in a region of a fixed width, using an otherwise standard dynamic programming line breaking algorithm, to minimize raggedness. In addition to a rule-based baseline segmenter, we use a very modest size text, manually annotated with positions of breaks, to train a maximum entropy classifier, relying on an extensive set of lexical and syntactic features, which can then predict whether or not to break after a certain word position in a sentence. We also use a simple genetic algorithm to search for a subset of the features optimizing F1, to arrive at a set of features that delivers 89.2% Precision, 90.2% Recall (89.7% F1) on a test set, improving the rule-based baseline by about 11 points and the classifier trained on all features by about 1 point in F1.
- Ossama Obeid, W. Zaghouani, Behrang Mohit, Nizar Habash, Kemal Oflazer, Nadi Tomeh. 2013. A Web-based Annotation Framework For Large-Scale Text Correction. Abstract: We demonstrate a web-based, languageindependent annotation framework used for manual correction of a large Arabic corpus. Our framework provides intuitive interfaces for annotating text and managing the annotation process. We describe the details of both the annotation and the administration interfaces as well as the back-end engine. We also show how this framework is able to speed up the annotation process by employing automated annotators to fix basic Arabic spelling errors.
- Mahmoud Azab, Ahmed S. Salama, Kemal Oflazer, Hideki Shima, J. Araki, T. Mitamura. 2013. An English Reading Tool as a NLP Showcase. Abstract: We introduce -SmartReaderan English reading tool for non-native English readers to overcome language related hindrances while reading a text. It makes extensive use of widely-available NLP tools and resources. SmartReader is a web-based application that can be accessed from standard browsers running on PCs or tablets. A user can choose a text document from the system’s library they want to read or can upload a new document of their own and the system will display an interactive version of such text, that provides the reader with an intelligent e-book functionality.
- Nathan Schneider, Behrang Mohit, Kemal Oflazer, Noah A. Smith. 2012. Coarse Lexical Semantic Annotation with Supersenses: An Arabic Case Study. Abstract: "Lightweight" semantic annotation of text calls for a simple representation, ideally without requiring a semantic lexicon to achieve good coverage in the language and domain. In this paper, we repurpose WordNet's supersense tags for annotation, developing specific guidelines for nominal expressions and applying them to Arabic Wikipedia articles in four topical domains. The resulting corpus has high coverage and was completed quickly with reasonable inter-annotator agreement.
- Emad Mohamed, Behrang Mohit, Kemal Oflazer. 2012. Transforming Standard Arabic to Colloquial Arabic. Abstract: We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-of-vocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects.
- Kemal Oflazer. 2012. 12 MORPHOLOGICAL ANALYSIS. Abstract: In the previous chapters, we have seen that a lot of information about the potential tags of tokens in a text is found by lexicon lookup. Another, often complementary source of information is morphological analysis, i.e. the process of decomposing words into their constituents. The information about the individual constituents can be used to determine the necessary information about the word as a whole. Such information may range from basic wordclass information assigned from a fixed inventory of tags to structural information consisting of the relationships between components of the word further annotated with various features and their values (cf. Chapter 10). The English word "redness" could thus either be analysed as having the tag NN (singular noun) hiding its internal details, or be analysed by a suitable word grammar to have the structureAdj (red) + N (+ness) where the internal structure of the word has been made explicit. This chapter will present issues in implementing morphological analysers to be used in wordclass tagging or other natural language processing activities, such as syntactic parsing, speech recognition, text-to-speech, spelling checking and correction, document indexing and retrieval. The purpose of this chapter, however, is not to provide a detailed coverage of various aspects of computational morphology; the reader is referred to several recent books covering this topic (see e.g. Sproat (1992) for a quite comprehensive treatment of computational morphology and Ritchie et al. (1992) for a description of a morphological analyser and lexicon for English). Instead, after
- Emad Mohamed, Behrang Mohit, Kemal Oflazer. 2012. Annotating and Learning Morphological Segmentation of Egyptian Colloquial Arabic. Abstract: We present an annotation and morphological segmentation scheme for Egyptian Colloquial Arabic (ECA) in which we annotate user-generated content that significantly deviates from the orthographic and grammatical rules of Modern Standard Arabic and thus cannot be processed by the commonly used MSA tools. Using a per letter classification scheme in which each letter is classified as either a segment boundary or not, and using a memory-based classifier, with only word-internal context, prove effective and achieve a 92% exact match accuracy at the word level. The well-known MADA system achieves 81% while the per letter classification scheme using the ATB achieves 82%. Error analysis shows that the major problem is that of character ambiguity since the ECA orthography overloads the characters which would otherwise be more specific in MSA, like the differences between y (U) and Y (U) and A (O§) , > ( O£), and < (O¥) which are collapsed to y (U) and A (O§) respectively or even totally confused and interchangeable. While normalization helps alleviate orthographic inconsistencies, it aggravates the problem of ambiguity.
- Kemal Oflazer. 2012. Türkçe ve Doğal Dil İşleme(Turkish Natural Language Processing). Abstract: Bu makalede Turkce’nin dogal dil isleme acisindan ilginc olan ozellikleri, ve karsilasilan sorun ve bulunan cozumlerin kus baki¸si bir taramasi yapilmistir. Cogu zorluklar dilin karma¸sik sozcuk yapisindan ve bu yapinin sozdizim ve istatistiksel modellemeyle olan iliskisinden kaynaklanmaktadir. Bu taramanin sonrasinda da Turkce dogal dil isleme icin gelistirilmis olan onemli kaynaklarin bir ozeti verilmistir.
- Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Kemal Oflazer, Noah A. Smith. 2012. Recall-Oriented Learning of Named Entities in Arabic Wikipedia. Abstract: We consider the problem of NER in Arabic Wikipedia, a semisupervised domain adaptation setting for which we have no labeled training data in the target domain. To facilitate evaluation, we obtain annotations for articles in four topical groups, allowing annotators to identify domain-specific entity types in addition to standard categories. Standard supervised learning on newswire text leads to poor target-domain recall. We train a sequence model and show that a simple modification to the online learner---a loss function encouraging it to "arrogantly" favor recall over precision---substantially improves recall and F1. We then adapt our model with self-training on unlabeled target-domain data; enforcing the same recall-oriented bias in the self-training stage yields marginal gains.
- Kemal Oflazer, T. Mitamura, Tomas By, Hideki Shima, E. Riebling. 2011. A Natural Language Processing-Based Active and Interactive Platform for Accessing English Language Content and Advanced Language Learning. Abstract: Abstract SmartReader is a general-purpose “reading appliance” being implemented at Carnegie Mellon University (Qatar and Pittsburgh) - building upon an earlier prototype version. It is an artificial intelligence system that employs advanced language processing technologies and can interact with the reader and respond to queries about the content, words and sentences in a text. We expect it to be used by students in Qatar and elsewhere to help improve their comprehension of English text. SmartReader is motivated by the observation that text is still the predominant medium for learning especially at the advanced level and that text, being ``bland’’, is hardly a conducive and motivating medium for learning, especially when one does not have access to tools that enable one get over language roadblocks, ranging from unknown words to unrecognized and forgotten names, to hard-to-understand sentences. SmartReader strives to make reading (English) textual material, an “active” and an “interactive” process with the user interacting with the text using anytime-anywhere contextually-guided query mechanism based-on contextual user intent recognition. With SmartReader, a user can -inquire about the contextually correct meaning or synonyms of a word or idiomatic and multi-word constructions, -select a person's name, and then get an immediate ``flashback’’ to the first (or the last) time the person was encountered in text to remind herself the details of the person, -extract a summary of a section to remember important aspects of the content at the point she left off, and continue reading with a significantly refreshed context, -select a sentence that she may not be able to understand fully and ask SmartReader to break it down, simplify or paraphrase to comprehend it better. -test her comprehension of the text in a page or a chapter, by asking SmartReader to dynamically generate quizzes and answering them. -ask questions about the content of the text and get answers in addition to many other functions. SmartReader is being implemented as a multi-platform (tablet/PC) client-server system using HTML5 technology, with Unstructured Information Management Architecture -UIMA technology (used recently in IBM's Watson Q/A system in the Jeopardy Challenge) as the underlying language processing framework.
- Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Kemal Oflazer, Noah A. Smith. 2011. Recall-Oriented Learning for Named Entity Recognition in Wikipedia. Abstract: We consider the problem of NER in Arabic Wikipedia, a semi-supervised domain adaptation setting for which we have no labeled training data in the target domain. To facilitate evaluation, we obtain annotations for articles in four topical groups, allowing annotators to identify domain-specific entity types in addition to standard categories. Standard supervised learning on newswire text leads to poor target-domain recall. We train a sequence model and show that a simple modification to the online learner—a loss function encouraging it to “arrogantly” favor recall over precision—substantially improves recall and F1. We then employ self-training on unlabeled target-domain data in order to adapt our model; enforcing the same recall-oriented bias in the self-training stage yields additional gains.
- Behrang Mohit, Nathan Schneider, Kemal Oflazer, Noah A. Smith. 2011. Annotating a Multi-Topic Corpus for Arabic Natural Language Processing. Abstract: Abstract Human-annotated data is an important resource for most natural language processing (NLP) systems. Most linguistically annotated text for Arabic NLP is in the news domain, but systems that rely on this data do not generalize well to other domains. We describe ongoing efforts to compile a dataset of 28 Arabic Wikipedia articles spanning four topical domains—sports, history, technology, and science. Each article in the dataset is annotated with three types of linguistic structure: named entities, syntax and lexical semantics. We adapted traditional approaches to linguistic annotation in order to make them accessible to our annotators (undergraduate native speakers of Arabic) and to better represent the important characteristics of the chosen domains. For the named entity (NE) annotation, we start with the task of marking boundaries of expressions in the traditional Person, Location and Organization classes. However, these categories do not fully capture the important entities discussed in domains li...
- A. C. Tantug, E. Adalı, Kemal Oflazer. 2011. Türkmenceden Türkçeye bilgisayarlı metin çevirisi. Abstract: Diller arasinda bilgisayar kullanilarak ceviri yapilmasi konusu, dogal dil isleme alaninin en onemli dallarindan bir tanesidir. Ancak teknolojideki ve yontemlerdeki gelismelere karsin, genel amacli, yuksek basarima sahip ceviri sistemleri henuz genel kullanima sunulamamistir. Bunun temel nedeni, diller arasindaki buyuk yapisal ve anlatim farkliliklardir. Bu noktadan hareketle, benzer diller arasinda Bilgisayarli Ceviri (BC) gerceklemenin daha kolay olabilecegi akla gelmektedir. Nitekim son yillarda Cekce-Slovakca, Cekce-Lehce, Ispanyolca-Katalanca gibi cok yakin diller arasinda yuksek basarimli ciktilar uretebilen sistemler gelistirilebilmistir. Ustelik bu sistemler, farkliliklarin derin oldugu, Japonca-Ingilizce gibi dil ciftleri arasinda BC icin gerek duyulan karmasik yontemlere gore daha basit ve kolay gerceklenebilir yontemler kullanmaktadirlar. Bu calisma kapsaminda, ayni dil ailesi icinde siniflandirilan ve bircok yonden benzerlikler gosteren Turkmence ile Turkce dilleri arasinda bir BC sistemi gelistirilmistir. Soz konusu bu diller ne kadar benzer ozellikler gosterse de, cozulmesi gereken farkliliklar azimsanmayacak boyuttadir. Turk Dilleri arasindaki farkliliklar, yukarida anilan dil ciftlerinden daha fazladir ve karsilikli anlasilabilirlik soz konusu degildir. Sistem, hem kural tabanli hem de istatistiksel bilesenlerden olusan karma bir ceviri modeli kullanarak, Turkmence tumcedeki sozcuklerin sirasini degistirmeden sozcuk-sozcuk Turkceye aktarim yapilmasi ilkesini temel almistir. Ancak bitisken yapili Turk Dillerinin karmasik bicimbilimsel ozellikleri nedeniyle, diger dillerde kullanilabilen basit dogrudan aktarim yontemleri gelistirilerek kullanilmistir. BLEU yontemi ile sistemin basarim olcumu yapilmis ve modelin basarili sonuclar uretebilecegi gosterilmistir. Anahtar Kelimeler: Bilgisayarli ceviri, Turk Dilleri, Turkmence, Turkce .
- M. J. Foster, H. T. Kung, Kemal Oflazer. 2011. MISE: Machine for ln-System Evaluation of Custom VLSI Chips. Abstract: The ultimate goal of any custom chip must be its effective use in a complete system. Integrating a chip into a system is usually a nontrivial task, especially for small research teams that do not have extensive hardware and software support As a result, chips designed by universities and research laboratories are seldom evaluated beyond functional checking in isolation. In-system evaluation, namely the evaluation of chips in a realistic application system, is a crucial step in the development of a chip—without it, the impact of the custom chip on a system cannot be demonstrated adequately. Unfortunately, in-system evaluation has often been regarded (mistakenly) as a "tedious but simple" interfacing problem. This paper identifies some of the key research problems that one encounters in specifying, designing, testing and demonstrating a custom chip in relation to the application system in which it will be used, and proposes a system called MISE (Machine For In-System Evaluation) to be a solution to the issues raised.
- A. Karatsolis, I. Cervesato, Khaled A. Harras, Y. Cooper, Kemal Oflazer, N. Abu-Ghazaleh, Thierry Sans. 2011. Getting CS undergraduates to communicate effectively. Abstract: In the last decade or so, the ACM, the IEEE and other organizations have acknowledged that there is a problem with the way communication is taught in the Computer Science curriculum: the writing, speaking, and presentation skills students learn in the classroom do not match what is expected of them in the workplace. The proposed solution, adopted by many undergraduate colleges, was to add a technical communication course to the CS curriculum. This does not appear to be enough, as mainstream accreditation boards are still emphasizing the need for improvement of communication skills instruction in their recent reports and recommendations. For the last two years, we have experimented with a complementary transversal approach where many "traditional CS" courses in our program have added a communication component to their syllabus, while at the same time our technical communication course has been revamped to expose students to realistic practices as promoted by situated learning theory. The results, so far anecdotal, point to improved student performance and attitude across several communication dimensions, in particular writing and presentation. We plan to develop this experiment by spreading it across more classes and by starting to collect rigorous measurements of students' communication performance.
- Mohit Behrang, Kemal Oflazer, Noah A. Smith. 2010. Named entity recognition from Arabic Wikipedia. Abstract: Abstract Named Entity Recognition (NER) is the problem of locating mentions to entities such as persons, locations and organizations. The named entity information is helpful for reducing the complexity of monolingual and multilingual processing tasks, such as information extraction, parsing and machine translation. We investigate the Arabic NER problem from the Arabic Wikipedia text. We employ statistical sequence labeling methods for solving the NER task. Previous studies suggest that sequence labeling methods, such as Conditional Random Fields, are the state of the art NER frameworks. The sequence labeling methods require human labeled training data. Most ofthe Arabic human labeled data for NER belong to the political news domain and the consequent trained models are biased towards the news domain. In contrast, our target test data (Arabic Wikipedia articles) has a very diverse set of topics. The domain mismatch between the train and test data results in poor NER performance. In order to reduce the coverage problem, we present three techniques: (1) we use the Wikipedia network structure to collect additional information about the text. Information such as monolingual and cross-lingual hyperlinks and text formatting lead us to use new features of the Wikipedia text in NER models. Moreover, we use cross-lingual projection to collect named entity information from English Wikipedia. (2) We use a domain adaptation technique to shift the model from the baseline political domain to domains relevant to our test data. Our model adaptation uses a small set of in-house-labeled Arabic Wikipedia articles. (3) We use self-training to port from a fully supervised to a semi-supervised learning framework: we collect a large volume of unlabeled Arabic Wikipedia articles to expand the underlying NER domain to new text domains. Our model expansion is gradual and iterative. In each iteration we add a new set of unlabeled articles to the training and use the current model to label and construct a larger model. Our NER evaluations are based on the standard precision and recall metrics.We evaluate our proposed framework in four different text domains ofArabic Wikipedia.
- Rishav Bhowmick, Michael Heilman, Kemal Oflazer, Behrang Mohit, Noah A. Smith. 2010. Rich entity recognition in English text. Abstract: AbstractEntity type recognition is used as a pre-processing step in common applications like summarization of text, classifying documents or automatic answering of questions posed in natural language. Here, ‘entity’ refers to concrete and abstract objects identified by proper and common nouns. Entity recognition focuses on detecting instances of types like person, location, organization, and so on. For example, an entity recognizer would take as input:George Washington was the first President of the United States of America. and output: George Washington was the first President of the United States of America .The task can be performed using machine learning techniques to train a system that recognizes entities with performance comparable to a human annotator. Challenges like the lack of a large annotated training data corpus, impossible nature of listing all entity types, and ambiguity in language make this problem h...
- Reyyan Yeniterzi, Kemal Oflazer. 2010. Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish. Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.
- Ilknur Durgar El-Kahlout, Kemal Oflazer. 2010. Exploiting Morphology and Local Word Reordering in English-to-Turkish Phrase-Based Statistical Machine Translation. Abstract: In this paper, we present the results of our work on the development of a phrase-based statistical machine translation prototype from English to Turkish-an agglutinative language with very productive inflectional and derivational morphology. We experiment with different morpheme-level representations for English-Turkish parallel texts. Additionally, to help with word alignment, we experiment with local word reordering on the English side, to bring the word order of specific English prepositional phrases and auxiliary verb complexes, in line with the morpheme order of the corresponding case-marked nouns and complex verbs, on the Turkish side. To alleviate the dearth of the parallel data available, we also augment the training data with sentences just with content word roots obtained from the original training data to bias root word alignment, and with highly reliable phrase-pairs from an earlier corpus alignment. We use a morpheme-based language model in decoding and a word-based language model in re-ranking the n-best lists generated by the decoder. Lastly, we present a scheme for repairing the decoder output by correcting words which have incorrect morphological structure or which are out-of-vocabulary with respect to the training data and language model, to further improve the translations. We improve from 15.53 BLEU points for our word-based baseline model to 25.17 BLEU points for an improvement of 9.64 points or about 62% relative.
- Steven P. Abney, S. Kurohashi, S. Bangalore, Irene Langkilde-Geary, C. Brew, Mirella Lapata, Sharon A. Caraballo, C. Leacock, Bob Carpenter, B. Levin, Stanley F. Chen, D. Litman, Kenneth Ward Church, I. Mani, Michael Collins, Christopher Manning, Ann A. Copestake, D. Marcu, M. Crocker, E. Marsi, P. Deane, Diana McCarthy, Mona T. Diab, I. D. Melamed, M. Dras, J. Minett, Jason Eisner, Robert C. Moore, E. Fosler-Lussier, Thomas Morton, George Foster, H. Ney, R. Frank, G. Ngai, Jianfeng Gao, Kemal Oflazer, Claire Gardent, Massimo Poesio, Tanja Gaustad van Zaanen, Judita Preiss, D. Gildea, Ehud Reiter, Andrew R. Golding, P. Resnik, Joshua Goodman, Roni Rosenfeld, G. Grefenstette, Frank Schilder, Mohammad Haji-Abdolhosseini, Lenhart K. Schubert, P. Heeman, Advaith Siddharthan, D. Higgins, R. Sproat, J. Hockenmaier, M. Strube, H. Horacek, M. Swerts, D. Inkpen, Simone Teufel, Martin Jansche, Kees van Deemter, Mark Johnson, Ye-Yi Wang, Frank Keller, B. Webber, A. Kilgarriff, J. Wiebe, Kevin Knight, Florian Wolf. 2010. Reviewers for Volume 31. Abstract: This journal has a knowledgeable and hardworking editorial board, whose members are listed on the inside front cover of each issue, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express his gratitude to the external reviewers listed below who anonymously reviewed papers for the journal during the preparation of this volume (Volume 31). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.
- Bernardine Dias, Kemal Oflazer, Noah Smith. 2010. 2010 Senior Thesis Project Reports Advisor: Kemal Oflazer and Noah Smith Rich Entity Type Recognition in Text Senior Thesis Table of Contents. Abstract: This technical report collects the final reports of the undergraduate Computer Science majors from the Qatar Campus of Carnegie Mellon University who elected to complete a senior research thesis in the academic year 2009–10 as part of their degree. These projects have spanned the students’ entire senior year, during which they have worked closely with their faculty advisors to plan and carry out their projects. This work counts as 18 units of academic credit each semester. In addition to doing the research, the students presented a brief midterm progress report each semester, presented a public poster session in December, presented an oral summary in the year-end campuswide Meeting of the Minds and submitted a written thesis in May.
- Kemal Oflazer, David Schlangen. 2009. Instructions for EACL-09 Proceedings. Abstract: This document contains the instructions for preparing a camera-ready manuscript for the proceedings of EACL-09. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. These instructions should be used for both papers submitted for review and for final versions of accepted papers. Authors are asked to conform to all the directions reported in this document.
- A. C. Tantug, Kemal Oflazer, Ilknur Durgar El-Kahlout. 2008. BLEU+: a Tool for Fine-Grained BLEU Computation. Abstract: We present a tool, BLEU+, which implements various extension to BLEU computation to allow for a better understanding of the translation performance, especially for morphologically complex languages. BLEU+ takes into account both closeness in morphological structure, closeness of the root words in the WordNet hierarchy while comparing tokens in the candidate and reference sentence. In addition to gauging performance at a finer level of granularity, BLEU+ also allows the computation of various upper bound oracle scores: comparing all tokens considering only the roots allows us to get an upper bound when all errors due to morphological structure are fixed, while comparing tokens in an error-tolerant way considering minor morpheme edit operations, allows us to get a (more realistic) upper bound when tokens that differ in morpheme insertions/deletions and substitutions are fixed. We use BLEU+ in the fine-grained evaluation of the output of our English-to-Turkish statistical MT system.
- Gülşen Eryiğit, E. Adalı, Kemal Oflazer. 2008. Türkçe'nin Olasılık Tabanlı Bağlılık Ayrıştırması. Abstract: Bu calisma, Turkce icin gelistirilmis ilk istatistiksel baglilik ayristiricisinin sonuclarini sunmaktadir. Turkce, tumce ici oge dizilisleri serbest, karmasik bir cekimsel ve turetimsel bicimbirime sahip olan bitisken bir dildir ve bu ozellikleri ile istatistiksel ayristirma konusunda ilginc sorunlar ortaya koymaktadir. Turkce’de, baglilik iliskileri “cekim kumesi” adi verilen sozcuk parcaciklari arasinda kurulmaktadir. Bu bagliliklarin bulunmasi amaci ile Turkce’nin karmasik yapisinin ayristirma sirasinda nasil modelleneceginin irdelenmesi gerekmektedir. Bu calismada, ayristirma icin farkli gosterim birimleri kullanan olasilik tabanli modeller incelenmistir. Baslangic olarak biri kural tabanli bir ayristirici olmak uzere uc dayanak model gelistirilmistir. Gerceklestirilen uc olasilik tabanli modelin, dayanak modellere ve birbirlerine oranla basarimlari degerlendirilmistir. Ayristiricinin egitimi ve sinamasi icin Odtu Sabanci Turkce agac yapili derlemi kullanilmistir. Calisma ayrica bu derlem uzerinde sinanmis ve sonuclai raporlanmis ilk calismadir. Bu ilk incelemede, derlemin sadece saga bagimli (iye sozcuklerin uydu sozcuklerin sag taraflarinda yer aldigi) turde ve kesismeyen bagliliklar iceren bir alt kumesini ayristirmaya odaklanilmistir. Eldeki derlemin boyutu nedeni ile gorunum bilgisi (sozcugun tumunun veya govdesinin ayristirma birimi gosterimlerinde bir ozellik olarak kullanilmasi) kullanmayan ve sadece birimler arasi etiketsiz bagliliklari bulmaya yonelik incelemeler yapilmistir. Sonuclarimiz, cekim kumeleri arasindaki dogru bagliklarin bulunma basarimi gozonune alindiginda, ayristirma birimi olarak cekim kumelerinin kullanildigi ve baglam bilgisinden yararlanan modelin en yuksek basarimi sagladigini gostermektedir. Anahtar Kelimeler: Baglilik ayristirmasi, dogal dil isleme, ayristirma, sentaks analizi .
- Gülşen Eryiğit, Joakim Nivre, Kemal Oflazer. 2008. Dependency Parsing of Turkish. Abstract: The suitability of different parsing methods for different languages is an important topic in syntactic parsing. Especially lesser-studied languages, typologically different from the languages for which methods have originally been developed, pose interesting challenges in this respect. This article presents an investigation of data-driven dependency parsing of Turkish, an agglutinative, free constituent order language that can be seen as the representative of a wider class of languages of similar type. Our investigations show that morphological structure plays an essential role in finding syntactic relations in such a language. In particular, we show that employing sublexical units called inflectional groups, rather than word forms, as the basic parsing units improves parsing accuracy. We test our claim on two different parsing methods, one based on a probabilistic model with beam search and the other based on discriminative classifiers and a deterministic parsing strategy, and show that the usefulness of sublexical units holds regardless of the parsing method. We examine the impact of morphological and lexical information in detail and show that, properly used, this kind of information can improve parsing accuracy substantially. Applying the techniques presented in this article, we achieve the highest reported accuracy for parsing the Turkish Treebank.
- G. Eryiğit, En, Joakim Nivre, Kemal Oflazer. 2008. Erratum: Dependency Parsing of Turkish. Abstract: The suitability of different parsing methods for different languages is an important topic in syntactic parsing. Especially lesser-studied languages, typologically different from the languages for which methods have originally been developed, pose interesting challenges in this respect. This article presents an investigation of data-driven dependency parsing of Turkish, an agglutinative, free constituent order language that can be seen as the representative of a wider class of languages of similar type. Our investigations show that morphological structure plays an essential role in finding syntactic relations in such a language. In particular, we show that employing sublexical units called inflectional groups, rather than word forms, as the basic parsing units improves parsing accuracy. We test our claim on two different parsing methods, one based on a probabilistic model with beam search and the other based on discriminative classifiers and a deterministic parsing strategy, and show that the usefulness of sublexical units holds regardless of the parsing method. We examine the impact of morphological and lexical information in detail and show that, properly used, this kind of information can improve parsing accuracy substantially. Applying the techniques presented in this article, we achieve the highest reported accuracy for parsing the Turkish Treebank.
- A. C. Tantug, E. Adali, Kemal Oflazer. 2007. A MT system from Turkmen to Turkish employing finite state and statistical methods. Abstract: In this work, we present a MT system from Turkmen to Turkish. Our system exploits the similarity of the languages by using a modified version of direct translation method. However, the complex inflectional and derivational morphology of the Turkic languages necessitate special treatment for word-by-word translation model. We also employ morphology-aware multi-word processing and statistical disambiguation processes in our system. We believe that this approach is valid for most of the Turkic languages and the architecture implemented using FSTs can be easily extended to those languages.
- Kemal Oflazer, Ilknur Durgar El-Kahlout. 2007. Exploring Different Representational Units in English-to-Turkish Statistical Machine Translation. Abstract: We investigate different representational granularities for sub-lexical representation in statistical machine translation work from English to Turkish. We find that (i) representing both Turkish and English at the morpheme-level but with some selective morpheme-grouping on the Turkish side of the training data, (ii) augmenting the training data with "sentences" comprising only the content words of the original training data to bias root word alignment, (iii) reranking the n-best morpheme-sequence outputs of the decoder with a word-based language model, and (iv) using model iteration all provide a non-trivial improvement over a fully word-based baseline. Despite our very limited training data, we improve from 20.22 BLEU points for our simplest model to 25.08 BLEU points for an improvement of 4.86 points or 24% relative.
- Ilknur Durgar El-Kahlout, Kemal Oflazer. 2006. Initial Explorations in English to Turkish Statistical Machine Translation. Abstract: This paper presents some very preliminary results for and problems in developing a statistical machine translation system from English to Turkish. Starting with a baseline word model trained from about 20K aligned sentences, we explore various ways of exploiting morphological structure to improve upon the baseline system. As Turkish is a language with complex agglutinative word structures, we experiment with morphologically segmented and disambiguated versions of the parallel texts in order to also uncover relations between morphemes and function words in one language with morphemes and functions words in the other, in addition to relations between open class content words. Morphological segmentation on the Turkish side also conflates the statistics from allomorphs so that sparseness can be alleviated to a certain extent. We find that this approach coupled with a simple grouping of most frequent morphemes and function words on both sides improve the BLEU score from the baseline of 0.0752 to 0.0913 with the small training data. We close with a discussion on why one should not expect distortion parameters to model word-local morpheme ordering and that a new approach to handling complex morphotactics is needed.
- A. C. Tantug, Kemal Oflazer, E. Adali. 2006. A prototype machine translation system between Turkmen and Turkish. Abstract: In this work, we present a prototype system for translation of Turkmen texts into Turkish. Although machine translation (MT) is a very hard task, it is easier to implement a MT system between very close language pairs which have similar syntactic structure and word order. We implement a direct translation system between Turkmen and Turkish which performs a word-to-word transfer. We also use a Turkish Language Model to find the most probable Turkish sentence among all possible candidate translations generated by our system.
- Gülşen Eryiğit, Kemal Oflazer, E. Adalı. 2006. Türkçe cümlelerin kural tabanlı bağlılık analizi. Abstract: Bu makalede, Turkce cumlelerin kural tabanli baglilik analizi yontemi ile ayristirilmalari sonucunda elde edilen basarim sunulmaktadir. Calisma, test verisi olarak kullanilan ODTU-Sabanci Agac Yapili Derlemi'nin butunu uzerindeki ilk kural tabanli sonuclari icermektedir. Uygulanan ayristirma algoritmasi ve kural yapilari detayli olarak verilmistir. Sonuclar Turkce'nin Baglilik Analizi konusunda yapilacak calismalara temel olma niteligindedir.
- Gülşen Eryiğit, Kemal Oflazer. 2006. Statistical Dependency Parsing for Turkish. Abstract: This paper presents results from the first statistical dependency parser for Turkish. Turkish is a free-constituent order language with complex agglutinative inflectional and derivational morphology and presents interesting challenges for statistical parsing, as in general, dependency relations are between “portions” of words called inflectional groups. We have explored statistical models that use different representational units for parsing. We have used the Turkish Dependency Treebank to train and test our parser but have limited this initial exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed.
- M. O. Külekci, Kemal Oflazer. 2006. An infrastructure for Turkish prosody generation in text-to-speech synthesis. Abstract: Text-to-speech engines benefit from natural language processing while generating the appropriate prosody. In this study, we investigate the natural language processing infrastructure for Turkish prosody generation in three steps as pronunciation disambiguation, phonological phrase detection and intonation level assignment. We focus on phrase boundary detection and intonation assignment. We propose a phonological phrase detection scheme based on syntactic analysis for Turkish and assign one of three intonation levels to words in detected phrases. Empirical observations on 100 sentences show that the proposed scheme works with approximately 85% accuracy.
- Ilknur Durgar El-Kahlout, Kemal Oflazer. 2006. Türkçe-İngilizce için istatistiksel bilgisayarlı çeviri sistemi. Abstract: Bu bildiride, Turkce Ingilizce dil cifti icin istatistiksel bilgisayarli ceviri sistemi anlatilmaktadir. Iki dil arasindaki yapisal farkliliklardan kaynaklanan problemler, bicimbirimsel analiz yapilarak eklerin ayri gosterimi ile ortadan kaldirilmistir. Yaklasim sozcuk obegi tabanli cozucu ile test edilmistir. Sistem performansi, eklerin bigram tabanli gruplandirilmasi ile iyilestirilmistir. Onerilen metot ile standart modele kiyasla daha iyi sonuclar elde edilmistir. 22000 cumlelik paralel metinler ile olusturulan sistemin performansi tatmin edici olmasa da bir baslangicdir.
- Özlem Çetinoğlu, Kemal Oflazer. 2006. Altsözcüksel birimlerle Türkçe için sözcüksel işlevsel gramer geliştirilmesi. Abstract: Bu bildiri Turkce’nin karmasik bicimbilimsel yapisi ve zengin turetme olaylarini ele alirken bir cozum olarak altsozcuksel birimler kullanmayi incelemekte ve onerilen yaklasimi Pargram projesi dahilinde gerceklenmekte olan Turkce sozcuksel islevsel gramer uzerinden anlatmaktadir. Izledigimiz yaklasim sayesinde kurallar daha duzenli ve ozlu bir sekilde yazilabilmekte, boylece hem genelleme imkani arttigi icin daha az sayida olan hem de icerik olarak karmasik olmayan kuralarla gramer kapsami genisletilebilmektedir. Ustelik turetmelerin sozcuklere anlambilimsel katkilari programin calismasi sirasinda yaratilan PRED degerleri sayesinde sistematik bir bicimde ifade edilebilmektedir. Calismamiz altsozcuksel birimlerin basit yapim ekleri ile kullanimina yer vermekte daha sonra ettirgen yapilar gibi gorece daha karmasik dil olaylarina deginmektedir. Oncelikli amacimiz kullandigimiz yaklasimi mumkun oldugunca birbirinden farkli dilbilimsel alanlarda incelemek oldugu icin bu bildiride sayisal bir degerlendirmeye yer verilmemistir.
- Özlem Çetinoğlu, Kemal Oflazer. 2006. Morphology-Syntax Interface for Turkish LFG. Abstract: This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes, in the development of a lexical functional grammar for Turkish. Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner. This in turn leads to more succinct and manageable rules. Further, the semantics of the derivations can also be systematically reflected in a compositional way by constructing PRED values on the fly. We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization, etc., which change verb valency. Our priority is to handle several linguistic phenomena in order to observe the effects of our approach on both the c-structure and the f-structure representation, and grammar writing, leaving the coverage and evaluation issues aside for the moment.
- H. Ng, Kemal Oflazer. 2005. Instructions for ACL-2005 Proceedings. Abstract: This document contains the instructions for preparing a camera-ready manuscript for the proceedings of ACL-2005. The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like. Authors are asked to conform to all the directions reported in this document.
- H. Erdogan, O. Buyuk, Kemal Oflazer. 2005. Incorporating language constraints in sub-word based speech recognition. Abstract: In large vocabulary continuous speech recognition (LVCSR) for agglutinative and inflectional languages, we encounter problems due to theoretically infinite full-word lexicon size. Sub-word lexicon units may be utilized to dramatically reduce the out-of-vocabulary rate in test data. One can develop language models based on sub-word units to perform LVCSR. However, it has not always been beneficial to use sub-word lexicon units, since shorter units have higher acoustic confusability among them and language model history is effectively shorter as compared to the history in full-word language models. To reduce the aforementioned problems, we propose using the longest possible sub-word units in our lexicon, namely half-words and full-words only. We also incorporate linguistic rules of half-word combination into our statistical language model. The language constraints are represented with a rule-based WFSM which can be combined with an N-gram language model to yield a better and smaller language model. We study the performance of the proposed system for Turkish LVCSR, when the language constraint takes the form of enforcing vowel harmony between stems and endings. We also introduce novel error-rate metrics that are more appropriate than word-error-rate for agglutinative languages. Using half-words with a bi-gram model yields a significant reduction in word-error-rate as compared to a bi-gram full-word model. In addition, combining a tri-gram half-word language model with the vowel-harmony WFSM improves the accuracy further when rescoring the bi-gram lattices
- Kemal Oflazer, Y. Yilmaz. 2004. Vi-xfst: A Visual Regular Expression Development Environment for Xerox Finite State Tool. Abstract: This paper describes Vi-xfst, a visual interface and a development environment, for developing finite state language processing applications using the Xerox Finite State Tool, xfst. Vi-xfst lets a user construct complex regular expressions via a drag-and-drop visual interface, treating simpler regular expressions as "Lego Blocks". It also enables the visualization of the structure of the regular expression components, providing a bird's eye view of the overall system, enabling a user to easily understand and track the structural and functional relationships among the components involved. Since the structure of a large regular expression (built in terms of other regular expressions) is now transparent, users can also interact with regular expressions at any level of detail, easily navigating among them for testing. Vi-xfst also keeps track of dependencies among the regular expressions at a very fine-grained level. So when a certain regular expression is modified as a result of testing, only the dependent regular expressions are recompiled resulting in an improvement in development process time, by avoiding file level recompiles which usually causes redundant regular expression compilations.
- E. Vural, Kemal Oflazer. 2004. A prosodic Turkish text-to-speech synthesizer. Abstract: We describe an implementation of a Turkish text-to-speech system using word stress as one of the sources for prosody, with an increased number of phonemes to cover the pronunciation variations in Turkish. The implementation is based on the Festival System (Black, A. and Lenzo, K., "Building Voices in the Festival Speech Synthesis System", Carnegie Mellon University).
- M. Kulekci, Kemal Oflazer. 2004. An overview of natural language processing techniques in text-to-speech systems. Abstract: The paper presents an overview of the use of natural language processing techniques at various stages of TTS systems.
- Ilknur Durgar El-Kahlout, Kemal Oflazer. 2004. Use of wordnet for retrieving words from their meanings. Abstract: This paper presents a Meaning to Word System (MTW) for Turkish 
Language, that finds a set of words, closely matching the defnition entered by the user. The approach of extracting words from meanings is based on checking the similarity between the user's denition and each entry of the Turkish database without considering any semantics or grammatical information. Results on unseen user queries indicate that in 66% of the queries the correct responses were in the rst 50 of the words returned, while for queries selected from the word definitions in a different dictionary in 92% of the queries correct responses were in the first 50 of the words returned. Our system make extensive uses of various linguistics resources including 
Turkish WordNet.
- E. Vural, Hakan Erdoǧan, Kemal Oflazer, Berrin Yanikoǧlu. 2004. Türkçe için tablet PC ortamında çevrimiçi yazı tanıma. Abstract: In this work an online handwritten text recognition system for Turkish has been developed using a Tablet PC as an interface. In recent years, although there has been great developments in the Tablet PC technology, still there are no applications for recognition in Turkish language. In this work, we have developed a prototype system using Hidden Markov Models which recognizes 
handwritten words from a small vocabulary list. This system has achieved a recognition rate over %90 percent.
- Orhan Bilgin, Özlem Çetinoğlu, Kemal Oflazer. 2004. Morphosemantic relations in and across wordnets: a preliminary study based on Turkish. Abstract: Morphological processes in a language can be effectively used to enrich 
individual wordnets with semantic relations. More importantly, morphological processes in a language can be used to discover less explicit semantic relations in other languages. This will both improve the internal connectivity of individual wordnets and also the overlap across different wordnets. Using morphology to improve the quality of wordnets and to automatically prepare synset glosses are two other possible applications.
- Orhan Bilgin, Özlem Çetinoğlu, Kemal Oflazer. 2004. Building a wordnet for Turkish. Abstract: This paper summarizes the development process of a wordnet for Turkish as part of the Balkanet project. After discussing the basic method-ological issues that had to be resolved during the course of the project, the paper presents the basic steps of the construction process in chronological order. Two applications using Turkish wordnet are summarized and links to resources for wordnet builders are provided at the end of the paper.
- Kemal Oflazer, Ozlem Cetinoglu, Bilge Say. 2004. Integrating Morphology with Multi-word Expression Processing in Turkish. Abstract: This paper describes a multi-word expression processor for preprocessing Turkish text for various language engineering applications. In addition to the fairly standard set of lexicalized collocations and multi-word expressions such as named-entities, Turkish uses a quite wide range of semi-lexicalized and non-lexicalized collocations. After an overview of relevant aspects of Turkish, we present a description of the multi-word expressions we handle. We then summarize the computational setting in which we employ a series of components for tokenization, morphological analysis, and multi-word expression extraction. We finally present results from runs over a large corpus and a small gold-standard corpus.
- E. Vural, Hakan Erdogan, Kemal Oflazer, B. Yanikoglu. 2004. An online handwriting recognition system for Turkish. Abstract: An online handwritten text recognition system for Turkish has been developed using a tablet PC as an interface. In recent years, although there have been great developments in tablet PC technology, there are still no applications for recognition in the Turkish language. We have developed a prototype system using hidden Markov models which recognizes handwritten words from a small vocabulary list. This system has achieved a recognition rate of over 90%.
- Kemal Oflazer. 2003. Lenient morphological analysis. Abstract: This paper presents a scheme that allows one to relax the all-or-none nature of two-level constraints in two-level morphology in a controlled manner, so that word forms with violations of some of the two-level constraints can be analyzed and ranked. The problem has been motivated by a recent phenomenon in Turkish with imported words that violate a fundamental assumption of Turkish that pronunciation and orthography have almost a one-to-one correspondence, and by a problem in Basque words with differing amounts of competence errors. We present the formulation of our proposal, and provide details of implementations for both problems using the XRCE Finite State Toolkit.
- Kemal Oflazer, Sharon Inkelas. 2003. A pronunciation lexicon for turkish based on two-level morphology. Abstract: This paper describes the implementation of a full-scale pronunciation lexicon for Turkish based on a two-level morphological analyzer. The system produces at its output, a parallel representation of the pronunciation and the morphological analysis of the word form so that morphological disambiguation can be used to disambiguate pronunciation when necessary. The pronunciation representation is based on the SAMPA standard and also encodes the position of the primary stress. The computation of the position of the primary stress depends on an interplay of any exceptional stress in root words and stress properties of certain morphemes, and requires that a full morphological analysis be done. The system has been implemented using XRCE Finite State Toolkit.
- Kemal Oflazer. 2003. Language engineering for lesser-studied languages. Abstract: The subject topic of this publication falls into the general area of natural language processing. Special emphasis is given to languages that, for various reasons, have not been the subject of study in this discipline. This book will be of interest to both computer scientists who would like to build language processing systems and linguists interested in learning about natural language processing.
- N. Atalay, Kemal Oflazer, Bilge Say. 2003. The Annotation Process in the Turkish Treebank. Abstract: We present a progress report of the Turkish Treebank concentrating on various aspects of its design and implementation. In addition to a review of the corpus compilation process and the design of the annotation scheme, we describe the details of various pre-processing stages and the computer-assisted annotation process.
- Kemal Oflazer. 2003. Dependency Parsing with an Extended Finite-State Approach. Abstract: This article presents a dependency parsing scheme using an extended finite-state approach. The parser augments input representation with channels so that links representing syntactic dependency relations among words can be accommodated and iterates on the input a number of times to arrive at a fixed point. Intermediate configurations violating various constraints of projective dependency representations such as no crossing links and no independent items except sentential head are filtered via finite-state filters. We have applied the parser to dependency parsing of Turkish.
- Kemal Oflazer, Sharon Inkelas. 2002. A Finite State Pronunciation Lexicon for Turkish. Abstract: This paper describes the implementation of a full-scale pronunciation lexicon for Turkish using finite state technology. The system produces at its output, a parallel representation of the pronunciation and the morphological analysis of the word form so that morphological disambiguation can be used to disambiguate pronunciation. The pronunciation representation is based on the SAMPA standard and also encodes the position of the primary stress. The computation of the position of the primary stress depends on an interplay of any exceptional stress in root words and stress properties of certain morphemes, and requires that a full morphological analysis be done. The system has been implemented using XRCE Finite State Toolkit.
- Kemal Oflazer, S. Nirenburg, M. McShane. 2001. Bootstrapping Morphological Analyzers by Combining Human Elicitation and Machine Learning. Abstract: This paper presents a semiautomatic technique for developing broad-coverage finite-state morphological analyzers for use in natural language processing applications. It consists of three componentselicitation of linguistic information from humans, a machine learning bootstrapping scheme, and a testing environment. The three components are applied iteratively until a threshold of output quality is attained. The initial application of this technique is for the morphology of low-density languages in the context of the Expedition project at NMSU Computing Research Laboratory. This elicit-build-test technique compiles lexical and inectional information elicited from a human into a finite-state transducer lexicon and combines this with a sequence of morphographemic rewrite rules that is induced using transformation-based learning from the elicited examples. The resulting morphological analyzer is then tested against a test set, and any corrections are fed back into the learning procedure, which then builds an improved analyzer.
- G. Tur, Dilek Z. Hakkani-Tür, Kemal Oflazer. 2000. Name Tagging Using Lexical, Contextual, and Morphological Information. Abstract: This paper presents a probabilistic model for automatically tagging names in a Turkish text. We used four different information sources to model names, and successfully combined them. Our first information source is based on the surface forms of the words. Then we combined the contextual cues with the lexical model, and obtained a significant improvement. After this, we modeled the morphological analyses of the words, and finally, we modeled the tag sequence, and reached an F-measure of 91.56% in Turkish name tagging. Our results are important in the sense that, using linguistic information, i.e. morphological analyses of the words, and a corpus large enough to train a statistical model helps this basic information extraction task.
- L. Karttunen, Kemal Oflazer. 2000. Introduction to the Special issue on finite state methods in NLP. Abstract: The idea for this special issue came up during the preparations of the International Workshop on Finite-State Methods in Natural Language Processing, that was held at Bilkent University in Ankara, Turkey in the summer of 1998. The number of the submissions had exceeded our initial expectations and we were able to select quite a good set of papers from those submitted. Further, the workshop and the preceding tutorial by Kenneth Beesley, on finite-state methods, was attended by quite a large number of participants. This led us to believe that interest in the theory and applications of finitestate machinery was alive and well, and that some of the papers from this workshop along with further additional submissions could make a very good special issue for this journal. The five papers in this issue are the result of this process. The last decade has seen a quite a substantial surge in the use of finite-state methods in all aspects of natural language applications. Fueled by the theoretical contributions of Kaplan and Kay (1994), Mohri’s recent contributions on the use of finite-state techniques in various NLP problems (Mohri 1996, 1997), the success of finite-state approaches especially in computational morphology, for example, Koskenniemi (1983), Karttunen (1983), and Karttunen, Kaplan, and Zaenen (1992), and, finally, the availability of state-of-the-art tools for building and manipulating large-scale finite-state systems (Karttunen 1993; Karttunen and Beesley 1992; Karttunen et al. 1996; Mohri, Pereira, and Riley 1998; van Noord 1999), recent years have seen many successful applications of finite-state approaches in tagging, spell checking, information extraction, parsing, speech recognition, and text-to-speech applications. This is a remarkable comeback considering that in the dawn of modern linguistics (Chomsky 1957), finitestate grammars were dismissed as fundamentally inadequate. As a result, most of the work in computational linguistics in the past few decades has been focused on far more powerful formalisms. Recent publications on finite-state technology include two collections of papers (Roche and Schabes 1997; Kornai 1999) with contributions covering a wide range of these topics. This special issue, we hope, will add to these contributions. The five papers in this collection cover many aspects of finite-state theory and applications. The papers Treatment of Epsilon Moves in Subset Construction by van Noord and Incremental Construction of Minimal Acyclic Finite-State Automata and Transducers by Daciuk, Watson, Watson, and Mihov, address two fundamental aspects in the construction of finite-state recognizers. Van Noord presents results for various methods for producing a deterministic automaton with no epsilon transitions from a nondeterministic automaton with a large number of epsilon transitions, especially those resulting from finite-state approximations of context-free and more powerful formalisms. Daciuk et al. present a new method for constructing minimal, deterministic, acyclic
- Gökhan Tür, Dilek Z. Hakkani-Tür, Kemal Oflazer. 1999. A Statistical Information Extraction System for Turkish. Abstract: Information Extraction (IE) is the process of analyzing natural language text or speech, and collecting information about speciied types of entities, relationships, or events, such as marking person, location, and organization names or determining the topic changes. Although in recent years there have been numerous studies in processing Turk-ish text, we are not aware of any studies of developing an IE system for Turkish. Furthermore, neither of these studies on processing Turk-ish text employ statistical techniques due to the problems in nding a training corpus and adapting these techniques to Turkish. As a doctoral thesis, I propose to investigate and develop a statistical information extraction system for Turkish.
- Kemal Oflazer. 1999. Dependency Parsing with an Extended Finite State Approach. Abstract: This paper presents a dependency parsing scheme using an extended finite state approach. The parser augments input representation with "channels" so that links representing syntactic dependency relations among words can be accommodated, and iterates on the input a number of times to arrive at a fixed point. Intermediate configurations violating various constraints of projective dependency representations such as no crossing links, no independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish.
- Kemal Oflazer, S. Nirenburg. 1999. Practical Bootstrapping of Morphological Analyzers. Abstract: This paper presents a semi-automatic technique for developing broad-coverage finite-state morphological analyzers for any language. It consists of three components-elicitation of linguistic information from humans, a machine learning bootstrapping scheme and a testing environment. The three components are applied iteratively until a threshold of output quality is attained. The initial application of this technique is for morphology of low-density languages in the context of the Expedition project at NMSU CRL. This elicitbuild-test technique compiles lexical and inflectional information elicited from a human into a finite state transducer lexicon and combines this with a sequence of morphographemic rewrite rules that is induced using transformation-based learning from the elicited examples. The resulting morphological analyzer is then tested against a test suite, and any corrections are fed back into the learning procedure that builds an improved analyzer.
- G. Tur, Kemal Oflazer. 1998. Tagging English by Path Voting Constraints. Abstract: We describe a constraint-based tagging approach where individual constraint rules vote on sequences of matching tokens and tags. Disambiguation of all tokens in a sentence is performed at the very end by selecting tags that appear on the path that receives the highest vote. This constraint application paradigm makes the outcome of the disambiguation independent of the rule sequence, and hence relieves the rule developer from worrying about potentially conflicting rule sequencing. The approach can also combine statistically and manually obtained constraints, and incorporate negative constraint rules to rule out certain patterns. We have applied this approach to tagging English text from the Wall Street Journal and the Brown Corpora. Our results from the Wall Street Journal Corpus indicate that with 4(}0 statistically derived constraint rules and about 800 hand-crafted constraint rules, we can attain an average accuracy of 97.89~ on the training corpus and an average accuracy of 97.50~o on the testing corpus. We can also relax the single tag per token limitation and allow ambiguous tagging which lets us trade recall and precision. 1 I n t r o d u c t i o n Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information. Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, (e.g., Church (1988), Cutting et al. (1992), DeR,ose (1988)), or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen et al., 1992; Voutilainen and Tapanainen, 1993). Brill (1992; 1994; 1995) has presented a transformationbased learning approach. This paper extends a novel approach to constraint-based tagging first applied for Turkish (Oflazer and Tiir, 1997), which relieves the rule developer from worrying about contlicting rule ordering requirements and constraints. The approach depends on assigning votes to constraints via statistical and/or manual means, and then letting constraints vote on matching sequences on tokens, as depicted in Figure 1. This approach does not reflect the outcome of matching constraints to the set of morphological parses immediately as usually done in constraint-based systems. Only after all applicable rules are applied to a sentence, tokens are disambiguated in parallel. Thus, the outcome of the rule applications is independe,d of the order of rule applications. W1 W2 W3 W4 Wn Tokens tl tl .tl tl . . . tl R1 R3 R2 .-. Rm voting Rules Figure 1: Voting Constraint Rules
- Kemal Oflazer, G. Tur. 1998. Implementing Voting Constraints with Finite State Transducers. Abstract: We describe a constraint-based morphological disambiguation system in which individual constraint rules vote on matching morphological parses followed by its implementation using finite state transducers. Voting constraint rules have a number of desirable properties: The outcome of the disambiguation is independent of the order of application of the local contextual constraint rules. Thus the rule developer is relieved from worrying about conflicting rule sequencing. The approach can also combine statistically and manually obtained constraints, and incorporate negative constraints that rule out certain patterns. The transducer implementation has a number of desirable properties compared to other finite state tagging and light parsing approaches, implemented with automata intersection. The most important of these is that since constraints do not remove parses there is no risk of an overzealous constraint "killing a sentence" by removing all parses of a token during intersection. After a description of our approach we present preliminary results from tagging the Wall Street Journal Corpus with this approach. With about 400 statistically derived constraints and about 570 manual constraints, we can attain an accuracy of 97.82% on the training corpus and 97.29% on the test corpus. We then describe a finite state implementation of our approach and discuss various related issues.
- L. Karttunen, Kemal Oflazer. 1998. Proceedings of the International Workshop on Finite State Methods in Natural Language Processing. Abstract: Recent years has seen a substantial increase in the use of finite state techniques in many aspects of natural language processing as mature tools for building large scale finite-state systems from various research laboratories and universities become available. This trend was by no means foreseen as late as ten years ago given the well-known demonstration by Noam Chomsky in 1957 that finite-state methods are inherently incapable of representing the full richness of constructions in a natural language. Nevertheless, it is evident now that there are many subsets of natural language that are adequately covered by finite-state means and that there are many other areas where finite-state approximations of more powerful formalisms are of great practical benefit. The discovery that systems of phonological rewrite rules and optimality constraints are within the finite-state domain has made an important theoretical leap forward. We expect that similar discoveries are yet to be made in other areas of linguistics. 
 
FSMNLP'98, International Workshop on Finite State Methods in Natural Language Processing was conceived, with support and motivation from EACL, as a forum to bring together recent contributions in all aspects of the theory and applications of finite state machinery in language processing. As you may have already observed by reviewing the workshop programme and by looking at the papers to be presented, the mix of the contributions is international and shows wide-spread interest in this technology. They range over a variety of topics of great interest to the NLP community.
- Gökhan Tür, Kemal Oflazer. 1998. Tagging English by Path Voting Constraints. Abstract: We describe a constraint-based tagging approach where individual constraint rules vote on sequences of matching tokens and tags. Disambiguation of all tokens in a sentence is performed at the very end by selecting tags that appear on the path that receives the highest vote. This constraint application paradigm makes the outcome of the disambiguation independent of the rule sequence, and hence relieves the rule developer from worrying about potentially conflicting rule sequencing. The approach can also combine statistically and manually obtained constraints, and incorporate negative constraint rules to rule out certain patterns. We have applied this approach to tagging English text from the Wall Street Journal and the Brown Corpora. Our results from the Wall Street Journal Corpus indicate that with 400 statistically derived constraint rules and about 800 hand-crafted constraint rules, we can attain an average accuracy of 97.89% on the training corpus and an average accuracy of 97.50% on the testing corpus. We can also relax the single tag per token limitation and allow ambiguous tagging which lets us trade recall and precision.
- Kemal Oflazer, Gökhan Tür. 1997. Morphological Disambiguation by Voting Constraints. Abstract: We present a constraint-based morphological disambiguation system in which individual constraints vote on matching morphological parses, and disambiguation of all the tokens in a sentence is performed at the end by selecting parses that receive the highest votes. This constraint application paradigm makes the outcome of the disambiguation independent of the rule sequence, and hence relieves the rule developer from worrying about potentially conflicting rule sequencing. Our results for disambiguating Turkish indicate that using about 500 constraint rules and some additional simple statistics, we can attain a recall of 95--96% and a precision of 94--95% with about 1.01 parses per token. Our system is implemented in Prolog and we are currently investigating an efficient implementation based on finite state transducers.
- Kemal Oflazer. 1997. Error-Tolerant Retrieval of Trees. Abstract: We present an efficient algorithm for retrieving from a database of trees, all trees that differ from a given query tree by a small number additional or missing leaves, or leaf label changes. It has natural language processing applications in searching for matches in example-based translation systems, and retrieval from lexical databases containing entries of complex feature structures. For large randomly generated synthetic tree databases (some having tens of thousands of trees), and on databases constructed from Wall Street Journal treebank, it can retrieve for trees with a small error, in a matter of tenths of a second to about a second.
- Kemal Oflazer, Gökhan Tür. 1996. Combining Hand-crafted Rules and Unsupervised Learning in Constraint-based Morphological Disambiguation. Abstract: This paper presents a constraint-based morphological disambiguation approach that is applicable languages with complex morphology-specifically agglutinative languages with productive inflectional and derivational morphological phenomena. In certain respects, our approach has been motivated by Brill's recent work (Brill, 1995b), but with the observation that his transformational approach is not directly applicable to languages like Turkish. Our system combines corpus independent handcrafted constraint rules, constraint rules that are learned via unsupervised learning from a training corpus, and additional statistical information from the corpus to be morphologically disambiguated. The hand-crafted rules are linguistically motivated and tuned to improve precision without sacrificing recall. The unsupervised learning process produces two sets of rules: (i) choose rules which choose morphological parses of a lexical item satisfying constraint effectively discarding other parses, and (ii) delete rules, which delete parses satisfying a constraint. Our approach also uses a novel approach to unknown word processing by employing a secondary morphological processor which recovers any relevant inflectional and derivational information from a lexieal item whose root is unknown. With this approach, well below 1% of the tokens remains as unknown in the texts we have experimented with. Our results indicate that by combining these hand-crafted, statistical and learned information sources, we can attain a recall of 96 to 97% with a corresponding precision of 93 to 94%, and ambiguity of 1.02 to 1.03 parses per token. Automatic morphological disambiguation is a very crucial component in higher level analysis of natural language text corpora. Morphological disambiguation facilitates parsing, essentially by performing a certain amount of ambiguity resolution using relatively cheaper methods (e.g., Gfing6rdii and Oflazer (1995)). There has been a large number of studies in tagging and morphological disambiguation using various techniques. Part-of-speech tagging systems have used either a statistical approach where a large corpora has been used to train a probabilistic model which then has been used to tag new text, assigning the most likely tag for a given word in a given context (e.g., Church (1988), Cutting et al. (1992), DeRose (1988)). Another approach is the rule-based or constraint-based approach, recently most prominently exemplified by the Constraint Grammar work (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen et al., 1992; Voutilainen and Tapanainen, 1993), where a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context. Brill (1992; 1994; 1995a) has presented a transformation-based learning approach, which induces rules from tagged corpora. Recently he has extended this work so that learning can proceed in an unsupervised manner using an untagged corpus (Brill, 1995b). Levinger et al. (1995) have recently reported on an approach that learns morpholexical probabilities from untagged corpus and have the used the resulting information in morphological disambiguation in Hebrew. In contrast to languages like English, for which there is a very small number of possible word forms with a given root word, and a small number of tags associated with a given lexical form, languages like Turkish or Finnish with very productive agglutinative morphology where it is possible to produce thousands of forms (or even millions (Hankamer, 1989)) for a given root word, pose a challenging problem for morphological disambiguation. In English, for example, a word such as make or set can be verb
- Kemal Oflazer, Okan Yilmaz. 1996. A Constraint-based Case Frame Lexicon. Abstract: We present a constraint-based case frame lexicon architecture for bi-directional mapping between a syntactic case frame and a semantic frame. The lexicon uses a semantic sense as the basic unit and employs a multi-tiered constraint structure for the resolution of syntactic information into the appropriate senses and/or idiomatic usage. Valency changing transformations such as morphologically marked passivized or causativized forms are handled via lexical rules that manipulate case frames templates. The system has been implemented in a typed-feature system and applied to Turkish.
- Kemal Oflazer. 1996. Error-tolerant Tree Matching. Abstract: This paper presents an efficient algorithm for retrieving from a database of trees, all trees that match a given query tree approximately, that is, within a certain error tolerance. It has natural language processing applications in searching for matches in example-based translation systems, and retrieval from lexical databases containing entries of complex feature structures. The algorithm has been implemented on SparcStations, and for large randomly generated synthetic tree databases (some having tens of thousands of trees) it can associatively search for trees with a small error, in a matter of tenths of a second to few seconds.
- Dilek Z. Hakkani-Tür, Kemal Oflazer. 1996. Tactical Generation in a Free Constituent Order Language. Abstract: This paper describes tactical generation in Turkish, a free constituent order language, in which the order of the constituents may change according to the information structure of the sentences to be generated. In the absence of any information regarding the information structure of a sentence (i.e. topic, focus, background, etc.), the constituents of the sentence obey a default order, but the order is almost freely changeable, depending on the constraints of the text flow or discourse. We have used a recursively structured finite state machine (much like a Recursive Transition Network (RTN)) for handling the changes in constituent order, implemented as a right-linear grammar backbone. Our implementation environment is the GenKit system, developed at Carnegie Mellon University, Center for Machine Translation. Morphological realization has been implemented using an external morphological analysis/generation component which performs concrete morpheme selection and handles morphographemic processes.
- Kemal Oflazer, Okan Yilmaz. 1995. A Constraint-based Case Frame Lexicon Architecture. Abstract: In Turkish, (and possibly in many other languages) verbs often convey several meanings (some totally unrelated) when they are used with subjects, objects, oblique objects, adverbial adjuncts, with certain lexical, morphological, and semantic features, and co-occurrence restrictions. In addition to the usual sense variations due to selectional restrictions on verbal arguments, in most cases, the meaning conveyed by a case frame is idiomatic and not compositional, with subtle constraints. In this paper, we present an approach to building a constraint-based case frame lexicon for use in natural language processing in Turkish, whose prototype we have implemented under the TFS system developed at Univ. of Stuttgart. 
A number of observations that we have made on Turkish have indicated that we need something beyond the traditional transitive and intransitive distinction, and utilize a framework where verb valence is considered as the obligatory co-existence of an arbitrary subset of possible arguments along with the obligatory exclusion of certain others, relative to a verb sense. Additional morphological lexical and semantic constraints on the syntactic constituents organized as a 5-tier constraint hierarchy, are utilized to map a given syntactic structure case-fraame to a specific verb sense.
- Kemal Oflazer. 1995. Error-tolerant Finite-state Recognition with Applications to Morphological Analysis and Spelling Correction. Abstract: This paper presents the notion of error-tolerant recognition with finite-state recognizers along with results from some applications. Error-tolerant recognition enables the recognition of strings that deviate mildly from any string in the regular set recognized by the underlying finite-state recognizer. Such recognition has applications to error-tolerant morphological processing, spelling correction, and approximate string matching in information retrieval. After a description of the concepts and algorithms involved, we give examples from two applications: in the context of morphological analysis, error-tolerant recognition allows misspelled input word forms to be corrected and morphologically analyzed concurrently. We present an application of this to error-tolerant analysis of the agglutinative morphology of Turkish words. The algorithm can be applied to morphological analysis of any language whose morphology has been fully captured by a single (and possibly very large) finite-state transducer, regardless of the word formation processes and morphographemic phenomena involved. In the context of spelling correction, error-tolerant recognition can be used to enumerate candidate correct forms from a given misspelled string within a certain edit distance. Error-tolerant recognition can be applied to spelling correction for any language, if (a) it has a word list comprising all inflected forms, or (b) its morphology has been fully described by a finite-state transducer. We present experimental results for spelling correction for a number of languages. These results indicate that such recognition works very efficiently for candidate generation in spelling correction for many European languages (English, Dutch, French, German, and Italian, among others) with very large word lists of root and inflected forms (some containing well over 200,000 forms), generating all candidate solutions within 10 to 45 milliseconds (with an edit distance of 1) on a SPARCStation 10/41. For spelling correction in Turkish, error-tolerant recognition operating with a (circular) recognizer of Turkish words (with about 29,000 states and 119,000 transitions) can generate all candidate words in less than 20 milliseconds, with an edit distance of 1.
- Kemal Oflazer. 1995. Error-tolerant Finite State Recognition. Abstract: Error-tolerant recognition enables the recognition of strings that deviate slightly from any string in the regular set recognized by the underlying finite state recognizer. In the context of natural language processing, it has applications in error-tolerant morphological analysis, and spelling correction. After a description of the concepts and algorithms involved, we give examples from these two applications: In morphological analysis, error-tolerant recognition allows misspelled input word forms to be corrected, and morphologically analyzed concurrently. The algorithm can be applied to the moiphological analysis of any language whose morphology is fully captured by a single (and possibly very large) finite state transducer, regardless of the word formation processes (such as agglutination or productive compounding) and morphographemic phenomena involved. We present an application to error tolerant analysis of agglutinative morphology of Turkish words. In spelling correction, error-tolerant recognition can be used to enumerate correct candidate forms from a given misspelled string within a certain edit distance. It can be applied to any language whose morphology is fully described by a finite state transducer, or with a word list comprising all inflected forms with very large word lists of root and inflected forms (some containing well over 200,000 forms), generating all candidate solutions within 10 to 45 milliseconds (with edit distance 1) on a SparcStation 10/41. For spelling correction in Turkish, error-tolerant recognition operating with a (circular) recognizer of Turkish words (with about 29,000 states and 119,000 transitions) can generate all candidate words in less than 20 milliseconds (with edit distance 1). Spelling correction using a recognizer constructed from a large word German list that simulates compounding, also indicates that the approach is applicable in such cases.
- H. Altay Güvenir, Kemal Oflazer. 1995. Using a Corpus for Teaching Turkish Morphology. Abstract: This paper reports on the preliminary phase of our ongoing research towards developing an intelligent tutoring environment for Turkish grammar. One of the components of this environment is a corpus search tool which, among other aspects of the language, will be used to present the learner sample sentences along with their morphological analyses. Following a brief introduction to the Turkish language and its morphology, the paper describes the morphological analysis and ambiguity resolution used to construct the corpus used in the search tool. Finally, implementation issues and details involving the user interface of the tool are discussed.
- Grammar Formalism, Kemal Oflazer. 1995. Parsing Turkish Using the Lexical Functional. Abstract: This paper describes our work on parsing Turkish using the lexical-functional grammar formalism (11). This work represents the first effort for wide-coverage syntactic parsing of Turkish. Our implementation is based on Tomita's parser developed at Carnegie Mellon University Center for Machine Translation. The grammar covers a substantial subset of Turkish including structurally simple and complex sentences, and deals with a reasonable amount of word order freeness. The complex agglutinative morphology of Turkish lexical structures is handled using a separate two-level morphological analyzer, which has been incorporated into the syntactic parser. After a discussion of the key relevant issues regarding Turkish grammar, we discuss aspects of our system and present results from our implementation. Our initial results suggest that our system can parse about 82% of the sentences directly and almost all the remaining with very minor pre-editing.
- Kemal Oflazer, Cemaleddin Güzey. 1994. Spelling Correction in Agglutinative Languages. Abstract: This paper presents an approach to spelling correction in agglutinative languages that is based on two-level morphology and a dynamic programming based search algorithm. Spelling correction in agglutinative languages is significantly different than in languages like English. The concept of a word in such languages is much wider that the entries found in a dictionary, owing to {}~productive word formation by derivational and inflectional affixations. After an overview of certain issues and relevant mathematical preliminaries, we formally present the problem and our solution. We then present results from our experiments with spelling correction in Turkish, a Ural--Altaic agglutinative language. Our results indicate that we can find the intended correct word in 95\% of the cases and offer it as the first candidate in 74\% of the cases, when the edit distance is 1.
- Kemal Oflazer, Ilker Kuruöz. 1994. Tagging and Morphological Disambiguation of Turkish Text. Abstract: Automatic text tagging is an important component in higher level analysis of text corpora, and its output can be used in many natural language processing applications. In languages like Turkish or Finnish, with agglutinative morphology, morphological disambiguation is a very crucial process in tagging, as the structures of many lexical forms are morphologically ambiguous. This paper describes a POS tagger for Turkish text based on a full-scale two-level specification of Turkish morphology that is based on a lexicon of about 24,000 root words. This is augmented with a multiword and idiomatic construct recognizer, and most importantly morphological disambiguator based on local neighborhood constraints, heuristics and limited amount of statistical information. The tagger also has functionality for statistics compilation and fine tuning of the morphological analyzer, such as logging erroneous morphological parses, commonly used roots, etc. Preliminary results indicate that the tagger can tag about 98-99% of the texts accurately with very minimal user intervention. Furthermore for sentences morphologically disambiguated with the tagger, an LFG parser developed for Turkish, generates, on the average, 50% less ambiguous parses and parses almost 2.5 times faster. The tagging functionality is not specific to Turkish, and can be applied to any language with a proper morphological analysis interface.
- Z. Gungordu, Kemal Oflazer. 1994. Parsing Turkish with the Lexical Functional Grammar Formalism. Abstract: This paper describes our work on parsing Turkish using the lexical-functional grammar formalism. This work represents the first significant effort for parsing Turkish. Our implementation is based on Tomita's parser developed at Carnegie-Mellon University Center for Machine Translation. The grammar covers a substantial subset of Turkish including simple and complex sentences, and deals with a reasonable amount of word order freeness. The complex agglutinative morphology of Turkish lexical structures is handled using a separate two-level morphological analyzer. After a discussion of key relevant issues regarding Turkish grammar, we discuss aspects of our system and present results from our implementation. Our initial results suggest that our system can parse about 82\% of the sentences directly and almost all the remaining with very minor pre-editing.
- Kemal Oflazer, Cem H. Bozsahin. 1994. An Outline of Turkish Morphology. Abstract: A new and distinct cultivar of Geranium plant named 'Balcolink', characterized by its numerous pink-colored semi-double flowers; upright and trailing growth habit; basal branching; vigor; and medium green leaves with a dark maroon-colored zonation pattern.
- Kemal Oflazer. 1993. Genetic Synthesis of Unsupervised Learning Algorithms. Abstract: This paper presents new unsupervised learning algorithms that have been synthesized using a genetic approach. A set of such learning algorithms has been compared with the classical Kohonen's Algorithm on the Self-Organizing Map and has been found to provide a better performance measure. This study indicates that there exist many unsupervised learning algorithms that lead to an organization similar to that of Kohonen's Algorithm, and that genetic algorithms can be used to search for optimal algorithms and optimal architectures for the unsupervised learning.
- Aysin Solak, Kemal Oflazer. 1993. Design and Implementation of a Spelling Checker for Turkish. Abstract: This paper presents the design and implementation of a spelling checker for Turkish. Turkish is an agglutinative language in which words are formed by affixing a sequence of morphemes to a root word
- Kemal Oflazer. 1993. Solving tangram puzzles: A connectionist approach. Abstract: We present a connectionist approach for solving Tangram puzzles. Tangram is an ancient Chinese puzzle where the object is to decompose a given figure into seven basic geometric figures. One connectionist approach models Tangram pieces and their possible placements and orientations as connectionist neuron units which receive excitatory connections from input units defining the puzzle and lateral inhibitory connections from competing or conflicting units. the network of these connectionist units, operating as a Boltzmann Machine, relaxes into a configuration in which units defining the solution receive no inhibitory input from other units. We present results from an implementation of our model using the Rochester Connectionist Simulator. © 1993 John Wiley & Sons, Inc.
- Kemal Oflazer. 1993. Two-level Description of Turkish Morphology. Abstract: Cet article etudie une description morphologique a deux niveaux de la structure lexicale du turc. Cette description a ete implemantee a l'aide d'un environnement PC-KIMMO et est basee sur un lexique de racine lexicale comprenant 23 OOO racines
- Aysin Solak, Kemal Oflazer. 1992. Parsing Agglutinative Word Structures and Its Application to Spelling Checking for Turkish. Abstract: Most of the research on parsing natural languages has been concerned with English, or with other languages morphologically similar to English. Parsing agglutinative word structures has attracted relatively little attention most probably because agglutinative languages contain word structures of considerable complexity, and parsing words in such languages requires morphological analysis techniques. In this paper, we present the design and implementation of a morphological root-driven parser for Turkish word structures which has been incorporated into a spelling checking kernel for on-line Turkish text. The agglutinative nature of the language and the resulting complex word formations. various phonetic harmony rules and subtle exceptions present certain difficulties not usually encountered in the spelling checking of languages like English and make this a very challenging problem.
- Cem Yüceer, Kemal Oflazer. 1992. A rotation, scaling and translation invariant pattern classification system. Abstract: Presents a hybrid pattern classification system which can classify patterns in a rotation, scaling, and translation invariant manner. The system is based on preprocessing the input image to map it into a rotation, scaling, and translation invariant canonical form, which is then classified by a multilayer feedforward neural net. Results from a number of classification problems are also presented in the paper.<<ETX>>
- E. André, Mark W. Johnson, M. de Rijke, Jason Baldridge, Laura Kallmeyer, M. Riley, S. Bangalore, A. Kehler, Fabio Rinaldi, P. Blackburn, A. Kilgarriff, G. Ritchie, Johan Bos, Kevin Knight, Brian Roark, Antal van den Bosch, Philipp Koehn, J. Rogers, T. Brants, R. Koeling, D. Roth, C. Brew, A. Korhonen, Giorgio Satta, Ted Briscoe, András Kornai, K. Schuler, Razvan C. Bunescu, Emiel Krahmer, F. Sebastiani, J. Burstein, S. Kulick, Julie C. Sedivy, L. Cavedon, Mirella Lapata, Violeta Seretan, Keh-Jiann Chen, Gina-Anne Levow, Stuart M. Shieber, Timothy Chklovski, David Lewis, Advaith Siddharthan, Massimiliano Ciaramita, G. Ligozat, Michel Simard, S. Clark, Jimmy J. Lin, David Smith, Michael Collins, I. Mani, H. Somers, Ann A. Copestake, Christopher Manning, Radu Soricut, J. Curran, D. Marcu, R. Sproat, Ido Dagan, M. Maybury, Amanda Stent, Mona T. Diab, Mandar Mitra, M. Strube, P. Edmonds, Mehryar Mohri, M. Surdeanu, D. Eichmann, Alessandro Moschitti, M. Swerts, T. M. Ellison, M. Nederhof, Kristina Toutanova, K. Erk, A. Nenkova, S. Tseng, George Foster, H. Ney, G. Tur, Pascale Fung, S. Oepen, Sriram Venkatapathy, Claire Gardent, Kemal Oflazer, Clare R. Voss, Josef van Genabith, Tom O'Hara, B. Webber, D. Gildea, Gerald Penn, David Weir, Roxana Girju, Massimo Poesio, R. Wicentowski, Claire Grover, Sameer Pradhan, J. Wiebe, Nizar Habash, D. Prescher, Florian Wolf, S. Harabagiu, A. Prince, Fei Xia, P. Heeman, J. Pustejovsky, Nianwen Xue, J. Hockenmaier, D. R. Scott, Wen-tau Yih, R. Hwa, P. Resnik, Deniz Yuret, Su-Ching Jian, S. Riezler, Fabio Massimo Zanzotto. 1990. Reviewers, Volume 33. Abstract: Jennifer Adair Jennifer Aldrich Angela Baum Michelle Bauml Doris Bergen Gloria Boutte Amanda Branscombe Lorraine Breffni Amy Broemmel Christopher Brown Nancy Brown Deborah Bruns Julie Bullard Jan G. Burcham Kathryn Castle Lori Caudle Christine Chaille Dong Hwa Choi Kenneth Counselman Leslie Couse Kay Cutler Sara Davis Anne Dorsey Angela Eckhoff Roy Evans Beatrice Fennimore Nancy Freeman Doris Fromberg Vicki Garavuso Dawn Garbett Jennifer Gilliard Dierdre Greer Sophia Han Sanna Harjusola-Webb Helen Hedges Julie Herron Rebecca Huss-Keeler Mary Jensen Jim Johnson Ithel Jones Laura Kates Virginia Keen Jill Klefstad Byran Korth Janice Kroeger Vickie Lake Karen LaParo Yuen-Ling Joyce Li Shannon McNair Daniel Meier Monica Miller-Marsh Mary Jane Moran Mark Nagasawa Stacey Neuharth-Pritchett Shelley Nicholson John Nimmo Deborah Norris Nadjwa Norton Cynthia Paris Will Parnell Jean Plaisir Beth Powers-Costello Patricia Ramsey Susan Recchia Stuart Reifel Frances Rust Sharon Ryan Catherine Scott-Little Lynda Kathryn Sharp Meagan Shedd Frances Sherwood Sara Sherwood Mariana Souto-Manning Dolores Stegelin Andrew Stremmel Judit Szente
- Kemal Oflazer. 1987. Partitioning in parallel processing of production systems. Abstract: This thesis presents research on certain issues related to parallel processing of production systems. It first presents a parallel production system interpreter that has been implemented on a four-processor multiprocessor. This parallel interpreter is based on Forgy's OPS5 interpreter and exploits production-level parallelism in production systems. Runs on the multiprocessor system indicate that it is possible to obtain speed-up of around 1.7 in the match computation for certain production systems when productions are split into three sets that are processed in parallel. However for production systems that are already relatively fast on uniprocessors, the communication overhead imposed by the implementation environment essentially offsets any gains when productions are split for parallel match. 
The next issue addressed is that of partitioning a set of rules to processors in a parallel interpreter with production-level parallelism, and the extent of additional improvement in performance. The partitioning problem is formulated and an algorithm for approximate solutions is presented. Simulation results from a number of OPS5 production systems indicate that partitionings using information about run time behaviour of the production systems can improve the match performance by a factor of 1.10 to 1.25, compared to partitionings obtained using various simpler schemes. 
The thesis next presents a parallel processing scheme for OPS5 production systems that allows some redundancy in the match computation. This redundancy enables the processing of a production to be divided into units of medium granularity each of which can be processed in parallel. Subsequently, a parallel processor architecture for implementing the parallel processing algorithm is presented. This architecture is based on an array of simple processors which can be clustered into groups of potentially different sizes, each group processing an affected production during a cycle of execution. Simulation results for a number of production systems indicate that the proposed algorithm performs better than other proposed massively parallel architectures like DADO, or NON-VON that use much larger number of processors. However, for certain systems, the performance is in the same range or sometimes worse than that can be obtained while a parallel interpreter based on Forgy's RETE algorithm such as an interpreter using production-level parallelism implemented on a small number of powerful processors, or an interpreter based on Gupta's parallel version of Forgy's RETE algorithm, implemented on a shared memory multiprocessor with 32 - 64 processors.
- Kemal Oflazer. 1983. Design and implementation of a single-chip 1-D median filter. Abstract: The design and implementation of a VLSI chip for the one-dimensional median filtering operation is presented. The device is designed to operate on 8-bit sample sequences with a window size of five samples. Extensive pipelining and employment of systolic data-flow concepts at the bit level enable the chip to filter at rates up to ten megasamples per second. A configuration for using the chip for approximate two-dimensional median filtering operation is also presented.
- E. Ozkarahan, Kemal Oflazer. 1978. Microprocessor Based Modular Database Processors. Abstract: It is a well accepted fact that the introduction of LSI into computer hardware design has changed the design philosophy. With the LSI's being the basic building blocks of computer structures, it is now possible to achieve modular hardware which can be dynamic or reconfigurable under software guidance. As the database machines are evolving into a new architectural class exploiting cellular and associative features, it is now possible to realize their design in view of the new hardware constraints introduced by the LSI technology. Starting with an existing static database machine design with utilizes a special purpose hardwired microprocessor, various ways of accomplishing a modular dynamic design with off-the-shelf microporocessors are being investigated. Trade-offs are discussed and the details of proposed architectural configurations are given. For each approach considered, the rationale for microprocessor applicability, along with requirements placed on microprocessor characteristics, are specified.
- I. Cervesato, Kemal Oflazer, Mark Stehlik, Khaled A. Harras, Houda Bouamor, Alexander R. W. Cheek, Sabih Bin Wasi. None. 2015 Senior Thesis Project Reports State-of-the-art, Real-time, Interactive Twitter Sentiment Analysis Using Machine Learning . . . . . 1 State-of-the-art, Real-time, Interactive Twitter Sentiment Analysis Using Machine Learning Author Sabih Bin Wasi Table of Contents. Abstract: This technical report collects the final reports of the undergraduate Computer Science majors from the Qatar Campus of Carnegie Mellon University who elected to complete a senior research thesis in the academic year 2014–15 as part of their degree. These projects have spanned the students' entire senior year, during which they have worked closely with their faculty advisors to plan and carry out their projects. This work counts as 18 units of academic credit each semester. In addition to doing the research, the students presented a brief midterm progress report each semester, presented a public poster session in December, presented an oral summary in the year-end campus-wide Meeting of the Minds and submitted a written thesis in May. work and helped me achieve the high system performance on time. Also, I am grateful to my friends Rukhsar Neyaz, Afroz Aziz and Abdullah Zafar for motivating me throughout. This work wouldn't have been possible without the support of my parents who allowed me to fly away from home and always encouraged me to participate in research activities. Abstract With more than 500M tweets sent per day containing opinions and messages, Twitter has become a gold­mine for organizations to monitor their brand reputations and analyze their performance and public sentiments about their products. In this work, we present our efforts in building a machine learning based system for sentiment analysis of Twitter messages. Our system takes as input an arbitrary tweet and assigns it to one of the following classes that best reflects its sentiment: positive, negative or neutral. We adopt a supervised text classification approach and use a combination of published­proven features and novel features. Our system outperforms the state­of­the­art systems, especially those tested on the SemEval 2014 testset. Using our prediction model, we also build an interactive visualization tool that uses standard design principles to provide interpretation and trust to the big­data fed into the system. Our system can handle Twitter sentiment analysis of millions of Tweets in pseudo real­time. Furthermore we give an extra emphasis to the adaptive nature of the tool in order to enable brand managers dig deep into patterns found in visualizations ­­ resulting in effective public sentiment monitoring on social media.
- Kemal Oflazer. 1899. A reconfigurable VLSI architecture for a database processor. Abstract: This work brings together the processing potential offered by regularly structured VLSI processing units and the architecture of a database processor---the Relational Associative Processor (RAP). The main motivations are to integrate a RAP cell processor on a few VLSI chips and improve performance by employing procedures exploiting these VLSI chips and the system level reconfigurability of processing resources. The resulting VLSI database processor consists of parallel processing cells that can be reconfigured into a large processor to execute the hard operations of projection and semijoin efficiently. It is shown that such a configuration can provide 2 to 3 orders of magnitude of performance improvement over previous implementations of the RAP system in the execution of such operations.
