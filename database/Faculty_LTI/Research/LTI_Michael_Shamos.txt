Michael Shamos
Paper count: 40
- Kannan Srinivasan, M. Shamos. 2013. C12) United States Patent. Abstract: 5,848,396 A * 5,918,014 A * 6,085,229 A * 6,286,005 B1 * 6,338,066 B1 * 6,356,879 B2 * 6,430,539 B1 * 12/1998 Gerace ........................ 705/10 6/1999 Robinson .................... 709/219 7/2000 Newman eta!. ............ 709/203 9/2001 Cannon ...................... 707/10
- M. Shamos, Alec Yasinsac. 2012. Realities of E-voting Security. Abstract: There has been substantial debate about e-voting security in the past 10 years. As a result, e-voting has evolved away from lever machines and punched cards, through touchscreen voting machines, to a voter-verifiable paper audit trail phase, and voter-marked paper ballots paired with optical scan tabulators. Still, the debate over e-voting security continues. The guest editors introduce the articles in this special issue, which cover technology aimed at both improving election integrity and providing confidence through postelection audits. These articles advance the e-voting security debate and will contribute to long-term election integrity.
- F. Preparata, M. Shamos, D. Stieren, R. Veale, Joonsoo Choi. 2007. 8 Practical Considerations in Our Approach 7 Testing Software. Abstract: Navy coordinate measuring machines: a study of needs. Simulation of simplicity: a technique to cope with degenerate cases in geometric algorithms. A review of current geometric tolerancing theories and inspection data analysis algorithms. 9 CONCLUSION 13 error from data rounding and coding error remains. But as we noted, imperfect input data is not a problem of the algorithm but calls for algorithms diierent than those designed for \perfect data". As for coding error, we suggest that testing correctness of code is orders of magnitude easier with our exact algorithms than with traditional xed-precision algorithms. 9 Conclusion We propose to implement non-heuristic algorithms motivated by CG, using exact computational techniques. Most of the sources of uncertainty in current metrology will be abolished by our approach. No other approaches in the quest for reference softwares comes close to this guarantee. We also note that exact CG algorithms will also be useful in the larger picture of software testing. To highlight the problems of oating-point computation, recall the running example of computing the minimum annulus for a set of input points. It would seem that the closer the input points are to being co-circular (perfect form), the more devastating an error can be expected in a oating-point computation! We conclude with a parable 3 on the economic value of dimensional metrology. In the manufacture of the tops of aluminium soda cans, it is empirically established that their thickness must be at least 0.03 inch (otherwise, the cans might spontaneously explode) but no more than 0.07 inch (in order for the pop-top mechanism to work). Since the metrological uncertainty in measuring this thickness is 0.01 inch, these can tops are speciied to have a nominal thickness of 0.04 inch. Now, if we could reduce the metrological uncertainty by a further 0.001 inch, leading to a nominal thickness of 0.039 inch, the material savings would be $73 million per year. We call this a parable because the basic message does not depend on the literalness of these numbers. Acknowledgements We are greatly indebted to discussions with Vijay Srinivasan, Ted Hopp and Herb Voelcker, who inform us about the eld of tolerancing and metrology; to J urgen Sellen for his comments and feedback on this paper. 3 We are indebted to Ted Hopp as our source for this. 8 PRACTICAL CONSIDERATIONS IN OUR APPROACH 12 in manufactured discs), we can devise G to generate points taken …
- David Meyers, M. Shamos. 2007. 3.2 Other Uses. Abstract: An automatic contour tiler (CTI) has been designed and implemented for use with planar, simple, possibly concave, non-intersecting \wireloop" contours which are typical in medical applications. Without user interaction or guidance, CTI connects 2D contours into 3D branching structures and then produces the tiles by extracting the surface of the resulting volume. CTI is a perturbing tiler based on Boissonnat's method. Previous ideas are extended by o ering implementation suggestions { above and beyond theoretical considerations { that result in a robust program even in the face of ill-formed contours. CTI is one of a suite of tools written to an NCI standard. This results in very portable code and makes it practical and economical to produce portable tools regardless of the site's local data formats.
- L. Sweeney, M. Shamos. 2004. A Multiparty Computation for Randomly Ordering Players and Making Random Selections. Abstract: Consider a set of players who wish to randomly arrange themselves without a trusted third-party. For example, if there are 3 players, a, b and c, then a trusted third party could order them as abc, acb, bac, bca, cab, or cba. In the absence of a trusted third party, the players want to select one of these permutations for themselves at random. In this writing, a protocol (named “RandomSelect”) is presented using multiplayer computation. From a bag of all possible ways the players could be ordered, RandomSelect provides a means for players to make local choices that when combined, jointly select a permutation randomly. The RandomSelect protocol supports any number (n) of two or more players and computes properly even if n-1 players collude. Communication is O(n) using a broadcast channel. More generally, necessary and sufficient conditions for a class of functions called “RandomOrder” functions are defined. A RandomOrder function uses n inputs to make a random selection of a string from a bag of n! strings where all possible selections are uniformly distributed over the possible inputs and over the strings. Any RandomOrder function can be used in the RandomSelect protocol. Bio-terrorism surveillance is used as an example application.
- P. L. Vora, B. Adida, R. Bucholz, D. L. Dill, D. W. Jones, A. D. Rubin, M. Shamos, M. Yung. 2004. Inside Risks. Abstract: T he recent spate of security issues and allegations of “lost votes” in the U.S. demonstrates the inadequacy of the standards used to evaluate our election systems. The current standards (the FEC Voting Systems Standards) along with the revision being developed by IEEE 1583 (see the article by Deutsch and Berger in last month’s Communications) are poor from another perspective: they establish a single pass/fail threshold for all systems, thereby eliminating incentives for existing suppliers to improve their products and rendering the market unattractive to new entrants. Moreover, they fail to precisely define the properties that should be required of a voting system. Instead, the standards rely on specific designs that are more than 15 years old. These legacy designs handicap promising new approaches, such as the various voter-verified printing schemes. New systems are unnecessarily burdened, while their substantial advantages go unrecognized. A set of well-defined properties would encourage the development and commercialization of better voting systems, especially when combined with objective ways to measure performance with respect to those properties. The overall result would then resemble the quantitative federal ratings for automobiles, where features such as vehicle safety and fuel efficiency form a basis for Consumer Reports-style comparative tables. Similarly, specific performance rating guidelines for different aspects of voting systems would provide meaningful metrics upon which system developers could compete. Decision makers, both regulatory and purchasing, would then be free to establish their own minimums for these metrics. Such a rating system can thus cleanly disentangle the development of the technical evaluation process from the various political and regulatory processes. The Chair of the U.S. Federal Election Assistance Commission (EAC), DeForest B. Soaries, Jr., recently asked the technical community for assistance in determining a new standard. This community is no stranger to the area of voting system properties and standards: a number of authors have tried to characterize requirements, and, in 2002, the Workshop on Election Standards and Technology addressed similar issues. The performance properties for voting systems might include the following: integrity of the votes (both voter verification, “I can check that my vote was captured correctly” and public verification, “anyone can check that all recorded votes were counted correctly”); ballot secrecy (both voter privacy and resistance to vote selling and coercion); robustness (including resistance to denial of service attacks); usability and accuracy (including access for the disabled); and transparency (both of mechanism and election data). The inherent differences in system architectures can be characterized abstractly on two levels. Architectures are first compared by how well each can satisfy the overall properties, then are characterized by the kinds of building blocks they need and by the assumptions they need to make about those blocks. A standard should provide an objective way to measure, for a particular actual system implementation, how well its building block instances ensure the properties required of them by the architecture of that system. Suitable performance evaluation and measurement standards already exist for several types of building blocks: FCC 47CFR shielding and emissions, FIPS rating of tamper-resistant equipment, and the Common Criteria for software. For some properties, objective and repeatable measures of overall performance can be defined. For example, the accuracy of a user interface in capturing voter intent can be experimentally tested in a practical and repeatable manner, with the result expressed as an error rate. “Tiger team” and code review security evaluation (while certainly not foolproof) should play a role along with ordinary reliability testing. Ideally, this process of developing the properties and characterizing architectures would be exceptionally transparent, such as that for Internet RFCs, and would be subject to appropriate peer review. The refinement and adaptation of the measurement techniques would proceed as an ongoing parallel activity. The EAC’s request for assistance is a unique chance to positively affect the quality of our election systems, by tackling this new scientific and technical challenge and building a solid foundation. The aim should be to impact the 2006 elections, though the timing is already tight: the EAC is required to present technical recommendations to the House Administration Committee in April 2005. The technical community is faced with a significant need, a rare opportunity, and a growing urgency for coordinated technical effort in this area. (See www.vspr.org for further details.) c Evaluation of Voting Systems
- F. Preparata, M. Shamos. 1993. Segmenting Handwritten Signatures at Their Perceptually Important Points. Abstract: Abstmct-This correspondence describes a new algorithm for segmenting continuous handwritten signatures sampled by a digithr. The segmentation points are found by means of a two-step procedure. The principal step is to construct a function that weights the perceptual importance of every signature point aceording to its specific neighboring points. The second step points out the various local maxima of this function that correspond where the signature should be segmented. The method is well illustrated and tested on a number of signatures that quire different kinds of segmentation decisions.
- F. Preparata, M. Shamos. 1992. Initializability Consideration in Sequential Machine Synthesis. Abstract: It is shown that a finite state machine, whose state encoding is obtained only to reduce the amount of logic in the final implementation, may not be initializable by a logic simulator or a test generator even when the circuit is functionally initializable (i.e., has synchronizing sequences). A fault simulator or a sequential circuit test generator, that assumes all memory elements initially to be in the unknown state, will be totally ineffective for such a design. Proper consideration for initializability during state assignment and logic optimization can guarantee the success for gate level analysis tools. In this paper, the conditions for initializability of finite state machines are derived and an automatic state assignment algorithm for logic minimality and initializability is given. Experimental results show that, in most cases, this method does not require more hardware than the other methods that may produce an uninitializable design. A partial reset technique, recommended for machines without a synchronizing sequence, is also discussed.
- S. Omohundro, R. Paul, M. Shamos, D. Walters, J. Cowan, J. Crutchfield, Bruce S. McNamara, L. Devroye, S. Harnad, J. Friedman, Jon Louis Bently, R. Finkel, D. R. Hougen. 1990. Geometric Learning Algorithms Geometric Learning Algorithms. Abstract: Emergent computation in the form of geometric learning is central to the development of motor and perceptual systems in biological organisms and promises to have a similar impact on emerging technologies including robotics, vision, speech, and graphics. This paper examines some of the trade-offs involved in different implementation strategies, focussing on the tasks of learning discrete classifications and smooth nonlinear mappings. The trade-offs between local and global representations are discussed, a spectrum of distributed network implementations are examined, and an important source of computational inefficiency is identified. Efficient algorithms based on k-d trees and the Delaunay triangulation are presented and the relevance to biological networks is discussed. Finally, extensions of both the tasks and the implementations are given.
- W. Dodrill, D. Lidtke, Cynthia Brown, M. Shamos, Mary Dee Harris Fosberg, Philip L. Miller. 1981. Plagiarism in computer sciences courses(Panel Discussion). Abstract: What constitutes cheating on programming assignments? What methods can be used to detect cheating? What should be done with offenders? How can cheating be eliminated in programming courses? These are all pertinent questions, but they are directed more towards treating symptoms rather than towards correcting some very fundamental problems.
 How can student interest in computer programming be stimulated? What can be done to reduce the frustrations inherent in writing and debugging code? What should be expected (and what should not be expected) of students taking introductory programming courses? How can individual performance and achievement be measured effectively for grading purposes?
 With critical problems of computer fraud and software theft increasing all the time, making Computer Science students aware of the ethics of the computer industry seems not only appropriate but necessary.
- G. Toussaint, B. Bhattacharya, J. Bentley, M. G. Faust, F. Preparata, S. Akl, M. Shamos, D. McCallum, D. Avis, H. ElGindy, R O Duda, P. Hart. 1981. 7. Acknowledgments 8. References 6. Concluding Remarks. Abstract: Optimal algorithms for computing the minimum distance between two finite planar sets, " Proc. A fast algorithm for the planar convex hull problem, " internal manuscript, [25] B. K. Bhattacharya and G. T. Toussaint. " A time-and-storage efficient implementation of an optimal planar convex hull algorithm, " Divide and conquer for linear expected time, " Inform. A linear algorithm for finding the convex hull of a simple polygon, " Inform. Applications of a two-dimensional hidden-line algorithm to other geometric problems, " Tech. [17] J. L. Bentley, et al., " On the average number of maxima in a set of vectors and applications , " [18] L. Devroye, " A note on finding convex hulls via maximal vectors, " Inform. [20] R. Graham, " An efficient algorithm for determining the convex hull of a planar set, " Inform .It should be noted that if metrics other than the euclidean are used, then the O(n log n) complexity can be reduced for the set-of-points problem. In [13] it is shown that with the L 1 and L ∞ metrics, the maximum distance between two arbitrary sets of points can be computed in O(n) worst-case time in two dimensions. Furthermore, in d dimensions the L ∞ maximum distance can be computed in O(nd) worst-case time. Finally, we note that for the euclidean metric we can obtain O(n) expected running time for any fixed dimensions d with a slight modification of the algorithm MAXDIST-1. Lemma 2.1 generalizes to higher dimensions. Furthermore, the 2 d sets of maximal vectors [17] of S i are a superset of the vertices of the convex hull of S i. Thus in the modified versions of MAXDIST-1 we first find the maximal vector sets S mi of S i , i = 1, 2 and then we use BRUTE-FORCE to compute d max (S m1 , S m2). Bentley et al. [17] have shown that the maximal vectors can be found in O(n) expected time whenever the underlying density f can be written as a d-fold product of densities: f(x 1 ,..., x d) =. These are very general conditions which are satisfied for example for the normal density. Furthermore, Devroye [18] has shown that, under the above condition, if an O(n p) (p ≥ 1) algorithm is used on the resulting set of maximal vectors the algorithm has average complexity O(n). Since computing the maximum distance by BRUTE-FORCE …
- M. Shamos, R. Sproull, C. Thompson. 1979. Doctor of Philosophy Candidate __________ C_l_a_rk __ D_. __ Th_o_mp __ S_on __________________ -+. Abstract: The established techniques of computational complexity can be applied to the new problems posed by very !urge-scale integrated (VLSI) circuits. This thesis develops a "VLSI model of computation" and derives upper and lower bounds on the silicon area and time required to snlve the problems of sorting and discrete Fourier transformation. In particular, the area (A) and time (T) taken by any VLSI chip using any algorithm to perform an N point Fourier transform. must satisfy AT2 > LN/8J2Jog2N. A more general result for both sorting and Fourier transformation is that AT2x :: ncNt+XIog 2 XN), for all X in the range 05x51. Also, the energy dissipated by a VLSI chip during the solution of either of these problems is at least n(N3121og N). The tightness of these bounds is ·demonstrated by the existence of nearly optimal• circuits. This thesis describes both a fast chip (T :: O(log3N), A = O(N2 /!og112N)) based on the shuffle-exchnnge interconnection pattern, and a slow chip (T :: O(N112 ), A = O(N log2N)) based on the mesh pattern. A Complexity Theory for VlSI (Thesis summary) C. D. Thompson 14 September 1979 Carnegie-Mellon University Computer Science Department This research is supported in part by the National Science Foundation under Grant MCS 78,...236-76 and a graduate fellowship, and in part by the Office of Naval Research under Contract N00014-76-C-0370. Table of
- M. Shamos. 1978. Computational geometry.. Abstract: We consider our points in space and generate an “S-coordinate” median hyperplane that will chop this space (and point set) in “half” such that we are left with two regions of space: Pleft defines the points contained entirely to the left of the hyperplane and Pright defines the points entirely to the right of the hyperplane. We can asume that for all points in P, the k-nearest neighbor balls will be generated on each side independently via divide and conquer. Clearly, some such k-nearest neighbor balls will be contained entirely in Pleft or Pright (i.e., does not cross the dividing plane). We know that such balls are correct and should be included in the final solution. Unfortunately, when a ball crosses the dividing plane, the ball may be incorrect. For balls that cross the boundary, there may be a closer k-point on the other side of the plane that was invisible during the divide and conquer step and as such the radius of the ball may need to shrink accordingly. We will need some sort of correction step to correct any such “boundary ball” problems (Figure 1). This correction is a simple point-location problem. Given a collection our collection of balls B in an h-ply system, we want to form a data structure containing our balls such that given a query “q” we can return {Bi : q ∈ Bi} [to our advantage, we know that Bi isn’t too big].
- J. Bentley, M. Shamos. 1978. A Problem in Multivariate Statistics: Algorithm, Data Structure and Applications.. Abstract: Abstract : Problems and applications are investigated which are associated with computing the empirical cumulative distribution function of N points in k-dimensional space and a multidimensional divide-and-conquer technique is employed that gives rise to a compact data structure for geometric and statistical search problems. A large number of important statistical quantities are computed much faster than was previously possible.
- M. Shamos. 1977. Recent technical reports. Abstract: Certain questions concerning the arithmetic complexity of unlvarlate polynomial evaluation are considered. Given an operator which maps polynomials to sets of polynomials, the objective is to investigate the relative savings in arithmetic operations achievable by evaluating some polynomlai h(x) ~ g(f(x)) rather than f(x). The main technical results concern the operator that maps f to the set of nontrlvlal polynomial multiples of f. It is shown that there exist polynomials f, g, and h, with h fg, such that h requires substantlally fewer arithmetic operations than either f or g. However, if the coefficients of f are algebraically independent, then any h fg Is as hard to evaluate as f. Several observations and open questions concerning other operators are discussed.
- M. Shamos, G. Yuval. 1976. Lower bounds from complex function theory. Abstract: We employ elementary results from the theory of several complex variables to obtain a quadratic lower bound on the complexity of computing the mean distance between points in the plane. This problem has 2N inputs and a single output and we show that exactly N(N-1)/2 square roots must be computed by any program over +, -, ×, ÷,) √, log and comparisons, even allowing an arbitrary field of constants. The argument is based on counting the total number of sheets of the Riemann surface of the analytic continuation to the complex domain of the (real) function computed by any algorithm which solves the problem. While finding an exact answer requires O(N2) operations, we show that an ε-approximate solution can be obtained in O(N) time for any ε ≫ 0, even if no square roots are permitted.
- M. Shamos. 1976. Geometry and statistics: problems at the interface. Abstract: In this paper we approach the analysis of statistics algorithms from a geometric viewpoint and use techniques from computational geometry to develop new, fast algorithms for computing familiar statistical quantities. Such fundamental procedures as sorting and selection play an important role in nonparametric estimation as well as in correlation and regression and we use known results to obtain lower bounds on the time required to perform various statistical tests. For some problems, computing the test statistic is NP-hard. While geometric insight is helpful in understanding statistical calculations, the reverse is also true-we employ statistical methods to analyze the average case of geometric al gorithms.
- M. Shamos, Dan Hoey. 1976. Geometric intersection problems. Abstract: We develop optimal algorithms for forming the intersection of geometric objects in the plane and apply them to such diverse problems as linear programming, hidden-line elimination, and wire layout. Given N line segments in the plane, finding all intersecting pairs requires O(N2) time. We give an O(N log N) algorithm to determine whether any two intersect and use it to detect whether two simple plane polygons intersect. We employ an O(N log N) algorithm for finding the common intersection of N half-planes to show that the Simplex method is not optimal. The emphasis throughout is on obtaining upper and lower bounds and relating these results to other problems in computational geometry.
- J. Bentley, M. Shamos. 1976. Divide-and-conquer in multidimensional space. Abstract: We investigate a divide-and-conquer technique in multidimensional space which decomposes a geometric problem on N points in k dimensions into two problems on N/2 points in k dimensions plus a single problem on N points in k−1 dimension. Special structure of the subproblems is exploited to obtain an algorithm for finding the two closest of N points in 0(N log N) time in any dimension. Related results are discussed, along with some conjectures and unsolved geometric problems.
- M. Shamos, Dan Hoey. 1975. Closest-point problems. Abstract: A number of seemingly unrelated problems involving the proximity of N points in the plane are studied, such as finding a Euclidean minimum spanning tree, the smallest circle enclosing the set, k nearest and farthest neighbors, the two closest points, and a proper straight-line triangulation. For most of the problems considered a lower bound of O(N log N) is shown. For all of them the best currently-known upper bound is O(N2) or worse. The purpose of this paper is to introduce a single geometric structure, called the Voronoi diagram, which can be constructed rapidly and contains all of the relevant proximity information in only linear space. The Voronoi diagram is used to obtain O(N log N) algorithms for all of the problems.
- M. Shamos. 1975. Geometric complexity. Abstract: The complexity of a number of fundamental problems in computational geometry is examined and a number of new fast algorithms are presented and analyzed. General methods for obtaining results in geometric complexity are given and upper and lower bounds are obtained for problems involving sets of points, lines, and polygons in the plane. An effort is made to recast classical theorems into a useful computational form and analogies are developed between constructibility questions in Euclidean geometry and computability questions in modern computational complexity.
- M. Shamos, M. Tavel. 1973. An absorber theory of acoustical radiation. Abstract: An absorber theory of acoustical radiation is developed by analogy to Wheeler‐Feynman electrodynamics. Using a method due to Feynman the acoustic index of refraction for a system of soft spherical scatterers is calculated. The effects of these scatterers on an acoustic source are summed to obtain the usual radiation reaction.
- M. Shamos. None. LOWER BOUNDS FROM COMPLEX FUNCTION THEORY t. Abstract: We employ elementary results from the theOr! of several complex variables to obtain a quadratic lower bound on the complexity of computing the mean distance between points in the plane. This problem' has 2N inputs and a single output and we show that exactly N(N-l)/2 square roots must be computed by any program over +,-, x, f, ;-, tog and comparisons, even allowing an arbitrary field of constants. The argument is based on counting the total number of sheets of the Riemann surface of the analytic continuation to the complex domain of the (real) function computed by any algorithm which solves the problem. While finding an exact answer requires O(N 2) operations , we show that an e-approximate solution can be obtained in O(N) time for any E > 0 , even if no square roots are permitted.
- M. Shamos. None. Image Understanding Robust Picture Processing Operators and Their Implementation as Circuits. Abstract: The increasing use of s atellites in image acquisition has made real-time data compression and summary essential. To reduce bandwidth and alleviate the load on land-based computers it is desirable to perform as much picture processing as possible via LSI circuitry aboard the satellite. Such circuits must be able to deal with a wide variety of images and must exhibit a high degree of reliability. In this paper we use some results from the theory of selection networks to produce a family of robust image smoothing operators suitable for LSI implementation. The circuits are (1) decomposable into small functional units, (2) easily testable, and (3) statistically insensitive to spikes or noise in the data.
