Mona Diab
Paper count: 228
- Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues. 2023. Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology. Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.
- A. S. Bergman, Mona T. Diab. 2022. Towards Responsible Natural Language Annotation for the Varieties of Arabic. Abstract: When building NLP models, there is a tendency to aim for broader coverage, often overlooking cultural and (socio)linguistic nuance. In this position paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the inclusion of cultural and linguistic expertise in the process. We present a playbook for responsible dataset creation for polyglossic, multidialectal languages. This work is informed by a study on Arabic annotation of social media content.
- Daniel Licht, Cynthia Gao, Janice Lam, Francisco Guzmán, Mona T. Diab, Philipp Koehn. 2022. Consistent Human Evaluation of Machine Translation across Language Pairs. Abstract: Obtaining meaningful quality scores for machine translation systems through human evaluation remains a challenge given the high variability between human evaluators, partly due to subjective expectations for translation quality for different language pairs. We propose a new metric called XSTS that is more focused on semantic equivalence and a cross-lingual calibration method that enables more consistent assessment. We demonstrate the effectiveness of these novel contributions in large scale evaluation studies across up to 14 language pairs, with translation both into and out of English.
- Badr AlKhamissi, Mona T. Diab. 2022. Meta AI at Arabic Hate Speech 2022: MultiTask Learning with Self-Correction for Hate Speech Classification. Abstract: In this paper, we tackle the Arabic Fine-Grained Hate Speech Detection shared task and demonstrate significant improvements over reported baselines for its three subtasks. The tasks are to predict if a tweet contains (1) Offensive language; and whether it is considered (2) Hate Speech or not and if so, then predict the (3) Fine-Grained Hate Speech label from one of six categories. Our final solution is an ensemble of models that employs multitask learning and a self-consistency correction method yielding 82.7% on the hate speech subtask—reflecting a 3.4% relative improvement compared to previous work.
- Shuguang Chen, Gustavo Aguilar, A. Srinivasan, Mona T. Diab, T. Solorio. 2022. CALCS 2021 Shared Task: Machine Translation for Code-Switched Data. Abstract: To date, efforts in the code-switching literature have focused for the most part on language identification, POS, NER, and syntactic parsing. In this paper, we address machine translation for code-switched social media data. We create a community shared task. We provide two modalities for participation: supervised and unsupervised. For the supervised setting, participants are challenged to translate English into Hindi-English (Eng-Hinglish) in a single direction. For the unsupervised setting, we provide the following language pairs: English and Spanish-English (Eng-Spanglish), and English and Modern Standard Arabic-Egyptian Arabic (Eng-MSAEA) in both directions. We share insights and challenges in curating the"into"code-switching language evaluation data. Further, we provide baselines for all language pairs in the shared task. The leaderboard for the shared task comprises 12 individual system submissions corresponding to 5 different teams. The best performance achieved is 12.67% BLEU score for English to Hinglish and 25.72% BLEU score for MSAEA to English.
- Pedram Hosseini, C. Wolfe, Mona T. Diab, David A. Broniatowski. 2022. GisPy: A Tool for Measuring Gist Inference Score in Text. Abstract: Decision making theories such as Fuzzy-Trace Theory (FTT) suggest that individuals tend to rely on gist, or bottom-line meaning, in the text when making decisions. In this work, we delineate the process of developing GisPy, an opensource tool in Python for measuring the Gist Inference Score (GIS) in text. Evaluation of GisPy on documents in three benchmarks from the news and scientific text domains demonstrates that scores generated by our tool significantly distinguish low vs. high gist documents. Our tool is publicly available to use at: https: //github.com/phosseini/GisPy.
- Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, Marjan Ghazvininejad. 2022. A Review on Language Models as Knowledge Bases. Abstract: Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufficiently large (web) corpus will encode a significant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem a LM should have to fully act as a KB, and review the recent literature with respect to those aspects.
- Jennifer Tracey, Owen Rambow, Michael Arrigo, Claire Cardie, Adam Dalton, H. Dang, Mona T. Diab, Bonnie Dorr, Louise Guthrie, M. Markowska, S. Muresan, Vinodkumar Prabhakaran, Samira Shaikh, T. Strzalkowski, J. Wiebe. 2022. BeSt: The Belief and Sentiment Corpus. Abstract: We present the BeSt corpus, which records cognitive state: who believes what (i.e., factuality), and who has what sentiment towards what. This corpus is inspired by similar source-and-target corpora, specifically MPQA and FactBank. The corpus comprises two genres, newswire and discussion forums, in three languages, Chinese (Mandarin), English, and Spanish. The corpus is distributed through the LDC.
- Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Ves Stoyanov, Xian Li. 2021. Few-shot Learning with Multilingual Language Models. Abstract: Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few-and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model suc-ceeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in 5 languages and find it has limitations similar to comparably sized GPT-3 models.
- Parsa Farinneya, Mohammad Mahdi Torabi pour, Sardar Hamidian, Mona T. Diab. 2021. Active Learning for Rumor Identification on Social Media. Abstract: Social media has emerged as a key channel for seeking information. Online users spend several hours reading, posting, and searching for news on microblogging platforms daily. However, this could act as a double-edged sword especially when not all information online is reliable. Moreover, the inherently unmoderated nature of social media renders identifying unverified information ever more challenging. Most of the existing approaches for rumor tracking are not scalable because of their dependency on a significant amount of labeled data. In this work, we investigate this problem from different angles. We design an ActiveTransfer Learning (ATL) strategy to identify rumors with a limited amount of annotated data. We go beyond that and investigate the impact of leveraging various machine learning approaches in addition to different contextual representations. We discuss the impact of multiple classifiers on a limited amount of annotated data followed by an interactive approach to gradually update the models by adding the least certain samples (LCS) from the pool of unlabeled data. Our proposed Active Learning (AL) strategy achieves faster convergence in terms of the F-score while requiring fewer annotated samples (42% of the whole dataset for the best model).
- Pedram Hosseini, David A. Broniatowski, Mona T. Diab. 2021. Commonsense Knowledge-Augmented Pretrained Language Models for Causal Reasoning Classification. Abstract: Commonsense knowledge can be leveraged 001 for identifying causal relations in text. In this 002 work, we convert triples in ATOMIC 2020 , a wide 003 coverage commonsense reasoning knowledge 004 graph, to natural language text and continually 005 pretrain a BERT pretrained language model. 006 We evaluate the resulting model on answer- 007 ing commonsense reasoning questions. Our 008 results show that a continually pretrained lan- 009 guage model augmented with commonsense 010 reasoning knowledge outperforms our base- 011 line on two commonsense causal reasoning 012 benchmarks, COPA and BCOPA-CE, without 013 additional improvement on the base model or 014 using quality-enhanced data for ﬁne-tuning.
- Peter Hase, Mona T. Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srini Iyer. 2021. Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs. Abstract: Do language models have beliefs about the world? Dennett (1995) famously argues that even thermostats have beliefs, on the view that a belief is simply an informational state decoupled from any motivational state. In this paper, we discuss approaches to detecting when models have beliefs about the world, and we improve on methods for updating model beliefs to be more truthful, with a focus on methods based on learned optimizers or hypernetworks. Our main contributions include: (1) new metrics for evaluating belief-updating methods that focus on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing model updates (SLAG) that improves the performance of learned optimizers, and (3) the introduction of the belief graph, which is a new form of interface with language models that shows the interdependencies between model beliefs. Our experiments suggest that models possess belief-like qualities to only a limited extent, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work. Code is available at https://github.com/peterbhase/SLAG-Belief-Updating
- Adithya Renduchintala, Denise Díaz, Kenneth Heafield, Xian Li, Mona T. Diab. 2021. Gender bias amplification during Speed-Quality optimization in Neural Machine Translation. Abstract: Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.
- Wei-Jen Ko, Ahmed El-Kishky, Adithya Renduchintala, Vishrav Chaudhary, Naman Goyal, Francisco Guzmán, Pascale Fung, Philipp Koehn, Mona T. Diab. 2021. Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data. Abstract: The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.
- Pedram Hosseini, David A. Broniatowski, Mona T. Diab. 2021. Predicting Directionality in Causal Relations in Text. Abstract: In this work, we test the performance of two bidirectional transformer-based language models, BERT and SpanBERT, on predicting directionality in causal pairs in the textual content. Our preliminary results show that predicting direction for inter-sentence and implicit causal relations is more challenging. And, SpanBERT performs better than BERT on causal samples with longer span length. We also introduce CREST which is a framework for unifying a collection of scattered datasets of causal relations.
- Pedram Hosseini, David A. Broniatowski, Mona T. Diab. 2021. Knowledge-Augmented Language Models for Cause-Effect Relation Classification. Abstract: Previous studies have shown the efficacy of knowledge augmentation methods in pretrained language models. However, these methods behave differently across domains and downstream tasks. In this work, we investigate the augmentation of pretrained language models with knowledge graph data in the cause-effect relation classification and commonsense causal reasoning tasks. After automatically verbalizing triples in ATOMIC2020, a wide coverage commonsense reasoning knowledge graph, we continually pretrain BERT and evaluate the resulting model on cause-effect pair classification and answering commonsense causal reasoning questions. Our results show that a continually pretrained language model augmented with commonsense reasoning knowledge outperforms our baselines on two commonsense causal reasoning benchmarks, COPA and BCOPA-CE, and a Temporal and Causal Reasoning (TCR) dataset, without additional improvement in model architecture or using quality-enhanced data for fine-tuning.
- Alexander R. Fabbri, Xiaojian Wu, Srini Iyer, Haoran Li, Mona T. Diab. 2021. AnswerSumm: A Manually-Curated Dataset and Pipeline for Answer Summarization. Abstract: Community Question Answering (CQA) fora such as Stack Overflow and Yahoo! Answers contain a rich resource of answers to a wide range of community-based questions. Each question thread can receive a large number of answers with different perspectives. One goal of answer summarization is to produce a summary that reflects the range of answer perspectives. A major obstacle for this task is the absence of a dataset to provide supervision for producing such summaries. Recent works propose heuristics to create such data, but these are often noisy and do not cover all answer perspectives present. This work introduces a novel dataset of 4,631 CQA threads for answer summarization curated by professional linguists. Our pipeline gathers annotations for all subtasks of answer summarization, including relevant answer sentence selection, grouping these sentences based on perspectives, summarizing each perspective, and producing an overall summary. We analyze and benchmark state-of-the-art models on these subtasks and introduce a novel unsupervised approach for multi-perspective data augmentation that boosts summarization performance according to automatic evaluation. Finally, we propose reinforcement learning rewards to improve factual consistency and answer coverage and analyze areas for improvement.
- T. Solorio, Mahsa Shafaei, C. Smailis, Mona T. Diab, Theodore Giannakopoulos, Heng Ji, Yang Liu, Rada Mihalcea, S. Muresan, I. Kakadiaris. 2021. White Paper: Challenges and Considerations for the Creation of a Large Labelled Repository of Online Videos with Questionable Content. Abstract: Abstract This white paper presents a summary of the discussions regarding critical considerations to develop an extensive repository of online videos annotated with labels indicating questionable content. The main discussion points include: 1) the type of appropriate labels that will result in a valuable repository for the larger AI community; 2) how to design the collection and annotation process, as well as the distribution of the corpus to maximize its potential impact; and, 3) what actions we can take to reduce risk of trauma to annotators.
- Nada AlMarwani, Mona T. Diab. 2021. Discrete Cosine Transform as Universal Sentence Encoder. Abstract: Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While averaging is the most commonly used efficient sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. However, as with most other sentence encoders, the DCT sentence encoder was only evaluated in English. To this end, we utilize DCT encoder to generate universal sentence representation for different languages such as German, French, Spanish and Russian. The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets
- Chunting Zhou, Jiatao Gu, Mona T. Diab, P. Guzmán, Luke Zettlemoyer, Marjan Ghazvininejad. 2020. Detecting Hallucinated Content in Conditional Neural Sequence Generation. Abstract: Neural sequence models can generate highly fluent sentences but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input, which can cause a lack of trust in the model. To better assess the faithfulness of the machine outputs, we propose a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. We also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of our proposed approach -- we obtain an average F1 of around 0.6 across all the benchmark datasets and achieve significant improvements in sentence-level hallucination scoring compared to baseline methods. We also release our annotated data and code for future research at this https URL.
- M. Aminian, Mohammad Sadegh Rasooli, Mona T. Diab. 2020. Multitask Learning for Cross-Lingual Transfer of Broad-coverage Semantic Dependencies. Abstract: We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic parsing as the auxiliary task in our multitask setup. Our annotation projection experiments from English to Czech show that our multitask setup yields 3.1% (4.2%) improvement in labeled F1-score on in-domain (out-of-domain) test set compared to a single-task baseline.
- Esin Durmus, He He, Mona T. Diab. 2020. FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization. Abstract: Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.
- Efsun Sarioglu Kayi, Linyong Nan, Bohan Qu, Mona T. Diab, K. McKeown. 2020. Detecting Urgency Status of Crisis Tweets: A Transfer Learning Approach for Low Resource Languages. Abstract: We release an urgency dataset that consists of English tweets relating to natural crises, along with annotations of their corresponding urgency status. Additionally, we release evaluation datasets for two low-resource languages, i.e. Sinhala and Odia, and demonstrate an effective zero-shot transfer from English to these two languages by training cross-lingual classifiers. We adopt cross-lingual embeddings constructed using different methods to extract features of the tweets, including a few state-of-the-art contextual embeddings such as BERT, RoBERTa and XLM-R. We train classifiers of different architectures on the extracted features. We also explore semi-supervised approaches by utilizing unlabeled tweets and experiment with ensembling different classifiers. With very limited amounts of labeled data in English and zero data in the low resource languages, we show a successful framework of training monolingual and cross-lingual classifiers using deep learning methods which are known to be data hungry. Specifically, we show that the recent deep contextual embeddings are also helpful when dealing with very small-scale datasets. Classifiers that incorporate RoBERTa yield the best performance for English urgency detection task, with F1 scores that are more than 25 points over our baseline classifier. For the zero-shot transfer to low resource languages, classifiers that use LASER features perform the best for Sinhala transfer while XLM-R features benefit the Odia transfer the most.
- Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, K. Krstovski, Mona T. Diab, S. Muresan. 2020. DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking. Abstract: The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating endto- end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction. We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking – multiple propositions, temporal reasoning, and ambiguity and lexical variation – and introduce a resource with these types of claims. Then we present a system designed to be resilient to these “attacks” using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions. We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval.
- Jason Krone, Yi Zhang, Mona T. Diab. 2020. Learning to Classify Intents and Slot Labels Given a Handful of Examples. Abstract: Intent classification (IC) and slot filling (SF) are core components in most goal-oriented dialogue systems. Current IC/SF models perform poorly when the number of training examples per class is small. We propose a new few-shot learning task, few-shot IC/SF, to study and improve the performance of IC and SF models on classes not seen at training time in ultra low resource scenarios. We establish a few-shot IC/SF benchmark by defining few-shot splits for three public IC/SF datasets, ATIS, TOP, and Snips. We show that two popular few-shot learning algorithms, model agnostic meta learning (MAML) and prototypical networks, outperform a fine-tuning baseline on this benchmark. Prototypical networks achieves significant gains in IC performance on the ATIS and TOP datasets, while both prototypical networks and MAML outperform the baseline with respect to SF on all three datasets. In addition, we demonstrate that joint training as well as the use of pre-trained language models, ELMo and BERT in our case, are complementary to these few-shot learning methods and yield further gains.
- Sawsan Alqahtani, Ajay K. Mishra, Mona T. Diab. 2020. A Multitask Learning Approach for Diacritic Restoration. Abstract: In many languages like Arabic, diacritics are used to specify pronunciations as well as meanings. Such diacritics are often omitted in written text, increasing the number of possible pronunciations and meanings for a word. This results in a more ambiguous text making computational processing on such text more difficult. Diacritic restoration is the task of restoring missing diacritics in the written text. Most state-of-the-art diacritic restoration models are built on character level information which helps generalize the model to unseen data, but presumably lose useful information at the word level. Thus, to compensate for this loss, we investigate the use of multi-task learning to jointly optimize diacritic restoration with related NLP problems namely word segmentation, part-of-speech tagging, and syntactic diacritization. We use Arabic as a case study since it has sufficient data resources for tasks that we consider in our joint modeling. Our joint models significantly outperform the baselines and are comparable to the state-of-the-art models that are more complex relying on morphological analyzers and/or a lot more data (e.g. dialectal data).
- M. Aminian, Mohammad Sadegh Rasooli, Mona T. Diab. 2020. Mutlitask Learning for Cross-Lingual Transfer of Semantic Dependencies. Abstract: We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with an annotation projection method. We transfer supervised semantic dependency parse annotations from a rich-resource language to a low-resource language through parallel data, and train a semantic parser on projected data. We make use of supervised syntactic parsing as an auxiliary task in a multitask learning framework, and show that with different multitask learning settings, we consistently improve over the single-task baseline. In the setting in which English is the source, and Czech is the target language, our best multitask model improves the labeled F1 score over the single-task baseline by 1.8 in the in-domain SemEval data (Oepen et al., 2015), as well as 2.5 in the out-of-domain test set. Moreover, we observe that syntactic and semantic dependency direction match is an important factor in improving the results.
- Or Levi, Pedram Hosseini, Mona T. Diab, David A. Broniatowski. 2019. Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues. Abstract: The blurry line between nefarious fake news and protected-speech satire has been a notorious struggle for social media platforms. Further to the efforts of reducing exposure to misinformation on social media, purveyors of fake news have begun to masquerade as satire sites to avoid being demoted. In this work, we address the challenge of automatically classifying fake news versus satire. Previous work have studied whether fake news and satire can be distinguished based on language differences. Contrary to fake news, satire stories are usually humorous and carry some political or social message. We hypothesize that these nuances could be identified using semantic and linguistic cues. Consequently, we train a machine learning method using semantic representation, with a state-of-the-art contextual language model, and with linguistic features based on textual coherence metrics. Empirical evaluation attests to the merits of our approach compared to the language-based baseline and sheds light on the nuances between fake news and satire. As avenues for future work, we consider studying additional linguistic features related to the humor aspect, and enriching the data with current news events, to help identify a political or social message.
- Sardar Hamidian, Mona T. Diab. 2019. GWU NLP at SemEval-2019 Task 7: Hybrid Pipeline for Rumour Veracity and Stance Classification on Social Media. Abstract: Social media plays a crucial role as the main resource news for information seekers online. However, the unmoderated feature of social media platforms lead to the emergence and spread of untrustworthy contents which harm individuals or even societies. Most of the current automated approaches for automatically determining the veracity of a rumor are not generalizable for novel emerging topics. This paper describes our hybrid system comprising rules and a machine learning model which makes use of replied tweets to identify the veracity of the source tweet. The proposed system in this paper achieved 0.435 F-Macro in stance classification, and 0.262 F-macro and 0.801 RMSE in rumor verification tasks in Task7 of SemEval 2019.
- F. Alghamdi, Mona T. Diab. 2019. Leveraging Pretrained Word Embeddings for Part-of-Speech Tagging of Code Switching Data. Abstract: Linguistic Code Switching (CS) is a phenomenon that occurs when multilingual speakers alternate between two or more languages/dialects within a single conversation. Processing CS data is especially challenging in intra-sentential data given state-of-the-art monolingual NLP technologies since such technologies are geared toward the processing of one language at a time. In this paper, we address the problem of Part-of-Speech tagging (POS) in the context of linguistic code switching (CS). We explore leveraging multiple neural network architectures to measure the impact of different pre-trained embeddings methods on POS tagging CS data. We investigate the landscape in four CS language pairs, Spanish-English, Hindi-English, Modern Standard Arabic- Egyptian Arabic dialect (MSA-EGY), and Modern Standard Arabic- Levantine Arabic dialect (MSA-LEV). Our results show that multilingual embedding (e.g., MSA-EGY and MSA-LEV) helps closely related languages (EGY/LEV) but adds noise to the languages that are distant (SPA/HIN). Finally, we show that our proposed models outperform state-of-the-art CS taggers for MSA-EGY language pair.
- Hanan Aldarmaki, Mona T. Diab. 2019. Context-Aware Cross-Lingual Mapping. Abstract: Cross-lingual word vectors are typically obtained by fitting an orthogonal matrix that maps the entries of a bilingual dictionary from a source to a target vector space. Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings. In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity. We incorporate context in the transformation matrix by directly mapping the averaged embeddings of aligned sentences in a parallel corpus. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality.
- Shabnam Tafreshi, Mona T. Diab. 2019. GWU NLP Lab at SemEval-2019 Task 3: EmoContext: Effective Contextual Information in Models for Emotion Detection in Sentence-level in a Multigenre Corpus. Abstract: In this paper we present an emotion classifier model submitted to the SemEval-2019 Task 3: EmoContext. The task objective is to classify emotion (i.e. happy, sad, angry) in a 3-turn conversational data set. We formulate the task as a classification problem and introduce a Gated Recurrent Neural Network (GRU) model with attention layer, which is bootstrapped with contextual information and trained with a multigenre corpus. We utilize different word embeddings to empirically select the most suited one to represent our features. We train the model with a multigenre emotion corpus to leverage using all available training sets to bootstrap the results. We achieved overall %56.05 f1-score and placed 144.
- Nada AlMarwani, Hanan Aldarmaki, Mona T. Diab. 2019. Efficient Sentence Embedding using Discrete Cosine Transform. Abstract: Vector averaging remains one of the most popular sentence embedding methods in spite of its obvious disregard for syntactic structure. While more complex sequential or convolutional networks potentially yield superior classification performance, the improvements in classification accuracy are typically mediocre compared to the simple vector averaging. As an efficient alternative, we propose the use of discrete cosine transform (DCT) to compress word sequences in an order-preserving manner. The lower order DCT coefficients represent the overall feature patterns in sentences, which results in suitable embeddings for tasks that could benefit from syntactic features. Our results in semantic probing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of DCT to preserve word order information.
- Arshit Gupta, Peng Zhang, Garima Lalwani, Mona T. Diab. 2019. CASA-NLU: Context-Aware Self-Attentive Natural Language Understanding for Task-Oriented Chatbots. Abstract: Natural Language Understanding (NLU) is a core component of dialog systems. It typically involves two tasks - Intent Classification (IC) and Slot Labeling (SL), which are then followed by a dialogue management (DM) component. Such NLU systems cater to utterances in isolation, thus pushing the problem of context management to DM. However, contextual information is critical to the correct prediction of intents in a conversation. Prior work on contextual NLU has been limited in terms of the types of contextual signals used and the understanding of their impact on the model. In this work, we propose a context-aware self-attentive NLU (CASA-NLU) model that uses multiple signals over a variable context window, such as previous intents, slots, dialog acts and utterances, in addition to the current user utterance. CASA-NLU outperforms a recurrent contextual NLU baseline on two conversational datasets, yielding a gain of up to 7% on the IC task. Moreover, a non-contextual variant of CASA-NLU achieves state-of-the-art performance on standard public datasets - SNIPS and ATIS.
- Hanan Aldarmaki, Mona T. Diab. 2019. C L ] 8 M ar 2 01 9 Context-Aware Crosslingual Mapping. Abstract: Cross-lingual word vectors are typically obtained by fitting an orthogonal matrix that maps the entries of a bilingual dictionary from a source to a target vector space. Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings. In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity. We incorporate context in the transformation matrix by directly mapping the averaged embeddings of aligned sentences in a parallel corpus. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality.
- M. Aminian, Mohammad Sadegh Rasooli, Mona T. Diab. 2019. Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic Roles. Abstract: We describe a transfer method based on annotation projection to develop a dependency-based semantic role labeling system for languages for which no supervised linguistic information other than parallel data is available. Unlike previous work that presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank.
- Sawsan Alqahtani, Ajay K. Mishra, Mona T. Diab. 2019. Efficient Convolutional Neural Networks for Diacritic Restoration. Abstract: Diacritic restoration has gained importance with the growing need for machines to understand written texts. The task is typically modeled as a sequence labeling problem and currently Bidirectional Long Short Term Memory (BiLSTM) models provide state-of-the-art results. Recently, Bai et al. (2018) show the advantages of Temporal Convolutional Neural Networks (TCN) over Recurrent Neural Networks (RNN) for sequence modeling in terms of performance and computational resources. As diacritic restoration benefits from both previous as well as subsequent timesteps, we further apply and evaluate a variant of TCN, Acausal TCN (A-TCN), which incorporates context from both directions (previous and future) rather than strictly incorporating previous context as in the case of TCN. A-TCN yields significant improvement over TCN for diacritization in three different languages: Arabic, Yoruba, and Vietnamese. Furthermore, A-TCN and BiLSTM have comparable performance, making A-TCN an efficient alternative over BiLSTM since convolutions can be trained in parallel. A-TCN is significantly faster than BiLSTM at inference time (270% 334% improvement in the amount of text diacritized per minute).
- Denis Peskov, Nancy Clarke, Jason Krone, Brigitta Fodor, Yi Zhang, Adel Youssef, Mona T. Diab. 2019. Multi-Domain Goal-Oriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data. Abstract: The need for high-quality, large-scale, goal-oriented dialogue datasets continues to grow as virtual assistants become increasingly wide-spread. However, publicly available datasets useful for this area are limited either in their size, linguistic diversity, domain coverage, or annotation granularity. In this paper, we present strategies toward curating and annotating large scale goal oriented dialogue data. We introduce the MultiDoGO dataset to overcome these limitations. With a total of over 81K dialogues harvested across six domains, MultiDoGO is over 8 times the size of MultiWOZ, the other largest comparable dialogue dataset currently available to the public. Over 54K of these harvested conversations are annotated for intent classes and slot labels. We adopt a Wizard-of-Oz approach wherein a crowd-sourced worker (the “customer”) is paired with a trained annotator (the “agent”). The data curation process was controlled via biases to ensure a diversity in dialogue flows following variable dialogue policies. We provide distinct class label tags for agents vs. customer utterances, along with applicable slot labels. We also compare and contrast our strategies on annotation granularity, i.e. turn vs. sentence level. Furthermore, we compare and contrast annotations curated by leveraging professional annotators vs the crowd. We believe our strategies for eliciting and annotating such a dialogue dataset scales across modalities and domains and potentially languages in the future. To demonstrate the efficacy of our devised strategies we establish neural baselines for classification on the agent and customer utterances as well as slot labeling for each domain.
- Hanan Aldarmaki, Mona T. Diab. 2019. Scalable Cross-Lingual Transfer of Neural Sentence Embeddings. Abstract: We develop and investigate several cross-lingual alignment approaches for neural sentence embedding models, such as the supervised inference classifier, InferSent, and sequential encoder-decoder models. We evaluate three alignment frameworks applied to these models: joint modeling, representation transfer learning, and sentence mapping, using parallel text to guide the alignment. Our results support representation transfer as a scalable approach for modular cross-lingual alignment of neural sentence embeddings, where we observe better performance compared to joint models in intrinsic and extrinsic evaluations, particularly with smaller sets of parallel data.
- Aditi Chaudhary, Siddharth Dalmia, Junjie Hu, Xinjian Li, Austin Matthews, Aldrian Obaja Muis, Naoki Otani, Shruti Rijhwani, Zaid A. W. Sheikh, Nidhi Vyas, Xinyi Wang, Jiateng Xie, Ruochen Xu, Chunting Zhou, Peter J. Jansen, Yiming Yang, Lori S. Levin, Florian Metze, T. Mitamura, David R. Mortensen, Graham Neubig, E. Hovy, A. Black, J. Carbonell, Graham Horwood, Shabnam Tafreshi, Mona T. Diab, Efsun Sarioglu Kayi, N. Farra, K. McKeown. 2019. The ARIEL-CMU Systems for LoReHLT18. Abstract: This paper describes the ARIEL-CMU submissions to the Low Resource Human Language Technologies (LoReHLT) 2018 evaluations for the tasks Machine Translation (MT), Entity Discovery and Linking (EDL), and detection of Situation Frames in Text and Speech (SF Text and Speech).
- Sawsan Alqahtani, Hanan Aldarmaki, Mona T. Diab. 2019. Homograph Disambiguation through Selective Diacritic Restoration. Abstract: Lexical ambiguity, a challenging phenomenon in all natural languages, is particularly prevalent for languages with diacritics that tend to be omitted in writing, such as Arabic. Omitting diacritics leads to an increase in the number of homographs: different words with the same spelling. Diacritic restoration could theoretically help disambiguate these words, but in practice, the increase in overall sparsity leads to performance degradation in NLP applications. In this paper, we propose approaches for automatically marking a subset of words for diacritic restoration, which leads to selective homograph disambiguation. Compared to full or no diacritic restoration, these approaches yield selectively-diacritized datasets that balance sparsity and lexical disambiguation. We evaluate the various selection strategies extrinsically on several downstream applications: neural machine translation, part-of-speech tagging, and semantic textual similarity. Our experiments on Arabic show promising results, where our devised strategies on selective diacritization lead to a more balanced and consistent performance in downstream applications.
- Gustavo Aguilar, F. Alghamdi, Víctor Soto, Mona T. Diab, Julia Hirschberg, T. Solorio. 2018. Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task. Abstract: In the third shared task of the Computational Approaches to Linguistic Code-Switching (CALCS) workshop, we focus on Named Entity Recognition (NER) on code-switched social-media data. We divide the shared task into two competitions based on the English-Spanish (ENG-SPA) and Modern Standard Arabic-Egyptian (MSA-EGY) language pairs. We use Twitter data and 9 entity types to establish a new dataset for code-switched NER benchmarks. In addition to the CS phenomenon, the diversity of the entities and the social media challenges make the task considerably hard to process. As a result, the best scores of the competitions are 63.76% and 71.61% for ENG-SPA and MSA-EGY, respectively. We present the scores of 9 participants and discuss the most common challenges among submissions.
- F. Alghamdi, Mona T. Diab. 2018. WASA: A Web Application for Sequence Annotation. Abstract: Data annotation is an important and necessary task for all NLP applications. Designing and implementing a web-based application that enables many annotators to annotate and enter their input into one central database is not a trivial task. These kinds of web-based applications require a consistent and robust backup for the underlying database and support to enhance the efficiency and speed of the annotation. Also, they need to ensure that the annotations are stored with a minimal amount of redundancy in order to take advantage of the available resources(e.g, storage space). In this paper, we introduce WASA, a web-based annotation system for managing large-scale multilingual Code Switching (CS) data annotation. Although WASA has the ability to perform the annotation for any token sequence with arbitrary tag sets, we will focus on how WASA is used for CS annotation. The system supports concurrent annotation, handles multiple encodings, allows for several levels of management control, and enables quality control measures while seamlessly reporting annotation statistics from various perspectives and at different levels of granularity. Moreover, the system is integrated with a robust language specific date prepossessing tool to enhance the speed and efficiency of the annotation. We describe the annotation and the administration interfaces as well as the backend engine.
- Hanan Aldarmaki, Mona T. Diab. 2018. Evaluation of Unsupervised Compositional Representations. Abstract: We evaluated various compositional models, from bag-of-words representations to compositional RNN-based models, on several extrinsic supervised and unsupervised evaluation benchmarks. Our results confirm that weighted vector averaging can outperform context-sensitive models in most benchmarks, but structural features encoded in RNN models can also be useful in certain classification tasks. We analyzed some of the evaluation datasets to identify the aspects of meaning they measure and the characteristics of the various models that explain their performance variance.
- Shabnam Tafreshi, Mona T. Diab. 2018. Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning. Abstract: Detection and classification of emotion categories expressed by a sentence is a challenging task due to subjectivity of emotion. To date, most of the models are trained and evaluated on single genre and when used to predict emotion in different genre their performance drops by a large margin. To address the issue of robustness, we model the problem within a joint multi-task learning framework. We train this model with a multigenre emotion corpus to predict emotions across various genre. Each genre is represented as a separate task, we use soft parameter shared layers across the various tasks. our experimental results show that this model improves the results across the various genres, compared to a single genre training in the same neural net architecture.
- W. Zaghouani, Sawsan Alqahtani, Mona T. Diab. 2018. Building a Rich Lexical Resource for Standard Arabic. Abstract: Language ambiguity is an inherent characteristic of natural languages. It refers to the phenomenon where an instance can be interpreted in multiple ways. Ambiguity is at the core of the problems faced by natural language processing applications (Obeid et al. 2013). Although humans have the ability to resolve such ambiguity based on their prior knowledge and context, there are instances (sentences, words,... etc) that require multiple readings to resolve it within a context (Hawwari et al. 2013; Diab et al. 2008). The problem of natural language ambiguity is further exacerbated by conventional orthographic decisions where not all phonemes are explicitly represented (Maamouri et al. 2010; Maamouri et al. 2012). Arabic standard orthography is one of these languages that is underspecified for some of the characters such as short vowels, gemination, glottal stops, etc which are collectively represented as diacritics (Zaghouani et al. 2012; Zaghouani et al. 2016). Most typical text in Arabic is rendered undiacritized, i.e. missing explicit short vowels and other diacritics, thereby compounding the natural linguistic ambiguity of the language. Fully orthographically specified Modern standard Arabic (MSA) consists of letters (consonants and long vowels) and diacritic marks. Diacritic marks involve short vowels (u i a), gemination marks, nunation, and the absence of vowels. These diacritics are helpful in denoting the pronunciations and meanings of such underspecified words (Jeblee et al. 2014). A resource that lists words in their typical underspecified form and their corresponding possible meanings would be quite useful for multiple purposes such as building NLP tools, psycho-linguistic and socio-linguistic studies, as well as pedagogical applications. In this abstract, we present a monolingual lexical resource for MSA, which provides for each undiactrized word: various possible diacritic alternatives, part of speech information (POS), and frequency of usage information, in addition to usage examples. It is a large-scale automatically acquired inventory of words from multiple genres. The main objective of this inventory is to explicitly mark undiacritized forms of Arabic words when they are ambiguous. We use the morphological analyzer and disambiguator, MADAMIRA, to generate the desired features: POS, diacritic alternatives, and lemmas. Our lexical resource represents different aspects of ambiguity at the word level: POS (syntactic level) and diacritic alternatives (lexical level). At the syntactic level, ambiguity indicates that the undiacritized word can be given multiple possible POS tags. If there is only one possible POS tag for the undiacritized word, then the word is syntactically unambiguous. For lexical ambiguity, an undiacritized word may have multiple readings due to multiple possible diacritizations or the same diacritized form would have multiple meanings (similar to the bank «financial institution» /bank «river bank», in English). We account for all three ambiguity cases in our presented resource. The absence of diacritics adds an additional layer of ambiguity in MSA. Diacritics help specify the exact meanings or even reduce the number of possible senses for a given undiacritized word. Although this sounds appealing and has proven beneficial in some tasks, full diacritization might also have performance degradation in some natural language processing applications and human reading speed. We observed three types of ambiguity caused by diacritics: ambiguity within POS tags, ambiguity for the same grapheme without considering POS tags, and ambiguity that is related to case and mood information. The former type concerns structural and grammatical level of ambiguity whereas the first two types are lexical which is our focus in this paper. It has been claimed that frequency may play a significant role in disambiguation where words that frequently occur tend to be less ambiguous and that such frequency varies depending on the genre. The presented lexical resource provides three types of frequencies: undiacritized words, diacritized words, diacritized words within a particular POS in addition to fine-grained frequencies for each genre so that researchers would be able to pick certain genres suitable for their studies. This lexical resource shows gaps in the frequency distributions among the alternative choices for each undiacritized word which may lead to having multiple choices for the same undiacritized word that have equal or close frequency approximation.The main objective of this lexical resource is to help lexical-decision making based on explicitly marking within-POS ambiguity which means having multiple diacritic alternatives for the same undiacritized words within a particular POS. It also provides lexical information that is automatically generated including diacritic alternatives, POS, word length, frequencies (within and across varying corpora of different domains and genres) in addition to explicitly marking undiacritized words that have multiple possible POS, as well as it provides usage examples. This resource will be used for readability experiments where we evaluate the impact of ambiguity and level of diacritization in human readings. ReferencesDiab Mona, Aous Mansouri, Martha Palmer, Olga Babko-Malaya,Wajdi Zaghouani, Ann Bies, Mohammed Maamouri. A Pilot Arabic Propbank; LREC 2008, Marrakech, Morocco, May 28-30, 2008.Hawwari, A.; Zaghouani, W.; O»Gorman, T.; Badran, A.; Diab, M., «Building a Lexical Semantic Resource for Arabic Morphological Patterns,» Communications, Signal Processing, and their Applications (ICCSPA), 2013, vol., no., pp.1,6, 12-14 Feb. 2013. Jeblee Serena; Houda Bouamor; Wajdi Zaghouani; Kemal Oflazer. CMUQ@QALB-2014: An SMT-based System for Automatic Arabic Error Correction. In Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing (ANLP), Doha, Qatar, October 2014.Maamouri Mohamed, Ann Bies, Seth Kulick, Wajdi Zaghouani, Dave Graff and Mike Ciul. 2010. From Speech to Trees: Applying Treebank Annotation to Arabic Broadcast News. In Proceedings of LREC 2010, Valetta, Malta, May 17-23, 2010.Maamouri Mohammed, Wajdi Zaghouani, Violetta Cavalli-Sforza, Dave Graff and Mike Ciul. 2012. Developing ARET: An NLP-based Educational Tool Set for Arabic Reading Enhancement. In Proceedings of The 7th Workshop on Innovative Use of NLP for Building Educational Applications, NAACL-HLT 2012, Montreal, Canada.Obeid Ossama, Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Kemal Oflazer and Nadi Tomeh. A Web-based Annotation Framework For Large- Scale Text Correction. In Proceedings of IJCNLP’2013, Nagoya, Japan.Zaghouani Wajdi, Nizar Habash, Ossama Obeid, Behrang Mohit, Houda Bouamor, Kemal Oflazer. 2016. Building an Arabic machine translation post-edited corpus: Guidelines and annotation. In Proceedings of the International Conference on Language Resources and Evaluation (LREC»2016).Zaghouani Wajdi, Abdelati Hawwari and Mona Diab. 2012. A Pilot PropBank Annotation for Quranic Arabic. In Proceedings of the first workshop on Computational Linguistics for Literature, NAACL-HLT 2012, Montreal, Canada.
- Christopher Hidey, Mona T. Diab. 2018. Team SWEEPer: Joint Sentence Extraction and Fact Checking with Pointer Networks. Abstract: Many tasks such as question answering and reading comprehension rely on information extracted from unreliable sources. These systems would thus benefit from knowing whether a statement from an unreliable source is correct. We present experiments on the FEVER (Fact Extraction and VERification) task, a shared task that involves selecting sentences from Wikipedia and predicting whether a claim is supported by those sentences, refuted, or there is not enough information. Fact checking is a task that benefits from not only asserting or disputing the veracity of a claim but also finding evidence for that position. As these tasks are dependent on each other, an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that is relevant. We thus jointly model sentence extraction and verification on the FEVER shared task. Among all participants, we ranked 5th on the blind test set (prior to any additional human evaluation of the evidence).
- Abeer Aldayel, H. Al-Khalifa, S. Alaqeel, N. Abanmy, M. Al-Yahya, Mona T. Diab. 2018. ARC-WMI : Towards Building Arabic Readability Corpus for Written Medicine Information. Abstract: Developing easy-to-read written medicine information continues to be a challenge in health communication. Readability aims to gauge the difficulty level of a text. Various formulas and machine learning algorithms have proposed to judge the readability of health materials and assist writers in identifying possible problems related to text difficulty. For this reason, having corpus annotated with readability levels is fundamental to evaluating the readability formulas and training machine learning algorithms. Arabic suffers from a lack of annotated corpora to evaluate text readability, especially for health materials. To address this shortage, we describe a baseline results towards constructing readability corpus ARC-WMI, a new Arabic collection of written medicine information annotated with readability levels. We compiled a corpus of 4476 sentences with over 61k words, extracted from 94 sources of Arabic written medicine information. These sentences were manually annotated and assigned a readability level (“Easy,” “Intermediate,” or “Difficult”) by a panel of five health-care professionals.
- Shabnam Tafreshi, Mona T. Diab. 2018. Sentence and Clause Level Emotion Annotation, Detection, and Classification in a Multi-Genre Corpus. Abstract: Predicting emotion categories (e.g. anger, joy, sadness) expressed by a sentence is challenging due to inherent multi-label smaller pieces such as phrases and clauses. To date, emotion has been studied in single genre, while models of human behaviors or situational awareness in the event of disasters require emotion modeling in multi-genres. In this paper, we expand and unify existing annotated data in different genres (emotional blog post, news title, and movie reviews) using an inventory of 8 emotions from Plutchik’s Wheel of Emotions tags. We develop systems for automatically detecting and classifying emotions in text, in different textual genres and granularity levels, namely, sentence and clause levels in a supervised setting. We explore the effectiveness of clause annotation in sentence-level emotion detection and classiﬁcation (EDC). To our knowledge, our EDC system is the ﬁrst to target the clause level; further we provide emotion annotation for movie reviews dataset for the ﬁrst time.
- Nada AlMarwani, Mona T. Diab. 2017. GW_QA at SemEval-2017 Task 3: Question Answer Re-ranking on Arabic Fora. Abstract: This paper describes our submission to SemEval-2017 Task 3 Subtask D, “Question Answer Ranking in Arabic Community Question Answering”. In this work, we applied a supervised machine learning approach to automatically re-rank a set of QA pairs according to their relevance to a given question. We employ features based on latent semantic models, namely WTMF, as well as a set of lexical features based on string lengths and surface level matching. The proposed system ranked first out of 3 submissions, with a MAP score of 61.16%.
- Mohamed Al-Badrashiny, A. Hawwari, Mona T. Diab. 2017. A Layered Language Model based Hybrid Approach to Automatic Full Diacritization of Arabic. Abstract: In this paper we present a system for automatic Arabic text diacritization using three levels of analysis granularity in a layered back off manner. We build and exploit diacritized language models (LM) for each of three different levels of granularity: surface form, morphologically segmented into prefix/stem/suffix, and character level. For each of the passes, we use Viterbi search to pick the most probable diacritization per word in the input. We start with the surface form LM, followed by the morphological level, then finally we leverage the character level LM. Our system outperforms all of the published systems evaluated against the same training and test data. It achieves a 10.87% WER for complete full diacritization including lexical and syntactic diacritization, and 3.0% WER for lexical diacritization, ignoring syntactic diacritization.
- Hanan Aldarmaki, Mahesh Mohan, Mona T. Diab. 2017. Unsupervised Word Mapping Using Structural Similarities in Monolingual Embeddings. Abstract: Most existing methods for automatic bilingual dictionary induction rely on prior alignments between the source and target languages, such as parallel corpora or seed dictionaries. For many language pairs, such supervised alignments are not readily available. We propose an unsupervised approach for learning a bilingual dictionary for a pair of languages given their independently-learned monolingual word embeddings. The proposed method exploits local and global structures in monolingual vector spaces to align them such that similar words are mapped to each other. We show empirically that the performance of bilingual correspondents that are learned using our proposed unsupervised method is comparable to that of using supervised bilingual correspondents from a seed dictionary.
- Mona T. Diab, Nizar Habash, I. Zitouni. 2017. NLP for Arabic and Related Languages. Abstract: presented, using FrameNet based tool for semantic annotation. Results conducted on a corpus of motion events expressions conﬁrm the cross-linguistic nature of Frame Semantics approach and the suitability of the theory for Arabic processing.
- Efsun Sarioglu Kayi, Mona T. Diab, L. Pauselli, M. Compton, Glen A. Coppersmith. 2017. Predictive Linguistic Features of Schizophrenia. Abstract: Schizophrenia is one of the most disabling and difficult to treat of all human medical/health conditions, ranking in the top ten causes of disability worldwide. It has been a puzzle in part due to difficulty in identifying its basic, fundamental components. Several studies have shown that some manifestations of schizophrenia (e.g., the negative symptoms that include blunting of speech prosody, as well as the disorganization symptoms that lead to disordered language) can be understood from the perspective of linguistics. However, schizophrenia research has not kept pace with technologies in computational linguistics, especially in semantics and pragmatics. As such, we examine the writings of schizophrenia patients analyzing their syntax, semantics and pragmatics. In addition, we analyze tweets of (self proclaimed) schizophrenia patients who publicly discuss their diagnoses. For writing samples dataset, syntactic features are found to be the most successful in classification whereas for the less structured Twitter dataset, a combination of features performed the best.
- Carlos Ramisch, S. Cordeiro, Agata Savary, V. Vincze, V. Mititelu, Archna Bhatia, Maja Buljan, Marie Candito, P. Gantar, Voula Giouli, Tunga Güngör, A. Hawwari, U. Iñurrieta, J. Kovalevskaite, Simon Krek, Timm Lichte, Chaya Liebeskind, J. Monti, Carla Parra Escartín, Behrang Q. Zadeh, Renata Ramisch, Nathan Schneider, I. Stoyanova, Ashwini Vaidya, Abigail Walsh, Cristina Aceta, I. Aduriz, Jean-Yves Antoine, Špela Arhar Holdt, Gözde Berk, Agnė Bielinskienė, G. Blagus, Loïc Boizou, Claire Bonial, Valeria Caruso, Jaka Cibej, Matthieu Constant, Paul Cook, Mona T. Diab, Tsvetana Dimitrova, Rafael Ehren, Mohamed Elbadrashiny, Hevi Elyovich, Berna Erden, A. Estarrona, A. Fotopoulou, Vassiliki Foufi, Kristina Geeraert, M. V. Gompel, I. Gonzalez, A. Gurrutxaga, Yaakov Ha-Cohen Kerner, R. Ibrahim, M. Ionescu, K. Jain, I. Jazbec, Teja Kavčič, N. Klyueva, Kristina Kocijan, V. Kovacs, Taja Kuzman, S. Leseva, Nikola Ljubesic, R. Malka, Stella Markantonatou, Héctor Martínez Alonso, Ivanka Matas, John P. McCrae, Helena de Medeiros Caseli, M. Onofrei, Emilia Palka-Binkiewicz, Stella Papadelli, Y. Parmentier, A. Pascucci, C. Pasquer, Maria Pia di Buono, Vandana Puri, Annalisa Raffone, Shraddha Ratori, A. Riccio, Federico Sangati, Vishakha Shukla, K. Simkó, J. Šnajder, Clarissa Somers, S. Srivastava, Valentina Stefanova, Shiva Taslimipoor, Natasa Theoxari, Maria Andreeva Todorova, Ruben Urizar, Aline Villavicencio, Leonardo Zilio. 2017. Annotated corpora and tools of the PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions (edition 1.1). Abstract: This multilingual resource contains corpora in which verbal MWEs have been manually annotated. VMWEs include idioms (let the cat out of the bag), light-verb constructions (make a decision), verb-particle constructions (give up), inherently reflexive verbs (help oneself), and multi-verb constructions (make do). VMWEs were annotated according to the universal guidelines in 19 languages. The corpora are provided in the cupt format, inspired by the CONLL-U format. The corpora were used in the 1.1 edition of the PARSEME Shared Task (2018). 
 
For most languages, morphological and syntactic information ­­­­– not necessarily using UD tagsets – including parts of speech, lemmas, morphological features and/or syntactic dependencies are also provided. Depending on the language, the information comes from treebanks (e.g., Universal Dependencies) or from automatic parsers trained on treebanks (e.g., UDPipe). 
 
This item contains training, development and test data, as well as the evaluation tools used in the PARSEME Shared Task 1.1 (2018). 
 
The annotation guidelines are available online: http://parsemefr.lif.univ-mrs.fr/parseme-st-guidelines/1.1
- K. Salih, O. Elfaki, Yassin H M Hamid, Widad M A Eldouch, Mona T. Diab, Sulafa O Abdelgadir. 2017. Prevalence of Helicobacter Pylori among Sudanese children admitted to a specialized children hospital.. Abstract: The infection by Helicobacter Pylori (HP), a gram-negative bacillus, is more prevalent in the developing countries, and more often among younger people reaching up to 10% of the population in comparison to only 0.5% in more developed world. Generally HP is asymptomatic in children. This study aimed to determining the prevalence of Hp among Sudanese children and to recognize associated epidemiological features. It was a prospective cross sectional study at Gaafar Ibn Aouf children hospital (GCH) in the period between December 2010 and May 2011. GCH is the largest specialized referral hospital in Khartoum and in the Sudan. Those who were diagnosed before as Hp positive were excluded. Full history, Physical examination was performed. A blood sample was taken from every patient and serum was kept at -20°C to be tested for Hp IgG antibodies through ELISA kit (Monobind; Inc., California, USA) as directed by the Manufacturer, and 20 U/mL for antibodies was considered significant and positive. Using SPSS Version 21, categorical variables were compared with Chi-square test where P < 0.05 was taken as statistically significant. This current study included 128/312 (40.1%) children admitted to the hospital who were +ve for HP (56.3% = male). The prevalence of HP is 56.3% among Sudanese children Prevalence of HP increased with age and was directly related to mother and father level of education, socioeconomic status and positive family history.
- Daniel Matthew Cer, Mona T. Diab, Eneko Agirre, I. Lopez-Gazpio, Lucia Specia. 2017. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation. Abstract: Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).
- M. Aminian, Mohammad Sadegh Rasooli, Mona T. Diab. 2017. Transferring Semantic Roles Using Translation and Syntactic Information. Abstract: Our paper addresses the problem of annotation projection for semantic role labeling for resource-poor languages using supervised annotations from a resource-rich language through parallel data. We propose a transfer method that employs information from source and target syntactic dependencies as well as word alignment density to improve the quality of an iterative bootstrapping method. Our experiments yield a 3.5 absolute labeled F-score improvement over a standard annotation projection method.
- Nada AlMarwani, Mona T. Diab. 2017. Arabic Textual Entailment with Word Embeddings. Abstract: Determining the textual entailment between texts is important in many NLP tasks, such as summarization, question answering, and information extraction and retrieval. Various methods have been suggested based on external knowledge sources; however, such resources are not always available in all languages and their acquisition is typically laborious and very costly. Distributional word representations such as word embeddings learned over large corpora have been shown to capture syntactic and semantic word relationships. Such models have contributed to improving the performance of several NLP tasks. In this paper, we address the problem of textual entailment in Arabic. We employ both traditional features and distributional representations. Crucially, we do not depend on any external resources in the process. Our suggested approach yields state of the art performance on a standard data set, ArbTE, achieving an accuracy of 76.2 % compared to state of the art of 69.3 %.
- Mohamed Al-Badrashiny, Arfath Pasha, Mona T. Diab, Nizar Habash, Owen Rambow, Wael Salloum, R. Eskander. 2016. SPLIT: Smart Preprocessing (Quasi) Language Independent Tool. Abstract: Text preprocessing is an important and necessary task for all NLP applications. A simple variation in any preprocessing step may drastically affect the final results. Moreover replicability and comparability, as much as feasible, is one of the goals of our scientific enterprise, thus building systems that can ensure the consistency in our various pipelines would contribute significantly to our goals. The problem has become quite pronounced with the abundance of NLP tools becoming more and more available yet with different levels of specifications. In this paper, we present a dynamic unified preprocessing framework and tool, SPLIT, that is highly configurable based on user requirements which serves as a preprocessing tool for several tools at once. SPLIT aims to standardize the implementations of the most important preprocessing steps by allowing for a unified API that could be exchanged across different researchers to ensure complete transparency in replication. The user is able to select the required preprocessing tasks among a long list of preprocessing steps. The user is also able to specify the order of execution which in turn affects the final preprocessing output.
- Heba Elfardy, Mona T. Diab. 2016. CU-GWU Perspective at SemEval-2016 Task 6: Ideological Stance Detection in Informal Text. Abstract: We present a supervised system that uses lexical, sentiment, semantic dictionaries and latent and frame semantic features to identify the stance of a tweeter towards an ideological target. We evaluate the performance of the proposed system on subtask A in SemEval-2016 Task 6: “Detecting Stance in Tweets”. The system yields an average F β =1 score of 63.6% on the task’s test set and has been ranked the 6 th by the task organizers out of 19 judged systems.
- A. Hawwari, Mohammed Attia, Mahmoud A. Ghoneim, Mona T. Diab. 2016. Explicit Fine grained Syntactic and Semantic Annotation of the Idafa Construction in Arabic. Abstract: Idafa in traditional Arabic grammar is an umbrella construction that covers several phenomena including what is expressed in English as noun-noun compounds and Saxon and Norman genitives. Additionally, Idafa participates in some other constructions, such as quantifiers, quasi-prepositions, and adjectives. Identifying the various types of the Idafa construction (IC) is of importance to Natural Language processing (NLP) applications. Noun-Noun compounds exhibit special behavior in most languages impacting their semantic interpretation. Hence distinguishing them could have an impact on downstream NLP applications. The most comprehensive syntactic representation of the Arabic language is the LDC Arabic Treebank (ATB). In the ATB, ICs are not explicitly labeled and furthermore, there is no distinction between ICs of noun-noun relations and other traditional ICs. Hence, we devise a detailed syntactic and semantic typification process of the IC phenomenon in Arabic. We target the ATB as a platform for this classification. We render the ATB annotated with explicit IC labels but with the further semantic characterization which is useful for syntactic, semantic and cross language processing. Our typification of IC comprises 3 main syntactic IC types: FIC, GIC, and TIC, and they are further divided into 10 syntactic subclasses. The TIC group is further classified into semantic relations. We devise a method for automatic IC labeling and compare its yield against the CATiB treebank. Our evaluation shows that we achieve the same level of accuracy, but with the additional fine-grained classification into the various syntactic and semantic types.
- Ayah Zirikly, Bart Desmet, Mona T. Diab. 2016. The GW/LT3 VarDial 2016 Shared Task System for Dialects and Similar Languages Detection. Abstract: This paper describes the GW/LT3 contribution to the 2016 VarDial shared task on the identification of similar languages (task 1) and Arabic dialects (task 2). For both tasks, we experimented with Logistic Regression and Neural Network classifiers in isolation. Additionally, we implemented a cascaded classifier that consists of coarse and fine-grained classifiers (task 1) and a classifier ensemble with majority voting for task 2. The submitted systems obtained state-of-the art performance and ranked first for the evaluation on social media data (test sets B1 and B2 for task 1), with a maximum weighted F1 score of 91.94%.
- Mona T. Diab, Mahmoud A. Ghoneim, A. Hawwari, F. Alghamdi, Nada AlMarwani, Mohamed Al-Badrashiny. 2016. Creating a Large Multi-Layered Representational Repository of Linguistic Code Switched Arabic Data. Abstract: We present our effort to create a large Multi-Layered representational repository of Linguistic Code-Switched Arabic data. The process involves developing clear annotation standards and Guidelines, streamlining the annotation process, and implementing quality control measures. We used two main protocols for annotation: in-lab gold annotations and crowd sourcing annotations. We developed a web-based annotation tool to facilitate the management of the annotation process. The current version of the repository contains a total of 886,252 tokens that are tagged into one of sixteen code-switching tags. The data exhibits code switching between Modern Standard Arabic and Egyptian Dialectal Arabic representing three data genres: Tweets, commentaries, and discussion fora. The overall Inter-Annotator Agreement is 93.1%.
- Mohamed Al-Badrashiny, Mona T. Diab. 2016. The George Washington University System for the Code-Switching Workshop Shared Task 2016. Abstract: We describe our work in the EMNLP 2016 second code-switching shared task; a generic language independent framework for linguistic code switch point detection (LCSPD). The system uses characters level 5-grams and word level unigram language models to train a conditional random fields (CRF) model for classifying input words into various languages. We participated in the Modern Standard Arabic (MSA)-dialectal Arabic (DA) and SpanishEnglish tracks, obtaining a weighted average F-scores of 0.83 and 0.91 on MSA-DA and EN-SP respectively.
- Sardar Hamidian, Mona T. Diab. 2016. Rumor Identification and Belief Investigation on Twitter. Abstract: Social media users spend several hours a day to read, post and search for news on microblogging platforms. Social media is becoming a key means for discovering news. However, verifying the trustworthiness of this information is becoming even more challenging. In this study, we attempt to address the problem of rumor detection and belief investigation on Twitter. Our deﬁnition of rumor is an unveriﬁable statement, which spreads mis-information or disinformation. We adopt a supervised rumors classiﬁcation task using the standard dataset. By employing the Tweet Latent Vector (TLV) feature, which creates a 100-d vector representative of each tweet, we increased the rumor retrieval task precision up to 0.972. We also introduce the belief score and study the belief change among the rumor posters between 2010 and 2016.
- Eneko Agirre, Carmen Banea, Daniel Matthew Cer, Mona T. Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, J. Wiebe. 2016. SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. Abstract: Comunicacio presentada al 10th International Workshop on Semantic Evaluation (SemEval-2016), celebrat els dies 16 i 17 de juny de 2016 a San Diego, California.
- Houda Bouamor, W. Zaghouani, Mona T. Diab, Ossama Obeid, Kemal Oflazer, Mahmoud A. Ghoneim, A. Hawwari. 2016. On Arabic Multi-Genre Corpus Diacritization. Abstract: One of the characteristics of writing in Modern Standard Arabic (MSA) is that the commonly used orthography is mostly consonantal and does not provide full vocalization of the text. It sometimes includes optional diacritical marks (henceforth, diacritics or vowels). Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants. Diacritics on the other hand comprise short vowels, gemination markers, nunation markers, as well as other markers (such as hamza, the glottal stop which appears in conjunction with a small number of letters, dots on letters, elongation and emphatic markers) which in all, if present, render a more or less exact precise reading of a word. In this study, we are mostly addressing three types of diacritical marks: short vowels, nunation, and shadda (gemination). Diacritics are extremely useful for text readability and understanding. Their absence in Arabic text adds another layer of lexical and morphological ambiguity. Naturally occurring Arabic text has some percentage of these diacritics present depending on genre and domain. For instance, religious text such as the Quran is fully diacritized to minimize chances of reciting it incorrectly. So are children's educational texts. Classical poetry tends to be diacritized as well. However, news text and other genre are sparsely diacritized (e.g., around 1.5% of tokens in the United Nations Arabic corpus bear at least one diacritic (Diab et al., 2007)). In general, building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available diacritized MSA corpora are generally limited to the newswire genres (those distributed by the LDC) or religion related texts such as Quran or the Tashkeela corpus. In this paper we present a pilot study where we annotate a sample of non-diacritized text extracted from five different text genres. We explore different annotation strategies where we present the data to the annotator in three modes: basic (only forms with no diacritics), intermediate (basic forms–POS tags), and advanced (a list of forms that is automatically diacritized). We show the impact of the annotation strategy on the annotation quality. It has been noted in the literature that complete diacritization is not necessary for readability Hermena et al. (2015) as well as for NLP applications, in fact, (Diab et al., 2007) show that full diacritization has a detrimental effect on SMT. Hence, we are interested in discovering the optimal level of diacritization. Accordingly, we explore different levels of diacritization. In this work, we limit our study to two diacritization schemes: FULL and MIN. For FULL, all diacritics are explicitly specified for every word. For MIN, we explore what is a minimum and optimal number of diacritics that needs to be added in order to disambiguate a given word in context and make a sentence easily readable and unambiguous for any NLP application. We conducted several experiments on a set of sentences that we extracted from five corpora covering different genres. We selected three corpora from the currently available Arabic Treebanks from the Linguistic Data Consortium (LDC). These corpora were chosen because they are fully diacritized and had undergone significant quality control, which will allow us to evaluate the anno tation accuracy as well as our annotators understanding of the task. We select a total of 16,770 words from these corpora for annotation. Three native Arabic annotators with good linguistic background annotated the corpora samples. Diab et al. (2007), define six different diacritization schemes that are inspired by the observation of the relevant naturally occurring diacritics in different texts. We adopt the FULL diacritization scheme, in which all the diacritics should be specified in a word. Annotators were asked to fully diacritize each word. The text genres were annotated following the different strategies: - Basic: In this mode, we ask for annotation of words where all diacritics are absent, including the naturally occurring ones. The words are presented in a raw tokenized format to the annotators in context. - Intermediate: In this mode, we provide the annotator with words along with their POS information. The intuition behind adding POS is to help the annotator disambiguate a word by narrowing down on the diacritization possibilities. - Advanced: In this mode, the annotation task is formulated as a selection task instead of an editng task. Annotators are provided with a list of automatically diacritized candidates and are asked to choose the correct one, if it appears in the list. Otherwise, if they are not satisfied with the given candidates, they can manually edit the word and add the correct diacritics. This technique is designed in order to reduce annotation time and especially reduce annotator workload. For each word, we generate a list of vowelized candidates using MADAMIRA (Pasha et al., 2014). MADAMIRA is able to achieve a lemmatization accuracy 99.2% and a diacritization accuracy of 86.3%. We present the annotator with the top three candidates suggested by MADAMIRA, when possible. Otherwise, only the available candidates are provided. We also provided annotators with detailed guidelines, describing our diacritization scheme and specifying how to add diacritics for each annotation strategy. We described the annotation procedure and specified how to deal with borderline cases. We also provided in the guidelines many annotated examples to illustrate the various rules and exceptions. In order to determine the most optimized annotation setup for the annotators, in terms of speed and efficiency, we test the results obtained following the three annotation strategies. These annotations are all conducted for the FULL scheme. We first calculated the number of words annotated per hour, for each annotator and in each mode. As expected, following the Advanced mode, our three annotators could annotate an average of 618.93 words per hour which is double those annotated in the Basic mode (only 302.14 words). Adding POS tags to the Basic forms, as in the Intermediate mode, does not accelerate the process much. Only − 90 more words are diacritized per hour compared to the basic mode. Then, we evaluated the Inter-Annotator Agreement (IAA) to quantify the extent to which independent annotators agree on the diacritics chosen for each word. For every text genre, two annotators were asked to annotate independently a sample of 100 words. We measured the IAA between two annotators by averaging WER (Word Error Rate) over all pairs of words. The higher the WER between two annotations, the lower their agreement. The results obtained show clearly that the Advanced mode is the best strategy to adopt for this diacritization task. It is the less confusing method on all text genres (with WER between 1.56 and 5.58). We also conducted a preliminary study for a minimum diacritization scheme. This is a diacritization scheme that encodes the most relevant differentiating diacritics to reduce confusability among words that look the same (homographs) when undiacritized but have different readings. Our hypothesis in MIN is that there is an optimal level of diacritization to render a text unambiguous for processing and enhance its readability. We showed the difficulty in defining such a scheme and how subjective this task can be. Acknowledgement This publication was made possible by grant NPRP-6-1020-1-199 from the Qatar National Research Fund (a member of the Qatar Foundation).
- Hanan Aldarmaki, Mona T. Diab. 2016. Learning Cross-lingual Representations with Matrix Factorization. Abstract: We present a matrix factorization model for learning cross-lingual representations. Using sentence-aligned corpora, the proposed model learns distributed representations by factoring the given data into language-dependent factors and one shared factor. Moreover, the model can quickly learn shared representations for more than two languages without undermining the quality of the monolingual components. The model achieves an accuracy of 88% on English to German cross-lingual document classiﬁcation, and 0.8 Pearson correlation on Spanish-English cross-lingual semantic textual similarity. While the results do not beat state-of-the-art performance in these tasks, we show that the crosslingual models are at least as good as their monolingual counterparts.
- M. Aminian, Mohamed Al-Badrashiny, Mona T. Diab. 2016. Automatic Verification and Augmentation of Multilingual Lexicons. Abstract: We present an approach for automatic verification and augmentation of multilingual lexica. We exploit existing parallel and monolingual corpora to extract multilingual correspondents via tri-angulation. We demonstrate the efficacy of our approach on two publicly available resources: Tharwa, a three-way lexicon comprising Dialectal Arabic, Modern Standard Arabic and English lemmas among other information (Diab et al., 2014); and BabelNet, a multilingual thesaurus comprising over 276 languages including Arabic variant entries (Navigli and Ponzetto, 2012). Our automated approach yields an F1-score of 71.71% in generating correct multilingual correspondents against gold Tharwa, and 54.46% against gold BabelNet without any human intervention.
- F. Alghamdi, Giovanni Molina, Mona T. Diab, T. Solorio, A. Hawwari, Víctor Soto, Julia Hirschberg. 2016. Part of Speech Tagging for Code Switched Data. Abstract: We address the problem of Part of Speech tagging (POS) in the context of linguistic code switching (CS). CS is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances, known as intra-sentential or inter-sentential CS, respectively. Processing CS data is especially challenging in intra-sentential data given state of the art monolingual NLP technology since such technology is geared toward the processing of one language at a time. In this paper we explore multiple strategies of applying state of the art POS taggers to CS data. We investigate the landscape in two CS language pairs, Spanish-English and Modern Standard Arabic-Arabic dialects. We compare the use of two POS taggers vs. a unified tagger trained on CS data. Our results show that applying a machine learning framework using two state of the art POS taggers achieves better performance compared to all other approaches that we investigate.
- Owen Rambow, Daniel Bauer, Axinia Radeva, M. Alagesan, Gregorios A. Katsios, T. Strzalkowski, Claire Cardie, Mona T. Diab, Michael Arrigo, Jennifer Tracey, Adam Dalton, Greg A. Dubbin. 2016. The 2016 TAC KBP BeSt Evaluation. Abstract: This paper provides a summary of the Belief and Sentiment (BeSt) evaluation that was part of the 2016 NIST TAC KBP evaluation. The evaluation is based on the accuracy of adding belief or sentiment links in a knowledge base between existing knowledge base objects. This is the first evaluation to cover both belief and sentiment.
- W. Zaghouani, Houda Bouamor, A. Hawwari, Mona T. Diab, Ossama Obeid, Mahmoud A. Ghoneim, Sawsan Alqahtani, Kemal Oflazer. 2016. Guidelines and Framework for a Large Scale Arabic Diacritized Corpus. Abstract: This paper presents the annotation guidelines developed as part of an effort to create a large scale manually diacritized corpus for various Arabic text genres. The target size of the annotated corpus is 2 million words. We summarize the guidelines and describe issues encountered during the training of the annotators. We also discuss the challenges posed by the complexity of the Arabic language and how they are addressed. Finally, we present the diacritization annotation procedure and detail the quality of the resulting annotations.
- Owen Rambow, Tao Yu, Axinia Radeva, A. R. Fabbri, Christopher Hidey, T. Peng, K. McKeown, S. Muresan, Sardar Hamidian, Mona T. Diab, Debanjan Ghosh. 2016. The Columbia-GWU System at the 2016 TAC KBP BeSt Evaluation. Abstract: We present the components of the ColumbiaGWU contribution to the 2016 TAC KBP BeSt Evaluation.
- Heba Elfardy, Mona T. Diab. 2016. Addressing Annotation Complexity: The Case of Annotating Ideological Perspective in Egyptian Social Media. Abstract: Automatically detecting the stance of people toward political and ideological topics –namely their “Ideological Perspective”– from social media is a rapidly growing research area with a wide range of applications. Research in such a ﬁeld faces several challenges among which is the lack of annotated corpora and associated guidelines for collecting annotations. The problem is even more pronounced in situations where there is no clear taxonomy for the common community perspectives and ideologies. The challenges are exacerbated when the communities where we need to gather these annotations are in a state of turmoil causing subjectivity and intimidation to be factors in the annotation process. Accordingly, we present the process for creating a robust and succinct set of guidelines for annotating “Egyp-tian Ideological Perspectives”. We collect social media data discussing Egyptian politics and develop an iterative feedback annotation framework reﬁning the annotation task and associated guidelines at-tempting to circumvent both weaknesses. Our efforts lead to a signiﬁcant increase in inter-annotator agreement measures from 75.7% to 92% overall agreement.
- Mohamed Al-Badrashiny, Mona T. Diab. 2016. LILI: A Simple Language Independent Approach for Language Identification. Abstract: We introduce a generic Language Independent Framework for Linguistic Code Switch Point Detection. The system uses characters level 5-grams and word level unigram language models to train a conditional random fields (CRF) model for classifying input words into various languages. We test our proposed framework and compare it to the state-of-the-art published systems on standard data sets from several language pairs: English-Spanish, Nepali-English, English-Hindi, Arabizi (Refers to Arabic written using the Latin/Roman script)-English, Arabic-Engari (Refers to English written using Arabic script), Modern Standard Arabic(MSA)-Egyptian, Levantine-MSA, Gulf-MSA, one more English-Spanish, and one more MSA-EGY. The overall weighted average F-score of each language pair are 96.4%, 97.3%, 98.0%, 97.0%, 98.9%, 86.3%, 88.2%, 90.6%, 95.2%, and 85.0% respectively. The results show that our approach despite its simplicity, either outperforms or performs at comparable levels to state-of-the-art published systems.
- Hanan Aldarmaki, Mona T. Diab. 2016. GWU NLP at SemEval-2016 Shared Task 1: Matrix Factorization for Crosslingual STS. Abstract: We present a matrix factorization model for learning cross-lingual representations for sentences. Using sentence-aligned corpora, the proposed model learns distributed representations by factoring the given data into language-dependent factors and one shared factor. As a result, input sentences from both languages can be mapped into fixed-length vectors and then compared directly using the cosine similarity measure, which achieves 0.8 Pearson correlation on Spanish-English semantic textual similarity.
- W. Zaghouani, A. Hawwari, Sawsan Alqahtani, Houda Bouamor, Mahmoud A. Ghoneim, Mona T. Diab, Kemal Oflazer. 2016. Using Ambiguity Detection to Streamline Linguistic Annotation. Abstract: Arabic writing is typically underspecified for short vowels and other markups, referred to as diacritics. In addition to the lexical ambiguity exhibited in most languages, the lack of diacritics in written Arabic adds another layer of ambiguity which is an artifact of the orthography. In this paper, we present the details of three annotation experimental conditions designed to study the impact of automatic ambiguity detection, on annotation speed and quality in a large scale annotation project.
- Mohammed Attia, Ayah Zirikly, Mona T. Diab. 2016. The Power of Language Music: Arabic Lemmatization through Patterns. Abstract: The interaction between roots and patterns in Arabic has intrigued lexicographers and morphologists for centuries. While roots provide the consonantal building blocks, patterns provide the syllabic vocalic moulds. While roots provide abstract semantic classes, patterns realize these classes in specific instances. In this way both roots and patterns are indispensable for understanding the derivational, morphological and, to some extent, the cognitive aspects of the Arabic language. In this paper we perform lemmatization (a high-level lexical processing) without relying on a lookup dictionary. We use a hybrid approach that consists of a machine learning classifier to predict the lemma pattern for a given stem, and mapping rules to convert stems to their respective lemmas with the vocalization defined by the pattern.
- Mohamed Al-Badrashiny, A. Hawwari, Mahmoud A. Ghoneim, Mona T. Diab. 2016. SAMER: A Semi-Automatically Created Lexical Resource for Arabic Verbal Multiword Expressions Tokens Paradigm and their Morphosyntactic Features. Abstract: Although MWE are relatively morphologically and syntactically fixed expressions, several types of flexibility can be observed in MWE, verbal MWE in particular. Identifying the degree of morphological and syntactic flexibility of MWE is very important for many Lexicographic and NLP tasks. Adding MWE variants/tokens to a dictionary resource requires characterizing the flexibility among other morphosyntactic features. Carrying out the task manually faces several challenges since it is a very laborious task time and effort wise, as well as it will suffer from coverage limitation. The problem is exacerbated in rich morphological languages where the average word in Arabic could have 12 possible inflection forms. Accordingly, in this paper we introduce a semi-automatic Arabic multiwords expressions resource (SAMER). We propose an automated method that identifies the morphological and syntactic flexibility of Arabic Verbal Multiword Expressions (AVMWE). All observed morphological variants and syntactic pattern alternations of an AVMWE are automatically acquired using large scale corpora. We look for three morphosyntactic aspects of AVMWE types investigating derivational and inflectional variations and syntactic templates, namely: 1) inflectional variation (inflectional paradigm) and calculating degree of flexibility; 2) derivational productivity; and 3) identifying and classifying the different syntactic types. We build a comprehensive list of AVMWE. Every token in the AVMWE list is lemmatized and tagged with POS information. We then search Arabic Gigaword and All ATBs for all possible flexible matches. For each AVMWE type we generate: a) a statistically ranked list of MWE-lexeme inflections and syntactic pattern alternations; b) An abstract syntactic template; and c) The most frequent form. Our technique is validated using a Golden MWE annotated list. The results shows that the quality of the generated resource is 80.04%.
- Sawsan Alqahtani, Mahmoud A. Ghoneim, Mona T. Diab. 2016. Investigating the Impact of Various Partial Diacritization Schemes on Arabic-English Statistical Machine Translation. Abstract: Most diacritics in Arabic represent short vowels. In Arabic orthography, such diacritics are considered optional. The absence of these diacritics naturally leads to significant word ambiguity to top the inherent ambiguity present in fully diacritized words. Word ambiguity is a significant impediment for machine translation. Despite the ambiguity presented by lack of diacritization, context helps ameliorate the situation. Identifying the appropriate amount of diacritic restoration to reduce word sense ambiguity in the context of machine translation is the object of this paper. Diacritic marks help reduce the number of possible lexical word choices assigned to a source word which leads to better quality translated sentences. We investigate a variety of (linguistically motivated) partial diacritization schemes that preserve some of the semantics that in essence complement the implicit contextual information present in the sentences. We also study the effect of training data size and report results on three standard test sets that represent a combination of different genres. The results show statistically significant improvements for some schemes compared to two baselines: text with no diacritics (the typical writing system adopted for Arabic) and text that is fully diacritized.
- Mona T. Diab. 2016. Processing Dialectal Arabic: Exploiting Variability and Similarity to Overcome Challenges and Discover Opportunities. Abstract: We recently witnessed an exponential growth in dialectal Arabic usage in both textual data and speech recordings especially in social media. Processing such media is of great utility for all kinds of applications ranging from information extraction to social media analytics for political and commercial purposes to building decision support systems. Compared to other languages, Arabic, especially the informal variety, poses a significant challenge to natural language processing algorithms since it comprises multiple dialects, linguistic code switching, and a lack of standardized orthographies, to top its relatively complex morphology. Inherently, the problem of processing Arabic in the context of social media is the problem of how to handle resource poor languages. In this talk I will go over some of our insights to some of these problems and show how there is a silver lining where we can generalize some of our solutions to other low resource language contexts.
- Heba Elfardy, Mona T. Diab, Chris Callison-Burch. 2015. Ideological Perspective Detection Using Semantic Features. Abstract: In this paper, we propose the use of word sense disambiguation and latent semantic features to automatically identify a person’s perspective from his/her written text. We run an Amazon Mechanical Turk experiment where we ask Turkers to answer a set of constrained and open-ended political questions drawn from the American National Election Studies (ANES). We then extract the proposed features from the answers to the open-ended questions and use them to predict the answer to one of the constrained questions, namely, their preferred Presidential Candidate. In addition to this newly created dataset, we also evaluate our proposed approach on a second standard dataset of “Ideological-Debates”. This latter dataset contains topics from four domains: Abortion, Creationism, Gun Rights and GayRights. Experimental results show that using word sense disambiguation and latentsemantics, whether separately or combined, beats the majority and random baselines on the cross-validation and held-out-test sets for both the ANES and the four domains of the “Ideological Debates” datasets. Moreover combining both feature sets outperforms a stronger unigram-only classification system.
- Mohammed Attia, Mohamed Al-Badrashiny, Mona T. Diab. 2015. GWU-HASP-2015@QALB-2015 Shared Task: Priming Spelling Candidates with Probability. Abstract: In this paper, we describe our system HASP-2015 (Hybrid Arabic Spelling and Punctuation Corrector) in which we introduce significant improvements over our previous version HASP-2014 and with which we participated in the QALB2015 Second Shared Task on Arabic Error Correction. Our system utilizes probabilistic information on errors and their possible corrections in the training data and combine that with an open-source reference dictionary (or word list) for detecting errors and generating and filtering candidates. We enhance our system further by allowing it to generate candidates for common semantic and grammatical errors. Eventually, an n-gram language model is used for selecting best candidates. We use a CRF (Conditional Random Fields) classifier for correcting punctuation errors in a two-pass process where first the system learns punctuation placement, and then it learns to identify punctuation types.
- Hanan Aldarmaki, Mona T. Diab. 2015. Robust Part-of-speech Tagging of Arabic Text. Abstract: We present a new and improved part of speech tagger for Arabic text that incorporates a set of novel features and constraints. This framework is presented within the MADAMIRA software suite, a state-of-the-art toolkit for Arabic language processing. Starting from a linear SVM model with basic lexical features, we add a range of features derived from morphological analysis and clustering methods. We show that using these features significantly improves part-of-speech tagging accuracy, especially for unseen words, which results in better generalization across genres. The final model, embedded in a sequential tagging framework, achieved 97.15% accuracy on the main test set of newswire data, which is higher than the current MADAMIRA accuracy of 96.91% while being 30% faster.
- Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Matthew Cer, Mona T. Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I. Lopez-Gazpio, M. Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, J. Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. Abstract: In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new datasets in English and Spanish. The annotations for both subtasks leveraged crowdsourcing. The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs. In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair. The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years. 7 teams participated with 29 runs.
- Sardar Hamidian, Mona T. Diab. 2015. Rumor Detection and Classification for Twitter Data. Abstract: With the pervasiveness of online media data as a source of information verifying the validity of this information is becoming even more important yet quite challenging. Rumors spread a large quantity of misinformation on microblogs. In this study we address two common issues within the context of microblog social media. First we detect rumors as a type of misinformation propagation and next we go beyond detection to perform the task of rumor classification. WE explore the problem using a standard data set. We devise novel features and study their impact on the task. We experiment with various levels of preprocessing as a precursor of the classification as well as grouping of features. We achieve and f-measure of over 0.82 in RDC task in mixed rumors data set and 84 percent in a single rumor data set using a two-step classification approach.
- Houda Bouamor, W. Zaghouani, Mona T. Diab, Ossama Obeid, Kemal Oflazer, Mahmoud A. Ghoneim, A. Hawwari. 2015. A Pilot Study on Arabic Multi-Genre Corpus Diacritization. Abstract: Arabic script writing is typically underspecified for short vowels and other mark up, referred to as diacritics. Apart from the lexical ambiguity found in words, similar to that exhibited in other languages, the lack of diacritics in written Arabic script adds another layer of ambiguity which is an artifact of the orthography. Diacritization of written text has a significant impact on Arabic NLP applications. In this paper, we present a pilot study on building a diacritized multi-genre corpus in Arabic. We annotate a sample of nondiacritized words extracted from five text genres. We explore different annotation strategies: Basic where we present only the bare undiacritized forms to the annotators, Intermediate (Basic forms+their POS tags), and Advanced (automatically diacritized words). We present the impact of the annotation strategy on annotation quality. Moreover, we study different diacritization schemes in the process.
- Gregory Werner, Vinodkumar Prabhakaran, Mona T. Diab, Owen Rambow. 2015. Committed Belief Tagging on the Factbank and LU Corpora: A Comparative Study. Abstract: Level of committed belief is a modality in natural language, it expresses a speak-er/writers belief in a proposition. Initial work exploring this phenomenon in the literature both from a linguistic and computational modeling perspective shows that it is a challenging phenomenon to capture, yet of great interest to several downstream NLP applications. In this work, we focus on identifying relevant features to the task of determining the level of committed belief tagging in two corpora specifically annotated for the phenomenon: the LU corpus and the FactBank corpus. We perform a thorough analysis comparing tagging schemes, infrastructure machinery, feature sets, preprocessing schemes and data genres and their impact on performance in both corpora. Our best results are an F1 score of 75.7 on the FactBank corpus and 72.9 on the smaller LU corpus.
- Ayah Zirikly, Mona T. Diab, Yassine Benajiba. 2015. GWU English TAC-KBP EL Diagnostic Task with Name Mention. Abstract: This paper describes the entity linking system participating in the 2015 Knowledge Base Population (KBP) track at the Text Analysis Conference (TAC) by GWU’s Natural Language Processing (NLP) group (Care4Lang) in collaboration with the NLP consulting company Luki Labs. Our proposed system uses a supervised modeling approach with a feature set that targets the overlapping information between the query and the candidate entities from the KB. In addition, it uses an unsupervised approach to cluster the mentions that don’t have a reference in the KB. It is a first participation for both teams and the attained results are promising and encouraging for further research.
- Mohamed Al-Badrashiny, Heba Elfardy, Mona T. Diab. 2015. AIDA2: A Hybrid Approach for Token and Sentence Level Dialect Identification in Arabic. Abstract: In this paper, we present a hybrid approach for performing token and sentence levels Dialect Identification in Arabic. Specifically we try to identify whether each token in a given sentence belongs to Modern Standard Arabic (MSA), Egyptian Dialectal Arabic (EDA) or some other class and whether the whole sentence is mostly EDA or MSA. The token level component relies on a Conditional Random Field (CRF) classifier that uses decisions from several underlying components such as language models, a named entity recognizer and and a morphological analyzer to label each word in the sentence. The sentence level component uses a classifier ensemble system that relies on two independent underlying classifiers that model different aspects of the language. Using a featureselection heuristic, we select the best set of features for each of these two classifiers. We then train another classifier that uses the class labels and the confidence scores generated by each of the two underlying classifiers to decide upon the final class for each sentence. The token level component yields a new state of the art F-score of 90.6% (compared to previous state of the art of 86.8%) and the sentence level component yields an accuracy of 90.8% (compared to 86.6% obtained by the best state of the art system).
- Ayah Zirikly, Mona T. Diab. 2015. Named Entity Recognition for Arabic Social Media. Abstract: The majority of research on Arabic Named Entity Recognition (NER) addresses the the task for newswire genre, where the language used is Modern Standard Arabic (MSA), however, the need to study this task in social media is becoming more vital. Social media is characterized by the use of both MSA and Dialectal Arabic (DA), with often code switching between the two language varieties. Despite some common characteristics between MSA and DA, there are significant differences between which result in poor performance when MSA targeting systems are applied for NER in DA. Additionally, most NER systems rely primarily on gazetteers, which can be more challenging in a social media processing context due to an inherent low coverage. In this paper, we present a gazetteers-free NER system for Dialectal data that yields an F1 score of 72.68% which is an absolute improvement of 2 3% over a comparable state-ofthe-art gazetteer based DA-NER system.
- Vinodkumar Prabhakaran, Tomas By, Julia Hirschberg, Owen Rambow, Samira Shaikh, T. Strzalkowski, Jennifer Tracey, Michael Arrigo, Rupayan Basu, M. Clark, Adam Dalton, Mona T. Diab, Louise Guthrie, A. Prokofieva, S. Strassel, Gregory Werner, Y. Wilks, J. Wiebe. 2015. A New Dataset and Evaluation for Belief/Factuality. Abstract: The terms “belief” and “factuality” both refer to the intention of the writer to present the propositional content of an utterance as firmly believed by the writer, not firmly believed, or having some other status. This paper presents an ongoing annotation effort and an associated evaluation.
- M. Aminian, Mahmoud A. Ghoneim, Mona T. Diab. 2015. Unsupervised False Friend Disambiguation Using Contextual Word Clusters and Parallel Word Alignments. Abstract: Lexical false friends (FF) are the phenomena where words that look the same, do not have the same meaning or lexical usage. FF impose several challenges to statistical machine translation. We present a methodology which exploits word context modeling as well as information provided by word alignments for identifying false friends and choosing the right sense for them in the context. We show that our approach enhances SMT lexical choice for false friends across language variants. We demonstrate that our approach reduces word error rate (WER) and position independent error rate (PER) for Egyptian-English SMT by 0.6% and 0.1% compared to the baseline.
- Mohammed Attia, Mohamed Al-Badrashiny, Mona T. Diab. 2014. GWU-HASP: Hybrid Arabic Spelling and Punctuation Corrector. Abstract: In this paper, we describe our Hybrid Arabic Spelling and Punctuation Corrector (HASP). HASP was one of the systems participating in the QALB-2014 Shared Task on Arabic Error Correction. The system uses a CRF (Conditional Random Fields) classifier for correcting punctuation errors, an open-source dictionary (or word list) for detecting errors and generating and filtering candidates, an n-gram language model for selecting the best candidates, and a set of deterministic rules for text normalization (such as removing diacritics and kashida and converting Hindi numbers into Arabic numerals). We also experiment with word alignment for spelling correction at the character level and report some preliminary results.
- M. Aminian, Mahmoud A. Ghoneim, Mona T. Diab. 2014. Handling OOV Words in Dialectal Arabic to English Machine Translation. Abstract: Dialects and standard forms of a language typically share a set of cognates that could bear the same meaning in both varieties or only be shared homographs but serve as faux amis. Moreover, there are words that are used exclusively in the dialect or the standard variety. Both phenomena, faux amis and exclusive vocabulary, are considered out of vocabulary (OOV) phenomena. In this paper, we present this problem of OOV in the context of machine translation. We present a new approach for dialect to English Statistical Machine Translation (SMT) enhancement based on normalizing dialectal language into standard form to provide equivalents to address both aspects of the OOV problem posited by dialectal language use. We specifically focus on Arabic to English SMT. We use two publicly available dialect identification tools: AIDA and MADAMIRA, to identify and replace dialectal Arabic OOV words with their modern standard Arabic (MSA) equivalents. The results of evaluation on two blind test sets show that using AIDA to identify and replace MSA equivalents enhances translation results by 0.4% absolute BLEU (1.6% relative BLEU) and using MADAMIRA achieves 0.3% absolute BLEU (1.2% relative BLEU) enhancement over the baseline. We show our replacement scheme reaches a noticeable enhancement in SMT performance for faux amis words.
- Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Matthew Cer, Mona T. Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, J. Wiebe. 2014. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. Abstract: In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline mappings. For Spanish, since, to our knowledge, this is the first time that official evaluations are conducted, we used well-formed text, by featuring sentences extracted from encyclopedic content and newswire. The annotations for both tasks leveraged crowdsourcing. The Spanish subtask engaged 9 teams participating with 22 system runs, and the English subtask attracted 15 teams with 38 system runs.
- Heba Elfardy, Mohamed Al-Badrashiny, Mona T. Diab. 2014. A hybrid system for code switch point detection in informal Arabic text. Abstract: How to detect the switch between a standard and a dialectal form of a language in written text and why this is important for natural language processing tasks.
- Wael Salloum, Heba Elfardy, Linda Alamir-Salloum, Nizar Habash, Mona T. Diab. 2014. Sentence Level Dialect Identification for Machine Translation System Selection. Abstract: In this paper we study the use of sentencelevel dialect identification in optimizing machine translation system selection when translating mixed dialect input. We test our approach on Arabic, a prototypical diglossic language; and we optimize the combination of four different machine translation systems. Our best result improves over the best single MT system baseline by 1.0% BLEU and over a strong system selection baseline by 0.6% BLEU on a blind test set.
- Heba Elfardy, Mohamed Al-Badrashiny, Mona T. Diab. 2014. AIDA: Identifying Code Switching in Informal Arabic Text. Abstract: In this paper, we present the latest version of our system for identifying linguistic code switching in Arabic text. The system relies on Language Models and a tool for morphological analysis and disambiguation for Arabic to identify the class of each word in a given sentence. We evaluate the performance of our system on the test datasets of the shared task at the EMNLP workshop on Computational Approaches to Code Switching (Solorio et al., 2014). The system yields an average token-level F =1 score of 93.6%, 77.7% and 80.1%, on the first, second, and surprise-genre test-sets, respectively, and a tweet-level F =1 score of 4.4%, 36% and 27.7%, on the same test-sets.
- A. Hawwari, Mohammed Attia, Mona T. Diab. 2014. A Framework for the Classification and Annotation of Multiword Expressions in Dialectal Arabic. Abstract: In this paper we describe a framework for classifying and annotating Egyptian Arabic Multiword Expressions (EMWE) in a specialized computational lexical resource. The framework intends to encompass comprehensive linguistic information for each MWE including: a. phonological and orthographic information; b. POS tags; c. structural information for the phrase structure of the expression; d. lexicographic classification; e. semantic classification covering semantic fields and semantic relations; f. degree of idiomaticity where we adopt a three-level rating scale; g. pragmatic information in the form of usage labels; h. Modern Standard Arabic equivalents and English translations, thereby rendering our resource a three-way ‐ Egyptian Arabic, Modern Standard Arabic and English ‐ repository for MWEs.
- Mona T. Diab, Mohamed Al-Badrashiny, M. Aminian, Mohammed A. Attia, Heba Elfardy, Nizar Habash, A. Hawwari, Wael Salloum, Pradeep Dasigi, R. Eskander. 2014. Tharwa: A Large Scale Dialectal Arabic - Standard Arabic - English Lexicon. Abstract: We introduce an electronic three-way lexicon, Tharwa, comprising Dialectal Arabic, Modern Standard Arabic and English correspondents. The paper focuses on Egyptian Arabic as the first pilot dialect for the resource, with plans to expand to other dialects of Arabic in later phases of the project. We describe Tharwas creation process and report on its current status. The lexical entries are augmented with various elements of linguistic information such as POS, gender, rationality, number, and root and pattern information. The lexicon is based on a compilation of information from both monolingual and bilingual existing resources such as paper dictionaries and electronic, corpus-based dictionaries. Multiple levels of quality checks are performed on the output of each step in the creation process. The importance of this lexicon lies in the fact that it is the first resource of its kind bridging multiple variants of Arabic with English. Furthermore, it is a wide coverage lexical resource containing over 73,000 Egyptian entries. Tharwa is publicly available. We believe it will have a significant impact on both Theoretical Linguistics as well as Computational Linguistics research.
- T. Solorio, Elizabeth Blair, Suraj Maharjan, Steven Bethard, Mona T. Diab, Mahmoud A. Ghoneim, A. Hawwari, F. Alghamdi, Julia Hirschberg, Alison Chang, Pascale Fung. 2014. Overview for the First Shared Task on Language Identification in Code-Switched Data. Abstract: We present an overview of the ﬁrst shared task on language identiﬁcation on code-switched data. The shared task included code-switched data from four language pairs: Modern Standard Arabic-Dialectal Arabic (MSA-DA), Mandarin-English (MAN-EN), Nepali-English (NEP-EN), and Spanish-English (SPA-EN). A total of seven teams participated in the task and submitted 42 system runs. The evaluation showed that language identiﬁcation at the token level is more difﬁcult when the languages present are closely related, as in the case of MSA-DA, where the prediction performance was the lowest among all language pairs. In contrast, the language pairs with the higest F-measure where SPA-EN and NEP-EN. The task made evident that language identiﬁcation in code-switched data is still far from solved and warrants further research.
- Weiwei Guo, W. Liu, Mona T. Diab. 2014. Fast Tweet Retrieval with Compact Binary Codes. Abstract: The most widely used similarity measure in the field of natural language processing may be cosine similarity. However, in the context of Twitter, the large scale of massive tweet data inevitably makes it expensive to perform cosine similarity computations among tremendous data samples. In this paper, we exploit binary coding to tackle the scalability issue, which compresses each data sample into a compact binary code and hence enables highly efficient similarity computations via Hamming distances between the generated codes. In order to yield semantics sensitive binary codes for tweet data, we design a binarized matrix factorization model and further improve it in two aspects. First, we force the projection directions employed by the model nearly orthogonal to reduce the redundant information in their resulting binary bits. Second, we leverage the tweets’ neighborhood information to encourage similar tweets to have adjacent binary codes. Evaluated on a tweet dataset using hashtags to create gold labels in an information retrieval scenario, our proposed model shows significant performance gains over competing methods.
- Giovanni Molina, F. Alghamdi, Mahmoud A. Ghoneim, A. Hawwari, Nicolas Rey-Villamizar, Mona T. Diab, T. Solorio. 2014. Overview for the Second Shared Task on Language Identification in Code-Switched Data. Abstract: We present an overview of the first shared task on language identification on codeswitched data. The shared task included code-switched data from four language pairs: Modern Standard ArabicDialectal Arabic (MSA-DA), MandarinEnglish (MAN-EN), Nepali-English (NEPEN), and Spanish-English (SPA-EN). A total of seven teams participated in the task and submitted 42 system runs. The evaluation showed that language identification at the token level is more difficult when the languages present are closely related, as in the case of MSA-DA, where the prediction performance was the lowest among all language pairs. In contrast, the language pairs with the higest F-measure where SPA-EN and NEP-EN. The task made evident that language identification in code-switched data is still far from solved and warrants further research.
- Ayah Zirikly, Mona T. Diab. 2014. Named Entity Recognition System for Dialectal Arabic. Abstract: To date, majority of research for Arabic Named Entity Recognition (NER) addresses the task for Modern Standard Arabic (MSA) and mainly focuses on the newswire genre. Despite some common characteristics between MSA and Dialectal Arabic (DA), the significant differences between the two language varieties hinder such MSA specific systems from solving NER for Dialectal Arabic. In this paper, we present an NER system for DA specifically focusing on the Egyptian Dialect (EGY). Our system delivers ≈ 16% improvement in F1-score over state-of-theart features.
- Mona T. Diab, Nizar Habash. 2014. Natural Language Processing of Arabic and its Dialects. Abstract: This tutorial introduces the different challenges and current solutions to the automatic processing of Arabic and its dialects. The tutorial has two parts: First, we present a discussion of generic issues relevant to Arabic NLP and detail dialectal linguistic issues and the challenges they pose for NLP. In the second part, we review the state-of-the-art in Arabic processing covering several enabling technologies and applications, e.g., dialect identification, morphological processing (analysis, disambiguation, tokenization, POS tagging), parsing, and machine translation.
- Ayah Zirikly, Mona T. Diab. 2014. Named Entity Recognition for Dialectal Arabic. Abstract: To date, majority of research for Arabic Named Entity Recognition (NER) addresses the task for Modern Standard Arabic (MSA) and mainly focuses on the newswire genre. Despite some common characteristics between MSA and Dialectal Arabic (DA), the significant differences between the two language varieties hinder such MSA specific systems from solving NER for Dialectal Arabic. In this paper, we present an NER system for DA specifically focusing on the Egyptian Dialect (EGY). Our system delivers 16% improvement in F1-score over state-of-theart features.
- Muhammad Abdul-Mageed, Mona T. Diab. 2014. SANA: A Large Scale Multi-Genre, Multi-Dialect Lexicon for Arabic Subjectivity and Sentiment Analysis. Abstract: The computational treatment of subjectivity and sentiment in natural language is usually significantly improved by applying features exploiting lexical resources where entries are tagged with semantic orientation (e.g., positive, negative values). In spite of the fair amount of work on Arabic sentiment analysis over the past few years (e.g., (Abbasi et al., 2008; Abdul-Mageed et al., 2014; Abdul-Mageed et al., 2012; Abdul-Mageed and Diab, 2012a; Abdul-Mageed and Diab, 2012b; Abdul-Mageed et al., 2011a; Abdul-Mageed and Diab, 2011)), the language remains under-resourced as to these polarity repositories compared to the English language. In this paper, we report efforts to build and present SANA, a large-scale, multi-genre, multi-dialect multi-lingual lexicon for the subjectivity and sentiment analysis of the Arabic language and dialects.
- Arfath Pasha, Mohamed Al-Badrashiny, Mona T. Diab, Ahmed El Kholy, R. Eskander, Nizar Habash, Manoj Pooleery, Owen Rambow, Ryan Roth. 2014. MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic. Abstract: In this paper, we present MADAMIRA, a system for morphological analysis and disambiguation of Arabic that combines some of the best aspects of two previously commonly used systems for Arabic processing, MADA (Habash and Rambow, 2005; Habash et al., 2009; Habash et al., 2013) and AMIRA (Diab et al., 2007). MADAMIRA improves upon the two systems with a more streamlined Java implementation that is more robust, portable, extensible, and is faster than its ancestors by more than an order of magnitude. We also discuss an online demo (see http://nlp.ldeo.columbia.edu/madamira/) that highlights these aspects.
- Mona T. Diab, Nizar Habash, Owen Rambow, Ryan Roth. 2013. LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual. Abstract: The Linguistic Data Consortium (LDC) has developed hundreds of data corpora for natural language processing (NLP) research. Among these are a number of annotated treebank corpora for Arabic. Typically, these corpora consist of a single collection of annotated documents. NLP research, however, usually requires multiple data sets for the purposes of training models, developing techniques, and final evaluation. Therefore it becomes necessary to divide the corpora used into the required data sets (divisions). This document details a set of rules that have been defined to enable consistent divisions for old and new Arabic treebanks (ATB) and related corpora.
- Heba Elfardy, Mona T. Diab. 2013. Sentence Level Dialect Identification in Arabic. Abstract: This paper introduces a supervised approach for performing sentence level dialect identification between Modern Standard Arabic and Egyptian Dialectal Arabic. We use token level labels to derive sentence-level features. These features are then used with other core and meta features to train a generative classifier that predicts the correct label for each sentence in the given input text. The system achieves an accuracy of 85.5% on an Arabic online-commentary dataset outperforming a previously proposed approach achieving 80.9% and reflecting a significant gain over a majority baseline of 51.9% and two strong baseline systems of 78.5% and 80.4%, respectively.
- Arfath Pasha, Mohamed Al-Badrashiny, Mohamed Altantawy, Nizar Habash, Manoj Pooleery, Owen Rambow, Ryan Roth, Mona T. Diab. 2013. DIRA: Dialectal Arabic Information Retrieval Assistant. Abstract: DIRA is a query expansion tool that generates search terms in Standard Arabic and/or its dialects when provided with queries in English or Standard Arabic. The retrieval of dialectal Arabic text has recently become necessary due to the increase of dialectal content on social media. DIRA addresses the challenges of retrieving information in Arabic dialects, which have significant linguistic differences from Standard Arabic. To our knowledge, DIRA is the only tool in existence that automatically generates dialect search terms with relevant morphological variations from English or Standard Arabic query terms.
- Mahmoud A. Ghoneim, Mona T. Diab. 2013. Multiword Expressions in the Context of Statistical Machine Translation. Abstract: Incorporating semantic information in the Statistical Machine Translation (SMT) framework is starting to gain some popularity in both the semantics and translation communities. In this paper, we present encouraging results obtained from experiments conducted on English to Arabic SMT system using static, dynamic, and hybrid integration of fine-grained Multiword Expression (MWE). We achieve an improvement up to 0.82 absolute BLEU score by integrating MWEs over a vanilla SMT system. We empirically show that different MWE types require different integration methods in the SMT framework.
- Muhammad Abdul-Mageed, Mona T. Diab, Sandra Kübler. 2013. ASMA: A System for Automatic Segmentation and Morpho-Syntactic Disambiguation of Modern Standard Arabic. Abstract: In this paper, we present ASMA, a fast and efficient system for automatic segmentation and fine grained part of speech (POS) tagging of Modern Standard Arabic (MSA). ASMA performs segmentation both of agglutinative and of inflectional morphological boundaries within a word. In this work, we compare ASMA to two state of the art suites of MSA tools: AMIRA 2.1 (Diab et al., 2007; Diab, 2009) and MADA+TOKAN 3.2. (Habash et al., 2009). ASMA achieves comparable results to these two systems’ state-of-theart performance. ASMA yields an accuracy of 98.34% for segmentation, and an accuracy of 96.26% for POS tagging with ar ich tagset and 97.59% accuracy with an extremely reduced tagset. 1I ntroduction
- Nadi Tomeh, Nizar Habash, Ryan Roth, N. Farra, Pradeep Dasigi, Mona T. Diab. 2013. Reranking with Linguistic and Semantic Features for Arabic Optical Character Recognition. Abstract: Optical Character Recognition (OCR) systems for Arabic rely on information contained in the scanned images to recognize sequences of characters and on language models to emphasize fluency. In this paper we incorporate linguistically and semantically motivated features to an existing OCR system. To do so we follow ann-best list reranking approach that exploits recent advances in learning to rank techniques. We achieve 10.1% and 11.4% reduction in recognition word error rate (WER) relative to a standard baseline system on typewritten and handwritten Arabic respectively.
- Weiwei Guo, Mona T. Diab. 2013. Improving Lexical Semantics for Sentential Semantics: Modeling Selectional Preference and Similar Words in a Latent Variable Model. Abstract: Sentence Similarity [SS] computes a similarity score between two sentences. The SS task differs from document level semantics tasks in that it features the sparsity of words in a data unit, i.e. a sentence. Accordingly it is crucial to robustly model each word in a sentence to capture the complete semantic picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets.
- Weiwei Guo, Hao Li, Heng Ji, Mona T. Diab. 2013. Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media. Abstract: Many current Natural Language Processing [NLP] techniques work well assuming a large context of text as input data. However they become ineffective when applied to short texts such as Twitter feeds. To overcome the issue, we want to find a related newswire document to a given tweet to provide contextual support for NLP tasks. This requires robust modeling and understanding of the semantics of short texts. The contribution of the paper is two-fold: 1. we introduce the Linking-Tweets-toNews task as well as a dataset of linked tweet-news pairs, which can benefit many NLP applications; 2. in contrast to previous research which focuses on lexical features within the short texts (text-to-word information), we propose a graph based latent variable model that models the inter short text correlations (text-to-text information). This is motivated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task.
- Mona T. Diab. 2013. Semantic Textual Similarity: past present and future. Abstract: Similarity is at the core of scientific inquiry in general and is one of the basic functionalities in Natural Language Processing (NLP) in particular. To arrive at generalizations across different phenomena, we need to recognize patterns of similarity, or divergence, to make scientific claims. Semantic textual similarity plays a significant role in NLP research both directly and indirectly. For example, for document summarization, we need to compress redundant information which requires identifying where the text is similar; for question answering, we need to recognize the similarity between the questions and the answers; textual similarity is an important component of an entailment system; evaluating machine translation (MT) output relies on calculating the similarity between the system’s output and some reference gold translations; textual generation technology benefits from sentence similarity by generating different expressions. In this talk, I will address the problem of textual semantic similarity. We have run 2 major tasks of STS over the span of two years within the context of Semeval in 2012 and *SEM shared task in 2013. The task to date is one of the most successful to be carried out within our community by virtue of being quite popular. I will share with you the details of the task, some interesting insights into the scientific merits of this enterprise and lessons learned. Finally I will share some thoughts on the future.
- Amjad Abu-Jbara, Ben King, Mona T. Diab, Dragomir R. Radev. 2013. Identifying Opinion Subgroups in Arabic Online Discussions. Abstract: In this paper, we use Arabic natural language processing techniques to analyze Arabic debates. The goal is to identify how the participants in a discussion split into subgroups with contrasting opinions. The members of each subgroup share the same opinion with respect to the discussion topic and an opposing opinion to the members of other subgroups. We use opinion mining techniques to identify opinion expressions and determine their polarities and their targets. We opinion predictions to represent the discussion in one of two formal representations: signed attitude network or a space of attitude vectors. We identify opinion subgroups by partitioning the signed network representation or by clustering the vector space representation. We evaluate the system using a data set of labeled discussions and show that it achieves good results.
- Eneko Agirre, Daniel Matthew Cer, Mona T. Diab, Aitor Gonzalez-Agirre, Weiwei Guo. 2013. *SEM 2013 shared task: Semantic Textual Similarity. Abstract: In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar. This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED). CORE is similar in set up to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets. TYPED, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description. Several types of similarity have been defined, including similar author, similar time period or similar location. The annotation for both tasks leverages crowdsourcing, with relative high interannotator correlation, ranging from 62% to 87%. The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs.
- Pradeep Dasigi, Weiwei Guo, Mona T. Diab. 2012. Genre Independent Subgroup Detection in Online Discussion Threads: A Study of Implicit Attitude using Textual Latent Semantics. Abstract: We describe an unsupervised approach to the problem of automatically detecting subgroups of people holding similar opinions in a discussion thread. An intuitive way of identifying this is to detect the attitudes of discussants towards each other or named entities or topics mentioned in the discussion. Sentiment tags play an important role in this detection, but we also note another dimension to the detection of people’s attitudes in a discussion: if two persons share the same opinion, they tend to use similar language content. We consider the latter to be an implicit attitude. In this paper, we investigate the impact of implicit and explicit attitude in two genres of social media discussion data, more formal wikipedia discussions and a debate discussion forum that is much more informal. Experimental results strongly suggest that implicit attitude is an important complement for explicit attitudes (expressed via sentiment) and it can improve the sub-group detection performance independent of genre.
- Vinodkumar Prabhakaran, Michael Bloodgood, Mona T. Diab, B. Dorr, Lori S. Levin, C. Piatko, Owen Rambow, Benjamin Van Durme. 2012. Statistical Modality Tagging from Rule-based Annotations and Crowdsourcing. Abstract: We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.
- Muhammad Abdul-Mageed, Sandra Kübler, Mona T. Diab. 2012. SAMAR: A System for Subjectivity and Sentiment Analysis of Arabic Social Media. Abstract: In this work, we present SAMAR, a system for Subjectivity and Sentiment Analysis (SSA) for Arabic social media genres. We investigate: how to best represent lexical information; whether standard features are useful; how to treat Arabic dialects; and, whether genre specific features have a measurable impact on performance. Our results suggest that we need individualized solutions for each domain and task, but that lemmatization is a feature in all the best approaches.
- Heba Elfardy, Mona T. Diab. 2012. Simplified guidelines for the creation of Large Scale Dialectal Arabic Annotations. Abstract: The Arabic language is a collection of dialectal variants along with the standard form, Modern Standard Arabic (MSA). MSA is used in official Settings while the dialectal variants (DA) correspond to the native tongue of the Arabic speakers. Arabic speakers typically code switch between DA and MSA, which is reflected extensively in written online social media. Automatic processing such Arabic genre is very difficult for automated NLP tools since the linguistic difference between MSA and DA is quite profound. However, no annotated resources exist for marking the regions of such switches in the utterance. In this paper, we present a simplified Set of guidelines for detecting code switching in Arabic on the word/token level. We use these guidelines in annotating a corpus that is rich in DA with frequent code switching to MSA. We present both a quantitative and qualitative analysis of the annotations.
- Muhammad Abdul-Mageed, Mona T. Diab. 2012. AWATIF: A Multi-Genre Corpus for Modern Standard Arabic Subjectivity and Sentiment Analysis. Abstract: We present AWATIF, a multi-genre corpus of Modern Standard Arabic (MSA) labeled for subjectivity and sentiment analysis (SSA) at the sentence level. The corpus is labeled using both regular as well as crowd sourcing methods under three different conditions with two types of annotation guidelines. We describe the sub-corpora constituting the corpus and provide examples from the various SSA categories. In the process, we present our linguistically-motivated and genre-nuanced annotation guidelines and provide evidence showing their impact on the labeling task.
- Vinodkumar Prabhakaran, Owen Rambow, Mona T. Diab. 2012. Predicting Overt Display of Power in Written Dialogs. Abstract: We analyze overt displays of power (ODPs) in written dialogs. We present an email corpus with utterances annotated for ODP and present a supervised learning system to predict it. We obtain a best cross validation F-measure of 65.8 using gold dialog act features and 55.6 without using them.
- A. Hawwari, Kfir Bar, Mona T. Diab. 2012. Building an Arabic Multiword Expressions Repository. Abstract: We introduce a list of Arabic multiword expressions (MWE) collected from various dictionaries. The MWEs are grouped based on their syntactic type. Every constituent word in the expressions is manually annotated with its full context-sensitive morphological analysis. Some of the expressions contain semantic variables as place holders for words that play the same semantic role. In addition, we have automatically annotated a large corpus of Arabic text using a pattern-matching algorithm that considers some morphosyntactic features as expressed by a highly inflected language, such as Arabic. A sample part of the corpus is manually evaluated and the results are reported in this paper.
- Amjad Abu-Jbara, Pradeep Dasigi, Mona T. Diab, Dragomir R. Radev. 2012. Subgroup Detection in Ideological Discussions. Abstract: The rapid and continuous growth of social networking sites has led to the emergence of many communities of communicating groups. Many of these groups discuss ideological and political topics. It is not uncommon that the participants in such discussions split into two or more subgroups. The members of each subgroup share the same opinion toward the discussion topic and are more likely to agree with members of the same subgroup and disagree with members from opposing subgroups. In this paper, we propose an unsupervised approach for automatically detecting discussant subgroups in online communities. We analyze the text exchanged between the participants of a discussion to identify the attitude they carry toward each other and towards the various aspects of the discussion topic. We use attitude predictions to construct an attitude vector for each discussant. We use clustering techniques to cluster these vectors and, hence, determine the subgroup membership of each participant. We compare our methods to text clustering and other baselines, and show that our method achieves promising results.
- Vinodkumar Prabhakaran, Michael Bloodgood, Mona T. Diab, B. Dorr, Lori S. Levin, C. Piatko, Owen Rambow, Benjamin Van Durme. 2012. from Rule-based Annotations and Crowdsourcing. Abstract: We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.
- Eneko Agirre, Daniel Matthew Cer, Mona T. Diab, Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. Abstract: Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation >80%, well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.
- Weiwei Guo, Mona T. Diab. 2012. Weiwei: A Simple Unsupervised Latent Semantics based Approach for Sentence Similarity. Abstract: The Semantic Textual Similarity (STS) shared task (Agirre et al., 2012) computes the degree of semantic equivalence between two sentences. We show that a simple unsupervised latent semantics based approach, Weighted Textual Matrix Factorization that only exploits bag-of-words features, can outperform most systems for this task. The key to the approach is to carefully handle missing words that are not in the sentence, and thus rendering it superior to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Our system ranks 20 out of 89 systems according to the official evaluation metric for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers.
- Pradeep Dasigi, Weiwei Guo, Mona T. Diab. 2012. Genre Independent Subgroup Detection in Online Discussion Threads: A Pilot Study of Implicit Attitud. Abstract: We describe an unsupervised approach to the problem of automatically detecting subgroups of people holding similar opinions in a discussion thread. An intuitive way of identifying this is to detect the attitudes of discussants towards each other or named entities or topics mentioned in the discussion. Sentiment tags play an important role in this detection, but we also note another dimension to the detection of people's attitudes in a discussion: if two persons share the same opinion, they tend to use similar language content. We consider the latter to be an implicit attitude. In this paper, we investigate the impact of implicit and explicit attitude in two genres of social media discussion data, more formal wikipedia discussions and a debate discussion forum that is much more informal. Experimental results strongly suggest that implicit attitude is an important complement for explicit attitudes (expressed via sentiment) and it can improve the sub-group detection performance independent of genre.
- Nizar Habash, Mona T. Diab, Owen Rambow. 2012. Conventional Orthography for Dialectal Arabic. Abstract: Dialectal Arabic (DA) refers to the day-to-day vernaculars spoken in the Arab world. DA lives side-by-side with the official language, Modern Standard Arabic (MSA). DA differs from MSA on all levels of linguistic representation, from phonology and morphology to lexicon and syntax. Unlike MSA, DA has no standard orthography since there are no Arabic dialect academies, nor is there a large edited body of dialectal literature that follows the same spelling standard. In this paper, we present CODA, a conventional orthography for dialectal Arabic; it is designed primarily for the purpose of developing computational models of Arabic dialects. We explain the design principles of CODA and provide a detailed description of its guidelines as applied to Egyptian Arabic.
- Vinodkumar Prabhakaran, Owen Rambow, Mona T. Diab. 2012. Who’s (Really) the Boss? Perception of Situational Power in Written Interactions. Abstract: We study the perception of situational power in written dialogs in the context of organizational emails and contrast it to the power attributed by organizational hierarchy. We analyze various correlates of the perception of power in the dialog structure and language use by participants in the dialog. We also present an SVM-based machine learning system using dialog structure and lexical features to predict persons with situational power in a given communication thread.
- Heba Elfardy, Mona T. Diab. 2012. Token Level Identification of Linguistic Code Switching. Abstract: Typically native speakers of Arabic mix dialectal Arabic and Modern Standard Arabic in the same utterance. This phenomenon is known as linguistic code switching (LCS). It is a very challenging task to identify these LCS points in written text where we don’t have an accompanying speech signal. In this paper, we address automatic identification of LCS points in Arabic social media text by identifying token level dialectal words. We present an unsupervised approach that employs a set of dictionaries, sound-change rules, and language models to tackle this problem. We tune and test the performance of our approach against human-annotated Egyptian and Levantine discussion fora datasets. Two types of annotations on the token level are obtained for each dataset: context sensitive and context insensitive annotation. We achieve a token level Fβ=1 score of 74% and 72.4% on the context-sensitive development and test datasets, respectively. On the context insensitive annotated data, we achieve a token level Fβ=1 score of 84.4% and 84.9% on the development and test datasets, respectively.
- Vinodkumar Prabhakaran, Huzaifa Neralwala, Owen Rambow, Mona T. Diab. 2012. Annotations for Power Relations on Email Threads. Abstract: Social relations like power and influence are difficult concepts to define, but are easily recognizable when expressed. In this paper, we describe a multi-layer annotation scheme for social power relations that are recognizable from online written interactions. We introduce a typology of four types of power relations between dialog participants: hierarchical power, situational power, influence and control of communication. We also present a corpus of Enron emails comprising of 122 threaded conversations, manually annotated with instances of these power relations between participants. Our annotations also capture attempts at exercise of power or influence and whether those attempts were successful or not. In addition, we also capture utterance level annotations for overt display of power. We describe the annotation definitions using two example email threads from our corpus illustrating each type of power relation. We also present detailed instructions given to the annotators and provide various statistics on annotations in the corpus.
- Weiwei Guo, Mona T. Diab. 2012. Modeling Sentences in the Latent Space. Abstract: Sentence Similarity is the process of computing a similarity score between two sentences. Previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences. In this paper, we show that by carefully handling words that are not in the sentences (missing words), we can train a reliable latent variable model on sentences. In the process, we propose a new evaluation framework for sentence similarity: Concept Definition Retrieval. The new framework allows for large scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity.
- Weiwei Guo, Mona T. Diab. 2012. Learning the Latent Semantics of a Concept from its Definition. Abstract: In this paper we study unsupervised word sense disambiguation (WSD) based on sense definition. We learn low-dimensional latent semantic vectors of concept definitions to construct a more robust sense similarity measure wmfvec. Experiments on four all-words WSD data sets show significant improvement over the baseline WSD systems and LDA based similarity measures, achieving results comparable to state of the art WSD systems.
- W. Zaghouani, A. Hawwari, Mona T. Diab. 2012. A Pilot PropBank Annotation for Quranic Arabic. Abstract: The Quran is a significant religious text written in a unique literary style, close to very poetic language in nature. Accordingly it is significantly richer and more complex than the newswire style used in the previously released Arabic PropBank (Zaghouani et al., 2010; Diab et al., 2008). We present preliminary work on the creation of a unique Arabic proposition repository for Quranic Arabic. We annotate the semantic roles for the 50 most frequent verbs in the Quranic Arabic Dependency Treebank (QATB) (Dukes and Buckwalter 2010). The Quranic Arabic PropBank (QAPB) will be a unique new resource of its kind for the Arabic NLP research community as it will allow for interesting insights into the semantic use of classical Arabic, poetic literary Arabic, as well as significant religious texts. Moreover, on a pragmatic level QAPB will add approximately 810 new verbs to the existing Arabic PropBank (APB). In this pilot experiment, we leverage our knowledge and experience from our involvement in the APB project. All the QAPB annotations will be made freely available for research purposes.
- Weiwei Guo, Mona T. Diab. 2011. Semantic Topic Models: Combining Word Distributional Statistics and Dictionary Definitions. Abstract: In this paper, we propose a novel topic model based on incorporating dictionary definitions. Traditional topic models treat words as surface strings without assuming predefined knowledge about word meaning. They infer topics only by observing surface word co-occurrence. However, the co-occurred words may not be semantically related in a manner that is relevant for topic coherence. Exploiting dictionary definitions explicitly in our model yields a better understanding of word semantics leading to better text modeling. We exploit WordNet as a lexical resource for sense definitions. We show that explicitly modeling word definitions helps improve performance significantly over the baseline for a text categorization task.
- Muhammad Abdul-Mageed, Mona T. Diab. 2011. Subjectivity and Sentiment Annotation of Modern Standard Arabic Newswire. Abstract: Subjectivity and sentiment analysis (SSA) is an area that has been witnessing a flurry of novel research. However, only few attempts have been made to build SSA systems for morphologically-rich languages (MRL). In the current study, we report efforts to partially bridge this gap. We present a newly labeled corpus of Modern Standard Arabic (MSA) from the news domain manually annotated for subjectivity and domain at the sentence level. We summarize our linguistically-motivated annotation guidelines and provide examples from our corpus exemplifying the different phenomena. Throughout the paper, we discuss expression of subjectivity in natural language, combining various previously scattered insights belonging to many branches of linguistics.
- Pradeep Dasigi, Mona T. Diab. 2011. CODACT: Towards Identifying Orthographic Variants in Dialectal Arabic. Abstract: Dialectal Arabic (DA) is the spoken vernacular for over 300M people worldwide. DA is emerging as the form of Arabic written in online communication: chats, emails, blogs, etc. However, most existing NLP tools for Arabic are designed for processing Modern Standard Arabic, a variety that is more formal and scripted. Apart from the genre variation that is a hindrance for any language processing, even in English, DA has no orthographic standard, compared to MSA that has a standard orthography and script. Accordingly, a word may be written in many possible inconsistent spellings rendering the processing of DA very challenging. To solve this problem, such inconsistencies have to be normalized. This work is the first step towards addressing this problem, as we attempt to identify spelling variants in a given textual document. We present an unsupervised clustering approach that addresses the problem of identifying orthographic variants in DA. We employ different similarity measures that exploit string similarity and contextual semantic similarity. To our knowledge this is the first attempt at solving the problem for DA. Our approaches are tested on data in two dialects of Arabic - Egyptian and Levantine. Our system achieves the highest Entropy of 0.19 for Egyptian (corresponding to 68% cluster precision) and Levantine (corresponding to 64% cluster precision) respectively. This constitutes a significant reduction in entropy (from 0.47 for Egyptian and 0.51 for Levantine) and improvement in cluster precision (from 29% for both) from the baseline.
- G. Katz, Mona T. Diab. 2011. Introduction to the Special Issue on Arabic Computational Linguistics. Abstract: In recent years there has been a significant increase in the amount of research on the computational analysis of the Arabic language being carried out around the world. This includes both a steady growth of the computational linguistics research community in the Arab-speaking world, as evidenced by the emergence of scientific societies such as the Egyptian Society for Language Engineering1, as well as the existence of major initiatives on Arabic natural language processing (NLP) in Europe and North America (particularly in the United States by the DARPA-funded GALE program2). In addition, the past year alone saw the publication of two monographs on Arabic Computational Linguistics [Farghaly 2010; Habash 2010], a special issue of this journal [Shaalan and Farghaly 2009], a new release by the Linguistics Data Consortium of the Arabic Treebank [Maamouri et al. 2009] (among other Arabic-language resources), as well as a number of conferences and workshops on Arabic NLP around the world. At the 2010 Georgetown University Roundtable on Language3 (GURT), whose theme was Arabic Language and Linguistics, linguists from around the world and from a wide range of different linguistic traditions and sub-disciplines came together in Washington, D.C. to share their research on the Arabic language. A two-day special session dedicated to Arabic computational linguistics and a half-day preconference tutorial on Arabic NLP drew researchers from both academia and industry. The presentations in the special session spanned the range from applied NLP to natural language understanding, from automatic diacritization to speech act classification, providing attendees with a snapshot of the current state of Arabic NLP. The papers in this volume were selected from those presentations to represent the different approaches and techniques currently being used in the field of Arabic computational linguistics. Arabic NLP research faces a number of well-known challenges, due to both the morphological complexity of the Arabic language and to the particularities of Arabic orthography. These challenges have been addressed extensively in Arabic NLP research which has focused on Modern Standard Arabic (MSA) [Habash and Rambow 2005; Diab et al. 2007; Larkey et al. 2007; Buckwalter 2007; Dichy and Farghaly 2007]. Significant recent attention, however, has been paid to the problems for NLP
- Pradeep Dasigi, Mona T. Diab. 2011. Named Entity Transliteration Generation Leveraging Statistical Machine Translation Technology. Abstract: Automatically identifying that different orthographic variants of names are referring to the same name is a significant challenge for processing natural language processing since they typically constitute the bulk of the out-of-vocabulary tokens. The problem is exacerbated when the name is foreign. In this paper we address the problem of generating valid orthographic variants for proper names, namely transliterating proper names in different scripts. We attempt to solve the problem for three different language pairs: English! Hindi, English! Persian, and Arabic! English. We adopt a unified approach to the problem. We frame the problem from a statistical Machine Translation perspective. We further post edit the output applying linguistically informed rules particular to the language pair and re-rank the output using machine learning methods.
- Yassine Benajiba, Mona T. Diab. 2011. A Web Application for Dialectal Arabic Text Annotation. Abstract: Design and implementation of an application which allows many annotators to annotate data and enter the information into a central database is not a trivial task. Such an application has to guarantee a high level of security, consistent and robust back-ups for the underlying database, and aid in increasing the speed and efficiency of the annotation by providing the annotators with intuitive GUIs. Moreover it needs to ensure that the data is stored with a minimal amount of redundancy in order to simultaneously save all the information while not losing on speed. In this paper, we describe a web application which is used to annotate many Dialectal Arabic texts. It aims at optimizing speed, accuracy and efficiency while maintaining the security and integrity of the data.
- Mona T. Diab, Ankitdeep Kamboj. 2011. Feasibility of Leveraging Crowd Sourcing for the Creation of a Large Scale Annotated Resource for Hindi English Code Switched Data: A Pilot Annotation. Abstract: Linguistic code switching (LCS) occurs when speakers mix multiple languages in the same speech utterance. We find LCS pervasively in bilingual communities. LCS poses a serious challenge to Natural Language and Speech Processing. With the ubiquity of informal genres online, LCS is emerging as a very widespread phenomenon. This paper presents a first attempt at collecting and annotating a large repository of LCS data. We target Hindi English (Hinglish) LCS. We investigate the feasibility of leveraging crowd sourcing as a means for annotating the data on the word level. This paper briefly explains the setup of the experiment and data collection. It also presents statistics representing agreements among annotators over different possible categories of Hinglish words and analyzes the confidence with which a code switched word can be annotated in the correct category by humans.
- Mona T. Diab, Nizar Habash, Owen Rambow, Mohamed Altantawy, Yassine Benajiba. 2011. COLABA : Arabic Dialect Annotation and Processing. Abstract: In this paper, we describe COLABA, a large effort to create resources and processing tools for Dialectal Arabic Blogs. We describe the objectives of the project, the process flow and the interaction between the different components. We briefly describe the manual annotation effort and the resources created. Finally, we sketch how these resources and tools are put together to create DIRA, a termexpansion tool for information retrieval over dialectal Arabic collections using Modern Standard Arabic queries.
- Muhammad Abdul-Mageed, Mona T. Diab, M. Korayem. 2011. Subjectivity and Sentiment Analysis of Modern Standard Arabic. Abstract: Although Subjectivity and Sentiment Analysis (SSA) has been witnessing a flurry of novel research, there are few attempts to build SSA systems for Morphologically-Rich Languages (MRL). In the current study, we report efforts to partially fill this gap. We present a newly developed manually annotated corpus of Modern Standard Arabic (MSA) together with a new polarity lexicon. The corpus is a collection of newswire documents annotated on the sentence level. We also describe an automatic SSA tagging system that exploits the annotated data. We investigate the impact of different levels of preprocessing settings on the SSA classification task. We show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance.
- Weiwei Guo, Mona T. Diab. 2010. COLEPL and COLSLM: An Unsupervised WSD Approach to Multilingual Lexical Substitution, Tasks 2 and 3 SemEval 2010. Abstract: In this paper, we present a word sense disambiguation (WSD) based system for multilingual lexical substitution. Our method depends on having a WSD system for English and an automatic word alignment method. Crucially the approach relies on having parallel corpora. For Task 2 (Sinha et al., 2009) we apply a supervised WSD system to derive the English word senses. For Task 3 (Lefever & Hoste, 2009), we apply an unsupervised approach to the training and test data. Both of our systems that participated in Task 2 achieve a decent ranking among the participating systems. For Task 3 we achieve the highest ranking on several of the language pairs: French, German and Italian.
- Yassine Benajiba, I. Zitouni, Mona T. Diab, Paolo Rosso. 2010. Arabic Named Entity Recognition: Using Features Extracted from Noisy Data. Abstract: Building an accurate Named Entity Recognition (NER) system for languages with complex morphology is a challenging task. In this paper, we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute).
- Weiwei Guo, Mona T. Diab. 2010. Combining Orthogonal Monolingual and Multilingual Sources of Evidence for All Words WSD. Abstract: Word Sense Disambiguation remains one of the most complex problems facing computational linguists to date. In this paper we present a system that combines evidence from a monolingual WSD system together with that from a multilingual WSD system to yield state of the art performance on standard All-Words data sets. The monolingual system is based on a modification of the graph based state of the art algorithm In-Degree. The multilingual system is an improvement over an All-Words unsupervised approach, SALAAM. SALAAM exploits multilingual evidence as a means of disambiguation. In this paper, we present modifications to both of the original approaches and then their combination. We finally report the highest results obtained to date on the SENSEVAL 2 standard data set using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme.
- Marine Carpuat, Mona T. Diab. 2010. Task-based Evaluation of Multiword Expressions: a Pilot Study in Statistical Machine Translation. Abstract: We conduct a pilot study for task-oriented evaluation of Multiword Expression (MWE) in Statistical Machine Translation (SMT). We propose two different integration strategies for MWE in SMT, which take advantage of different degrees of MWE semantic compositionality and yield complementary improvements in SMT quality on a large-scale translation task.
- Steven P. Abney, S. Kurohashi, S. Bangalore, Irene Langkilde-Geary, C. Brew, Mirella Lapata, Sharon A. Caraballo, C. Leacock, Bob Carpenter, B. Levin, Stanley F. Chen, D. Litman, Kenneth Ward Church, I. Mani, Michael Collins, Christopher Manning, Ann A. Copestake, D. Marcu, M. Crocker, E. Marsi, P. Deane, Diana McCarthy, Mona T. Diab, I. D. Melamed, M. Dras, J. Minett, Jason Eisner, Robert C. Moore, E. Fosler-Lussier, Thomas Morton, George Foster, H. Ney, R. Frank, G. Ngai, Jianfeng Gao, Kemal Oflazer, Claire Gardent, Massimo Poesio, Tanja Gaustad van Zaanen, Judita Preiss, D. Gildea, Ehud Reiter, Andrew R. Golding, P. Resnik, Joshua Goodman, Roni Rosenfeld, G. Grefenstette, Frank Schilder, Mohammad Haji-Abdolhosseini, Lenhart K. Schubert, P. Heeman, Advaith Siddharthan, D. Higgins, R. Sproat, J. Hockenmaier, M. Strube, H. Horacek, M. Swerts, D. Inkpen, Simone Teufel, Martin Jansche, Kees van Deemter, Mark Johnson, Ye-Yi Wang, Frank Keller, B. Webber, A. Kilgarriff, J. Wiebe, Kevin Knight, Florian Wolf. 2010. Reviewers for Volume 31. Abstract: This journal has a knowledgeable and hardworking editorial board, whose members are listed on the inside front cover of each issue, but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The Editor of Computational Linguistics would like to express his gratitude to the external reviewers listed below who anonymously reviewed papers for the journal during the preparation of this volume (Volume 31). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service.
- Vinodkumar Prabhakaran, Owen Rambow, Mona T. Diab. 2010. Automatic Committed Belief Tagging. Abstract: We go beyond simple propositional meaning extraction and present experiments in determining which propositions in text the author believes. We show that deep syntactic parsing helps for this task. Our best feature combination achieves an F-measure of 64%, a relative reduction in F-measure error of 21% over not using syntactic features.
- W. Zaghouani, Mona T. Diab, Aous Mansouri, Sameer Pradhan, Martha Palmer. 2010. The Revised Arabic PropBank. Abstract: The revised Arabic PropBank (APB) reflects a number of changes to the data and the process of PropBanking. Several changes stem from Treebank revisions. An automatic process was put in place to map existing annotation to the new trees. We have revised the original 493 Frame Files from the Pilot APB and added 1462 new files for a total of 1955 Frame Files with 2446 framesets. In addition to a heightened attention to sense distinctions this cycle includes a greater attempt to address complicated predicates such as light verb constructions and multi-word expressions. New tools facilitate the data tagging and also simplify frame creation.
- K. Meftouh, S. Khoja, Nizar Habash, Mona T. Diab, Nasreedine Semmar, Kareem Darwish. 2009. Statistical Modeling of Arabic Language. Abstract: Human Language Technologies enable humans to communicate with computers and to use computers and the internet in a more natural way and in their own language, i.e. to participate in the information society in a totally natural way. Native speakers of languages that are not well served by language technology suffer from less access to information, and from less efficient tools, and higher productions costs for documents and translation [1]. Since 2001, Arabic language becomes a priority for several researchers through the world. this interest is probably due to a massive need of computer tools necessary to deal with the huge amount of Arabic data electronically available and, which is dramatically increasing daily. Some tools already exist for Arabic, but there is a long way to go before the Arabic tools reach the level which exists for the so called ”resource rich languages” as English, French or Chinese. this lack of perfection, in my opinion, is certainly due to lack of cooperation in the research community working on the Arabic language. So what we need is to consolidate efforts by sharing ideas, knowledge and resources to produce better tools. precisely MEDAR encourages and facilitates the exchange and sharing of resources and knowledge:As much as possible, MEDAR partners work from present open-source technology and build on it. This helps the Arabic HLT community in terms of sharing knowledge and resources, helping to minimize costs. MEDAR also updates partners on state-of-the-art Arabic HLT so that duplication of work is minimized. this update is also provided by the organization of meetings between researchers. this contact is certainly very
- Mona T. Diab, M. Krishna. 2009. Handling Sparsity for Verb Noun MWE Token Classification. Abstract: We address the problem of classifying multiword expression tokens in running text. We focus our study on Verb-Noun Constructions (VNC) that vary in their idiomaticity depending on context. VNC tokens are classified as either idiomatic or literal. Our approach hinges upon the assumption that a literal VNC will have more in common with its component words than an idiomatic one. Commonality is measured by contextual overlap. To this end, we set out to explore different contextual variations and different similarity measures handling the sparsity in the possible contexts via four different parameter variations. Our approach yields state of the art performance with an overall accuracy of 75.54% on a TEST data set.
- Yassine Benajiba, Mona T. Diab, Paolo Rosso. 2009. Arabic Named Entity Recognition: A Feature-Driven Study. Abstract: The named entity recognition task aims at identifying and classifying named entities within an open-domain text. This task has been garnering significant attention recently as it has been shown to help improve the performance of many natural language processing applications. In this paper, we investigate the impact of using different sets of features in three discriminative machine learning frameworks, namely, support vector machines, maximum entropy and conditional random fields for the task of named entity recognition. Our language of interest is Arabic. We explore lexical, contextual and morphological features and nine data-sets of different genres and annotations. We measure the impact of the different features in isolation and incrementally combine them in order to evaluate the robustness to noise of each approach. We achieve the highest performance using a combination of 15 features in conditional random fields using broadcast news data (Fbeta = 1=83.34).
- Mona T. Diab. 2009. Second Generation AMIRA Tools for Arabic Processing : Fast and Robust Tokenization , POS tagging , and Base Phrase Chunking. Abstract: In this paper, we address the problem of processing Modern St andard Arabic. We present the second generation of tools tha t process Arabic (AMIRA). AMIRA is a successor suite to the ASV MTools. The AMIRA toolkit includes a clitic tokenizer (TOK), part of speech tagger (POS) and base phrase chunker (B PC) shallow syntactic parser. The technology of AMIRA is based on supervised learning with no explicit dependence o xplicit modeling or knowledge of deep morphology. AMIRA is based on using a unified framework casting each of the component problems as a classification task. The underlying technology employs Support Vector Machines in a sequence modeling framework using the YAMCHA toolkit. The system is very fast and robust and allows for a number of va riable user settings depending on the disambiguation granularity. The AMIRA toolkit has been widely used for diff erent NLP (MT, IE, IR, NER, etc.) applications due to its speed and high performance.
- Weiwei Guo, Mona T. Diab. 2009. Improvements To Monolingual English Word Sense Disambiguation. Abstract: Word Sense Disambiguation remains one of the most complex problems facing computational linguists to date. In this paper we present modification to the graph based state of the art algorithm In-Degree. Our modifications entail augmenting the basic Lesk similarity measure with more relations based on the structure of WordNet, adding SemCor examples to the basic WordNet lexical resource and finally instead of using the LCH similarity measure for computing verb verb similarity in the In-Degree algorithm, we use JCN. We report results on three standard data sets using three different versions of WordNet. We report the highest performing monolingual unsupervised results to date on the Senseval 2 all words data set. Our system yields a performance of 62.7% using WordNet 1.7.1.
- K. Parton, K. McKeown, B. Coyne, Mona T. Diab, R. Grishman, Dilek Z. Hakkani-Tür, M. Harper, Heng Ji, Wei-Yun Ma, Adam Meyers, Sara Rosenthal, Ang Sun, Gökhan Tür, W. Xu, S. Yaman. 2009. Who, What, When, Where, Why? Comparing Multiple Approaches to the Cross-Lingual 5W Task. Abstract: Cross-lingual tasks are especially difficult due to the compounding effect of errors in language processing and errors in machine translation (MT). In this paper, we present an error analysis of a new cross-lingual task: the 5W task, a sentence-level understanding task which seeks to return the English 5W's (Who, What, When, Where and Why) corresponding to a Chinese sentence. We analyze systems that we developed, identifying specific problems in language processing and MT that cause errors. The best cross-lingual 5W system was still 19% worse than the best monolingual 5W system, which shows that MT significantly degrades sentence-level understanding. Neither source-language nor target-language analysis was able to circumvent problems in MT, although each approach had advantages relative to the other. A detailed error analysis across multiple systems suggests directions for future research on the problem.
- Mona T. Diab, Pravin Bhutada. 2009. Verb Noun Construction MWE Token Classification. Abstract: We address the problem of classifying multi-word expression tokens in running text. We focus our study on Verb-Noun Constructions (VNC) that vary in their idiomaticity depending on context. VNC tokens are classiﬁed as either idiomatic or literal. We present a supervised learning approach to the problem. We ex-periment with different features. Our approach yields the best results to date on MWE clas-siﬁcation combining different linguistically motivated features, the overall performance yields an F-measure of 84.58% corresponding to an F-measure of 89.96% for idiomaticity identiﬁcation and classiﬁcation and 62.03% for literal identiﬁ-cation and classiﬁcation.
- Yassine Benajiba, Mona T. Diab, Paolo Rosso. 2009. Using Language Independent and Language Specific Features to Enhance Arabic Named Entity Recognition. Abstract: The Named entity recognition task has been garnering significant attention as it has been shown to help improve the performance of many natural language processing applications. More recently, we are starting to see a surge in developing named entity recognition systems for languages other than English. With the relative abundance of resources for the Arabic language and a certain degree of maturation in the state of the art for processing Arabic, it is natural to see interest in developing NER systems for the language. In this paper, we investigate the impact of using different sets of features that are both language independent and language specific in a discriminative machine learning framework, namely, Support Vector Machines. We explore lexical, contextual and morphological features and nine data+sets of different ge nres and annotations. We systematically measure the impact of the different features in isolation and combined. We achieve the highest performance using a combination of all features, F1=82.71. Essentially combining language independent features with language specific ones yields the best performance on all the genres of text we investigate. However, on a class level, we observe that the different classes of named entities benefit differently from the morphological features employed.
- Mona T. Diab, Pravin Bhutada. 2009. Verb noun construction MWE token supervised classification. Abstract: We address the problem of classifying multiword expression tokens in running text. We focus our study on Verb-Noun Constructions (VNC) that vary in their idiomaticity depending on context. VNC tokens are classified as either idiomatic or literal. We present a supervised learning approach to the problem. We experiment with different features. Our approach yields the best results to date on MWE classification combining different linguistically motivated features, the overall performance yields an F-measure of 84.58% corresponding to an F-measure of 89.96% for idiomaticity identification and classification and 62.03% for literal identification and classification.
- Mona T. Diab, Lori S. Levin, T. Mitamura, Owen Rambow, Vinodkumar Prabhakaran, Weiwei Guo. 2009. Committed Belief Annotation and Tagging. Abstract: We present a preliminary pilot study of belief annotation and automatic tagging. Our objective is to explore semantic meaning beyond surface propositions. We aim to model people's cognitive states, namely their beliefs as expressed through linguistic means. We model the strength of their beliefs and their (the human) degree of commitment to their utterance. We explore only the perspective of the author of a text. We classify predicates into one of three possibilities: committed belief, non committed belief, or not applicable. We proceed to manually annotate data to that end, then we build a supervised framework to test the feasibility of automatically predicting these belief states. Even though the data is relatively small, we show that automatic prediction of a belief class is a feasible task. Using syntactic features, we are able to obtain significant improvements over a simple baseline of 23% F-measure absolute points. The best performing automatic tagging condition is where we use POS tag, word type feature AlphaNumeric, and shallow syntactic chunk information CHUNK. Our best overall performance is 53.97% F-measure.
- Ryan Roth, Owen Rambow, Nizar Habash, Mona T. Diab, C. Rudin. 2008. Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking. Abstract: We investigate the tasks of general morphological tagging, diacritization, and lemmatization for Arabic. We show that for all tasks we consider, both modeling the lexeme explicitly, and retuning the weights of individual classifiers for the specific task, improve the performance.
- Yassine Benajiba, Mona T. Diab, Paolo Rosso. 2008. Arabic Named Entity Recognition using Optimized Feature Sets. Abstract: The Named Entity Recognition (NER) task has been garnering significant attention in NLP as it helps improve the performance of many natural language processing applications. In this paper, we investigate the impact of using different sets of features in two discriminative machine learning frameworks, namely, Support Vector Machines and Conditional Random Fields using Arabic data. We explore lexical, contextual and morphological features on eight standardized data-sets of different genres. We measure the impact of the different features in isolation, rank them according to their impact for each named entity class and incrementally combine them in order to infer the optimal machine learning approach and feature set. Our system yields a performance of Fβ=1-measure=83.5 on ACE 2003 Broadcast News data.
- Mona T. Diab, Alessandro Moschitti, Daniele Pighin. 2008. Semantic Role Labeling Systems for Arabic using Kernel Methods. Abstract: There is a widely held belief in the natural language and computational linguistics communities that Semantic Role Labeling (SRL) is a significant step toward improving important applications, e.g. question answering and information extraction. In this paper, we present an SRL system for Modern Standard Arabic that exploits many aspects of the rich morphological features of the language. The experiments on the pilot Arabic Propbank data show that our system based on Support Vector Machines and Kernel Methods yields a global SRL F1 score of 82.17%, which improves the current state-of-the-art in Arabic SRL.
- Yassine Benajiba, Mona T. Diab, Paolo Rosso. 2008. ARABIC AMED E TITY RECOG ITIO : A SVM-BASED APPROACH. Abstract: The amed Entity Recognition ( ER) task has been garnering significant attention as it has been shown to help improve the performance of many atural Language Processing ( LP) applications. More recently, we are starting to see a surge in developing ER systems for languages other than English. With the relative abundance of resources for the Arabic language and a certain degree of maturation in the state of the art for processing Arabic, it is natural to see interest in developing ER systems for the language. In this paper, we investigate the impact of using different sets of features that are both language independent and language specific in a discriminative machine learning framework, namely, Support Vector Machines. We explore lexical, contextual and morphological features and nine data-sets of different genres and annotations. We systematically measure the impact of the different features in isolation and combined. We achieve the highest performance using a combination of all features. Combining all the features, our system yields an F1=82.71. Essentially combining language independent features with language specific ones yields the best performance on all the genres of text we
- Martha Palmer, O. Babko-Malaya, Ann Bies, Mona T. Diab, M. Maamouri, Aous Mansouri, W. Zaghouani. 2008. A Pilot Arabic Propbank. Abstract: In this paper, we present the details of creating a pilot Arabic proposition bank (Propbank). Propbanks exist for both English and Chinese. However the morphological and syntactic expression of linguistic phenomena in Arabic yields a very different type of process in creating an Arabic propbank. Hence, we highlight those characteristics of Arabic that make creating a propbank for the language a different challenge compared to the creation of an English Propbank.We believe that many of the lessons learned in dealing with Arabic could generalise to other languages that exhibit equally rich morphology and relatively free word order.
- Irina Matveeva, Chris Biemann, M. Choudhury, Mona T. Diab. 2008. Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing. Abstract: Recent years have shown an increased interest in bringing the field of graph theory into Natural Language Processing. In many NLP applications entities can be naturally represented as nodes in a graph and relations between them can be represented as edges. Recent research has shown that graph-based representations of linguistic units as diverse as words, sentences and documents give rise to novel and efficient solutions in a variety of NLP tasks, ranging from part of speech tagging, word sense disambiguation and parsing to information extraction, semantic role assignment, summarization and sentiment analysis. The contribution of the graph representation, in addition to its intuitiveness, resides in the possibility to relate linguistic entities beyond pairwise comparison. This volume contains papers accepted for presentation at the TextGraphs-3 2008 Workshop on Graph-Based Algorithms for Natural Language Processing. This event took place on August 24, 2008, in Manchester, UK, immediately following COLING 2008, the 22nd International Conference on Computational Linguistics. It was the third workshop on this topic, building on the success of the first and second TextGraphs workshop at HLT-NAACL 2006 and 2007. The workshop aimed at bringing together researchers working on problems related to the use of graph-based algorithms for Natural Language Processing and on the theory of graph-based methods. It addressed a broad spectrum of research areas to foster exchange of ideas and to help identify principles of using the graph notions that go beyond an ad-hoc usage. 
 
We issued calls for both regular and short, late-breaking papers. Six regular and three short papers were accepted for presentation, based on the careful reviews of our program committee. We are indebted to all program committee members for their thoughtful, high quality and elaborate reviews, especially considering our extremely tight time frame for reviewing. The papers appearing in this volume have surely benefited from their expert feedback. This year's workshop attracted papers employing graphs in a wide range of settings, so we are proud to present a very diverse program this year. N. Hathout acquires morphological structure from a lexicon employing the bipartite graph between headwords' formal semantic features. Mapping of text to a graph-based meaning representation is conducted by S. Muresan, using a recent grammar formalism. A. B. Masse et al. lay out a general theoretical framework for addressing the symbol grounding problem in digital dictionaries. A. Moschitti and F.M. Zanzotto use Kernel methods on tree pairs for recognizing textual entailment. Combining co-occurrence and phonological similarity, K. Ichioka and F. Fukumoto semantically cluster onomatopoetic words in Japanese. D. Rao et al. examine several random walk based approaches to measure word similarity. B. McGillivray et al. address cluster overlapping with correspondence analysis and apply their method to cluster English and Italian verbs and nouns. A domain-specific summarization method ranking nodes in a graph of concepts is introduced by L. Plaza Morales et al. The topology of associative concept dictionaries is modeled by H. Akama et al., who report interesting scale free properties of such networks.
- Mona T. Diab, Alessandro Moschitti, Daniele Pighin. 2007. CUNIT: A Semantic Role Labeling System for Modern Standard Arabic. Abstract: In this paper, we present a system for Arabic semantic role labeling (SRL) based on SVMs and standard features. The system is evaluated on the released SEMEVAL 2007 development and test data. The results show an Fβ=1 score of 94.06 on argument boundary detection and an overall Fβ=1 score of 81.43 on the complete semantic role labeling task using gold parse trees.
- Mona T. Diab. 2007. Improved Arabic Base Phrase Chunking with a new enriched POS tag set. Abstract: Base Phrase Chunking (BPC) or shallow syntactic parsing is proving to be a task of interest to many natural language processing applications. In this paper, A BPC system is introduced that improves over state of the art performance in BPC using a new part of speech tag (POS) set. The new POS tag set, ERTS, reflects some of the morphological features specific to Modern Standard Arabic. ERTS explicitly encodes definiteness, number and gender information increasing the number of tags from 25 in the standard LDC reduced tag set to 75 tags. For the BPC task, we introduce a more language specific set of definitions for the base phrase annotations. We employ a support vector machine approach for both the POS tagging and the BPC processes. The POS tagging performance using this enriched tag set, ERTS, is at 96.13% accuracy. In the BPC experiments, we vary the feature set along two factors: the POS tag set and a set of explicitly encoded morphological features. Using the ERTS POS tagset, BPC achieves the highest overall Fβ=1 of 96.33% on 10 different chunk types outperforming the use of the standard POS tag set even when explicit morphological features are present.
- K. Kirchhoff, Owen Rambow, Nizar Habash, Mona T. Diab. 2007. Semi-automatic error analysis for large-scale statistical machine translation. Abstract: This paper presents a general framework for semi-automatic error analysis in large-scale statistical machine translation (SMT) systems. The main objective is to relate characteristics of input documents (which can be either in text or audio form) to the system's overall translation performance and thus identify particularly problematic input characteristics (e.g. source, genre, dialect, etc.). Various measurements of these factors are extracted from the input, either automatically or by human annotation, and are related to translation performance scores by means of mutual information. We apply this analysis to a state-of-the-art large-scale SMT system operating on Chinese and Arabic text and audio documents, and demonstrate how the proposed error analysis can help identify system weaknesses.
- Mona T. Diab, M. Alkhalifa, S. ElKateb, C. Fellbaum, Aous Mansouri, Martha Palmer. 2007. SemEval-2007 Task 18: Arabic Semantic Labeling. Abstract: In this paper, we present the details of the Arabic Semantic Labeling task. We describe some of the features of Arabic that are relevant for the task. The task comprises two subtasks: Arabic word sense disambiguation and Arabic semantic role labeling. The task focuses on modern standard Arabic.
- Mona T. Diab, Mahmoud A. Ghoneim, Nizar Habash. 2007. Arabic diacritization in the context of statistical machine translation. Abstract: Diacritics in Arabic are optional orthographic symbols typically representing short vowels. Most Arabic text is underspecified for diacritics. However, we do observe partial diacritization depending on genre and domain. In this paper, we investigate the impact of Arabic diacritization on statistical machine translation (SMT). We define several diacritization schemes ranging from full to partial diacritization. We explore the impact of the defined schemes on SMT in two different modes which tease apart the effect of diacritization on the alignment and its consequences on decoding. Our results show that none of the partial diacritization schemes significantly varies in performance from the no-diacritization baseline despite the increase in the number of types in the data. However, a full diacritization scheme performs significantly worse than no diacritization. Crucially, our research suggests that the SMT performance is positively correlated with the increase in the number of tokens correctly affected by a diacritization scheme and the high F-score of the automatic assignment of the particular diacritic.
- Mona T. Diab, Nizar Habash. 2007. Arabic Dialect Processing Tutorial. Abstract: Language exists in a natural continuum, both historically and geographically. The term language as opposed to dialect is only an expression of power and dominance of one group/ideology over another. In the Arab world, politics (Arab nationalism) and religion (Islam) are what shape the perception of the distinction between the Arabic language and an Arabic dialect. This power relationship is similar to others that exist between languages and their dialects. However, the high degree of difference between standard Arabic and its dialects and the fact that standard Arabic is not any Arab's native language sets the Arabic linguistic situation apart.
- N. Snider, Mona T. Diab. 2006. Unsupervised Induction of Modern Standard Arabic Verb Classes. Abstract: We exploit the resources in the Arabic Treebank (ATB) and Arabic Gigaword (AG) to determine the best features for the novel task of automatically creating lexical semantic verb classes for Modern Standard Arabic (MSA). The verbs are classified into groups that share semantic elements of meaning as they exhibit similar syntactic behavior. The results of the clustering experiments are compared with a gold standard set of classes, which is approximated by using the noisy English translations provided in the ATB to create Levin-like classes for MSA. The quality of the clusters is found to be sensitive to the inclusion of syntactic frames, LSA vectors, morphological pattern, and subject animacy. The best set of parameters yields an Fβ=1 score of 0.456, compared to a random baseline of an Fβ=1 score of 0.205.
- N. Snider, Mona T. Diab. 2006. Unsupervised Induction of Modern Standard Arabic Verb Classes Using Syntactic Frames and LSA. Abstract: We exploit the resources in the Arabic Treebank (ATB) for the novel task of automatically creating lexical semantic verb classes for Modern Standard Arabic (MSA). Verbs are clustered into groups that share semantic elements of meaning as they exhibit similar syntactic behavior. The results of the clustering experiments are compared with a gold standard set of classes, which is approximated by using the noisy English translations provided in the ATB to create Levin-like classes for MSA. The quality of the clusters is found to be sensitive to the inclusion of information about lexical heads of the constituents in the syntactic frames, as well as parameters of the clustering algorithm. The best set of parameters yields an Fβ=1 score of 0.501, compared to a random baseline with an Fβ=1 score of 0.37.
- Mona T. Diab, Nizar Habash, Ńawilatan ζadīdatan, Nizar not-bought-not. 2006. Arabic Dialect Processing. Abstract: The existence of dialects for any language constitutes a challenge for Natural Language Processing (NLP) in general since it adds another set of variation dimensions from a known standard. The problem is particularly interesting and challenging in Arabic and its different dialects, where the diversion from the standard could, in some linguistic theories, warrant a classification as a different language. This problem would not be as pronounced if standard Arabic were to be a living language, however it is not. Any realistic and practical approach to processing Arabic will have to account for dialectal usage since it is so pervasive. In this tutorial, we will attempt to highlight different dialectal phenomena and how they migrate from the standard and why they pose challenges to NLP. Our tutorial will have four different parts: First, we will give you a background layout of issues for standard Arabic NLP. Then, we will present a high level generic view of dialects and different aspects of them that are of interest for the NLP community, addressing both text and speech issues in addition to standardization issues. We will focus in depth on two aspects of dialect processing in the third and fourth parts of the tutorial, namely, dialectal morphology and dialectal syntactic parsing. Throughout the presentation we will make references to the different resources available and draw contrastive links with standard Arabic and English. We will provide links to recent publications and available toolkits/resources for all four sections.
- David Chiang, Mona T. Diab, Nizar Habash, Owen Rambow, S. Shareef. 2006. Parsing Arabic Dialects. Abstract: The Arabic language is a collection of spoken dialects with important phonological, morphological, lexical, and syntactic differences, along with a standard written language, Modern Standard Arabic (MSA). Since the spoken dialects are not officially written, it is very costly to obtain adequate corpora to use for training dialect NLP tools such as parsers. In this paper, we address the problem of parsing transcribed spoken Levantine Arabic (LA).We do not assume the existence of any annotated LA corpus (except for development and testing), nor of a parallel corpus LAMSA. Instead, we use explicit knowledge about the relation between LA and MSA.
- M. Maamouri, Ann Bies, Tim Buckwalter, Mona T. Diab, Nizar Habash, Owen Rambow, Dalila Tabessi. 2006. Developing and Using a Pilot Dialectal Arabic Treebank. Abstract: In this paper, we describe the methodological procedures and issues that emerged from the development of a pilot Levantine Arabic Treebank (LATB) at the Linguistic Data Consortium (LDC) and its use at the Johns Hopkins University (JHU) Center for Language and Speech Processing workshop on Parsing Arabic Dialects (PAD). This pilot, consisting of morphological and syntactic annotation of approximately 26,000 words of Levantine Arabic conversational telephone speech, was developed under severe time constraints; hence the LDC team drew on their experience in treebanking Modern Standard Arabic (MSA) text. The resulting Levantine dialect treebanked corpus was used by the PAD team to develop and evaluate parsers for Levantine dialect texts. The parsers were trained on MSA resources and adapted using dialect-MSA lexical resources (some developed especially for this task) and existing linguistic knowledge about syntactic differences between MSA and dialect. The use of the LATB for development and evaluation of syntactic parsers allowed the PAD team to provide feedbasck to the LDC treebank developers. In this paper, we describe the creation of resources for this corpus, as well as transformations on the corpus to eliminate speech effects and lessen the gap between our pre-existing MSA resources and the new dialectal corpus
- Kareem Darwish, Mona T. Diab, Nizar Habash. 2005. Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages. Abstract: Welcome to the ACL 2005 Workshop on Computational Approaches to Semitic Languages. This workshop is a sequel to the ACL 2002 workshop and shares its goals of: (i) heightening awareness amongst Semitic-language researchers of shared breakthroughs and challenges, (ii) highlighting issues common to all Semitic languages as much as possible, (iii) encouraging the potential for developing coordinated approaches; and (iv) in addition, leveraging resource and tool creation for less prominent members of the Semitic language family. 
 
We received 21 submissions, we accepted 12. The accepted papers cover several languages: Modern Standard Arabic, Dialectal Arabic, Hebrew, and Amharic. They cover a span of topics in computational linguistics, from morphological analysis and disambiguation and diacritization to information retrieval and document classification using both symbolic and statistical approaches.
- Mona T. Diab. 2004. An Unsupervised Approach for Bootstrapping Arabic Sense Tagging. Abstract: To date, there are no WSD systems for Arabic. In this paper we present and evaluate a novel unsupervised approach, SALAAM, which exploits translational correspondences between words in a parallel Arabic English corpus to annotate Arabic text using an English WordNet taxonomy. We illustrate that our approach is highly accurate in ≤ 90.1% of the evaluated data items based on Arabic native judgement ratings and annotations. Moreover, the obtained results are competitive with state-of-the-art unsupervised English WSD systems when evaluated on English data.
- Mona T. Diab, K. Hacioglu, Dan Jurafsky. 2004. Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks. Abstract: To date, there are no fully automated systems addressing the community's need for fundamental language processing tools for Arabic text. In this paper, we present a Support Vector Machine (SVM) based approach to automatically tokenize (segmenting off clitics), part-of-speech (POS) tag and annotate base phrases (BPs) in Arabic text. We adapt highly accurate tools that have been developed for English text and apply them to Arabic text. Using standard evaluation metrics, we report that the SVM-TOK tokenizer achieves an Fβ=1 score of 99.12, the SVM-POS tagger achieves an accuracy of 95.49%, and the SVM-BP chunker yields an Fβ=1 score of 92.08.
- Mona T. Diab. 2004. Relieving the data Acquisition Bottleneck in Word Sense Disambiguation. Abstract: Supervised learning methods for WSD yield better performance than unsupervised methods. Yet the availability of clean training data for the former is still a severe challenge. In this paper, we present an unsupervised bootstrapping approach for WSD which exploits huge amounts of automatically generated noisy data for training within a supervised learning framework. The method is evaluated using the 29 nouns in the English Lexical Sample task of SENSEVAL 2. Our algorithm does as well as supervised algorithms on 31% of this test set, which is an improvement of 11% (absolute) over state-of-the-art bootstrapping WSD algorithms. We identify seven different factors that impact the performance of our system.
- Mona T. Diab, P. Resnik. 2002. An Unsupervised Method for Word Sense Tagging using Parallel Corpora. Abstract: We present an unsupervised method for word sense disambiguation that exploits translation correspondences in parallel corpora. The technique takes advantage of the fact that cross-language lexicalizations of the same concept tend to be consistent, preserving some core element of its semantics, and yet also variable, reflecting differing translator preferences and the influence of context. Working with parallel corpora introduces an extra complication for evaluation, since it is difficult to find a corpus that is both sense tagged and parallel with another language; therefore we use pseudo-translations, created by machine translation systems, in order to make possible the evaluation of the approach against a standard test set. The results demonstrate that word-level translation correspondences are a valuable source of information for sense disambiguation.
- Mona T. Diab. 2000. An Unsupervised Method for Multilingual Word Sense Tagging Using Parallel Corpora. Abstract: With an increasing number of languages making their way to our desktops everyday via the Internet, researchers have come to realize the lack of linguistic knowledge resources for scarcely represented/studied languages. In an attempt to bootstrap some of the required linguistic resources for some of those languages, this paper presents an unsupervised method for automatic multilingual word sense tagging using parallel corpora. The method is evaluated on the English Brown corpus and its translation into three different languages: French, German and Spanish. A preliminary evaluation of the proposed method yielded results of up to 79% accuracy rate for the English data on 81.8% of the SemCor manually tagged data.
- Mona T. Diab, S. Finch. 2000. A statistical word-level translation model for comparable corpora. Abstract: In this paper, we present a model of statistical word-level mapping for comparable corpora. The approach is based on the assumption that if two terms have close distributional profiles, their corresponding translations' distributional profiles should be close in a comparable corpus. The proposed model is described. A preliminary investigation on intralanguage comparable corpora is laid out. The preliminary results are >92% accurate, suggesting the feasibility of the model. The model needs to undergo some improvements and should be tested cross linguistically before assessing its significance.
- Terry P. Riopka, Mona T. Diab, Peter Book. 2000. Quantifying and Interpreting the Effect of Intelligent Information Exchange Between Chromosomes in a Human Simulation of a Genetic Algorithm. Abstract: A genetic algorithm is simulated using human bein gs as "chromosomes" in a preliminary study intended to quantify and interpre t the effect of intelligent information exchange on genetic algorithm performance. Two factors are varied: the amount of information supplied to the cohort and the type of data manipulation allowed during the exchange. A human simulated genetic algorithm is ru n for each combination of factors as well as a machine simulation for comparison. Qualit ative analysis of recorded conversations indicate extensive use of memory and development of block biases during genetic algorithm evolution. Informal analysis shows that genetic alg orithm simulations using complex data manipulations combined with exact knowledge of string fitnesses seem to out-perform a standard machine implementation for the given optim ization fitness function. Interestingly, polar combinations: simple data manipulation/minimum information and complex data manipulation/maximum information simulations seem to out-perform other combinations. A genetic algorithm can be interpreted as a process of simulated evolution in which chromosome strings gradually modify the information encoded in their s chemata. Some form of replication based on string fitness is typically used to simulate natural selec tion and incorporate the Darwinian notion of surviv al of the fittest. In addition, strings are altered accordin g to the probabilistic rules of various operators, of which crossovers and mutations are the most well known. In this information exchange phase, the strings (heretofore referred to as chromosomes) exchange symbols probabilistically to try to impr ove their fitness in order to survive through to the following generatio n. A building block hypothesis has been suggested but not theoretically proven to be the primary impetus for solution improvement over generations. Efforts to develop analytical methods to determine the efficac y of problem coding/operator combinations have met with limited success but, nevertheless, strongly su pport this contention. According to the hypothesis , chromosomes in the genetic population exchange varying sized blocks of consecutive symbols which, over time, tend to proliferate in the components of chro mosomes that survive from one generation to the nex t. In current implementations of genetic algorithms, the application of the various operators is probabilist ic. In most cases these probabilities are chosen based on empirical studies and heuristics and are often non- optimal for any given application. An alternative mechanism is proposed which uses memory and logical reasoning to determine how information is exchanged between chromosomes. If memory, logical reasoning and more complex data manipulation could be exploited to streamline the s earch for these building blocks, the efficacy of th e genetic algorithm search might be improved and coul d also lead to more robust automated genetic algori thm
- P. Resnik, Mona T. Diab. 2000. Measuring Verb Similarity - eScholarship. Abstract: Measuring Verb Similarity Philip Resnik and Mona Diab Department of Linguistics and Institute for Advanced Computer Studies University of Maryland College Park, MD USA f resnik,mdiab g @umiacs.umd.edu Abstract The way we model semantic similarity is closely tied to our understanding of linguistic representations. We present several models of semantic similarity, based on diering representational assumptions, and investigate their properties via comparison with human ratings of verb similarity. The results oer insight into the bases for human similarity judgments and provide a testbed for further investigation of the interactions among syn- tactic properties, semantic structure, and semantic con- tent. Introduction The way we model semantic similarity is closely tied to our understanding of how linguistic representations are acquired and used. Some models of similarity, such as Tversky's (1977), assume an explicit set of features over which a similarity measure can be computed, and re- cent computational methods for measuring word similar- ity can be thought of as an update of this idea on a large scale, representing words in terms of distributional fea- tures acquired via analysis of text corpora (e.g., Brown, Della Pietra, deSouza, Lai, & Mercer, 1992; Schutze, 1993). Other methods, following in the semantic net- works tradition of Quillian (1968), focus less on explicit features and more on relationships among lexical items within a conceptual taxonomy, sometimes going beyond taxonomic relationships to also take advantage of fre- quency information derived from corpora (e.g., Rada, Mili, Bicknell, & Blettner, 1989; Resnik, 1999). Although some of these approaches are not explicitly designed as cognitive models, we have proposed that pre- diction of human similarity can provide a useful point of comparison for computational measures of similarity, noting that one must be aware that such comparisons can be quite sensitive to the specic choice of test items (Resnik, 1999). To date, we are only aware of compar- isons having been done using noun similarity. In this paper, we consider the problem of measuring the semantic similarity of verbs. Verb similarity is in many respects a dierent problem from noun similar- ity, because verb representations are generally viewed as possessing properties that nouns do not, such as syn- tactic subcategorization restrictions, selectional prefer- ences, and event structure, and there are dependencies among these properties. 1 This means that particular Admittedly, the relevant contrast may turn out not to care must be taken in selecting items, as discussed below, and it also means that the same computational measures may be capturing dierent properties for verbs than for nouns. For example, the is-a relationship in WordNet's verb taxonomy (Fellbaum, 1998), central in the compu- tation of some measures, signies generalization accord- ing to manner, as in devour is-a eat ; concomitantly, the verb taxonomy is considerably wider and shallower than WordNet's noun taxonomy. Similarly, measures based on syntactic dependencies may be sensitive to syntactic adjuncts, such as locative and temporal modiers, that occur predominantly with verbs rather than with nouns. In what follows, we rst discuss several dierent mea- sures of word similarity and their properties. We then describe an experiment designed to obtain human sim- ilarity ratings for pairs of verbs, discuss the t of the alternative measures to the human ratings, and suggest some implications of these results for future work. Models of Verb Similarity We consider three classes of similarity measure, corre- sponding to three kinds of lexical representation. In the rst, verbs are associated with nodes in a semantic net- work. In the second, verbs are represented by distri- butional syntactic co-occurrence features obtained via analysis of a corpus. In the third, verbs are associated with lexical entries represented according to a theory of lexical conceptual structure. These classes of represen- tation can be viewed as occupying three dierent points on the spectrum from non-syntactic to syntactically rel- evant facets of verb meaning. Taxonomic Models Taxonomic models of lexical and conceptual knowledge have a long history. In this work we use WordNet version 1.5, a large scale taxonomic representation of concepts lexicalized in English. As a model of the lexicon, Word- Net's verb hierarchy is limited by design to paradigmatic relations, in explicit contrast to attempts to organize se- mantically coherent verb classes through shared syntac- tic behavior. The simplest and most traditional measure of semantic similarity in a taxonomy counts the number of edges in- be part-of-speech per se ; one could argue that some nouns carry similar kinds of participant information, observing, for example, that x's gift of y to z parallels x gave y to z . We are not attempting to address that issue here.
- Mona T. Diab, J. Schuster, P. Bock. 2000. A Preliminary Statistical Investigation into the Impace of an N-Gram Analysis Approach Based on World Syntactic Categories Toward Text Author Classification. Abstract: Abstract : Quantitative analysis of literary style has heretofore utilized semantic elements-word counts. This research attempts to identify quantifiable syntactic elements of style that can be used for author identification. The measurement of syntactic elements utilizes a dictionary with one part of speech per word and looks at phrases delimited by punctuation marks. Different size permutations of words - referred to as grams - are counted within each text. Correlations are measured amongst the gram frequencies of eight texts pertaining to four authors, both contemporary and non-contemporary. The correlations are performed across different gram sizes of words. The same treatment is applied to a target text, the Funeral Elegy text. The approach holds for classifying texts temporally consistently across the various gram sizes. Yet a finer grained investigation is required to certify the authorship of the Funeral Elegy text.
- P. Resnik, Mona T. Diab. 2000. Measuring Verb Similarity. Abstract: Abstract : The way we model semantic similarity is closely tied to our understanding of linguistic representations. We present several models of semantic similarity, based on differing rep- resentational assumptions, and investigate their properties via comparison with human ratings of verb similarity. The results offer insight into the bases for human similarity judgments and provide a testbed for further investigation of the interactions among syntactic properties, semantic structure, and semantic content.
- E. André, Mark W. Johnson, M. de Rijke, Jason Baldridge, Laura Kallmeyer, M. Riley, S. Bangalore, A. Kehler, Fabio Rinaldi, P. Blackburn, A. Kilgarriff, G. Ritchie, Johan Bos, Kevin Knight, Brian Roark, Antal van den Bosch, Philipp Koehn, J. Rogers, T. Brants, R. Koeling, D. Roth, C. Brew, A. Korhonen, Giorgio Satta, Ted Briscoe, András Kornai, K. Schuler, Razvan C. Bunescu, Emiel Krahmer, F. Sebastiani, J. Burstein, S. Kulick, Julie C. Sedivy, L. Cavedon, Mirella Lapata, Violeta Seretan, Keh-Jiann Chen, Gina-Anne Levow, Stuart M. Shieber, Timothy Chklovski, David Lewis, Advaith Siddharthan, Massimiliano Ciaramita, G. Ligozat, Michel Simard, S. Clark, Jimmy J. Lin, David Smith, Michael Collins, I. Mani, H. Somers, Ann A. Copestake, Christopher Manning, Radu Soricut, J. Curran, D. Marcu, R. Sproat, Ido Dagan, M. Maybury, Amanda Stent, Mona T. Diab, Mandar Mitra, M. Strube, P. Edmonds, Mehryar Mohri, M. Surdeanu, D. Eichmann, Alessandro Moschitti, M. Swerts, T. M. Ellison, M. Nederhof, Kristina Toutanova, K. Erk, A. Nenkova, S. Tseng, George Foster, H. Ney, G. Tur, Pascale Fung, S. Oepen, Sriram Venkatapathy, Claire Gardent, Kemal Oflazer, Clare R. Voss, Josef van Genabith, Tom O'Hara, B. Webber, D. Gildea, Gerald Penn, David Weir, Roxana Girju, Massimo Poesio, R. Wicentowski, Claire Grover, Sameer Pradhan, J. Wiebe, Nizar Habash, D. Prescher, Florian Wolf, S. Harabagiu, A. Prince, Fei Xia, P. Heeman, J. Pustejovsky, Nianwen Xue, J. Hockenmaier, D. R. Scott, Wen-tau Yih, R. Hwa, P. Resnik, Deniz Yuret, Su-Ching Jian, S. Riezler, Fabio Massimo Zanzotto. 1990. Reviewers, Volume 33. Abstract: Jennifer Adair Jennifer Aldrich Angela Baum Michelle Bauml Doris Bergen Gloria Boutte Amanda Branscombe Lorraine Breffni Amy Broemmel Christopher Brown Nancy Brown Deborah Bruns Julie Bullard Jan G. Burcham Kathryn Castle Lori Caudle Christine Chaille Dong Hwa Choi Kenneth Counselman Leslie Couse Kay Cutler Sara Davis Anne Dorsey Angela Eckhoff Roy Evans Beatrice Fennimore Nancy Freeman Doris Fromberg Vicki Garavuso Dawn Garbett Jennifer Gilliard Dierdre Greer Sophia Han Sanna Harjusola-Webb Helen Hedges Julie Herron Rebecca Huss-Keeler Mary Jensen Jim Johnson Ithel Jones Laura Kates Virginia Keen Jill Klefstad Byran Korth Janice Kroeger Vickie Lake Karen LaParo Yuen-Ling Joyce Li Shannon McNair Daniel Meier Monica Miller-Marsh Mary Jane Moran Mark Nagasawa Stacey Neuharth-Pritchett Shelley Nicholson John Nimmo Deborah Norris Nadjwa Norton Cynthia Paris Will Parnell Jean Plaisir Beth Powers-Costello Patricia Ramsey Susan Recchia Stuart Reifel Frances Rust Sharon Ryan Catherine Scott-Little Lynda Kathryn Sharp Meagan Shedd Frances Sherwood Sara Sherwood Mariana Souto-Manning Dolores Stegelin Andrew Stremmel Judit Szente
- Mona T. Diab. None. Feasibility of Bootstrapping an Arabic WordNet Leveraging Parallel Corpora and an English WordNet. Abstract: In this paper, we propose the automatic bootstrapping of a Modern Standard Arabic WordNet on the lexeme level using Arabic English parallel corpora and an English WordNet. We address the feasibility of such an endeavor and present a qualitative evaluation of the meaning correspondences cross linguistically between Arabic and English. We further present an automatic means of performing this task using an unsupervised Word Sense Disambiguation System. We test the feasibility of the bootstrapping by qualitatively evaluating the meaning definition projection of English words onto their Arabic translations. We manually evaluate 447 word instances of the Arabic words that correspond to correctly sense tagged English words using English WordNet 1.7. from the SENSEVAL 3 data. The words evaluated correspond to Nouns, verbs, adjectives in English. We find that for Arabic verbs, adjectives and nouns, on average 52.3% of all the words examined, the corresponding English WordNet set of definitions are sufficient as definitions for the Arabic translation word; 39.96% of the Arabic words correspond to specific subsets of the WordNet definitions; and finally, 7.8% of the Arabic words comprise supersets of their corresponding English WordNet translation definitions. These results are very encouraging as they are similar to those obtained by researchers building EuroWordNet.
