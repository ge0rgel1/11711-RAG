Rita Singh
Paper count: 146
- Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj. 2023. BASS: Block-wise Adaptation for Speech Summarization. Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
- Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj. 2023. Rethinking Voice-Face Correlation: A Geometry View. Abstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.
- Rita Singh. 2023. A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker’s Voice. Abstract: Over the past decades, many machine-learning- and artificial-intelligence-based technologies have been created to deduce biometric or bio-relevant parameters of speakers from their voice. These voice profiling technologies have targeted a wide range of parameters, from diseases to environmental factors, based largely on the fact that they are known to influence voice. Recently, some have also explored the prediction of parameters whose influence on voice is not easily observable through data-opportunistic biomarker discovery techniques. However, given the enormous range of factors that can possibly influence voice, more informed methods for selecting those that may be potentially deducible from voice are needed. To this end, this paper proposes a simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data. The links represent reasonable selection criteria for use by computational by profiling technologies only, and are not intended to establish any unknown biological facts. The proposed algorithm is validated using a simple example from medical literature—that of the clinically observed effects of specific chromosomal microdeletion syndromes on the vocal characteristics of affected people. In this example, the algorithm attempts to link the genes involved in these syndromes to a single example gene (FOXP2) that is known to play a broad role in voice production. We show that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected. Validation experiments and subsequent analyses confirm that the methodology could be potentially useful in predicting the existence of vocal signatures in naïve cases where their existence has not been otherwise observed.
- Wayne Zhao, Rita Singh. 2023. Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation. Abstract: During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker’s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker’s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker’s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.
- Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj. 2023. Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.
- Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj. 2023. The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features. Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.
- Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang. 2023. Pengi: An Audio Language Model for Audio Tasks. Abstract: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding
- Roshan Sharma, Tyler Vuong, Mark Lindsey, Hira Dhamyal, Rita Singh, B. Raj. 2022. Self-supervision and Learnable STRFs for Age, Emotion, and Country Prediction. Abstract: This work presents a multitask approach to the simultaneous estimation of age, country of origin, and emotion given vocal burst audio for the 2022 ICML Expressive Vocalizations Challenge E X V O -M ULTI T ASK track. The method of choice utilized a combination of spectro-temporal modulation and self-supervised features, followed by an encoder-decoder network organized in a multitask paradigm. We evaluate the complementarity between the tasks posed by examining independent task-speciﬁc and joint models, and explore the relative strengths of different feature sets. We also introduce a simple score fusion mechanism to leverage the complementarity of different feature sets for this task. We ﬁnd that robust data preprocessing in con-junction with score fusion over spectro-temporal receptive ﬁeld and HuBERT models achieved our best E X V O -M ULTI T ASK test score of 0.412. framework may be This is the hypothesis which to
- Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, B. Raj, Rita Singh. 2022. Describing emotions with acoustic property prompts for speech emotion recognition. Abstract: Emotions lie on a broad continuum and treating emotions as a discrete number of classes limits the ability of a model to capture the nuances in the continuum. The challenge is how to describe the nuances of emotions and how to enable a model to learn the descriptions. In this work, we devise a method to automatically create a description (or prompt) for a given audio by computing acoustic properties, such as pitch, loudness, speech rate, and articulation rate. We pair a prompt with its corresponding audio using 5 different emotion datasets. We trained a neural network model using these audio-text pairs. Then, we evaluate the model using one more dataset. We investigate how the model can learn to associate the audio with the descriptions, resulting in performance improvement of Speech Emotion Recognition and Speech Audio Retrieval. We expect our ﬁndings to motivate research describing the broad continuum of emotion.
- Rita Singh. 2022. Connecting human voice profiling to genomics: A predictive algorithm for linking speech phenotypes to genetic microdeletion syndromes. Abstract: Changes in vocal acoustic patterns are known to correlate with the occurrence of several diseases and syndromes, many of which do not directly affect the structures or processes that control voice production. In such cases, it is difficult to support the existence of correlated changes in voice. This paper presents a methodology for identifying potential genomic bases for such correlations, by finding links between specific genes involved in the conditions under study, and those involved in voice, speech or language generation. Syndromes associated with chromosomal microdeletions are examined as an illustrative case, with focus on their linkage to the FOXP2 gene which has been strongly implicated in speech and language disorders. A novel path-finding graph algorithm to detect pathway chains that connect the the former to the latter is proposed. Statistical analysis of ensembles of “voice” chains detected by this algorithm indicates that they are predictive of speech phenotypes for the syndromes. Algorithmic findings are validated against clinical findings in the literature pertaining to the actual speech phenotypes that have been found to be associated with these syndromes. This methodology may also potentially be used to predict the existence of voice biomarkers in naїve cases where the existence of voice biomarkers has not already been established.
- Roshan Sharma, Hira Dhamyal, B. Raj, Rita Singh. 2022. Unifying the Discrete and Continuous Emotion labels for Speech Emotion Recognition. Abstract: Traditionally, in paralinguistic analysis for emotion detection from speech, emotions have been identified with discrete or dimensional (continuous-valued) labels. Accordingly, models that have been proposed for emotion detection use one or the other of these label types. However, psychologists like Russell and Plutchik have proposed theories and models that unite these views, maintaining that these representations have shared and complementary information. This paper is an attempt to validate these viewpoints computationally. To this end, we propose a model to jointly predict continuous and discrete emotional attributes and show how the relationship between these can be utilized to improve the robustness and performance of emotion recognition tasks. Our approach comprises multi-task and hierarchical multi-task learning frameworks that jointly model the relationships between continuous-valued and discrete emotion labels. Experimental results on two widely used datasets (IEMOCAP and MSPPodcast) for speech-based emotion recognition show that our model results in statistically significant improvements in performance over strong baselines with non-unified approaches. We also demonstrate that using one type of label (discrete or continuous-valued) for training improves recognition performance in tasks that use the other type of label. Experimental results and reasoning for this approach (called the mismatched training approach) are also presented.
- Hira Dhamyal, B. Raj, Rita Singh. 2022. Positional Encoding for Capturing Modality Specific Cadence for Emotion Detection. Abstract: Emotion detection from a single modality, such as an audio or text stream, has been known to be a challenging task. While encouraging results have been obtained by using joint evidence from multiple streams, combining such evidence in optimal ways is an open challenge. In this paper, we claim that al-though the multi-modalities like audio, phoneme sequence ids and word sequence ids are related to each other, they also have their individual local ‘cadence’, which is important to be modelled for the task of emotion recognition. We model the local cadence by using separate ‘positional encodings’ for each modality in a transformer architecture. Our results show that emotion detection based on this strategy is better than when the modality specific cadence is ignored or normalized out by using a shared positional encoding. We also find that capturing the modality interdependence is not as important as is capturing of the local cadence of individual modalities. We conduct our experiments on the IEMOCAP and CMU-MOSI datasets to demonstrate the effectiveness of the proposed methodology for combining multi-modal evidence.
- Ankit Shah, Hira Dhamyal, Yang Gao, Rita Singh, B. Raj. 2022. On the pragmatism of using binary classifiers over data intensive neural network classifiers for detection of COVID-19 from voice. Abstract: Lately, there has been a global effort by multiple research groups to detect COVID-19 from voice. Different researchers use different kinds of information from the voice signal to achieve this. Various types of phonated sounds and the sound of cough and breath have all been used with varying degrees of success in automated voice-based COVID-19 detection apps. In this paper, we show that detecting COVID-19 from voice does not require custom-made non-standard features or complicated neural network classifiers rather it can be successfully done with just standard features and simple binary classifiers. In fact, we show that the latter is not only more accurate and interpretable and also more computationally efficient in that they can be run locally on small devices. We demonstrate this from a human-curated dataset collected and calibrated in clinical settings. On this dataset which comprises over 1000 speakers, a simple binary classifier is able to achieve 94% detection accuracy.
- Yandong Wen, Weiyang Liu, Adrian Weller, B. Raj, Rita Singh. 2021. SphereFace2: Binary Classification is All You Need for Deep Face Recognition. Abstract: State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we first identify the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the “competitive” nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this “one-vs-all” binary classification framework so that it can outperform current competitive methods. We conduct comprehensive experiments on popular benchmarks to demonstrate that SphereFace2 can consistently outperform current state-of-the-art deep face recognition methods.
- Rita Singh, Ankit Shah, Hira Dhamyal. 2021. An Overview of Techniques for Biomarker Discovery in Voice Signal. Abstract: This paper reflects on the effect of several categories of medical conditions on human voice, focusing on those that may be hypothesized to have effects on voice, but for which the changes themselves may be subtle enough to have eluded observation in standard analytical examinations of the voice signal. It presents three categories of techniques that can potentially uncover such elusive biomarkers and allow them to be measured and used for predictive and diagnostic purposes. These approaches include proxy techniques, model-based analytical techniques and data-driven AI techniques.
- Weiyang Liu, Yandong Wen, B. Raj, Rita Singh, Adrian Weller. 2021. SphereFace Revived: Unifying Hyperspherical Face Recognition. Abstract: This paper addresses the deep face recognition problem under an open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. To this end, hyperspherical face recognition, as a promising line of research, has attracted increasing attention and gradually become a major focus in face recognition research. As one of the earliest works in hyperspherical face recognition, SphereFace explicitly proposed to learn face embeddings with large inter-class angular margin. However, SphereFace still suffers from severe training instability which limits its application in practice. In order to address this problem, we introduce a unified framework to understand large angular margin in hyperspherical face recognition. Under this framework, we extend the study of SphereFace and propose an improved variant with substantially better training stability – SphereFace-R. Specifically, we propose two novel ways to implement the multiplicative margin, and study SphereFace-R under three different feature normalization schemes (no feature normalization, hard feature normalization and soft feature normalization). We also propose an implementation strategy – “characteristic gradient detachment” – to stabilize training. Extensive experiments on SphereFace-R show that it is consistently better than or competitive with state-of-the-art methods.
- Yang Gao, Tyler Vuong, Mahsa Elyasi, Gaurav Bharaj, Rita Singh. 2021. Generalized Spoofing Detection Inspired from Audio Generation Artifacts. Abstract: State-of-the-art methods for audio generation suffer from fingerprint artifacts and repeated inconsistencies across temporal and spectral domains. Such artifacts could be well captured by the frequency domain analysis over the spectrogram. Thus, we propose a novel use of long-range spectro-temporal modulation feature -- 2D DCT over log-Mel spectrogram for the audio deepfake detection. We show that this feature works better than log-Mel spectrogram, CQCC, MFCC, as a suitable candidate to capture such artifacts. We employ spectrum augmentation and feature normalization to decrease overfitting and bridge the gap between training and test dataset along with this novel feature introduction. We developed a CNN-based baseline that achieved a 0.0849 t-DCF and outperformed the previously top single systems reported in the ASVspoof 2019 challenge. Finally, by combining our baseline with our proposed 2D DCT spectro-temporal feature, we decrease the t-DCF score down by 14% to 0.0737, making it a state-of-the-art system for spoofing detection. Furthermore, we evaluate our model using two external datasets, showing the proposed feature's generalization ability. We also provide analysis and ablation studies for our proposed feature and results.
- Soham Deshmukh, B. Raj, Rita Singh. 2021. Improving weakly supervised sound event detection with self-supervised auxiliary tasks. Abstract: While multitask and transfer learning has shown to improve the performance of neural networks in limited data settings, they require pretraining of the model on large datasets beforehand. In this paper, we focus on improving the performance of weakly supervised sound event detection in low data and noisy settings simultaneously without requiring any pretraining task. To that extent, we propose a shared encoder architecture with sound event detection as a primary task and an additional secondary decoder for a self-supervised auxiliary task. We empirically evaluate the proposed framework for weakly supervised sound event detection on a remix dataset of the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 dB SNR. To ensure we retain the localisation information of multiple sound events, we propose a two-step attention pooling mechanism that provides a time-frequency localisation of multiple audio events in the clip. The proposed framework with two-step attention outperforms existing benchmark models by 22.3%, 12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an ablation study to determine the contribution of the auxiliary task and two-step attention pooling to the SED performance improvement.
- Yandong Wen, Weiyang Liu, B. Raj, Rita Singh. 2021. Self-Supervised 3D Face Reconstruction via Conditional Estimation. Abstract: We present a conditional estimation (CEST) framework to learn 3D facial parameters from 2D single-view images by self-supervised training from videos. CEST is based on the process of analysis by synthesis, where the 3D facial parameters (shape, reflectance, viewpoint, and illumination) are estimated from the face image, and then recombined to reconstruct the 2D face image. In order to learn semantically meaningful 3D facial parameters without explicit access to their labels, CEST couples the estimation of different 3D facial parameters by taking their statistical dependency into account. Specifically, the estimation of any 3D facial parameter is not only conditioned on the given image, but also on the facial parameters that have already been derived. Moreover, the reflectance symmetry and consistency among the video frames are adopted to improve the disentanglement of facial parameters. Together with a novel strategy for incorporating the reflectance symmetry and consistency, CEST can be efficiently trained with in-the-wild video clips. Both qualitative and quantitative experiments demonstrate the effectiveness of CEST.
- Rowland Chen, R. Dannenberg, B. Raj, Rita Singh. 2020. Artificial Creative Intelligence: Breaking the Imitation Barrier. Abstract: Not all knowledge is created equal. A hierarchical architecture is a method to classify knowledge for use in the field of human cognition and computational creativity. This paper introduces an Insight-Knowledge Object (IKO) model as a framework for Artificial Creative Intelligence (ACI), a step forward in the pursuit of replicating general human intelligence with computing machinery. To achieve ACI, it is hypothesized that a fundamental rethinking of the architecture of human cognition and knowledge processing is required. One possible novel architecture could be the IKO model. The authors include a description of on-going work at Carnegie Mellon University that applies the IKO model in practice with an artificial music improvisation embodiment.
- Soham Deshmukh, B. Raj, Rita Singh. 2020. Multi-Task Learning for Interpretable Weakly Labelled Sound Event Detection. Abstract: Weakly Labelled learning has garnered lot of attention in recent years due to its potential to scale Sound Event Detection (SED) and is formulated as Multiple Instance Learning (MIL) problem. This paper proposes a Multi-Task Learning (MTL) framework for learning from Weakly Labelled Audio data which encompasses the traditional MIL setup. To show the utility of proposed framework, we use the input TimeFrequency representation (T-F) reconstruction as the auxiliary task. We show that the chosen auxiliary task de-noises internal T-F representation and improves SED performance under noisy recordings. Our second contribution is introducing two step Attention Pooling mechanism. By having 2-steps in attention mechanism, the network retains better T-F level information without compromising SED performance. The visualisation of first step and second step attention weights helps in localising the audio-event in T-F domain. For evaluating the proposed framework, we remix the DCASE 2019 task 1 acoustic scene data with DCASE 2018 Task 2 sounds event data under 0, 10 and 20 db SNR resulting in a multi-class Weakly labelled SED problem. The proposed total framework outperforms existing benchmark models over all SNRs, specifically 22.3 %, 12.8 %, 5.9 % improvement over benchmark model on 0, 10 and 20 dB SNR respectively. We carry out ablation study to determine the contribution of each auxiliary task and 2-step Attention Pooling to the SED performance improvement. The code is publicly released
- Jiachen Lian, A. V. Kumar, Hira Dhamyal, B. Raj, Rita Singh. 2020. Masked Proxy Loss for Text-Independent Speaker Verification. Abstract: Open-set speaker recognition can be regarded as a metric learning problem, which is to maximize inter-class variance and minimize intra-class variance. Supervised metric learning can be categorized into entity-based learning and proxy-based learning. Most of the existing metric learning objectives like Contrastive, Triplet, Prototypical, GE2E, etc all belong to the former division, the performance of which is either highly dependent on sample mining strategy or restricted by insufficient label information in the mini-batch. Proxy-based losses mitigate both shortcomings, however, fine-grained connections among entities are either not or indirectly leveraged. This paper proposes a Masked Proxy (MP) loss which directly incorporates both proxy-based relationships and pair-based relationships. We further propose Multinomial Masked Proxy (MMP) loss to leverage the hardness of speaker pairs. These methods have been applied to evaluate on VoxCeleb test set and reach state-of-the-art Equal Error Rate(EER).
- Yang Gao, Jiachen Lian, B. Raj, Rita Singh. 2020. Detection and Evaluation of Human and Machine Generated Speech in Spoofing Attacks on Automatic Speaker Verification Systems. Abstract: Automatic speaker verification (ASV) systems utilize the biometric information in human speech to verify the speaker’s identity. The techniques used for performing speaker verification are often vulnerable to malicious attacks that attempt to induce the ASV system to return wrong results, allowing an impostor to bypass the system and gain access. Attackers use a multitude of spoofing techniques for this, such as voice conversion, audio replay, speech synthesis, etc. In recent years, easily available tools to generate deepfaked audio have increased the potential threat to ASV systems. In this paper, we compare the potential of human impersonation (voice disguise) based attacks with attacks based on machinegenerated speech, on black-box and white-box ASV systems. We also study countermeasures by using features that capture the unique aspects of human speech production, under the hypothesis that machines cannot emulate many of the finelevel intricacies of the human speech production mechanism. We show that fundamental frequency sequence-related entropy, spectral envelope, and aperiodic parameters are promising candidates for robust detection of deepfaked speech generated by unknown methods.
- Mahmoud Al Ismail, Soham Deshmukh, Rita Singh. 2020. Detection of Covid-19 Through the Analysis of Vocal Fold Oscillations. Abstract: Phonation, or the vibration of the vocal folds, is the primary source of vocalization in the production of voiced sounds by humans. It is a complex bio-mechanical process that is highly sensitive to changes in the speaker's respiratory parameters. Since most symptomatic cases of COVID-19 present with moderate to severe impairment of respiratory functions, we hypothesize that signatures of COVID-19 may be observable by examining the vibrations of the vocal folds. Our goal is to validate this hypothesis, and to quantitatively characterize the changes observed to enable the detection of COVID-19 from voice. For this, we use a dynamical system model for the oscillation of the vocal folds, and solve it using our recently developed ADLES algorithm to yield vocal fold oscillation patterns directly from recorded speech. Experimental results on a clinically curated dataset of COVID-19 positive and negative subjects reveal characteristic patterns of vocal fold oscillations that are correlated with COVID-19. We show that these are prominent and discriminative enough that even simple classifiers such as logistic regression yields high detection accuracies using just the recordings of isolated extended vowels.
- Jiachen Lian, A. V. Kumar, Hira Dhamyal, B. Raj, Rita Singh. 2020. Mask Proxy Loss for Text-Independent Speaker Recognition. Abstract: Open-set speaker recognition can be regarded as a metric learning problem, which is to maximize inter-class variance and minimize intra-class variance. Supervised metric learning can be categorized into entity-based learning and proxy-based learning\protect\footnote{Different from the definition in \cite{Proxyanchor}, we adopt the concept of entity-based learning rather than pair-based learning to illustrate the data-to-data relationship. Entity refers to real data point.}. Most of existing metric learning objectives like Contrastive, Triplet, Prototypical, GE2E, etc all belong to the former division, the performance of which is either highly dependent on sample mining strategy or restricted by insufficient label information in the mini-batch. Proxy-based losses mitigate both shortcomings, however, fine-grained connections among entities are either not or indirectly leveraged. This paper proposes a Mask Proxy (MP) loss which directly incorporates both proxy-based relationship and entity-based relationship. We further propose Multinomial Mask Proxy (MMP) loss to leverage the hardness of entity-to-entity pairs. These methods have been applied to evaluate on VoxCeleb test set and reach state-of-the-art Equal Error Rate(EER).
- Soham Deshmukh, Mahmoud Al Ismail, Rita Singh. 2020. Interpreting Glottal Flow Dynamics for Detecting Covid-19 From Voice. Abstract: In the pathogenesis of COVID-19, impairment of respiratory functions is often one of the key symptoms. Studies show that in these cases, voice production is also adversely affected - vocal fold oscillations are asynchronous, asymmetrical and more restricted during phonation. This paper proposes a method that analyzes the differential dynamics of the glottal flow waveform (GFW) during voice production to identify features in them that are most significant for the detection of COVID-19 from voice. Since it is hard to measure this directly in COVID-19 patients, we infer it from recorded speech signals and compare it to the GFW computed from physical model of phonation. For normal voices, the difference between the two should be minimal, since physical models are constructed to explain phonation under assumptions of normalcy. Greater differences implicate anomalies in the bio-physical factors that contribute to the correctness of the physical model, revealing their significance indirectly. Our proposed method uses a CNN-based 2-step attention model that locates anomalies in time-feature space in the difference of the two GFWs, allowing us to infer their potential as discriminative features for classification. The viability of this method is demonstrated using a clinically curated dataset of COVID-19 positive and negative subjects.
- Hira Dhamyal, Shahan Ali Memon, B. Raj, Rita Singh. 2019. The phonetic bases of vocal expressed emotion: natural versus acted. Abstract: Can vocal emotions be emulated? This question has been a recurrent concern of the speech community, and has also been vigorously investigated. It has been fueled further by its link to the issue of validity of acted emotion databases. Much of the speech and vocal emotion research has relied on acted emotion databases as valid proxies for studying natural emotions. To create models that generalize to natural settings, it is crucial to work with valid prototypes -- ones that can be assumed to reliably represent natural emotions. More concretely, it is important to study emulated emotions against natural emotions in terms of their physiological, and psychological concomitants. In this paper, we present an on-scale systematic study of the differences between natural and acted vocal emotions. We use a self-attention based emotion classification model to understand the phonetic bases of emotions by discovering the most 'attended' phonemes for each class of emotions. We then compare these attended-phonemes in their importance and distribution across acted and natural classes. Our tests show significant differences in the manner and choice of phonemes in acted and natural speech, concluding moderate to low validity and value in using acted speech databases for emotion classification tasks.
- Shahan Ali Memon, Hira Dhamyal, Oren Wright, Daniel Justice, Vijaykumar Palat, William Boler, Yandong Wen, B. Raj, Rita Singh. 2019. Detecting gender differences in perception of emotion in crowdsourced data. Abstract: Do men and women perceive emotions differently? Popular convictions place women as more emotionally perceptive than men. Empirical findings, however, remain inconclusive. Most prior studies focus on visual modalities. In addition, almost all of the studies are limited to experiments within controlled environments. Generalizability and scalability of these studies has not been sufficiently established. In this paper, we study the differences in perception of emotion between genders from speech data in the wild, annotated through crowdsourcing. While we limit ourselves to a single modality (i.e. speech), our framework is applicable to studies of emotion perception from all such loosely annotated data in general. Our paper addresses multiple serious challenges related to making statistically viable conclusions from crowdsourced data. Overall, the contributions of this paper are two fold: a reliable novel framework for perceptual studies from crowdsourced data; and the demonstration of statistically significant differences in speech-based emotion perception between genders.
- Felix Kreuk, Yossi Adi, B. Raj, Rita Singh, Joseph Keshet. 2019. Hide and Speak: Deep Neural Networks for Speech Steganography. Abstract: Steganography is the science of hiding a secret message within an ordinary public message, which referred to as Carrier. Traditionally, digital signal processing techniques, such as least significant bit encoding, were used for hiding messages. In this paper, we explore the use of deep neural networks as steganographic functions for speech data. To this end, we propose to jointly optimize two neural networks: the first network encodes the message inside a carrier, while the second network decodes the message from the modified carrier. We demonstrated the effectiveness of our method on several speech data-sets and analyzed the results quantitatively and qualitatively. Moreover, we showed that our approach could be applied to conceal multiple messages in a single carrier using multiple decoders or a single conditional decoder. Qualitative experiments suggest that modifications to the carrier are unnoticeable by human listeners and that the decoded messages are highly intelligible.
- Hariank Muthakana, Rita Singh. 2019. Uncertainty and Diversity in Deep Active Image Classification. Abstract: Deep neural networks have revolutionized computer vision, with state-of-the art performance across multiple tasks. An important part of training such networks is the availability of large, high-quality labeled datasets. This makes building new datasets a significant hurdle to approaching novel tasks or domains. In many cases, acquiring labels can be difficult, expensive, or time-consuming. Active learning seeks to improve label efficiency and lower overall labeling cost by allowing the learning system to intelligently pick samples to label. Active learning is well studied for classical machine learning models, but many of these approaches have been shown to be ineffective for deep models and modern image datasets. This raises the question of how to develop and use active strategies in these settings. In this work, we seek to build intuitions for deep active learning by conducting a comprehensive empirical analysis of existing approaches for image classification tasks. Critical to this analysis is the distinction between uncertainty and diversity-based strategies and how they perform in various settings. Our experiments show surprising results regarding the efficacy of existing approaches in commonly tested settings. We find that active learning is more useful in settings such as low data availability, class imbalance, and transfer learning. Finally, our results provide heuristics for the active learning practitioner to decide on a strategy to use, and more crucially whether to use active learning at all.
- Daanish Ali Khan, Linhong Li, Ninghao Sha, Zhuoran Liu, Abelino Jiménez, B. Raj, Rita Singh. 2019. Non-Determinism in Neural Networks for Adversarial Robustness. Abstract: Recent breakthroughs in the field of deep learning have led to advancements in a broad spectrum of tasks in computer vision, audio processing, natural language processing and other areas. In most instances where these tasks are deployed in real-world scenarios, the models used in them have been shown to be susceptible to adversarial attacks, making it imperative for us to address the challenge of their adversarial robustness. Existing techniques for adversarial robustness fall into three broad categories: defensive distillation techniques, adversarial training techniques, and randomized or non-deterministic model based techniques. In this paper, we propose a novel neural network paradigm that falls under the category of randomized models for adversarial robustness, but differs from all existing techniques under this category in that it models each parameter of the network as a statistical distribution with learnable parameters. We show experimentally that this framework is highly robust to a variety of white-box and black-box adversarial attacks, while preserving the task-specific performance of the traditional neural network model.
- Felix Kreuk, Yossi Adi, B. Raj, Rita Singh, Joseph Keshet. 2019. Hide and Speak: Towards Deep Neural Networks for Speech Steganography. Abstract: Steganography is the science of hiding a secret message within an ordinary public message, which is referred to as Carrier. Traditionally, digital signal processing techniques, such as least significant bit encoding, were used for hiding messages. In this paper, we explore the use of deep neural networks as steganographic functions for speech data. We showed that steganography models proposed for vision are less suitable for speech, and propose a new model that includes the short-time Fourier transform and inverse-short-time Fourier transform as differentiable layers within the network, thus imposing a vital constraint on the network outputs. We empirically demonstrated the effectiveness of the proposed method comparing to deep learning based on several speech datasets and analyzed the results quantitatively and qualitatively. Moreover, we showed that the proposed approach could be applied to conceal multiple messages in a single carrier using multiple decoders or a single conditional decoder. Lastly, we evaluated our model under different channel distortions. Qualitative experiments suggest that modifications to the carrier are unnoticeable by human listeners and that the decoded messages are highly intelligible.
- Wenbo Zhao, Rita Singh. 2019. Speech-Based Parameter Estimation of an Asymmetric Vocal Fold Oscillation Model and its Application in Discriminating Vocal Fold Pathologies. Abstract: So far, several physical models have been proposed for the study of vocal fold oscillations during phonation. The parameters of these models, such as vocal fold elasticity, resistance, etc. are traditionally determined through the observation and measurement of the vocal fold vibrations in the larynx. Since such direct measurements tend to be the most accurate, the traditional practice has been to set the parameter values of these models based on measurements that are averaged across an ensemble of human subjects. However, the direct measurement process is hard to revise outside of clinical settings. In many cases, especially in pathological ones, the properties of the vocal folds often deviate from their generic values—sometimes asymmetrically wherein the characteristics of the two vocal folds differ for the same individual. In such cases, it is desirable to find a more scalable way to adjust the model parameters on a case by case basis. In this paper, we present a novel and alternate way to determine vocal fold model parameters from the speech signal. We focus on an asymmetric model and show that for such models, differences in estimated parameters can be successfully used to discriminate between voices that are characteristic of different underlying vocal fold pathologies.
- Yandong Wen, Rita Singh, B. Raj. 2019. Reconstructing faces from voices. Abstract: Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling - reconstructing someone's face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task - cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance.
- Yandong Wen, B. Raj, Rita Singh. 2019. Face Reconstruction from Voice using Generative Adversarial Networks. Abstract: Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling - reconstructing someone's face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task - cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance. The code is publicly available in https://github.com/cmu-mlsp/reconstructing_faces_from_voices
- Daanish Ali Khan, Saquib Razak, B. Raj, Rita Singh. 2019. Human Behaviour Recognition Using Wifi Channel State Information. Abstract: Device-Free Human Behaviour Recognition is automatically recognizing physical activity from a series of observations, without directly attaching sensors to the subject. Behaviour Recognition has applications in security, health-care, and smart homes. The ubiquity of WiFi devices has generated recent interest in Channel State Information (CSI) that describes the propagation of RF signals for behaviour recognition, leveraging the relationship between body movement and variations in CSI streams. Existing work on CSI based behaviour recognition has established the efficacy of deep neural network classifiers, yielding performance that surpasses traditional techniques. In this paper, we propose a deep Recurrent Neural Network (RNN) model for CSI based Behaviour Recognition that utilizes a Convolutional Neural Network (CNN) feature extractor with stacked Long Short-Term Memory (LSTM) networks for sequence classification. We also examine CSI de-noising techniques that allow faster training and model convergence. Our model has yielded significant improvement in classification accuracy, compared to existing techniques.
- Hira Dhamyal, Tianyan Zhou, B. Raj, Rita Singh. 2019. Optimizing Neural Network Embeddings Using a Pair-Wise Loss for Text-Independent Speaker Verification. Abstract: This paper proposes a new loss function called the “quartet” loss for the better optimization of the neural networks for matching tasks. For such tasks, where neural network embeddings are the key component, the optimization of the network for better embeddings is critical. The embeddings are required to be class discriminative, resulting in minimal inter-class variation and maximal intra-class variation even for unseen classes for better generalization of the network. The quartet loss explicitly computes the distance metric between pairs of inputs and increases the gap between the similarity score distributions between the same class pairs and the different class pairs. We evaluate on the speaker verification task and demonstrate the performance of the loss on our proposed neural network.
- Yandong Wen, Mahmoud Al Ismail, Weiyang Liu, B. Raj, Rita Singh. 2018. Disjoint Mapping Network for Cross-modal Matching of Voices and Faces. Abstract: We propose a novel framework, called Disjoint Mapping Network (DIMNet), for cross-modal biometric matching, in particular of voices and faces. Different from the existing methods, DIMNet does not explicitly learn the joint relationship between the modalities. Instead, DIMNet learns a shared representation for different modalities by mapping them individually to their common covariates. These shared representations can then be used to find the correspondences between the modalities. We show empirically that DIMNet is able to achieve better performance than other current methods, with the additional benefits of being conceptually simpler and less data-intensive.
- Yang Gao, Rita Singh, B. Raj. 2018. Voice Impersonation Using Generative Adversarial Networks. Abstract: Voice impersonation is not the same as voice transformation, although the latter is an essential element of it. In voice impersonation, the resultant voice must convincingly convey the impression of having been naturally produced by the target speaker, mimicking not only the pitch and other perceivable signal qualities, but also the style of the target speaker. In this paper, we propose a novel neural-network based speech quality- and style-mimicry framework for the synthesis of impersonated voices. The framework is built upon a fast and accurate generative adversarial network model. Given spectrographic representations of source and target speakers' voices, the model learns to mimic the target speaker's voice quality and style, regardless of the linguistic content of either's voice, generating a synthetic spectrogram from which the time-domain signal is reconstructed using the Griffin-Lim method. In effect, this model reframes the well-known problem of style-transfer for images as the problem of style-transfer for speech signals, while intrinsically addressing the problem of durational variability of speech sounds. Experiments demonstrate that the model can generate extremely convincing samples of impersonated speech. It is even able to impersonate voices across different genders effectively. Results are qualitatively evaluated using standard procedures for evaluating synthesized voices.
- Yandong Wen, Mahmoud Al Ismail, B. Raj, Rita Singh. 2018. Optimal Strategies for Matching and Retrieval Problems by Comparing Covariates. Abstract: In many retrieval problems, where we must retrieve one or more entries from a gallery in response to a probe, it is common practice to learn to do by directly comparing the probe and gallery entries to one another. In many situations the gallery and probe have common covariates -- external variables that are common to both. In principle it is possible to perform the retrieval based merely on these covariates. The process, however, becomes gated by our ability to recognize the covariates for the probe and gallery entries correctly. 
In this paper we analyze optimal strategies for retrieval based only on matching covariates, when the recognition of the covariates is itself inaccurate. We investigate multiple problems: recovering one item from a gallery of $N$ entries, matching pairs of instances, and retrieval from large collections. We verify our analytical formulae through experiments to verify their correctness in practical settings.
- Yandong Wen, Tianyan Zhou, Rita Singh, B. Raj. 2018. A Corrective Learning Approach for Text-Independent Speaker Verification. Abstract: We present a conceptually plausible approach for text-independent speaker verification (TISV) which treats speech recordings as a collection of segments providing incremental evidence. This approach, called corrective learning, gradually improves an initial prediction of speaker identity based on incoming speech and the latest prediction. Specifically, we propose deep corrective learning networks (CLNets) that explicitly learn a mapping from a new speech segment and the current predictions, to a correction. Intuitively, the predictions eventually converge to the ground truth after several corrections. Trained on NIST SRE datasets, CLNets outperform current CNN and the i-vector baselines. Moreover, CLNets and i-vectors are complementary, and their fusion leads to significant performance improvements compared to what can be achieved by each of them individually.
- Shahan Ali Memon, Wenbo Zhao, B. Raj, Rita Singh. 2018. Neural Regression Trees. Abstract: Regression-via-Classification (RvC) is the process of converting a regression problem to a classification one. Current approaches for RvC use ad-hoc discretization strategies and are suboptimal. We propose a neural regression tree model for RvC. In this model, we employ a joint optimization framework where we learn optimal discretization thresholds while simultaneously optimizing the features for each node in the tree. We empirically show the validity of our model by testing it on two challenging regression tasks where we establish the state of the art.
- K. Osako, Yuki Mitsufuji, Rita Singh, B. Raj. 2017. Supervised monaural source separation based on autoencoders. Abstract: In this paper, we propose a new supervised monaural source separation based on autoencoders. We employ the autoencoder for the dictionary training such that the nonlinear network can encode the target source with high expressiveness. The dictionary is trained by each target source without the mixture signal, which makes the system independent from the context where the dictionaries will be used. In separation process, the decoder portions of the trained autoencoders are used as dictionaries to find the activations in a iterative manner such that a summation of the decoder outputs approximates the original mixture. The results of the instruments source separation experiments revealed that the separation performance of the proposed method was superior to that of the NMF.
- Rita Singh, Abelino Jiménez, Anders Øland. 2017. Voice disguise by mimicry: deriving statistical articulometric evidence to evaluate claimed impersonation. Abstract: Voice disguise by impersonation is often used in voice-based crimes by perpetrators who try to evade identification while sounding genuine. Voice evidence from these crimes is analysed to both detect impersonation, and match the impersonated voice to the natural voice of the speaker to prove its correct ownership. There are interesting situations, however, where a speaker might be confronted with voice evidence that perceptually sounds like their natural voice but may deny ownership of it, claiming instead that it is the production of an expert impersonator. This is a bizarre claim, but plausible since the human voice has a great degree of natural variation. It poses a difficult forensic problem: instead of detecting impersonation one must now prove the absence of it, and instead of matching the evidence with the natural voice of the person one must show that they cannot not have a common originator. The authors address the problem of disproving the denial of voice ownership from an articulatory-phonetic perspective, and propose a hypothesis-testing framework that may be used to solve it. The authors demonstrate their approach on data comprising voices of prominent political figures in USA, and their expert impersonators.
- Rita Singh, J. Baker, Luciana Pennant, Louis-Philippe Morency. 2017. Deducing the severity of psychiatric symptoms from the human voice. Abstract: Psychiatric illnesses are often associated with multiple symptoms, whose severity must be graded for accurate diagnosis and treatment. This grading is usually done by trained clinicians based on human observations and judgments made within doctor-patient sessions. Current research provides sufficient reason to expect that the human voice may carry biomarkers or signatures of many, if not all, these symptoms. Based on this conjecture, we explore the possibility of objectively and automatically grading the symptoms of psychiatric illnesses with reference to various standard psychiatric rating scales. Using acoustic data from several clinician-patient interviews within hospital settings, we use non-parametric models to learn and predict the relations between symptom-ratings and voice. In the process, we show that different articulatory-phonetic units of speech are able to capture the effects of different symptoms differently, and use this to establish a plausible methodology that could be employed for automatically grading psychiatric symptoms for clinical purposes.
- Wenbo Zhao, Yang Gao, Rita Singh. 2017. Speaker identification from the sound of the human breath. Abstract: This paper examines the speaker identification potential of breath sounds in continuous speech. Speech is largely produced during exhalation. In order to replenish air in the lungs, speakers must periodically inhale. When inhalation occurs in the midst of continuous speech, it is generally through the mouth. Intra-speech breathing behavior has been the subject of much study, including the patterns, cadence, and variations in energy levels. However, an often ignored characteristic is the {\em sound} produced during the inhalation phase of this cycle. Intra-speech inhalation is rapid and energetic, performed with open mouth and glottis, effectively exposing the entire vocal tract to enable maximum intake of air. This results in vocal tract resonances evoked by turbulence that are characteristic of the speaker's speech-producing apparatus. Consequently, the sounds of inhalation are expected to carry information about the speaker's identity. Moreover, unlike other spoken sounds which are subject to active control, inhalation sounds are generally more natural and less affected by voluntary influences. The goal of this paper is to demonstrate that breath sounds are indeed bio-signatures that can be used to identify speakers. We show that these sounds by themselves can yield remarkably accurate speaker recognition with appropriate feature representations and classification frameworks.
- Rita Singh, J. Lehman. 2016. Systems and methods for estimating age of a speaker based on speech. Abstract: There is provided a system comprising a microphone, configured to receive an input speech from an individual, an analog-to-digital (A/D) converter to convert the input speech to digital form and generate a digitized speech, a memory storing an executable code and an age estimation database, a hardware processor executing the executable code to receive the digitized speech, identify a plurality of boundaries in the digitized speech delineating a plurality of phonemes in the digitized speech, extract a plurality of formant-based feature vectors from each phoneme in the digitized speech based on at least one of a formant position, a formant bandwidth, and a formant dispersion, compare the plurality of formant-based feature vectors with age determinant formant-based feature vectors of the age estimation database, determine the age of the individual when the comparison finds a match in the age estimation database, and communicate an age-appropriate response to the individual.
- Rita Singh, B. Raj, D. Gençaga. 2016. Forensic anthropometry from voice: An articulatory-phonetic approach. Abstract: This paper addresses a problem that is of paramount importance in solving crimes wherein voice may be key evidence, or the only evidence: that of describing the perpetrator. The term Forensic anthropometry from voice refers to the deduction of the speaker's physical dimensions from voice. There are multiple studies in the literature that approach this problem in different ways, many of which depend on the availability of sufficient volumes of speech for analysis. However, in the case of many voice-based crimes, the voice evidence available may be limited. In such cases it is especially advantageous to regard the recorded signal as comprising multiple pieces of evidence. In this paper, we show how this can be done. We explain why, for any anthropometric measurement from speech, it makes sense to consider the contributions of each articulatory-phonetic unit independently of others, and to aggregate the deductions from them only in the aftermath. This approach is based on the hypothesis that the relative evidence given by different compositional units of speech can be more indicative of the anthropometric factor being deduced, than the evidence derived from the aggregate voice signal. We explain the applicability of this approach through experiments on standard speech databases.
- R. Iyer, Sanjeel Parekh, Vikas Mohandoss, Anush Ramsurat, B. Raj, Rita Singh. 2016. Content-based Video Indexing and Retrieval Using Corr-LDA. Abstract: Existing video indexing and retrieval methods on popular web-based multimedia sharing websites are based on user-provided sparse tagging. This paper proposes a very specific way of searching for video clips, based on the content of the video. We present our work on Content-based Video Indexing and Retrieval using the Correspondence-Latent Dirichlet Allocation (corr-LDA) probabilistic framework. This is a model that provides for auto-annotation of videos in a database with textual descriptors, and brings the added benefit of utilizing the semantic relations between the content of the video and text. We use the concept-level matching provided by corr-LDA to build correspondences between text and multimedia, with the objective of retrieving content with increased accuracy. In our experiments, we employ only the audio components of the individual recordings and compare our results with an SVM-based approach.
- Shareef Babu Kalluri, Ashwin Vijayakumar, Deepu Vijayasenan, Rita Singh. 2016. Estimating multiple physical parameters from speech data. Abstract: In this work, we explore prediction of different physical parameters from speech data. We aim to predict shoulder size and waist size of people from speech data in addition to the conventional height and weight parameters. A data-set with this information is created from 207 volunteers. A bag of words representation based on log magnitude spectrum is used as features. A support vector regression predicts the physical parameters from the bag of the words representation. The system is able to achieve a root mean square error of 6.6 cm for height estimation, 2.6cm for shoulder size, 7.1cm for waist size and 8.9 kg for weight estimation. The results of height estimation is on par with state of the art results.
- Rita Singh, J. Baker, Luciana Pennant, Louis-Philippe Morency. 2016. Voice-based Grading of Psychiatric Disorders. Abstract: It is well recognized that many psychiatric disorders and illnesses present with acoustic markers that manifest in the speech of the affected people. Studies continue to uncover positive relationships between the human voice and a widening range of mental conditions. There is not much literature, however, on the use of voice in the grading of psychiatric illnesses. Here the term grading refers to automatically estimating the degree of an illness measured in terms of the severity of multiple associated symptoms to gauge a patient’s condition. This is currently done manually by trained clinicians based on human observations within doctor-patient sessions. In this paper, we address the problem of psychiatric grading of patients who are under treatment for active mental illnesses, entirely through their voice. Using acoustic data from clinician interviews with sixteen patients recorded within a hospital setting, we use nonparametric models to learn the relations between manually assigned ratings and the articulatory-phonetic units of speech. In addition to discovering interesting relations that point to the physical effects of mental illnesses on specific articulators, our studies highlight the merits of using this approach in the development of voice-based automatic grading mechanisms for many symptoms associated with psychiatric illnesses.
- Rita Singh. 2016. Mereological algebras as mechanisms for reasoning about sounds. Abstract: This paper suggests the use and debates the appropriateness of Mereology for the study of real world sounds. Mereology is a formalism in mathematical logic that describes the universe in terms of parts and the wholes that are formed by the parts. This is in contrast with set theory, within which the universe is described as objects and the groups they belong to. Classification and traditional machine learning fit well into the description of the universe as a set of objects and their associated properties. In the case of sound, however, pieces of sound can extend and morph in time and frequency to form other recognizable sound entities without having clear partitions in the set theoretic sense. Our reasoning is that by treating sounds as composed entirely or parts, and wholes that are formed by parts, it may become easier to formalize the descriptions, mathematical manipulations and real-world interpretations for the universe of sounds. This paper is neither an exhaustive thesis on this subject, nor does it establish any formal system of Mereology for sound. The goal of this paper is to merely show that there are some promising possibilities with existing Mereological formalisms for manipulating the world of sounds differently, and perhaps more easily.
- Rita Singh, Joseph Keshet, D. Gençaga, B. Raj. 2016. The relationship of voice onset time and Voice Offset Time to physical age. Abstract: In a speech signal, Voice Onset Time (VOT) is the period between the release of a plosive and the onset of vocal cord vibrations in the production of the following sound. Voice Offset Time (VOFT), on the other hand, is the period between the end of a voiced sound and the release of the following plosive. Traditionally, VOT has been studied across multiple disciplines and has been related to many factors that influence human speech production, including physical, physiological and psychological characteristics of the speaker. The mechanism of extraction of VOT has however been largely manual, and studies have been carried out over small ensembles of individuals under very controlled conditions, usually in clinical settings. Studies of VOFT follow similar trends, but are more limited in scope due to the inherent difficulty in the extraction of VOFT from speech signals. In this paper we use a structured-prediction based mechanism for the automatic computation of VOT and VOFT. We show that for specific combinations of plosives and vowels, these are relatable to the physical age of the speaker. The paper also highlights the ambiguities in the prediction of age from VOT and VOFT, and consequently in the use of these measures in forensic analysis of voice.
- Rita Singh, D. Gençaga, B. Raj. 2016. Formant manipulations in voice disguise by mimicry. Abstract: The human voice can be disguised in many ways. The purpose of disguise could either be to impersonate another person, or to conceal the identity of the original speaker, or both. On the other hand, the goal of any biometric analysis on disguised voices could also be twofold: either to find out if the originator of the disguised voice is a given speaker, or to know how a speaker's voice can be manipulated so that the extent and type of disguise that the speaker can perform can be guessed a-priori. Any analysis toward the former goal must rely on the knowledge of what characteristics of a person's voice are least affected or unaffected by attempted disguise. Analysis towards the latter goal must use the knowledge of what sounds are typically most amenable to voluntary variation by the speaker, so that the extent to which given speakers can successfully disguise their voice can be estimated. Our paper attempts to establish a simple methodology for analysis of voice for both goals. We study the voice impersonations performed by an expert mimic, focusing specifically on formants and formant-related measurements, to find out the extent and type of formant manipulations that are performed by the expert at the level of individual phonemes. Expert mimicry is an extreme form of attempted disguise. Our study is presented with the expectation that non-expert attempts at voice disguise by mimicry will fall within the gold standard of manipulation patterns set by an expert mimic, and that it is therefore useful to establish this gold standard.
- Rita Singh, B. Raj, J. Baker. 2016. Short-term analysis for estimating physical parameters of speakers. Abstract: Conventional approaches to estimating speakers' physiometric parameters such as height, age, weight etc. from their voice analyze the speech signal at relatively coarse time resolutions, typically with analysis windows of 25ms or longer. At these resolutions the analysis effectively captures the structure of the supra-glottal vocal tract. In this paper we hypothesize that by analyzing the signal at a finer temporal resolution that is lower than a pitch period, it may be possible to analyze segments of the speech signal that are obtained entirely when the glottis is open, and thereby capture some of the sub-glottal structure that may be represented in the voice. To explore this hypothesis we propose an analysis approach that combines signal analysis techniques suited to fine-temporal-resolution analysis and well-known regression models. We test it on the prediction of heights and ages of speakers from a standard speech database. Our findings show that the higher-resolution analysis does provide benefits over conventional analysis for estimating speaker height, although it is less useful in predicting age.
- J. Lehman, Rita Singh. 2016. Estimation of Children's Physical Characteristics from Their Voices. Abstract: To date, multiple strategies have been proposed for the estimation of speakers’ physical parameters such as height, weight, age, gender etc. from their voices. These employ various types of feature measurements in conjunction with different regression and classification mechanisms. While some are quite effective for adults, they are not so for children’s voices. This is presumably because in children, the relationship between voice and physical parameters is relatively more complex. The vocal tracts of adults, and the processes that accompany speech production, are fully mature and do not undergo changes within small age differentials. In children, however, these factors change continuously with age, causing variations in style, content, enunciation, rate and quality of their speech. Strategies for the estimation of children’s physical parameters from their voice must take this variability into account. In this paper, using different formant-related measurements as exemplary analysis features generated within articulatory-phonetic guidelines, we demonstrate the nonlinear relationships of children’s physical parameters to their voice. We also show how such analysis can help us focus on the specific sounds that relate well to each parameter, which can be useful in obtaining more accurate estimates of the physical parameters.
- Rita Singh, K. Kumatani. 2015. Free energy for speech recognition. Abstract: Traditionally, speech recognizers have used a strictly Bayesian paradigm for finding the best hypothesis from amongst all possible hypotheses for the data to be recognized. The Bayes classification rule has been shown to be optimal when the class distributions represent the true distributions of the data to be classified. In reality, however, this condition is often not satisfied - the classifier itself is trained on some training data and may be deployed to classify data whose statistical characteristics are different from the training data. The Bayes classification rule may result in suboptimal performance under these conditions of mismatch. Classification may benefit from the use of modified classification rules in this case. The use of entropy as an optimization criterion for various classification tasks has been well established in the literature. In this paper we show that free energy, a thermodynamic concept directly related to entropy, can also be used as an objective criterion in classification. Furthermore, we show how this novel classification scheme can be used in the framework of existing Bayesian classification schemes implemented in current speech recognizers by simply modifying the class distributions a priori. Pilot experiments show that minimization of free energy results in more accurate recognition under conditions of mismatch.
- Soham De, Indradyumna Roy, Tarunima Prabhakar, Kriti Suneja, Sourish Chaudhuri, Rita Singh, B. Raj. 2015. Plagiarism Detection in Polyphonic Music using Monaural Signal Separation. Abstract: Given the large number of new musical tracks released each year, automated approaches to plagiarism detection are essential to help us track potential violations of copyright. Most current approaches to plagiarism detection are based on musical similarity measures, which typically ignore the issue of polyphony in music. We present a novel feature space for audio derived from compositional modelling techniques, commonly used in signal separation, that provides a mechanism to account for polyphony without incurring an inordinate amount of computational overhead. We employ this feature representation in conjunction with traditional audio feature representations in a classification framework which uses an ensemble of distance features to characterize pairs of songs as being plagiarized or not. Our experiments on a database of about 3000 musical track pairs show that the new feature space characterization produces significant improvements over standard baselines.
- K. Osako, Rita Singh, B. Raj. 2015. Complex recurrent neural networks for denoising speech signals. Abstract: Effective denoising of noise-corrupted speech signals remains a challenging problem. Existing solutions typically employ some combination of noise estimation and noise elimination, either by subtraction or by filtering. The estimation of noise and the denoising are generally treated as independent aspects of the problem. In this paper we propose a new neural-network-based approach for de-noising of speech signals. The approach integrates noise estimation and denoising into a single network design, while maintaining many of the aspects of conventional noise estimation and signal denoising through a recurrent gated structure. The network thus operates as a single integrated process that can be trained to jointly estimate noise and denoise the speech signal with minimal artifacts. Noise reduction experiments on noisy speech, both with digitally added synthetic noise and real car noise, show that the proposed algorithm can recover much of the degradation caused by the noise.
- Shoou-I Yu, Lu Jiang, Zhongwen Xu, Zhenzhong Lan, Shicheng Xu, Xiaojun Chang, Xuanchong Li, Zexi Mao, Chuang Gan, Yajie Miao, Xingzhong Du, Yang Cai, Lara J. Martin, Nikolas Wolfe, Anurag Kumar, Huan Li, Ming Lin, Zhigang Ma, Yi Yang, Deyu Meng, S. Shan, P. D. Sahin, Susanne Burger, Florian Metze, Rita Singh, B. Raj, T. Mitamura, R. Stern, Alexander Hauptmann. 2014. Informedia@TrecVID 2014: MED and MER. Abstract: We report on our system used in the TRECVID 2014 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. On the MED task, the CMU team achieved leading performance in the Semantic Query (SQ), 000Ex, 010Ex and 100Ex settings. Furthermore, SQ and 000Ex runs are significantly better than the submissions from the other teams. We attribute the good performance to 4 main components: 1) our large-scale semantic concept detectors trained on video shots for SQ/000Ex systems, 2) better features such as improved trajectories and deep learning features for 010Ex/100Ex systems, 3) a novel Multistage Hybrid Late Fusion method for 010Ex/100Ex systems and 4) our developed reranking methods for Pseudo Relevance Feedback for 000Ex/010Ex systems. On the MER task, our system utilizes a subset of features and detection results from the MED system from which the recounting is then generated. Recounting evidence is presented by selecting the most likely concepts detected in the salient shots of a video. Salient shots are detected by searching for shots which have high response when predicted by the video level event detector.
- P. Baljekar, J. Lehman, Rita Singh. 2014. Online word-spotting in continuous speech with recurrent neural networks. Abstract: In this paper we introduce a simplified architecture for gated recurrent neural networks that can be used in single-pass applications, where word-spotting needs to be done in real-time and phoneme-level information is not available for training. The network operates as a self-contained block in a strictly forward-pass configuration to directly generate keyword labels. We call these simple networks causal networks, where the current output is only weighted by the the past inputs and outputs. Since the basic network has a simpler architecture as compared to traditional memory networks used in keyword spotting, it also requires less data to train. Experiments on a standard speech database highlight the behavior and efficacy of such networks. Comparisons with a standard HMM-based keyword spotter show that these networks, while simple, are still more accurate.
- Anurag Kumar, Rita Singh, B. Raj. 2014. Detecting sound objects in audio recordings. Abstract: In this paper we explore the idea of defining sound objects and how they may be detected. We try to define sound objects and demonstrate by our experiments the existence of these objects. Most of current works on acoustic event detection focus on detecting a finite set of audio events and the detection of a generic object in sound is not done. The major reason for proposing the idea of sound objects is to work with a generic sound concept instead of working with a small set of acoustic events for detection as is the norm. Our definition tries to conform to notions present in human auditory perception. Our experimental results are promising, and show that the idea of sound objects is worth pursuing and that it could give a new direction to semi-supervised or unsupervised learning of acoustic event detection mechanisms.
- Rita Singh. 2014. Audio Classification with Thermodynamic Criteria. Abstract: Detecting sound events in audio recordings is a challenging problem. A detector must be trained for each sound to be classified. However, the recordings of the examples used to train the detector rarely match the conditions found in the test audio to be classified. If the event detection problem is posed as one of Bayes classification, the problem may be viewed as one of mismatch between the true distribution of the data and that represented by the classifier. The Bayes classification rule results in suboptimal performance under such mismatch, and a modified classification rule is required. Alternately stated, the classification rule must optimize a different objective criterion than the Bayes error rate computed from the training distributions. The use of entropy as an optimization criterion for various classification tasks has been well established in the literature. In this paper we show that free-energy, a thermodynamic concept directly related to entropy, can also be used as an objective criterion for classification in such scenarios. We demonstrate with examples on classification with HMMs that minimization of free-energy is an effective criterion for classification under conditions of mismatch.
- B. Lambert, B. Raj, Rita Singh. 2013. Discriminatively trained dependency language modeling for conversational speech recognition. Abstract: We present a discriminatively trained dependency parser-based language model. The model operates on utterances, rather than words, and so can utilize long-distance structural features of each sentence. We train the model discriminatively on n -best lists, using the perceptron algorithm to tune the model weights. Our features include standard n -gram style features, long-distance co-occurrence features, and syntactic structural features. We evaluate this model by re-ranking n best lists of recognized speech from the Fisher dataset of informal telephone conversations. We compare various combinations of feature types, and methods of training the model.
- A. Juárez, B. Raj, Rita Singh. 2013. Semi-supervised context-aware discovery of unknown audio concepts. Abstract: Both defining new audio categories and annotating data that belong to these is a problem yet hardly tackled, much less resolved. These problems need to be solved, however, if we are to escalate the labeling of audio data from subjective manual annotation onto automatic audio discovery upon the vast amounts of audio data available today. The lack of work on this matter is understandable: audio data overlaps different semantic categories through a single channel, it is most often noisy, and any application that works on large datasets needs to deal with these problems. Additionally, relating semantic concepts to audio data is a problem in itself: how can a system associate newlyencountered acoustic data to concepts of which there is no data available? In this paper we describe how we used a labeled dataset to train reliable concept detectors for several semantic categories, how we augmented unlabeled data with contextual features through co-occurrence to and duration of known concepts, and results that indicate the feasibility of this task. We believe the design presented can serve as a general discovery framework for audio-like sequential data in general.
- K. Kumatani, Rita Singh, F. Faubel, J. McDonough, Youssef Oualil. 2013. Joint constrained maximum likelihood regression for overlapping speech recognition. Abstract: Adaptation techniques for speech recognition are very effective in single-speaker scenarios. However, when distant microphones capture overlapping speech from multiple speakers, conventional speaker adaptation methods are less effective. The putative signal for any speaker contains interference from other speakers. Consequently, any adaptation technique adapts the model to the interfering speakers as well, which leads to degradation of recognition performance for the desired speaker. In this work, we develop a new feature-space adaptation method for overlapping speech. We first build a beamformer to enhance speech from each active speaker. After that, we compute speech feature vectors from the output of each beamformer. We then jointly transform the feature vectors from all speakers to maximize the likelihood of their respective acoustic models. Experiments run on the speech separation challenge data collected under the AMI project demonstrate the effectiveness of our adaptation method. An absolute word error rate (WER) reduction up to 14 % was achieved in the case of delay-and-sum beamforming. With minimum mutual information (MMI) beamforming, our adaptation method achieved a WER of 31.5 %. To the best of our knowledge, this is the lowest WER reported on this task.
- Sourish Chaudhuri, Rita Singh, B. Raj. 2013. BLOCK-SPARSE BASIS SETS FOR IMPROVED AUDIO CONTENT ESTIMATION. Abstract: Unsupervised lexicon learning techniques for audio-in-the-wild typically assume that only one of the lexical units is active at any given point in time (hard quantization) or use soft counts to avoid committing to one unit (soft quantization). In reality, the audio will usually be produced as a mixture of the different audio concepts in the lexicon. In this paper, we propose a model where the audio content is assumed to be generated by a mixture of a sparse subset of the lexical units thus guiding the system toward a better estimate of presence of the concepts. We present an approach that builds on current lexicon learning frameworks, and develop a novel algorithm to estimate the contribution of different sources by imposing block-sparsity constraints on the lexicon. Our proposed framework shows significant improvement over the standard lexicon learning framework on a retrieval task for audio-in-the-wild.
- Shubhranshu Barnwal, Rohit Barnwal, R. Hegde, Rita Singh, B. Raj. 2013. Doppler based speed estimation of vehicles using passive sensor. Abstract: This paper aims to develop a system for estimating a vehicle's speed by analyzing its drive by acoustics with a passive audio microphone. Analysis of the vehicle's acoustics would primarily use the phenomenon of Doppler shift, and the instant at which vehicle is at closest-point-of approach. This approach uses a technique called Seam carving to track harmonics formed by vehicle particularly its engine noise. The method proposed is computationally inexpensive and can very easily be developed into mobile application.
- Anurag Kumar, R. Hegde, Rita Singh, B. Raj. 2013. Event detection in short duration audio using Gaussian Mixture Model and Random Forest Classifier. Abstract: The amount of online multimedia files is increasing day by day with the ever increasing popularity of video sharing websites. This has led to a huge interest in content analysis of multimedia files. Audio being a major component of multimedia has the potential to help analyze different events occurring in a multimedia recording. In this paper we present an audio event detection mechanism based on Gaussian Mixture Model (GMM) and Random Forest Classifier. Experiments show that our proposed mechanism shows significant improvement in detection of specifically finer audio events in short duration recordings.
- Zhenzhong Lan, Lu Jiang, Shoou-I Yu, Chenqiang Gao, Shourabh Rawat, Yang Cai, Shicheng Xu, Haoquan Shen, Xuanchong Li, Yipei Wang, Waito Sze, Yan Yan, Zhigang Ma, Nicolas Ballas, Deyu Meng, Wei Tong, Yi Yang, Susanne Burger, Florian Metze, Rita Singh, B. Raj, R. Stern, T. Mitamura, Eric Nyberg, Alexander Hauptmann, A. Hauptmann. 2013. Informedia@TRECVID 2013. Abstract: In the first part of this three-part report we describe our system and novel approaches used in the TRECVID 2013 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. A separate section of the report (SIN) details methods and results for the Semantic Indexing task. The final section (SED) describes our approaches and results on the Surveillance Event Detection task.
- Rita Singh, K. Kumatani, Chen Liu, Bapat Ojas, R. Fastow. 2012. FREE-ENERGY BASED SCORING FOR SPEECH RECOGNITION. Abstract: Traditionally, speech recognizers have used a strictly Bayesian paradigm for finding the best hypothesis from amongst all possible hypotheses for the data at hand that is to be recognized. In fact, the Bayes classification rule has been shown to be optimal when the class distributions represent the true distributions of the data to be classified. In reality, however, this condition is not satisfied the classifer itself is trained on some training data and may be deployed to recognize data that are different from the training data. The use of Enropy as an optimization criterion for various classification tasks has been well established in the literature. In our work, we show that free-energy, a thermodynamic concept directly related to enropy, can also be used as an objective criteion in classification. Furthermore, we show how this novel classification scheme can be used in the framework of existing Bayesian classification schemes implemented in current recognizers by simply modifying the class distributions a-priori. Pilot experiments performed under mismatched and matched conditions further bring out the viability for free-enrgy for classification in speech recognizers.
- T. Virtanen, Rita Singh, B. Raj. 2012. Techniques for Noise Robustness in Automatic Speech Recognition. Abstract: Automatic speech recognition (ASR) systems are finding increasing use in everyday life. Many of the commonplace environments where the systems are used are noisy, for example users calling up a voice search system from a busy cafeteria or a street. This can result in degraded speech recordings and adversely affect the performance of speech recognition systems. As the use of ASR systems increases, knowledge of the state-of-the-art in techniques to deal with such problems becomes critical to system and application engineers and researchers who work with or on ASR technologies. This book presents a comprehensive survey of the state-of-the-art in techniques used to improve the robustness of speech recognition systems to these degrading external influences. Key features: Reviews all the main noise robust ASR approaches, including signal separation, voice activity detection, robust feature extraction, model compensation and adaptation, missing data techniques and recognition of reverberant speech. Acts as a timely exposition of the topic in light of more widespread use in the future of ASR technology in challenging environments. Addresses robustness issues and signal degradation which are both key requirements for practitioners of ASR. Includes contributions from top ASR researchers from leading research units in the field
- Shubhranshu Barnwal, Kamal Sahni, Rita Singh, B. Raj. 2012. Spectrographic seam patterns for discriminative word spotting. Abstract: This paper presents a novel method for deriving patterns for classification of speech sounds. In contrast to conventional methods that attempt to capture time-frequency patterns as represented by spectral envelopes or peaks, our method captures patterns of high-energy tracks, or seams, of maximum “whiteness” across frequency in spectrograms. Our hypothesis is that these seams could potentially carry relatively invariant signatures of underlying sounds. We present a method to derive feature vectors from seam patterns for discriminative word spotting. We show experimentally that spectrographic seam patterns are indeed distinctive for different spoken words, and are effective for word spotting.
- K. Kumatani, B. Raj, Rita Singh, J. McDonough. 2012. Microphone Array Post-filter based on Spatially-Correlated Noise Measurements for Distant Speech Recognition. Abstract: This paper presents a new microphone-array post-ﬁltering algorithm for distant speech recognition (DSR). Convention-ally, post-ﬁltering methods assume static noise ﬁeld models, and using this assumption, employ a Wiener ﬁlter mechanism for estimating the noise parameters. In contrast to this, we show how we can build the Wiener post-ﬁlter based on actual noise observations without any noise-ﬁeld assumption. The algorithm is framed within a state-of-the-art beamforming technique, namely maximum negentropy (MN) beamforming with super directivity. We investigate the effectiveness of the proposed post-ﬁlter on DSR through experiments on noisy data collected in a car under different acoustic conditions. Experiments show that the new post-ﬁltering mechanism is able to achieve up to 20% relative reduction of word error rates (WER) under the represented noise conditions, as compared to a single distant microphone. In contrast, super-directive (SD) beamforming followed by Zelinski post-ﬁltering achieves a relative WER reduction of only up to 11%. Other post-ﬁlters evaluated perform similarly in comparison to the proposed post-ﬁlter.
- Anurag Kumar, Pranay Dighe, Rita Singh, Sourish Chaudhuri, B. Raj. 2012. Audio event detection from acoustic unit occurrence patterns. Abstract: In most real-world audio recordings, we encounter several types of audio events. In this paper, we develop a technique for detecting signature audio events, that is based on identifying patterns of occurrences of automatically learned atomic units of sound, which we call Acoustic Unit Descriptors or AUDs. Experiments show that the methodology works as well for detection of individual events and their boundaries in complex recordings.
- Rita Singh, K. Kumatani, J. McDonough, Chen Liu. 2012. A signal-separation-based array postfilter for distant speech recognition. Abstract: In standard microphone array processing for distant speech recognition, the beamformed output is postﬁltered to reduce residual noise. Postﬁltering is usually performed through a Weiner ﬁlter whose parameters are estimated from both the beamformer output and the signals captured at the microphones themselves. Conventional postﬁltering methods assume diffuse or incoherent noise at the various microphones in order to estimate these parameters. When the noise does not conform to this assumption they perform poorly. We propose an alternate postﬁltering mechanism that attenuates noise by estimating and separating out the contributions of speech and noise explicitly. Experiments on a corpus of in-car two-channel recordings show that the proposed postﬁltering algorithm outperforms conventional postﬁlters signiﬁcantly under many noise conditions.
- K. Kumatani, T. Arakawa, Kazumasa Yamamoto, J. McDonough, B. Raj, Rita Singh, I. Tashev. 2012. Microphone array processing for distant speech recognition: Towards real-world deployment. Abstract: Distant speech recognition (DSR) holds out the promise of providing a natural human computer interface in that it enables verbal interactions with computers without the necessity of donning intrusive body- or head-mounted devices. Recognizing distant speech robustly, however, remains a challenge. This paper provides a overview of DSR systems based on microphone arrays. In particular, we present recent work on acoustic beamforming for DSR, along with experimental results verifying the effectiveness of the various algorithms described here; beginning from a word error rate (WER) of 14.3% with a single microphone of a 64-channel linear array, our state-of-the-art DSR system achieved a WER of 5.3%, which was comparable to that of 4.2% obtained with a lapel microphone. Furthermore, we report the results of speech recognition experiments on data captured with a popular device, the Kinect [1]. Even for speakers at a distance of four meters from the Kinect, our DSR system achieved acceptable recognition performance on a large vocabulary task, a WER of 24.1%, beginning from a WER of 42.5% with a single array channel.
- Sourish Chaudhuri, Rita Singh, B. Raj. 2012. Exploiting Temporal Sequence Structure for Semantic Analysis of Multimedia. Abstract: Automatic deduction of semantic labels for audiovisual data requires awareness of context, which in turn requires processing sequences of audiovisual scenes or events. The representation of such sequences is important for semantic analysis tasks. Whereas, conventionally, sequences of specific short-duration event labels, often hand-annotated for learning detectors or classifiers, have been used, we propose a new technique for audiovisual event categorization in this paper, wherein units of audio and image scenes are discovered automatically from data in a likelihood-maximization process. We show how these units for audio and video, respectively called AUDs and VIDs, can be used to learn the salient characteristics of broad-category semantic labels without requiring explicit error recovery measures. Experiments with the MED-11 dataset show that AUDs and VIDs are better able to retrieve semantic categories from mixed-content data as compared to vector quantization-based systems and systems that use library-based descriptors. Index: multimedia analysis, semantic labels, unsupervised lexicon learning, audiovisual data retrieval
- Sixie Yu, Zhongwen Xu, Duo Ding, Waito Sze, F. Vicente, Zhenzhong Lan, Yang Cai, Shourabh Rawat, Peter F. Schulam, N. Markandaiah, S. Bahmani, A. Juárez, Wei Tong, Yi Yang, Susanne Burger, Florian Metze, Rita Singh, B. Raj, R. Stern, T. Mitamura, Eric Nyberg, A. Hauptmann. 2012. Informedia @TRECVID 2012. Abstract: We report on our system used in the TRECVID 2012 Multimedia Event Detection (MED) and Multimedia Event Recounting (MER) tasks. For MED, it consists of three main steps: extracting features, training detectors and fusion. In the feature extraction part, we extract many low-level, high-level, and text features. Those features are then represented in three different ways which are spatial bag-of words with standard tiling, spatial bag-of-words with feature and event specific tiling and the Gaussian Mixture Model Super Vector. In the detector training and fusion, two classifiers and three fusion methods are employed. The results from both the official sources and our internal evaluations show good performance of our system. Our MER system utilizes a subset of features and detection results from the MED system from which the recounting is generated. 1. MED System 1.1 Features In order to encompass all aspects of a video, we extracted a wide variety of low-level and highlevel features. Table 1 summarizes the features used in our system. Among those features, most of them are widely used features in the community, for example, SIFT, STIP and MFCC. We extracted those features using standard code available from the authors with default parameters. Table 1: Features used for MED’12 system Visual Features Audio Features Low-level features 1. SIFT (Sande, Gevers, & Snoek, 2010) 2. Color SIFT (CSIFT) (Sande, Gevers, & Snoek, 2010) 3. Motion SIFT (MoSIFT) (Chen & Hauptmann, 2009) 4. Transformed Color Histogram (TCH) (Sande, Gevers, & Snoek, 2010) 5. STIP (Wang, Ullah, Klaser, Laptev, & Schmid, 2009) 6. Dense Trajectory (Wang, Klaser, Schmid, & Liu, 2011) 1. MFCC 2. Acoustic Unit Descriptors (AUDs) (Chaudhuri, Harvilla, & Raj, 2011) High-level features 1. Semantic Indexing Concepts (SIN) (Over, et al., 2012) 2. Object Bank (Li, Su, Xing, & Fei-Fei, 2010) 1. Acoustic Scene Analysis Text Features 1. Optical Character Recognition 1. Automatic Speech Recognition Besides those common features, we have two home-grown features which are Motion SIFT (MoSIFT) and Acoustic Unit Descriptors (AUDs). We will introduce these two features in the following subsections. 1.1 .1 Motion SIFT (MoSIFT) Feature The goal of developing the MoSIFT feature is to combine the features from the spatial domain and the temporal domain. Local spatio-temporal features around interest points provide compact and descriptive representations for video analysis and motion recognition. Current approaches tend to extend spatial descriptions by adding a temporal component to the appearance descriptor, which only implicitly captures motion information. MoSIFT detects interest points and encodes not only their local appearance but also explicitly models local motion. The idea is to detect distinctive local features through local appearance and motion. Figure 1 demonstrates the MoSIFT algorithm. Figure 1: System flow chart of the MoSIFT algorithm. The algorithm takes a pair of video frames to find spatio-temporal interest points at multiple scales. Two major computations are applied: SIFT point detection and optical flow computation according to the scale of the SIFT points. For the descriptor, MoSIFT adapts the idea of grid aggregation in SIFT to describe motions. Optical flow detects the magnitude and direction of a movement. Thus, optical flow has the same properties as appearance gradients. The same aggregation can be applied to optical flow in the neighborhood of interest points to increase robustness to occlusion and deformation. The two aggregated histograms (appearance and optical flow) are combined into the MoSIFT descriptor, which now has 256 dimensions. 1.1 .2 Acoustic Unit Descriptors (AUDs) We have developed an unsupervised lexicon learning algorithm that automatically learns units of sound. Each unit is such that it spans a set of audio frames, thereby taking local acoustic context into account. Using a maximum-likelihood estimation process, we can learn a set of such acoustic units unsupervised from audio data. Each of these units can be thought of as low-level fundamental units of sound, and each audio frame is generated by these units. We refer to these units as Acoustic Unit Descriptors (AUDs) and we expect that the distribution of these units will carry information about the semantic content of the audio stream. Each AUD is represented by a 5-state Hidden Markov Model (HMM) with a 4-gaussian mixture output density function. Ideally, with a perfect learning process, we would like to learn semantically interpretable lowerlevel units, such as a clap, a thud sound, a bang, etc. Naturally, it is hard to enforce semantic interpretability on the audio learning process at that level of detail. Further, because the space of all possible sounds is so large, many different sounds will be mapped into single sounds at learning time, since we can only learn a finite set of units. 1.2 Feature Representat ions In the previous section, we briefly describe the features we used in the system. In this section, we will describe the representations we used for the raw features extracted in Section 1. Three representations were used in our system. They were K-means based spatial bag-ofwords model with standard tiling (Lazebnik, Schmid, & Ponce, 2006), K-means based spatial bag-of-words with feature and event specific tiling (Viitaniemil & Laaksonen, 2009) and Gaussian Mixture Model Super Vector (Campbell & Sturim, 2006). Since the K-means based spatial bag-of-words model with standard tiling and Gaussian Mixture Model Super Vector are standard technology, we will focus on the K-means based spatial bag-of-words model with feature and event specific tiling. For simplicity, we will refer to it as tiling. Spatial bag-of-words model is a widely used representation of the low-level image/video features. The central idea of the spatial bag-of-words model is to divide the image into some small tiles and compute bag-of-words for each tile. Figure 2 shows a couple of tiling examples. Figure 2: Examples of tiling In general, the standard spatial bag-of-words tiling uses the 1x1, 2x2 and 4x4 tiling. However the use of those tilings is ad-hoc and some preliminary works have shown that other tilings might produce better performance (Viitaniemil & Laaksonen, 2009). In our system, we systematically tested 80 different tilings to select the best one for each feature and each event. Table 2 shows the performance of feature specific tiling v.s. the standard tiling. The scores are computed from our internal experiments and are the average over 20 MED12 pre-specified events. The PMiss @ TER=12.5 metric is an official evaluation metric specified in the MED 2012 Evaluation Plan. A smaller PMiss score signifies better performance. From the table, we can see clearly that for all of the five features, the feature specific tiling performs consistently at least 1% better than the standard tiling. Table 2: The performance of feature specific tiling and standard tiling Feature SIFT CSIFT TCH STIP MOSIFT Feature Specific Tiling 0.4209 0.4496 0.4914 0.5178 0.4330 Standard Tiling 0.4325 0.4618 0.5052 0.5234 0.4456 Figure 3 shows an example of the performance of event specific tiling v.s. standard tiling on Event 25 (marriage proposal), which is a difficult event identified in our experiments. It can be seen clearly that the event specific tiling can noticeably improve the performance over standard tiling. Figure 3: The comparison of event specific tiling and standard tiling on Event 25 1.3 Training and Fusion We used the standard MED’12 training dataset for our internal evaluation and the training of the models for our submission. For our internal evaluation, the MED’12 training dataset was further divided into the training set and testing set by randomly selecting half of the positive examples into the training set and the other half into the testing set. The negative examples consisted of only NULL videos which do not have label information. The two classifiers used in the system were kernel SVM and kernelized rigid regression. For simplicity, we will refer to it as kernel regression. For the K-means based feature representations, we used the Chi-squared kernel. For the GMM based representation RBF kernel was used. The parameters of the model were tuned by 5-fold cross validation and the PMiss @ TER = 12.5 metric was used as the evaluation metric. For combining features from multiple modalities and the outputs of different classifiers, we used fusion and ensemble methods. More specifically, for the same classifier, we used three fusion methods to fuse different features. The fusion methods were early fusion, late fusion and double fusion (Lan, Bao, Yu, Liu, & Hauptmann, 2012). In early fusion, the kernel matrices from different features were normalized first and then combined together. In late fusion, the prediction scores from the models trained using different features were combined. In our system, we also used a fusion method called double fusion, which combines early fusion and late fusion together. Finally, the results from different classifiers were ensembled together. Figure 4 shows the diagram of our system. Figure 4: The diagram of the system 0.55 0.57 0.59 0.61 0.63 0.65 0.67 0.69 0.71 0.73 0.75 CSIFT SIFT MOSIFT STIP TCH PM is s@ 12 .5 E025 Marriage_proposal baseline 1.4 Submiss ion In the following section we describe in detail the runs we submitted to NIST. Table 3 shows the official performance of each submission. 1.4 .1 Pre-Specified Submission 1.4.1.1 Submission 1: CMU_MED12_MED12TEST_PS_MEDFull_EKFull_AutoEAG_p_ensembleKRSVM_1 In this submission, using the features described in the previous section, we did the following to generate this run: 1. For each feature, train a SVM classifier and a kernel regression model. 2. Late fusion of all the results from SVM classifiers and kernel regression respectively. 3. Early fusion of all features except ASR. 4. Train a SVM classifier and a kernel
- Rita Singh. 2012. Compensating for denoising artifacts. Abstract: Noise degrades speech signals, affecting their perceptual quality, intelligibility, as well as their downstream processing, e.g. coding or recognition. One obvious solution to this is to denoise the signals, but denoising algorithms filter out an estimate of noise, which is often inexact. As a result, denoising can attenuate spectral components of speech, which may enhance perceptual quality but further reduce it's intelligibility. We address the latter issue and propose a method to restore lost spectral components in denoised speech. Our algorithm modifies the standard NMF formulation to represent clean speech as a composition of bases, and denoised speech as a composition of distortions of these bases. By decomposing the denoised signal into a composition of the distorted bases, the corresponding clean signal can be estimated as an identical composition of the clean bases.
- Kamal Sahni, Pranay Dighe, Rita Singh, B. Raj. 2012. Language identification using spectro-temporal patch features. Abstract: We present a novel approach for automatic Language Identification (LID) using spectro-temporal patch features. Our approach is based on the premise that speech and spoken phenomena are characterized by typical visible patterns in timefrequency representations of the signal, and that the manner of occurrence of these patterns is language specific. To model this, we derive a randomly selected library of spectro-temporal patterns from spoken examples from a language, and derive features from the correlations of this library to spectrograms derive from the speech signal. Under our hypothesis, the relative frequency of correlation peaks must be different for different languages. We model this by learning a discriminative classifier based on these features to detect the presence of the language in a recording. The proposed approach has been tested on two different datasets: the VoxForge multilingual speech data and the LDC2005S26 corpus. Preliminary results indicate that our proposed approach can achieve an accuracy of 85-93%, and perform significantly better than a non-phonetic HMM-based classifier.
- B. Raj, Rita Singh, J. Baker. 2011. A paired test for recognizer selection with untranscribed data. Abstract: Traditionally, the use of untranscribed speech has been restricted to unsupervised or semi-supervised training of acoustic models. Comparison of recognizers has required labeled data. In this paper we show how recognizers may be rank-ordered in terms of their performance using only a large quantity of untranscribed data, given a third “reference” recognizer. We develop statistical tests for comparing recognizers in this scenario. The accuracy of the reference system need not be known. Also, while the accuracy of the reference system affects the amount of data required, with enough data it only needs to perform better than chance. We show through detailed experiments that the rank ordering predicted from untranscribed data is indeed correct.
- Kshitiz Kumar, Rita Singh, B. Raj, R. Stern. 2011. Gammatone sub-band magnitude-domain dereverberation for ASR. Abstract: We present an algorithm for dereverberation of speech signals for automatic speech recognition (ASR) applications. Often ASR systems are presented with speech that has been recorded in environments that include noise and reverberation. The performance of ASR systems degrades with increasing levels of noise and reverberation. While many algorithms have been proposed for robust ASR in noisy environments, reverberation is still a challenging problem. In this paper, we present 1 an approach for dereverberation that models reverberation as a convolution operation in the speech spectral domain. Using a least-squares error criterion we decompose reverberated spectra into clean spectra convolved with a filter. We incorporate non-negativity and sparsity of the speech spectra as constraints within a non-negative matrix factorization (NMF) framework to achieve the decomposition. In ASR experiments where the system is trained with unreverberated and reverberated speech, we show that the proposed approach can provide upto 40% and 19% relative reduction respectively in performance.
- Kshitiz Kumar, B. Raj, Rita Singh, R. Stern. 2011. An iterative least-squares technique for dereverberation. Abstract: Some recent dereverberation approaches that have been effective for automatic speech recognition (ASR) applications, model reverberation as a linear convolution operation in the spectral domain, and derive a factorization to decompose spectra of reverberated speech in to those of clean speech and room-response filter. Typically, a general non-negative matrix factorization (NMF) framework is employed for this. In this work1 we present an alternative to NMF and propose an iterative least-squares deconvolution technique for spectral factorization. We propose an efficient algorithm for this and experimentally demonstrate it's effectiveness in improving ASR performance. The new method results in 40–50% relative reduction in word error rates over standard baselines on artificially reverberated speech.
- B. Raj, Rita Singh, T. Virtanen. 2011. Phoneme-Dependent NMF for Speech Enhancement in Monaural Mixtures. Abstract: The problem of separating speech signals out of monaural mixtures (with other non-speech or speech signals) has become increasingly popular in recent times. Among the various solutions proposed, the most popular methods are based on compositional models such as non-negative matrix factorization (NMF) and latent variable models. Although these techniques are highly effective they largely ignore the inherently phonetic nature of speech. In this paper we present a phoneme-dependent NMFbased algorithm to separate speech from monaural mixtures. Experiments performed on speech mixed with music indicate that the proposed algorithm can result in significant improvement in separation performance, over conventional NMF-based separation.
- B. Lambert, Rita Singh, B. Raj. 2010. Creating a linguistic plausibility dataset with non-expert annotators. Abstract: We describe the creation of a linguistic plausibility dataset that contains annotated examples of language judged to be linguistically plausible, implausible, and every-thing in between. To create the dataset we randomly generate sentences and have them annotated by crowd sourcing over the Amazon Mechanical Turk. Obtaining inter-annotator agreement is a difﬁcult problem because linguistic plausibility is highly subjective. The annotations obtained depend, among other factors, on the manner in which annotators are questioned about the plausibility of sentences. We describe our experiments on posing a number of different questions to the annotators, in order to elicit the responses with greatest agreement, and present several methods for analyzing the resulting responses. The generated dataset and annotations are being made available to public.
- Rita Singh, B. Raj, P. Smaragdis. 2010. Latent-variable decomposition based dereverberation of monaural and multi-channel signals. Abstract: We present an algorithm to dereverberate single- and multi-channel audio recordings. The proposed algorithm models the magnitude spectrograms of clean audio signals as histograms drawn from a multinomial process. Spectrograms of reverberated signals are obtained as histograms of draws from the PDF of the sum of two random variables, one representing the spectrogram of clean speech and the second the frequency decomposition of the room response. The spectrogram of the clean signal is computed as a maximum-likelihood estimate from the spectrogram of reverberant speech using an EM algorithm. Experimental evaluations show that the proposed algorithm is able to greatly reduce the reverberation effects in even highly reverberant signals captured in auditoria and other open spaces.
- B. Raj, T. Virtanen, Sourish Chaudhuri, Rita Singh. 2010. Non-negative matrix factorization based compensation of music for automatic speech recognition. Abstract: This paper proposes to use non-negative matrix factorization based speech enhancement in robust automatic recognition of mixtures of speech and music. We represent magnitude spectra of noisy speech signals as the non-negative weighted linear combination of speech and noise spectral basis vectors, that are obtained from training corpora of speech and music. We use overcomplete dictionaries consisting of random exemplars of the training data. The method is tested on theWall Street Journal large vocabulary speech corpus which is artificially corrupted with polyphonic music from the RWC music database. Various music styles and speech-tomusic ratios are evaluated. The proposed methods are shown to produce a consistent, significant improvement on the recognition performance in the comparison with the baseline method. Audio demonstrations of the enhanced signals are available at http://www.cs.tut.fi/ tuomasv.
- Rita Singh, B. Lambert, B. Raj. 2010. The use of sense in unsupervised training of acoustic models for ASR systems. Abstract: In unsupervised training of ASR systems, no annotated data are assumed to exist. Word-level annotations for training audio are generated iteratively using an ASR system. At each iteration a subset of data judged as having the most reliable transcriptions is selected to train the next set of acoustic models. Data selection however remains a difficult problem, particularly when the error rate of the recognizer providing the initial annotation is very high. In this paper we propose an iterative algorithm that uses a combination of likelihoods and a simple model of sense to select data. We show that the algorithm is effective for unsupervised training of acoustic models, particularly when the initial annotation is highly erroneous. Experiments conducted on Fisher-1 data using initial models from Switchboard, and a vocabulary and LM derived from the Google N-grams, show that performance on a selected held-out test set from Fisher data improves more with iterations relative to likelihood-based data selection.
- Lu Jiang, Zhongwen Xu, Zhenzhong Lan, Shicheng Xu, Xiaojun Chang, Xuanchong Li, Zexi, Mao, Chuang Gan, Yajie Miao, Xingzhong Du, Yang Cai, Lara Martin, Nikolas Wolfe, Anurag Kumar, Huan Li, Ming Lin, Yezhou Yang, Deyu Meng, S. Shan, P. D. Sahin, Susanne, Burger, Florian Metze, Rita Singh, B. Raj, T. Mitamura, R. Stern, Alexander, Hauptmann, Pinar Duygulu-Sahin, Alexander Hauptmann, Yicheng Zhao. 2010. MMM-TJU at TRECVID 2010. Abstract: Surveillance Event Detection Semantic event detection in the huge amount of surveillance video in both retrospective and real-time styles is essential to a variety of higher-level applications in the public security. In TRECVID 2010, to overcome the limitations of the traditional human action analysis method with human detection/tracking and domain knowledge, we evaluate the general framework for multiple human behaviors modeling with the philosophy of bag of spatiotemporal feature (BoSTF). The brief
- Dhananjay Bansal, N. Nair, Rita Singh, B. Raj. 2009. A joint decoding algorithm for multiple-example-based addition of words to a pronunciation lexicon. Abstract: We propose an algorithm that enables joint Viterbi decoding of multiple independent audio recordings of a word to derive its pronunciation. Experiments show that this method results in better pronunciation estimation and word recognition accuracy than that obtained either with a single example of the word or using conventional approaches to pronunciation estimation using multiple examples.
- Rita Singh, B. Raj. 2008. Discovery of temporal patterns in continuous nonrandom sound sequences.. Abstract: The problem addressed is that of automatically determining the minimal structures in structured sounds such as human speech. It is well established that human speech comprises consistent sound units (such as phonemes), a fact that is exploited by speech recognition systems, which only model the units, characterizing all speech as sequences of units. Currently, the units are typically manually defined, and their statistical models are assigned structures based on subjective judgments. In this work it is attempted to identify these units automatically through an analysis of data. The problem is treated as one of entropy minimization. The minimum entropy estimation principle assumes a structured universe and dictates the estimation of the most predictable model that the observations used to train the model will allow. For the current problem, this amounts to identifying a set of units such that every word can be represented by a minimum‐perplexity network over them. Experiments demonstrate that this procedur...
- Rita Singh, E. Gouvêa, B. Raj. 2007. Probabilistic deduction of symbol mappings for extension of lexicons. Abstract: This paper proposes a statistical mapping-based technique for guessing pronunciations of novel words from their spellings. The technique is based on the automatic determination and utilization of unidirectional mappings between n-tuples of characters and n-tuples of phonemes, and may be viewed as a statistical extension of analogy-based pronunciation guessing algorithms.
- B. Raj, Rita Singh, M. Shashanka, P. Smaragdis. 2007. Bandwidth Expansionwith a pólya URN Model. Abstract: We present a new statistical technique for the estimation of the high frequency components (4-8 kHz) of speech signals from narrow-band (0-4 kHz) signals. The magnitude spectra of broadband speech are modelled as the outcome of a Polya Urn process, that represents the spectra as the histogram of the outcome of several draws from a mixture multinomial distribution over frequency indices. The multinomial distributions that compose this process are learnt from a corpus of broadband (0-8 kHz) speech. To estimate high-frequency components of narrow-band speech, its spectra are also modelled as the outcome of draws from a mixture-multinomial process that is composed of the learnt multinomials, where the counts of the indices of higher frequencies have been obscured. The obscured high-frequency components are then estimated as the expected number of draws of their indices from the mixture-multinomial. Experiments conducted on bandlimited signals derived from the WSJ corpus show that the proposed procedure is able to accurately estimate the high frequency components of these signals.
- C. Kwan, Xiaokun Li, D. Lao, Yunbin Deng, Z. Ren, B. Raj, Rita Singh, R. Stern. 2007. Voice driven applications in non-stationary and chaotic environment. Abstract: Automated operations based on voice commands would become more and more important in many applications, including robotics, maintenance operations, etc. However, voice command recognition rates drop quite a lot under nonstationary and chaotic noise environments. In this research, we tried to significantly improve the speech recognition rates under non-stationary noise environments. First, 298 Navy acronyms have been selected for automatic speech recognition. Data sets were collected under 4 types of non-stationary noisy environments: factory, buccaneer jet, babble noise in a canteen, and destroyer. Within each noisy environment, 4 levels (5 dB, 15 dB, 25 dB, and clean) of signal-to-noise ratio (SNR) were introduced to corrupt the speech. Second, a new algorithm to estimate speech or no speech regions has been developed, implemented, and evaluated. Third, extensive simulations were carried out. It was found that the combination of the new algorithm, the proper selection of language model and a customized training of the speech recognizer based on clean speech yielded very high recognition rates, which are from 80% to 90% for the four different noisy conditions. Fourth, extensive comparative studies have also been carried out
- B. Raj, Rita Singh. 2005. Feature compensation with secondary sensor measurements for robust speech recognition. Abstract: This paper investigates the use of secondary sensor measurements to augment feature compensation methods for robust speech recognition. Secondary sensors measure secondary phenomena associated with human speech production. While such measurements do not provide sufficient information for speech recognition per-se, they do not degrade with the noise that corrupts the acoustic signal and can be used to guide algorithms that attempt to estimate noise compensation algorithms by restricting the region of the acoustic space within which the recorded speech must lie. In this paper we specifically, we investigate the use of measurements obtained from a Glottal ElectroMagnetic Sensor (GEMS) to improve the noise estimation performance of the Vector Taylor Series algorithm. We and show that this can result in significant improvement in performance of the VTS algorithm, and, consequently, recognition performance.
- B. Raj, Rita Singh, P. Smaragdis. 2005. Recognizing speech from simultaneous speakers. Abstract: In this paper we present and evaluate factored methods for recognition of simultaneous speech from multiple speakers in single-channel recordings. Factored methods decompose the problem of jointly recognizing the speech from each of the speakers by separately recognizing the speech from each speaker. In order to achieve this, the signal components of the target speaker in each case must be enhanced in some manner. We do this in two ways: using an NMF-based speaker separation algorithm that generates separated spectra for each speaker, and a mask estimation method that generates spectral masks for each speaker that must be used in conjunction with a missing-feature method that can recognize speech from partial spectral data. Experiments on synthetic mixtures of signals from the Wall Street Journal corpus show that both approaches can greatly improve the recognition of the individual signals in the mixture.
- William Walker, Paul Lamere, Philip Kwok, B. Raj, Rita Singh, E. Gouvêa, Peter Wolf, Joseph Woelfel. 2004. Sphinx-4: a flexible open source framework for speech recognition. Abstract: Sphinx-4 is a flexible, modular and pluggable framework to help foster new innovations in the core research of hidden Markov model (HMM) speech recognition systems. The design of Sphinx-4 is based on patterns that have emerged from the design of past systems as well as new requirements based on areas that researchers currently want to explore. To exercise this framework, and to provide researchers with a "researchready" system, Sphinx-4 also includes several implementations of both simple and state-of-the-art techniques. The framework and the implementations are all freely available via open source.
- Rita Singh, B. Raj. 2004. Classification in Likelihood Spaces. Abstract: In classification methods that explicitly model class-conditional probability distributions, the true distributions are often not known. These are estimated from the data available, to approximate the true distributions. Errors in classification that arise due to this approximation can be reduced to some extent if the estimated distributions are used merely to project data into a space of likelihoods and classification is performed in that space using discriminant functions. In this article, we discuss the rationale behind this, and also the general properties of likelihood projections. We demonstrate the utility of likelihood projections in improving classification performance through experiments carried out on a standard image database and a standard speech database.
- A. Raux, Rita Singh. 2004. Maximum - likelihod adaptation of semi-continuous HMMs by latent variable decomposition of state distributions. Abstract: Compared to fully-continuous HMMs, semi-continuous HMMs are more compact in size, require less data to train well and result in comparable recognition performance with much faster decoding speeds. Nevertheless, the use of semi-continuous HMMs in large vocabulary speech recognition systems has declined considerably in recent years. A significant factor that has contributed this is that systems that use semi-continuous HMMs cannot be easily adapted to new acoustic (environmental or speaker) conditions. While maximum likelihood (ML) adaptation techniques have been very successful for continuous density HMMs, these have not worked to a usable degree for semi-continuous HMMs. This paper presents a new framework for supervised and unsupervised ML adaptation of semi-continuous HMMs, built upon the paradigm of probabilistic latent semantic analysis. Experiments with a specific implementation developed under this framework demonstrate its effectiveness.
- B. Raj, Rita Singh, R. Stern. 2004. On tracking noise with linear dynamical system models. Abstract: This paper investigates the use of higher-order autoregressive vector predictors for tracking the noise in noisy speech signals. The autoregressive predictors form the state equation of a linear dynamical system that models the spectral dynamics of the noise process. Experiments show that the use of such models to track noise can lead to large gains in recognition performance on speech compensated for the estimated noise. However, predictors of order greater than 1 are not observed to improve the performance beyond that obtained with a first-order predictor. We analyze and explain why this is so.
- Rita Singh. 2003. HEATING HMM STATE DENSITIES FOR SPEECH RECOGNITON. Abstract: Currently, the best performing state-of-art large vocabulary speech recognition systems are statistical pattern classifiers which model sound units using hidden Markov models (HMMs). Given a sequence of data X derived from the speech signal, the classification problem that is solved is that of finding the class c(X) for which the following expression, given by Bayes classification rule, is maximized: (1)
- Paul Lamere, Philip Kwok, W. E. Gouvêa, Rita Singh, Bhik, I. Walker, Shankar raj, Peter Wolf. 2003. DESIGN OF THE CMU SPHI. Abstract: Sphinx-4 is an open source HMM-based speech recognition system written in the JavaTM programming language. The design of the Sphinx-4 decoder incorporates several new features in response to current demands on HMM-based large vocabulary systems. Some new design aspects include graph construction for multilevel parallel decoding with multiple feature streams without the use of compound HMMs, the incorporation of a generalized search algorithm that subsumes Viterbi decoding as a special case, token stack decoding for efficient maintenance of multiple paths during search, design of a generalized language HMM graph from grammars and language models of multiple standard formats, that can potentially toggle between flat search structure, tree search structure, etc. This paper describes a few of these design aspects, and reports some preliminary performance measures for speed and accuracy.
- Rita Singh, Manfred K. Warmuth, B. Raj, Paul Lamere. 2003. Classification with free energy at raised temperatures. Abstract: In this paper we describe a generalized classification method for HMM-based speech recognition systems, that uses free energy as a discriminant function rather than conventional probabilities. The discriminant function incorporates a single adjustable temperature parameter T. The computation of free energy can be motivated using an entropy regularization, where the entropy grows monotonically with the temperature. In the resulting generalized classification scheme, the values of and give the conventional Viterbi and forward algorithms, respectively, as special cases. We show experimentally that if the test data are mismatched with the classifier, classification at temperatures higher than one can lead to significant improvements in recognition performance. The temperature parameter is far more effective in improving performance on mismatched data than a variance scaling factor, which is another apparent single adjustable parameter that has a very similar analytical form.
- Rita Singh, B. Raj, R. Stern. 2002. Automatic generation of subword units for speech recognition systems. Abstract: Large vocabulary continuous speech recognition (LVCSR) systems traditionally represent words in terms of smaller subword units. Both during training and during recognition, they require a mapping table, called the dictionary, which maps words into sequences of these subword units. The performance of the LVCSR system depends critically on the definition of the subword units and the accuracy of the dictionary. In current LVCSR systems, both these components are manually designed. While manually designed subword units generalize well, they may not be the optimal units of classification for the specific task or environment for which an LVCSR system is trained. Moreover, when human expertise is not available, it may not be possible to design good subword units manually. There is clearly a need for data-driven design of these LVCSR components. In this paper, we present a complete probabilistic formulation for the automatic design of subword units and dictionary, given only the acoustic data and their transcriptions. The proposed framework permits easy incorporation of external sources of information, such as the spellings of words in terms of a nonideographic script.
- Xiang Li, Rita Singh, R. Stern. 2002. Combining search spaces of heterogeneous recognizers for improved speech recogniton. Abstract: In speech recognition systems, information from multiple sources such as different feature streams or acoustic models can be combined in many different ways to yield better recognition performance. It is theoretically expected that the best performance is obtainable through the simultaneous use of all sources of information, in a system capable of using these in parallel. Such systems, however, are extremely complex and difficult to construct. In this paper we propose a simple alternative criterion for combination which can factorize the complex recognizer into several simple recognizers, each of which is based on a single source of information. We use this criterion in simple experiments which combine lattices from recognizers built with different feature streams. Experimental results obtained on five different corpora show that the proposed method is effective in improving recognition performance.
- Xiang Li, Rita Singh, Richar. 2002. COMBINING SEARCH SPACES OF HETEROG IMPROVED SPEECH RE. Abstract: In speech recognition systems, information from multiple sources such as different feature streams or acoustic models can be combined in many different ways to yield better recognition performance. It is theoretically expected that the best performance is obtainable through the simultaneous use of all sources of information, in a system capable of using these in parallel. Such systems, however, are extremely complex and difficult to construct. In this paper we propose a simple alternative criterion for combination which can factorize the complex recognizer into several simple recognizers, each of which is based on a single source of information. We use this criterion in simple experiments which combine lattices from recognizers built with different feature streams. Experimental results obtained on five different corpora show that the proposed method is effective in improving recognition performance.
- A. Black, Ralf D. Brown, R. Frederking, K. Lenzo, J. Moody, Alexander I. Rudnicky, Rita Singh, E. Steinbrecher. 2002. RAPID DEVLOPEMENT OF SPEECH-TO-SPEECH TRANSLATION SYSTEMS. Abstract: This paper describes building of the basic components, par-ticularly speech recognition and synthesis, of a speech-to-speech translation system. This work is described within the framework of the “Tongues: small footprint speech-to-speech translation device” developed at CMU and Lockheed Martin for use by US Army Chaplains.
- A. Black, Ralf D. Brown, R. Frederking, Rita Singh, J. Moody, E. Steinbrecher. 2002. TONGUES: rapid development of a speech-to-speech translation system. Abstract: We carried out a one-year project to build a portable speech-to-speech translation system in a new language that could run on a small portable computer. Croatian was chosen as the target language. The resulting system was tested with real users on a trip to Croatia in the spring of 2001. We describe its basic components, the methods we used to build them, initial evaluation results, and related significant observations. This work was done in conjunction with the US Army Chaplain School; chaplains are often the only personnel in a position to communicate with local people over non-military issues such as medical supplies, refugees, etc. This paper thus reports on a realistic instance of rapidly deploying and field-testing a speech-to-speech translator using current technology.
- Rita Singh, R. Stern, B. Raj. 2002. Signal and Feature Compensa-tion Methods for Robust Speech Recognition. Abstract: A process is provided for dimerizing 1-olefins which comprises contacting such olefins in a reaction zone with a minor proportion of at least one catalyst selected from the group consisting of phosphoric acid-promoted and water-promoted boron trifluoride catalyst in a mole ratio of catalyst to olefins of from about 0.005:1 to about 0.1:1 and at a temperature from about 100 DEG C. to about 150 DEG C.
- D. Ellis, Rita Singh, S. Sivadas. 2001. Tandem acoustic modeling in large-vocabulary recognition. Abstract: In the tandem approach to modeling the acoustic signal, a neural-net preprocessor is first discriminatively trained to estimate posterior probabilities across a phone set. These are then used as feature inputs for a conventional hidden Markov model (HMM) based speech recognizer, which relearns the associations to subword units. We apply the tandem approach to the data provided for the first Speech in Noisy Environments (SPINE1) evaluation conducted by the Naval Research Laboratory (NRL) in August 2000. In our previous experience with the ETSI Aurora noisy digits (a small-vocabulary, high-noise task) the tandem approach achieved error-rate reductions of over 50% relative to the HMM baseline. For SPINE1, a larger task involving more spontaneous speech, we find that, when context-independent models are used, the tandem features continue to result in large reductions in word-error rates relative to those achieved by systems using standard MFC or PLP features. However, these improvements do not carry over to context-dependent models. This may be attributable to several factors which are discussed in the paper.
- Rita Singh, M. Seltzer, B. Raj, R. Stern. 2001. Speech in Noisy Environments: robust automatic segmentation, feature extraction, and hypothesis combination. Abstract: The first evaluation for Speech in Noisy Environments (SPINE1) was conducted by the Naval Research Labs (NRL) in August, 2000. The purpose of the evaluation was to test existing core speech recognition technologies for speech in the presence of varying types and levels of noise. In this case the noises were taken from military settings. Among the strategies used by Carnegie Mellon University's successful systems designed for this task were session-adaptive segmentation, robust mel-scale filtering for the computation of cepstra, the use of parallel front-end features and noise-compensation algorithms, and parallel hypotheses combination through word-graphs. This paper describes the motivations behind the design decisions taken for these components, supported by observations and experiments.
- Paul Lamere, Philip Kwok, E. Gouvêa, B. Raj, Rita Singh, William Walker, Manfred K. Warmuth, Peter Wolf. 2001. THE CMU SPHINX-4 SPEECH RECOGNITION SYSTEM. Abstract: The Sphinx-4 speech recognition system is the latest addition to Carnegie Mellon University's repository of Sphinx speech recognition systems. It has been jointly designed by Carnegie Mellon University, Sun Microsystems Laboratories and Mitsubishi Electric Research Laboratories. It is differently designed from the earlier Sphinx systems in terms of modularity, flexibility and algorithmic aspects. It uses newer search strategies, is universal in its acceptance of various kinds of grammars and language models, types of acoustic models and feature streams. Algorithmic innovations included in the system design enable it to incorporate multiple information sources in an elegant manner. The system is entirely developed on the JavaTM platform and is highly portable, flexible, and easier to use with multithreading. This paper describes the salient features of the Sphinx-4 decoder and includes preliminary performance measures relating to speed and accuracy.
- Xiang Li, Rita Singh, R. Stern. 2001. LATTICE COMBINATION FOR IMPROVED SPEECH RECOGNITON. Abstract: Combining recognition outputs from different features or different systems will generally improve the recognition accuracy compared to that obtained with any single feature/system alone. Several attempts have been made to combine different systems together, but they all restricted to the use of only single best hypothesis from different feature/systems during combination. We present a new multiple hypothesis based combination method named lattice combination. The idea of lattice combination is to construct a mixed lattice by combining and modifying lattices from individual feature sets or systems together, and output the best path from the combined lattice as the final hypothesis. Experiments in five different database indicate the consistent improvements in recognition accuracy of lattice combination over conventional methods.
- Jon P. Nedel, Rita Singh, R. Stern. 2000. Phone transition acoustic modeling: application to speaker independent and spontaneous speech systems. Abstract: HMM-based large vocabulary speech recognition systems usually have a very large number of statistical parameters. For better estimation, the number of parameters is reduced by sharing them across models. The parameter sharing is decided by regression trees which are built using phonetic classes designed either by a human expert or by data-driven methods. In situations where neither of these are reliable, it may be useful to have techniques for non-decision-tree based state tying which perform comparably to those based on traditional methods. In this paper we propose two methods for non-decision tree based parameter learning in HMM-based systems. In the first method (context-dependent state tying), we restructure acoustic models to explicitly capture the transitions between phones in continuous speech. In the second method (transition-based subword units), we redefine the basic sound units used to model speech to model transitions between sounds explicitly. Experiments show that context-dependent state tying is a viable option for large vocabulary systems. They also show that using transition-based subword units can improve performance on spontaneous speech.
- Alexander I. Rudnicky, Christina L. Bennett, A. Black, A. Chotimongkol, K. Lenzo, Alice H. Oh, Rita Singh. 2000. Task and domain specific modelling in the Carnegie Mellon communicator system. Abstract: The Carnegie Mellon Communicator is a telephone-based dialog system that supports planning in a travel domain. The implementation of such a system requires two complimentary components, an architecture capable of managing interaction and the task, as well as a knowledge base that captures the speech, language and task characteristics specific to the domain. Given a suitable architecture, the principal effort in development in taken up in the acquisition and processing of a domain knowledge base. This paper describes a variety of techniques we have applied to modeling in acoustic, language, task, generation and synthesis components of the system.
- Rita Singh, B. Raj, R. Stern. 2000. Automatic generation of phone sets and lexical transcriptions. Abstract: Large vocabulary automatic speech recognition systems model words as sequences of a small set of basic sub-word units (the phoneset), which the systems are trained to classify. All words in the system's vocabulary are transcribed in terms of this set in a dictionary. The phoneset and dictionary are specific to a language and are typically designed manually. The system's performance is critically dependent on the quality of the phoneset and the accuracy of the dictionary. The authors attempt to generate the phoneset and dictionary automatically, using only the training data and their transcriptions. We treat this as a joint optimization problem with a maximum a posteriori solution for the dictionary and a maximum likelihood solution for the phoneset and its acoustic models. Experiments with the DARPA Resource Management corpus show that the automatically generated phoneset and dictionary result in recognition accuracies close to those obtained using manually designed ones.
- Rita Singh, B. Raj, R. Stern. 2000. Structured redefinition of sound units by merging and splitting for improved speech recognition. Abstract: The performance of speech recognition systems degrades when the basic sound units used are poorly defined or incon-sistently used. Several attempts have been made to improve dictionaries automatically, either by redefining pronuncia-tions of words in terms of existing sound units, or by redefining the sound units themselves completely. The problem with these approaches is that, while the former is limited by the sound units used, the latter discards all human information that has been incorporated into an expert-designed recognition dictionary. In this paper we propose a new merging-and-splitting algorithm that attempts to redefine the basic sound units used in the dictionary, while maintaining the expert knowledge built into a manually designed dictionary. Sound units from an existing dictionary are merged based on their inherent confusability, as measured by a Monte-Carlo based metric, and subsequently split to maximize the likelihood of the training data. Experiments with the Resource Management database indicate that this approach results in an improvement in recognition accuracy when context-indepen-dent models are used for recognition. When context-dependent models are used, the improvement observed is reduced.
- Jon P. Nedel, Rita Singh, R. Stern. 2000. Automatic subword unit refinement for spontaneous speech recognition via phone splitting. Abstract: Spontaneous speech is highly variable and rarely conforms to conventional assumptions and linguistically defined pronunciation rules. Specifically, there may be many different continuous speech realizations for each expertly defined phonetic unit in the dictionary. The phones may be realized in a clean and complete fashion as in read speech, or they may be realized in a sloppy and incomplete fashion as in highly spontaneous speech. For spontaneous speech, therefore, it may be beneficial to model incompletely realized variants of any phonetic unit as separate units. In this paper we test this hypothesis by introducing two possible modeling classes for the phones AA and IY in the standard English CMU recognition dictionary. We propose three different automatic methods of segregating the training data properly in order to identify and label the appropriate variants. Each of these methods results in improved recognition performance over the baseline, leading to the conclusion that finer modeling frameworks can be helpful to parameterize properly and recognize spontaneous speech.
- M. Ravishankar, Rita Singh, B. Raj, R. Stern. 1999. THE 1999 CMU 10X REAL TIME BROADCAST NEWS TRANSCRIPTION SYSTEM. Abstract: CMU's 10X real time system is the HMM-based SPHINX-III system with a newly developed fast decoder. The fast decoder uses a subvector clustered version of the acoustic models for Gaussian computation and a lexical tree search structure. It was developed in September, 1999, and is currently a first-pass decoder, capable of generating word lattices. It was designed to optimize speed, recognition accuracy as well as memory requirements. For the 1999 Hub 4 evaluation task, the system used two sets of acoustic models - full-bandwidth and narrow-bandwidth. The acoustic models were 6000 senone, 32 Gaussians per state, 3-state HMMs with no skips permitted across states. The system used a single 39 dimensional feature stream consisting of cepstra and cepstral differences. The lattices generated were rescored using a DAG algorithm. The DAG-rescored hypotheses were designated as those of the primary system. The contrastive system consisted of the output of the first pass Viterbi search, with no DAG rescoring of lattices. A trigram language model consisting of 57,000 unigrams, 10 million bigrams and 14.9 million trigrams was used. No adaptation passes were done. In this paper we describe the various components of the primary system. The first-pass word error rate on the 1998 Hub 4 evaluation set was 20.4% with this system. The overall word error rate scored by NIST for the 1999 Hub 4 evaluation set was 27.6%.
- Rita Singh, B. Raj, R. Stern. 1999. Domain adduced state tying for cross-domain acoustic modelling. Abstract: In situations when automatic speech recognition (ASR) systems are rapidly deployed for a new task, the availability of within-domain training data may be limited. In such cases one needs to build the ASR system from other, possibly out-of-domain databases. We refer to the process of building ASR systems for one task domain using data from other domains as cross-domain modelling or CDM. Conventional CDM-based systems perform poorly because the disparity between the triphonetic distributions of the training and test domains is not well accounted for. In this paper we describe two techniques to impose the acousticphonetic structure of the task domain on acoustic models built from out-of-domain data. The first technique, called Extrinsic CDM, combines decision tree structures obtained from a database close in domain to the task domain with acoustic models that are trained from a third less domain-relevant database. In the second technique, called Intrinsic CDM, the task domain data is used to impose the triphonetic distribution of the task domain on the decision trees built from an out-of-domain large database. Both these techniques result in acoustic models which perform better than conventional CDM models.
- Rita Singh, B. Raj, R. Stern. 1999. Automatic clustering and generation of contextual questions for tied states in hidden Markov models. Abstract: Most current automatic speech recognition systems based on HMMs cluster or tie together subsets of the subword units with which speech is represented. This tying improves the recognition accuracy when systems are trained with limited data, and is performed by classifying the sub-phonetic units using a series of binary tests based on speech production, called "linguistic questions". This paper describes a new method for automatically determining the best combinations of subword units to form these questions. The hybrid algorithm proposed clusters state distributions of context-independent phones to obtain questions for triphonetic contexts. Experiments confirm that the questions thus generated can replace manually generated questions and can provide improved recognition accuracy. Automatic generation of questions has the additional important advantage of extensibility to languages for which the phonetic structure is not well understood by the system designer, and can be effectively used in situations where the subword units are not phonetically motivated.
- B. Raj, Rita Singh, R. Stern. 1998. Inference of missing spectrographic features for robust speech recognition. Abstract: Two types of algorithms are introduced that recover missing time-frequency regions of log-spectral representations of speech. These compensation algorithms modify the incoming feature vector without any changes to the speech recognition system, in contrast to previously-described approaches. The first approach clusters the log-spectral vectors representing clean speech. Missing data are recovered by estimating the spectral cluster in each analysis frame on the basis of the feature values that are present. The second approach uses MAP procedures to estimate the values of missing data elements based on their correlation with the features that are present. Greatest recognition accuracy was obtained using the correlation-based approach, presumably because of its ability to exploit the temporal as well as spectral structure of speech. The recognition accuracy provided by these algorithms approaches but does not exceed that obtained by traditional marginalization. Nevertheless, it is believed that these algorithms provide greater computational efficiency and enable greater flexibility in recognition system structure.
