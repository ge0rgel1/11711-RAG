Robert Frederking
Paper count: 91
- R. Iyer, Miguel Ballesteros, Chris Dyer, R. Frederking. 2020. Transition-Based Dependency Parsing using Perceptron Learner. Abstract: Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora. In this paper, we tackle transition-based dependency parsing using a Perceptron Learner. Our proposed model, which adds more relevant features to the Perceptron Learner, outperforms a baseline arc-standard parser. We beat the UAS of the MALT and LSTM parsers. We also give possible ways to address parsing of non-projective trees.
- Vaibhav, Po-Yao (Bernie) Huang, R. Frederking. 2019. RWR-GAE: Random Walk Regularization for Graph Auto Encoders. Abstract: Node embeddings have become an ubiquitous technique for representing graph data in a low dimensional space. Graph autoencoders, as one of the widely adapted deep models, have been proposed to learn graph embeddings in an unsupervised way by minimizing the reconstruction error for the graph data. However, its reconstruction loss ignores the distribution of the latent representation, and thus leading to inferior embeddings. To mitigate this problem, we propose a random walk based method to regularize the representations learnt by the encoder. We show that the proposed novel enhancement beats the existing state-of-the-art models by a large margin (upto 7.5\%) for node clustering task, and achieves state-of-the-art accuracy on the link prediction task for three standard datasets, cora, citeseer and pubmed. Code available at this https URL.
- Ting-Yao Hu, Chirag Raman, Salvador Medina Maza, Liangke Gui, T. Baltrušaitis, R. Frederking, Louis-Philippe Morency, A. Black, M. Eskénazi. 2017. Integrating Verbal and Nonvebval Input into a Dynamic Response Spoken Dialogue System. Abstract:  In this work, we present a dynamic response spoken dialogue system (DRSDS). It is capable of understanding the verbal and nonverbal language of users and making instant, situation-aware response. Incorporating with two external systems, MultiSense and email summarization, we built an email reading agent on mobile device to show the functionality of DRSDS.
- A. Chotimongkol, Alexander I. Rudnicky, R. Frederking, Roni Rosenfeld, Rong Zhang. 2015. Improving Speech Recognizer Performance in a Dialog System Using N-best Hypotheses Reranking. Abstract: This thesis investigates N-best hypotheses reranking techniques for improving speech recognition accuracy. We have focused on improving the accuracy of a speech recognizer used in a dialog system. Our post-processing approach uses a linear regression model to predict the error rate of each hypothesis from hypothesis features, and then outputs the one that has the lowest (recomputed) error rate. We investigated 15 different features sampled from 3 components of a dialog system: a decoder, a parser and a dialog manager. These features are speech recognizer score, acoustic model score, language model score, N-best word rate, N-best homogeneity with speech recognizer score, N-best homogeneity with language model score, N-best homogeneity with acoustic model score, unparsed words, gap number, fragmentation transitions, highest-in-coverage, slot bigram, conditional slot, expected slots and conditional slot bigram. We also used a linear rescaling with clipping technique to normalize feature values to deal with differences in order of magnitude. A searching strategy was used to discover the optimal feature set for reordering; three search algorithms were examined: stepwise regression, greedy search and brute force search. To improve reranking accuracy and reduce computation we examined techniques for selecting utterances likely to benefit from reranking then applying reranking only to utterances so identified. Besides the conventional performance metric, word error rate, we also proposed concept error rate as an alternative metric. An experiment with human subjects revealed that concept error rate is the metric that better conforms to the criteria used by humans when they evaluated hypotheses quality. The reranking model, that performed the best, combined 6 features together to predict error rate. These 6 features are speech recognizer score, language model score, acoustic model score, slot bigram, N-best homogeneity with speech recognizer score and N-best word rate. This optimal set of features was obtained using greedy search. This model can improve the word error rate significantly beyond the speech recognizer baseline. The reranked word error rate is 11.14%, which is a 2.71% relative improvement from the baseline. The reranked concept error rate is 9.68%, which is a 1.22% relative improvement from the baseline. Adding an utterance selection module into a reranking process did not improve the reranking performance beyond the number achieved by reranking every utterance. However, some selection criteria achieved the same overall error rate by reranking just a small number (8.37%) of the utterances. When comparing the performance of the proposed reranking technique to the performance of a human on the same reranking task, the proposed method did as well as a native speaker, suggesting that an automatic reordering process is quite competitive.
- Lori S. Levin, T. Mitamura, B. MacWhinney, Davida Fromm, J. Carbonell, Wes Feely, R. Frederking, A. Gershman, Carlos Ramírez. 2014. Resources for the Detection of Conventionalized Metaphors in Four Languages. Abstract: This paper describes a suite of tools for extracting conventionalized metaphors in English, Spanish, Farsi, and Russian. The method depends on three significant resources for each language: a corpus of conventionalized metaphors, a table of conventionalized conceptual metaphors (CCM table), and a set of extraction rules. Conventionalized metaphors are things like “escape from poverty” and “burden of taxation”. For each metaphor, the CCM table contains the metaphorical source domain word (such as “escape”) the target domain word (such as “poverty”) and the grammatical construction in which they can be found. The extraction rules operate on the output of a dependency parser and identify the grammatical configurations (such as a verb with a prepositional phrase complement) that are likely to contain conventional metaphors. We present results on detection rates for conventional metaphors and analysis of the similarity and differences of source domains for conventional metaphors in the four languages.
- Wes Feely, Mehdi Manshadi, R. Frederking, Lori S. Levin. 2014. The CMU METAL Farsi NLP Approach. Abstract: While many high-quality tools are available for analyzing major languages such as English, equivalent freely-available tools for important but lower-resourced languages such as Farsi are more difficult to acquire and integrate into a useful NLP front end. We report here on an accurate and efficient Farsi analysis front end that we have assembled, which may be useful to others who wish to work with written Farsi. The pre-existing components and resources that we incorporated include the Carnegie Mellon TurboParser and TurboTagger (Martins et al., 2010) trained on the Dadegan Treebank (Rasooli et al., 2013), the Uppsala Farsi text normalizer PrePer (Seraji, 2013), the Uppsala Farsi tokenizer (Seraji et al., 2012a), and Jon Dehdaris PerStem (Jadidinejad et al., 2010). This set of tools (combined with additional normalization and tokenization modules that we have developed and made available) achieves a dependency parsing labeled attachment score of 89.49%, unlabeled attachment score of 92.19%, and label accuracy score of 91.38% on a held-out parsing test data set. All of the components and resources used are freely available. In addition to describing the components and resources, we also explain the rationale for our choices.
- Luís Marujo, A. Gershman, J. Carbonell, R. Frederking, J. Neto. 2012. Supervised Topical Key Phrase Extraction of News Stories using Crowdsourcing, Light Filtering and Co-reference Normalization. Abstract: Fast and effective automated indexing is critical for search and personalized services. Key phrases that consist of one or more words and represent the main concepts of the document are often used for the purpose of indexing. In this paper, we investigate the use of additional semantic features and pre-processing steps to improve automatic key phrase extraction. These features include the use of signal words and freebase categories. Some of these features lead to significant improvements in the accuracy of the results. We also experimented with 2 forms of document pre-processing that we call light filtering and co-reference normalization. Light filtering removes sentences from the document, which are judged peripheral to its main content. Co-reference normalization unifies several written forms of the same named entity into a unique form. We also needed a Gold Standard ― a set of labeled documents for training and evaluation. While the subjective nature of key phrase selection precludes a true Gold Standard, we used Amazon's Mechanical Turk service to obtain a useful approximation. Our data indicates that the biggest improvements in performance were due to shallow semantic features, news categories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of deeper semantic features such as Freebase sub-categories was not beneficial by itself, but in combination with pre-processing, did cause slight improvements in the nDCG scores.
- Rushin Shah, Bo Lin, Kevin Dela Rosa, A. Gershman, R. Frederking. 2011. Improving cross-document co-reference with semi-supervised information extraction modelsi. Abstract: In this paper, we consider the problem of cross-document co-reference (CDC). Existing approaches tend to treat CDC as an information retrieval based problem and use features such as TF-IDF cosine similarity to cluster documents and/or co-reference chains. We augmented these features with features based on biographical attributes, such as occupation, nationality, gender, etc., obtained by using semisupervised attribute extraction models. Our results suggest that the addition of these features boosts the performance of our CDC system considerably. The extraction of such specific attributes allows us to use features, such as semantic similarity, mutual information and approximate name similarity which have not been used so far for CDC with traditional bag-of-words models. Our system achieves F0.5 scores of 0.82 and 0.81 on the WePS-1 and WePS-2 datasets, which rival the best reported scores for this problem.
- V. Pedro, Jaime CarbonellChair, Eric Nyberg, R. Frederking, Eduard HovyUSC. 2011. THESIS PROPOSAL. Abstract: Resting-state functional MRI (rs-fMRI) is increasingly used for probing functional connectivity of the human brain. The spontaneous activity identified by rs-fMRI plays a key role in understanding the normal brain’s functional organization. It also holds valuable diagnostic and prognostic information towards various neurological or psychiatric diseases including Alzheimer’s disease, depression, schizophrenia, etc [1]. The blood oxygenation level-dependent (BOLD) signal of fMRI detects the locations of increased neuro activity by measuring the blood oxygen levels at consecutive time points. The higher the temporal correlation between two spatially distant regions, the more likely that there is a functional connection between those regions.
- Kevin Dela Rosa, Rushin Shah, Bo Lin, A. Gershman, R. Frederking. 2011. Topical Clustering of Tweets. Abstract: In the emerging field of micro-blogging and social communication services, users post millions of short messages every day. Keeping track of all the messages posted by your friends and the conversation as a whole can become tedious or even impossible. In this paper, we presented a study on automatically clustering and classifying Twitter messages, also known as “tweets”, into different categories, inspired by the approaches taken by news aggregating services like Google News. Our results suggest that the clusters produced by traditional unsupervised methods can often be incoherent from a topical perspective, but utilizing a supervised methodology that utilize the hash-tags as indicators of topics produce surprisingly good results. We also offer a discussion on temporal effects of our methodology and training set size considerations. Lastly, we describe a simple method of finding the most representative tweet in a cluster, and provide an analysis of the results.
- R. Frederking. 2011. Integrated Natural Language Dialogue : A Computational Model. Abstract: 1. Introduction.- 1.1. Ellipsis in natural language.- 1.2. Ellipsis in natural language interfaces.- 1.3. Computational architecture.- 1.4. Outline of the book.- 2. Relevant Previous Work.- 2.1. Work on ellipsis resolution.- 2.1.1. Semantic grammar ellipsis resolution.- 2.1.2. Case frame ellipsis resolution.- 2.1.3. PSLI3 ellipsis resolution.- 2.2. Work on dialogue modelling.- 2.2.1. Syntactic focusing in discourse.- 2.2.2. Task-oriented focusing in dialogue.- 2.2.3. Formal discourse representation.- 2.2.4. Speech acts and dialogue modelling.- 2.2.5. Discourse grammar.- 2.2.6. Dialogue modelling in Psli3.- 2.3. Work on chart parsing.- 2.4. How Psli3 differs from other systems.- 3. An Analysis of Natural Language Dialogue.- 3.1. An Analysis of intersentential ellipsis.- 3.1.1. Top-level taxonomy of intersentential ellipsis.- 3.1.2. Understanding elliptical utterances.- 3.1.3. Other factors in intended effects.- 3.1.4. Analysis of short story dialogues.- 3.2. Other dialogue phenomena.- 3.2.1. Noun phrase references.- 3.2.2. Interactive recovery from user errors.- 3.3. Relationship of theory to implementation.- 4. The Implementation.- 4.1. Top-level organization.- 4.2. Semantic chart parsing.- 4.2.1. An example parse.- 4.2.2. Current syntactic coverage.- 4.2.3. Verb and noun phrase reference.- 4.2.4. Error recognition and recovery.- 4.2.5. Natural language generation.- 4.3. Ellipsis handling in the chart.- 4.3.1. Verb phrase reformulation ellipsis.- 4.3.2. Noun phrase ellipsis resolution.- 4.4. Correction, elaboration, and dialogue charts.- 4.4.1. Correction ellipsis.- 4.4.2. Elaboration ellipsis.- 4.4.3. Dialogue charts.- 4.5. The response phase.- 4.6. Future chart developments.- 5. The Program in Action.- 5.1. Introduction.- 5.2. The initial full sentence.- 5.3. Functional verb phrase ellipsis.- 5.4. Constituent verb phrase ellipsis.- 5.5. Another functional verb phrase ellipsis.- 5.6. Conclusion.- 6. Conclusion.- 6.1. Contributions.- 6.1.1. Ellipsis coverage.- 6.2. Future work.- 6.2.1. Straightforward extensions.- 6.2.2. Extensions within the framework.- 6.2.3. Extensions of the current framework.- References.
- Haizhou Li, Min Zhang, A. Kumaran, Microsoft Research, Kalika Bali, Rafael E. Banchs, Barcelonamedia, Sivaji Bandyopadhyay, Marta Ruiz Costa-jussà, Upc, G. Grefenstette, Exalead, France, Mitesh M. Khapra, Olivia Kwong, Hong Kong, A. McCallum, Jong-Hoon Oh, Sunita Sarawagi, Iit-Bombay, India, S. Sarkar, Iit-Kharagpur, R. Sproat, Vasudeva Varma, Iiit-Hyderabad, Haifeng Wang, C. Wutiwiwatchai, Nectec, V. Pervouchine, Vladimir, Sittichai Jiampojamarn, Kenneth Dwyer, S. Bergsma, Aditya Bhargava, Qing Dou, Mi-Young, Yan Song, Chunyu Kit, Hai, Amitava Das, Tanik Saikh, Tapabrata Mondal, Asif Ekbal, Iman Saleh, Kareem Darwish, Aly, Eva Sourjikova, A. Frank, Simone Paolo, Yu Chen, Y. Ouyang, Wenjie Li, De-Kui Zheng, T. Zhao, Bo Lin, Rushin Shah, R. Frederking, A. Gershman, Mi-Young Kim, Grzegorz Kondrak, A. Finch, E. Sumita, Haizhen Zhao, A. Thangthai, Aly Fahmy, Simone Paolo Ponzetto, A. A. Hamid, Friday. 2010. Shared Task Organizing Committee -transliteration Mining: Whitepaper of News 2010 Shared Task on Transliteration Generation Transliteration Generation and Mining with Limited Training Resources Transliteration Using a Phrase-based Statistical Machine Translation System to Re-score the Output of a Jo. Abstract: ii Preface Named Entities play a significant role in Natural Language Processing and Information Retrieval. While identifying and analyzing named entities in a given natural language is a challenging research problem by itself, the phenomenal growth in the Internet user population, especially among the non-English speaking parts of the world, has extended this problem to the crosslingual arena. We specifically focus The purpose of the NEWS workshop is to bring together researchers across the world interested in identification, analysis, extraction, mining and transformation of named entities in monolingual or multilingual natural language text. The workshop scope includes many interesting specific research areas pertaining to the named entities, such as, orthographic and phonetic characteristics, corpus analysis, unsupervised and supervised named entities extraction in monolingual or multilingual corpus, transliteration modelling, and evaluation methodologies, to name a few. For this years edition, 11 research papers were submitted, each of which was reviewed by at least 3 reviewers from the program committee. 7 papers were chosen for publication, covering main research areas, from named entities recognition, extraction and categorization, to distributional characteristics of named entities, and finally a novel evaluation metrics for co-reference resolution. All accepted research papers are published in the workshop proceedings. This year, as parts of the NEWS workshop, we organized two shared tasks: one on Machine Transliteration Generation, and another on Machine Transliteration Mining, participated by research teams from around the world, including industry, government laboratories and academia. The transliteration generation task was introduced in NEWS 2009. While the focus of the 2009 shared task was on establishing the quality metrics and on baselining the transliteration quality based on those metrics, the 2010 shared task expanded the scope of the transliteration generation task to about dozen languages, and explored the quality depending on the direction of transliteration, between the languages. We collected significantly large, hand-crafted parallel named entities corpora in dozen different languages from 8 language families, and made available as common dataset for the shared task. We published the details of the shared task and the training and development data six months ahead of the conference that attracted an overwhelming response from the research community. Totally 7 teams participated in the transliteration generation task. The approaches ranged from traditional unsupervised learning methods (such as, Phrasal SMT-based, Conditional Random Fields, etc.) to somewhat unique approaches (such as, DirectTL approach), combined with several model combinations for results re-ranking. A report of …
- Bo Lin, Rushin Shah, R. Frederking, A. Gershman. 2010. ENCORE : Experiments with a Synthetic Entity Co-reference Resolution Tool. Abstract: We present ENCORE, a system for entity co-reference resolution that synthesizes the outputs of several off-the-shelf co-reference resolution systems. To boost precision, we filter the output using a named entity recognition tool called SYNERGY which itself is a synthesis of several off-the-shelf NER systems. ENCORE is designed to work under two conditions: NP-CR which resolves noun phrase co-reference and NE-CR which resolves co-references only for named entities. We report the results of our experiments with ENCORE that show 2% to 40% improvements in precision, recall and F-scores over the underlying systems. This opens a promising approach which leverages the existing “black box” state-of-the-art tools without attempting to re-create their achievements and focuses the development efforts on the differences in their output.
- Rushin Shah, Bo Lin, A. Gershman, R. Frederking. 2010. SYNERGY: A Named Entity Recognition System for Resource-scarce Languages such as Swahili using Online Machine Translation. Abstract: Developing Named Entity Recognition (NER) for a new language using standard techniques requires collecting and annotating large training resources, which is costly and time-consuming. Consequently, for many widely spoken languages such as Swahili, there are no freely available NER systems. We present here a new technique to perform NER for new languages using online machine translation systems. Swahili text is translated to English, the best off-the-shelf NER systems are applied to the resulting English text and the English named entities are mapped back to words in the Swahili text. Our system, called SYNERGY, addresses the problem of NER for a new language by breaking it into three relatively easier problems: Machine Translation to English, English NER and word alignment between English and the new language. SYNERGY achieves good precision as well as recall for Swahili. We also apply SYNERGY to Arabic, for which freely available NERs do exist, in order to compare its performance to other NERs. We ﬁnd that SYNERGY’s performance is close to the state-of-the-art in Arabic NER, with the advantage of requiring vastly less time and effort to build.
- Bo Lin, Rushin Shah, R. Frederking, A. Gershman. 2010. CONE: Metrics for Automatic Evaluation of Named Entity Co-Reference Resolution. Abstract: Human annotation for Co-reference Resolution (CRR) is labor intensive and costly, and only a handful of annotated corpora are currently available. However, corpora with Named Entity (NE) annotations are widely available. Also, unlike current CRR systems, state-of-the-art NER systems have very high accuracy and can generate NE labels that are very close to the gold standard for unlabeled corpora. We propose a new set of metrics collectively called CONE for Named Entity Co-reference Resolution (NE-CRR) that use a subset of gold standard annotations, with the advantage that this subset can be easily approximated using NE labels when gold standard CRR annotations are absent. We define CONE B3 and CONE CEAF metrics based on the traditional B3 and CEAF metrics and show that CONE B3 and CONE CEAF scores of any CRR system on any dataset are highly correlated with its B3 and CEAF scores respectively. We obtain correlation factors greater than 0.6 for all CRR systems across all datasets, and a best-case correlation factor of 0.8. We also present a baseline method to estimate the gold standard required by CONE metrics, and show that CONE B3 and CONE CEAF scores using this estimated gold standard are also correlated with B3 and CEAF scores respectively. We thus demonstrate the suitability of CONE B3 and CONE CEAF for automatic evaluation of NE-CRR.
- Udhyakumar Nallasamy, A. Black, Tanja Schultz, R. Frederking. 2008. NineOneOne: Recognizing and Classifying Speech for Handling Minority Language Emergency Calls. Abstract: In this paper, we describe NineOneOne (9-1-1), a system designed to recognize and translate Spanish emergency calls for better dispatching. We analyze the research challenges in adapting speech translation technology to 9-1-1 domain. We report our initial research towards building the system and the results of our initial experiments.
- Richard C. Wang, A. Tomasic, R. Frederking, Isaac Simmons, William W. Cohen. 2008. Learning to Extract Gene-Protein Names from Weakly-Labeled Text. Abstract: Abstract : Training a named entity recognizer (NER) has always been a difficult task due to the effort required to generate a significant amount of annotated training data. In this paper, we reduce or eliminate the effort required to create training data by automatically converting other sources of data into annotated training data. The performance of this approach is tested on a gene-protein name extractor by using the mouse and fly data obtained from the BioCreAtIvE challenge. Results show that our methods are effective and that our trained NER system outperforms all of our baseline results.
- U. Hahn, Thilo Götz, E. Brown, H. Cunningham, Eric Nyberg, P. Ogren, P. Wetzler, S. J. Bethard, U. Hahn, E. Buyko, R. Landefeld, M. Mühlhausen, M. Poprat, K. Tomanek, J. Wermter, Eric Nyberg, E. Riebling, R. C. Wang, R. Frederking, G. Savova, K. Kipper-Schuler, J. Buntrock, C. Chute, I. Sominsky, A. Coden, M. Tanenblatt, T. Heinze, M. Light, Frank Schilder, C. Müller, Torsten Zesch, M. Müller, D. Bernhard, K. Ignatova, Iryna Gurevych, M. Mühlhäuser, D. Ferrucci, D. Roesner, G. Wilcock, M. Kunze, D. Rösner, V. Vi. 2008. Towards Enhanced Interoperability for Large HLT Systems : UIMA for NLP. Abstract: We introduce JCORE, a full-fledged UIMA -compliant component repository for complex text analytics developed at the Jena University Language & Information Engineering (J ULIE) Lab. JCORE is based on a comprehensive type system and a variety of document readers, analysis engines, and CAS consumers. We survey these components and then turn to a discussion of lessons we learnt, with particular emphasis on managing the underlying type system. We briefly sketch two complex NLP applications which can easily be built from the components contained in JC ORE.
- Eric Nyberg, E. Riebling, Richard C. Wang, R. Frederking. 2008. Integrating a Natural Language Message Pre-Processor with UIMA. Abstract: This paper describes the use of the Unstructured Information Management Architecture (UIMA) to integrate a set of natural language processing (NLP) tools in the RADAR system. The challenge was to define a common data model and a set of component interfaces for these tools, so that they could be integrated into a single system. The integrated system is used to pre-process each email arriving in the RADAR user’s IMAP store. We present a UIMA collection processing engine for RADAR, including a common type system for text analysis results and annotators for each of the NLP tools. The paper also includes an analysis of system performance and a discussion of the lessons learned through use the of UIMA for this integration task.
- Udhyakumar Nallasamy, A. Black, Tanja Schultz, R. Frederking, J. Weltman. 2008. Speech Translation for Triage of Emergency Phonecalls in Minority Languages. Abstract: We describe Ayudame, a system designed to recognize and translate Spanish emergency calls for better dispatching. We analyze the research challenges in adapting speech translation technology to 9-1-1 domain. We report our initial research in 9-1-1 translation system design, ASR experiments, and utterance classification for translation.
- Christian Monson, A. F. Llitjós, Vamshi Ambati, Lori S. Levin, A. Lavie, A. Alvarez, Roberto Aranovich, J. Carbonell, R. Frederking, Erik Peterson, Katharina Probst. 2008. Linguistic Structure and Bilingual Informants Help Induce Machine Translation of Lesser-Resourced Languages. Abstract: Producing machine translation (MT) for the many minority languages in the world is a serious challenge. Minority languages typically have few resources for building MT systems. For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development. For these reasons, our research programs on minority language MT have focused on leveraging to the maximum extent two resources that are available for minority languages: linguistic structure and bilingual informants. All natural languages contain linguistic structure. And although the details of that linguistic structure vary from language to language, language universals such as context-free syntactic structure and the paradigmatic structure of inflectional morphology, allow us to learn the specific details of a minority language. Similarly, most minority languages possess speakers who are bilingual with the major language of the area. This paper discusses our efforts to utilize linguistic structure and the translation information that bilingual informants can provide in three sub-areas of our rapid development MT program: morphology induction, syntactic transfer rule learning, and refinement of imperfect learned rules.
- Jonathan Clark, R. Frederking, Lori S. Levin. 2008. Inductive Detection of Language Features via Clustering Minimal Pairs: Toward Feature-Rich Grammars in Machine Translation. Abstract: Syntax-based Machine Translation systems have recently become a focus of research with much hope that they will outperform traditional Phrase-Based Statistical Machine Translation (PBSMT). Toward this goal, we present a method for analyzing the morphosyntactic content of language from an Elicitation Corpus such as the one included in the LDC's upcoming LCTL language packs. The presented method discovers a mapping between morphemes and linguistically relevant features. By providing this tool that can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved. We conclude by outlining how the resulting output can then be used in inducing a morphosyntactically feature-rich grammar for AVENUE, a modern syntax-based MT system.
- Jonathan Clark, R. Frederking, Lori S. Levin. 2008. Toward Active Learning in Data Selection: Automatic Discovery of Language Features During Elicitation. Abstract: Data Selection has emerged as a common issue in language technologies. We define Data Selection as the choosing of a subset of training data that is most effective for a given task. This paper describes deductive feature detection, one component of a data selection system for machine translation. Feature detection determines whether features such as tense, number, and person are expressed in a language. The database of the The World Atlas of Language Structures provides a gold standard against which to evaluate feature detection. The discovered features can be used as input to a Navigator, which uses active learning to determine which piece of language data is the most important to acquire next.
- P. Bouillon, Farzad Ehsani, R. Frederking, Manny Rayner. 2008. 22nd International Conference on Computational Linguistics Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications. Abstract: The concept classifier has been used as a translation unit in speech-to-speech translation systems. However, the sparsity of the training data is the bottle neck of its effectiveness. Here, a new method based on using a statistical machine translation system has been introduced to mitigate the effects of data sparsity for training classifiers. Also, the effects of the background model which is necessary to compensate the above problem, is investigated. Experimental evaluation in the context of crosslingual doctor-patient interaction application show the superiority of the proposed method.
- A. Alvarez, Lori S. Levin, R. Frederking. 2007. An assessment of language elicitation without the supervision of a linguist. Abstract: The AVENUE machine translation system is designed for resource poor scenarios in which parallel corpora are not available. In this situation, parallel corpora are created by bilingual consultants who translate an elicitation corpus into their languages. We have described the elicitation corpus in other publications. This paper is concerned with evaluation of the elicitation corpus: is it suitably designed so that a bilingual consultant can produce reliable data without the supervision of a linguist? We evaluated two translations of the English elicitation corpus, one into Thai and one into Bengali. Two types of evaluation were conducted: an error analysis of the translations produced by the Thai and Bengali consultants, and a comparison of Example Based MT trained on the original translations and on corrected translations.
- V. Pedro, Jaime CarbonellChair, Eric Nyberg, R. Frederking, Eduard HovyUSC. 2007. Thesis Proposal. Abstract: In the last decades the number of available ontologies has grown considerably. These resources offer the promise of easily-accessible, open-domain ontological information, but the existence of such diverse ontologies raises the issue of information merging and reuse. A comparison of available ontologies reveals both redundant and complementary coverage, but the variety of frameworks and languages used for ontology development makes it a challenge to merge query results from different ontologies. This research proposes to address this problem by building an ontological middleware level for only small fragments of ontologies in an on-demand basis by querying multiple ontologies and merging the query results from multiple knowledge base systems. We then follow ontological chains and inferences across ontologies, using partial query results from one ontology to query another. This is a more complex version of cross-data-base joins, where the data schemas are sufficiently compatible. An initial evaluation used the federated ontology search for answer type checking in question answering. The results of the evaluation show that it is possible to obtain results that outperform querying ontologies independently in both precision and recall. Solutions to additional problems in querying multiple ontologies such as concept identification and ontology selection will be provided using string and structural similarity measures. We propose the creation of an evaluation framework using the question and answer sets in the TREC evaluations to account for a wider set of ontological operations. 1. PROBLEM STATEMENT ................................................................................................................ 1 1.1. MOTIVATION................................................................................................................................ 1 1.2. MOTIVATING EXAMPLE................................................................................................................ 3 1.2.1. Question Answering................................................................................................................ 4 1.2.2. Example 1 ............................................................................................................................... 4 1.3. THESIS STATEMENT ..................................................................................................................... 9 1.4. EXPECTED CONTRIBUTIONS ......................................................................................................... 9 1.5. DOCUMENT STRUCTURE ............................................................................................................ 10 2. RELATED RESEARCH .................................................................................................................. 11 2.1. ONTOLOGY SELECTION .............................................................................................................. 11 2.2. ONTOLOGY MAPPING................................................................................................................. 12 2.3. ONTOLOGY EVALUATION........................................................................................................... 14 3. METHODOLOGY ........................................................................................................................... 15 3.1. BASIC DEFINITIONS.................................................................................................................... 15 3.1.1. Ontologies and Graphs......................................................................................................... 15 3.1.2. Query .................................................................................................................................... 16 3.1.3. Result .................................................................................................................................... 16 3.2. ONTOLOGICAL SEARCH.............................................................................................................. 17 3.2.1. Operators.............................................................................................................................. 18 3.2.2. Query .................................................................................................................................... 22 3.2.3. Resource Description and Selection ..................................................................................... 24 3.2.4. Merging ................................................................................................................................ 25 3.2.5. Scoring Results ..................................................................................................................... 30 3.3. KNOWN ISSUES WITH CURRENT WORK ....................................................................................... 31 3.3.1. Identification of the correct concepts and relations in ontologies........................................ 31 3.3.2. Concept and edge similarity between concepts in different ontologies ................................ 32 3.3.3. Chains of indirect inference.................................................................................................. 32 3.3.4. Insensitivity in Boosting graphs of different structural properties ....................................... 32 4. PRELIMINARY RESULTS ............................................................................................................ 33 4.1.1. Type Checking ...................................................................................................................... 33 4.1.2. Experimental Setup............................................................................................................... 33 4.1.3. Results and Analysis ............................................................................................................. 34 5. ROADMAP........................................................................................................................................ 36 5.1. THESIS SCOPE ............................................................................................................................ 36 5.2. RESEARCH ACTIVITIES ............................................................................................................... 37 5.2.1. Graph Matching ................................................................................................................... 37 5.2.2. Ontological Metadata........................................................................................................... 38 5.2.3. Query and Resource Description.......................................................................................... 39 5.2.4. Resource Selection................................................................................................................ 40 5.2.5. Result Merging ..................................................................................................................... 42 5.2.6. Result Scoring....................................................................................................................... 42 5.3. EVALUATION.............................................................................................................................. 43 5.3.1. Factoid TREC QA Task ........................................................................................................ 43 5.3.2. Possible evaluation tasks...................................................................................................... 44 5.3.3. Condition of Success............................................................................................................. 45 5.4. TIMETABLE ................................................................................................................................ 46 6. REFERENCES.................................................................................................................................. 47
- P. Bouillon, Farzad Ehsani, R. Frederking, Manny Rayner. 2006. Proceedings of the Workshop on Medical Speech Translation. Abstract: Medical applications have emerged as one of the most popular domains for speech translation, and several functional systems now exist. Despite this, there is so far no established consensus on any of the central questions, including the following: 
 
•Does medical speech translation pose special problems, and if so, what are they? 
 
•What do the users (both doctors and patients) actually want? What constitutes acceptable performance, given that medicine is a safety-critical area? 
 
•What are the alternatives to speech translation for non-L1 speakers in healthcare situations? 
 
•What are the most important tasks, sub-domains and language pairs? 
 
•What architectures are most suitable for medical speech translation applications? (Fixed-phrase, ad hoc phrasal rules, rule-based, statistical...) 
 
•What evaluation/data collection methodologies are appropriate to medical speech translation? 
 
•What requirements are there on hardware platforms? What options currently exist? 
 
•How close are we to having applications that can be used in the field? 
 
In this one day workshop, our aim has been to get together as many as possible of the key players in this field, so that we can exchange information and clarify the above and other issues. We expect the workshop to be of interest to people working in all three component communities - speech technology, machine translation, and medicine. 
 
The main body of the workshop consists of two parts: oral presentation of papers, followed by a demo session. We will end with a panel discussion, which will include representatives of both the system developer and medical user communities.
- Jeff Good, A. Alvarez, Lori S. Levin, R. Frederking. 2006. Parallel Reverse Treebanks for the Discovery of Morpho-Syntactic Markings. Abstract: This paper describes a corpus of syntactic structures and associated sentences. However, it is not a traditional treebank. The syntactic structures are created first and are then associated with sentences in a human language. We therefore call it a reverse treebank (RTB).1 The RTB has been created for elicitation of sentences in low resource languages. First, a corpus of feature structures is created using a tool suite built by the authors. The second step is to add sentences in a widely spoken language like English or Spanish that express the meanings of each feature structure. We will call this language the Elicitation Language. The third step is to have a bilingual informant translate the sentences into a low resource language. Using an elicitation tool, the informant can also graphically align the words of the Elicitation Language to the words of the low resource language. The result is a high quality parallel, word aligned corpus annotated with feature structures, which we will call a parallel RTB. RTB sentences may have multiple clauses, but they are generally short in comparison to naturally occurring sentences in treebanks. The reason is that parallel RTBs provide small, but highly structured corpora for machine learning with small amounts of resources. Corpora such as these have been used for automatic learning of transfer rules for machine translation [8].
- A. Alvarez, Lori S. Levin, R. Frederking, Simon Fung, D. Gates, Jeff Good. 2006. The MILE Corpus for Less Commonly Taught Languages. Abstract: This paper describes a small, structured English corpus that is designed for translation into Less Commonly Taught Languages (LCTLs), and a set of re-usable tools for creation of similar corpora. The corpus systematically explores meanings that are known to affect morphology or syntax in the world's languages. Each sentence is associated with a feature structure showing the elements of meaning that are represented in the sentence. The corpus is highly structured so that it can support machine learning with only a small amount of data. As part of the REFLEX program, the corpus will be translated into multiple LCTLs, resulting in parallel corpora can be used for training of MT and other language technologies. Only the untranslated English corpus is described in this paper.
- Eric Nyberg, R. Frederking, T. Mitamura, M. Bilotti, K. Hannan, L. Hiyakumoto, Jeongwoo Ko, Frank Lin, L. Lita, V. Pedro, A. Schlaikjer. 2005. JAVELIN I and II Systems at TREC 2005. Abstract: The JAVELIN team at Carnegie Mellon University submitted three question-answering runs for the TREC 2005 evaluation. The JAVELIN I system was used to generate a single submission to the main track, and the JAVELIN II system was used to generate two submissions to the relationship track. In the sections that follow, we separately describe each system and the submission(s) it produced, and conclude with a brief summary.
- Eric Nyberg, T. Mitamura, R. Frederking, V. Pedro, M. Bilotti, A. Schlaikjer, K. Hannan. 2005. Extending the JAVELIN QA System with Domain Semantics ∗. Abstract: This paper presents the current status of work to extend the JAVELIN QA system with domain semantics for question answering in restricted domains. We discuss how the original architecture was extended, and how the system modules must be adjusted to incorporate knowledge from existing ontologies and information provided by third-party annotation tools.
- A. Alvarez, Lori S. Levin, R. Frederking, Erik Peterson, Jeff Good. 2005. Semi-Automated Elicitation Corpus Generation. Abstract: In this document we will describe a semi-automated process for creating elicitation corpora. An elicitation corpus is translated by a bilingual consultant in order to produce high quality word aligned sentence pairs. The corpus sentences are automatically generated from detailed feature structures using the GenKit generation program. Feature structures themselves are automatically generated from information that is provided by a linguist using our corpus specification software. This helps us to build small, flexible corpora for testing and development of machine translation systems.
- R. Frederking, Kathryn Taylor. 2004. Machine translation : from real users to research : 6th Conference of the Association for Machine Translation in the Americas, AMTA 2004, Washington, DC, USA, September 28 - October 2, 2004 : proceedings. Abstract: Case Study: Implementing MT for the Translation of Pre-sales Marketing and Post-sales Software Deployment Documentation at Mycom International.- A Speech-to-Speech Translation System for Catalan, Spanish, and English.- Multi-align: Combining Linguistic and Statistical Techniques to Improve Alignments for Adaptable MT.- A Modified Burrows-Wheeler Transform for Highly Scalable Example-Based Translation.- Designing a Controlled Language for the Machine Translation of Medical Protocols: The Case of English to Chinese.- Normalizing German and English Inflectional Morphology to Improve Statistical Word Alignment.- System Description: A Highly Interactive Speech-to-Speech Translation System.- A Fluency Error Categorization Scheme to Guide Automated Machine Translation Evaluation.- Online MT Services and Real Users' Needs: An Empirical Usability Evaluation.- Counting, Measuring, Ordering: Translation Problems and Solutions.- Feedback from the Field: The Challenge of Users in Motion.- The Georgetown-IBM Experiment Demonstrated in January 1954.- Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.- The PARS Family of Machine Translation Systems for Dutch System Description/Demonstration.- Rapid MT Experience in an LCTL (Pashto).- The Significance of Recall in Automatic Metrics for MT Evaluation.- Alignment of Bilingual Named Entities in Parallel Corpora Using Statistical Model.- Weather Report Translation Using a Translation Memory.- Keyword Translation from English to Chinese for Multilingual QA.- Extraction of Name and Transliteration in Monolingual and Parallel Corpora.- Error Analysis of Two Types of Grammar for the Purpose of Automatic Rule Refinement.- The Contribution of End-Users to the TransType2 Project.- An Experiment on Japanese-Uighur Machine Translation and Its Evaluation.- A Structurally Diverse Minimal Corpus for Eliciting Structural Mappings Between Languages.- Investigation of Intelligibility Judgments.- Interlingual Annotation for MT Development.- Machine Translation of Online Product Support Articles Using a Data-Driven MT System.- Maintenance Issues for Machine Translation Systems.- Improving Domain-Specific Word Alignment with a General Bilingual Corpus.- A Super-Function Based Japanese-Chinese Machine Translation System for Business Users.
- R. Frederking. 2004. Grice’s Maxims: “Do the Right Thing”. Abstract: Grice’s maxims are hopelessly vague, and in fact harmful, because they form a misleading taxonomy. While his cooperative principle may be useful at a high level of theoretical analysis, it too is vague, and should not be directly implemented in computational natural language systems. Answers are suggested to a number of this symposium’s topics based on this position. Examples are presented to show that the maxims are too vague and too general, and that they are not really used by computational systems that claim to be based on them. The historical origins of the maxims in Kant’s philosophy are revealed. A comparison is made with Relevance Theory, which seems to provide a better approach to the same phenomena. I conclude by suggesting that it may be too early in the history of computational linguistics to expect to find such broad
- A. Waibel, A. Badran, A. Black, R. Frederking, D. Gates, A. Lavie, Lori S. Levin, K. Lenzo, L. Tomokiyo, Jürgen Reichert, Tanja Schultz, D. Wallace, M. Woszczyna, Jing Zhang. 2003. Speechalator: two-way speech-to-speech translation on a consumer PDA. Abstract: This paper describes a working two-way speech-to-speech translation system that runs in near real-time on a consumer handheld computer. It can translate from English to Arabic and Arabic to English in the domain of medical interviews. We describe the general architecture and frameworks within which we developed each of the components: HMM-based recognition, interlingua translation (both rule and statistically based), and unit selection synthesis.
- Eric Nyberg, R. Frederking. 2003. JAVELIN: A Flexible, Planner-Based Architecture for Question Answering. Abstract: The JAVELIN system integrates a flexible, planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text. The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus. The operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session.
- A. Waibel, A. Badran, A. Black, R. Frederking, D. Gates, A. Lavie, Lori S. Levin, K. Lenzo, L. Tomokiyo, Jürgen Reichert, Tanja Schultz, D. Wallace, M. Woszczyna, Jing Zhang. 2003. Speechalator: Two-Way Speech-to-Speech Translation in Your Hand. Abstract: This demonstration involves two-way automatic speech-to-speech translation on a consumer off-the-shelf PDA. This work was done as part of the DARPA-funded Babylon project, investigating better speech-to-speech translation systems for communication in the field. The development of the Speechalator software-based translation system required addressing a number of hard issues, including a new language for the team (Egyptian Arabic), close integration on a small device, computational efficiency on a limited platform, and scalable coverage for the domain.
- Eric Nyberg, T. Mitamura, Jamie Callan, J. Carbonell, R. Frederking, Kevyn Collins-Thompson, L. Hiyakumoto, Yifen Huang, C. Huttenhower, S. Judy, Jeongwoo Ko, Anna Kupsc, L. Lita, V. Pedro, David Svoboda, Benjamin Van Durme. 2003. The JAVELIN Question-Answering System at TREC 2003: A Multi-Strategh Approach with Dynamic Planning. Abstract: The JAVELIN system evaluated at TREC 2003 is an integrated architecture for open-domain question answering. JAVELIN employs a modular approach that addresses individual aspects of the QA task in an abstract manner. The System implements a planner that controls the execution and information o w, as well as a multiple answer seeking strategies used differently depending on the type of question.
- T. Mitamura, Eric Nyberg, R. Frederking. 2003. Teaching machine translation in a graduate language technologies program. Abstract: This paper describes a graduate-level machine translation (MT) course taught at the Language Technologies Institute at Carnegie Mellon University. Most of the students in the course have a background in computer science. We discuss what we teach (the course syllabus), and how we teach it (lectures, homeworks, and projects). The course has evolved steadily over the past several years to incorporate refinements in the set of course topics, how they are taught, and how students “learn by doing”. The course syllabus has also evolved in response to changes in the field of MT and the role that MT plays in various social contexts.
- R. Frederking, Eric Nyberg, T. Mitamura, Jaime G. Carbonnell. 2002. Design and Evolution of a Language Technologies Curriculum. Abstract: The Language Technologies Institute (LTI) of the School of Computer Science at Carnegie Mellon University is one of the largest programs of its kind. We present here the initial design and subsequent evolution of our MS and PhD programs in Language Technologies. The motivations for the design and evolution are also presented.
- R. Frederking, E. Steinbrecher, Ralf D. Brown, Alexander I. Rudnicky, J. Moody. 2002. Speech Translation on a Tight Budget without Enough Data. Abstract: The Tongues speech-to-speech translation system was developed for the US Army chaplains, with fairly stringent constraints on time, budget, and available data. The resulting prototype was required to undergo a quite realistic field test. We describe the development and architecture of the system, the field test, and our analysis of its results. The system performed quite well, especially given its development constraints.
- A. Black, Ralf D. Brown, R. Frederking, K. Lenzo, J. Moody, Alexander I. Rudnicky, Rita Singh, E. Steinbrecher. 2002. RAPID DEVLOPEMENT OF SPEECH-TO-SPEECH TRANSLATION SYSTEMS. Abstract: This paper describes building of the basic components, par-ticularly speech recognition and synthesis, of a speech-to-speech translation system. This work is described within the framework of the “Tongues: small footprint speech-to-speech translation device” developed at CMU and Lockheed Martin for use by US Army Chaplains.
- A. Black, Ralf D. Brown, R. Frederking, Rita Singh, J. Moody, E. Steinbrecher. 2002. TONGUES: rapid development of a speech-to-speech translation system. Abstract: We carried out a one-year project to build a portable speech-to-speech translation system in a new language that could run on a small portable computer. Croatian was chosen as the target language. The resulting system was tested with real users on a trip to Croatia in the spring of 2001. We describe its basic components, the methods we used to build them, initial evaluation results, and related significant observations. This work was done in conjunction with the US Army Chaplain School; chaplains are often the only personnel in a position to communicate with local people over non-military issues such as medical supplies, refugees, etc. This paper thus reports on a realistic instance of rapidly deploying and field-testing a speech-to-speech translator using current technology.
- R. Frederking, A. Black, Ralf D. Brown, J. Moody, E. Steinbrecher. 2002. Field Testing the Tongues Speech-to-Speech Machine Translation System. Abstract: The Tongues portable, rapid-development, speech-to-speech machine translation system was developed specifically to allow a realistic field-test of a deployable prototype. In this paper we will describe the system, its field-testing using regular US Army officers and naive Croatians, and the evaluation of these tests. The evaluation includes analysis of answers to a questionnaire, analysis of system transcript logs, and the authors’ qualitative observations. The overall result of the test was that while the system did successfully aid translation, it requires further development before it would be ready for regular field use. 1. The Tongues System The Tongues system was funded by the US Army to support the mission of the US Army chaplains, who are increasingly called upon to deal with local populations, usually without the benefit of human translators. It is thus intended to be used by a trained US Army chaplain with a completely naive and untrained non-English speaker. The architecture and user interface of the Tongues system were based in large measure on the Diplomat system (Frederking et al., 2000). The speech recognition system used was the open-source Sphinx II (Huang et al., 1992); the translation system was a EBMT/MEMT (ExampleBased MT/Multi-Engine MT) system (Brown, 1996; Frederking and Nirenburg, 1994; Brown and Frederking, 1995) very similar to that in Diplomat; and the synthesis system was the open-source Festival (Black et al., 1998). While the initial system was specifically to demonstrate translation in both directions between English and Croatian, the design was also required to allow rapid development for new languages. To ensure rapid development, the entire project was only allowed to take one calendar year, including contractual arrangements, hiring language experts, etc. The total development effort was similarly restricted: six senior research personnel (the authors of this paper) provided an estimated total of about two (2) fulltime person-years of effort. In addition to the senior staff, there were also part-time Croatian informants, chaplains, and some student programmers. We should note that some of the translation data used to train the system was collected for the Diplomat project (Frederking et al., 2000). In addition to rapid development, the system was not permitted to be restricted to a narrowly-limited domain, but had to be wide-coverage. (Both of these properties were important for the chaplains’ envisioned activities.) Since we were to build a broad-coverage system in a short period of time on a small budget, data-driven approaches were the only reasonable choice. In order to provide in-domain conversational data, we arranged at the start of the project to record a number of chaplains in role-playing conversations of the type they expected the device to encounter. Fortunately, the chaplains were familiar with role-playing exercises, and all had relevant field experiences to re-enact. Both sides of the conversations were spoken in English. These were digitally recorded with head-mounted microphones at 16KHz in stereo (one speaker on each channel), as this was closest to the intended audio channel characteristics of the eventual system. In all, we recorded 46 conversations, ranging from a few minutes to 20 minutes length. This provided a total of 4.25 hours of actual speech. The recorded conversations were hand-transcribed at the word level, and translated into Croatian by native Croatian speakers. The English recordings were used for training the English speech recognition models. The transcripts and their translations were added to the EBMT system’s example base of parallel sentences. A subset of the Croatian translations were read by native Croatian speakers to create data for the Croatian speech recognizer, as described elsewhere (Black et al., 2002). This simple approach appears to be surprisingly adequate. Simply stringing together a recognizer, translator, and synthesizer does not make a very useful speech-to-speech translation system. A good interface is necessary to make the parts work together in such a way that a user can actually derive benefit from it. Using our experience from the earlier Diplomat system, we designed the Tongues interface to be asymmetric, with the Croatian side being as simple as possible, and any necessary complexity handled on the English side, since the chaplain would be trained and practiced in using the system. Even the English side was not terribly complex (see Figure 1). We included a back-translation capability, to allow a user with no knowledge of the target language to better assess the quality of the translation. (We could not use the approach of generating paraphrases from meaning representations, since the system does not use any meaning representations.) We also included several user-requested features, such as built-in pre-recorded instructions and explanations for the Croatian (since the Croatian speaker is completely naive regarding the device and the chaplain’s intentions), emergency key phrases (such as “Don’t move!”), and enhancements such as being able to modify the translation lexicon in the field, so that the system could be tuned to more specific tasks. The final system ran on a Windows-based Toshiba Libretto, running at 200MHz with 192MB of memory. At the time of the project (2000) this was the best combination of speed and size that was readily available. The system was equipped with a custom touchscreen, so that the Croatian-speaker would not need to type or use a mouse at all. Aware that the system might be used in situations where the non-English participant would be unfamiliar with computer technology, we included a microphone/speaker handset that looks like a conventional telephone handset. This has the advantage of provided a close-talking microphone, thus making speech recognition easier, while coming in a form factor that will be familiar to most people. We have provided a more detailed description of the development of the Tongues system elsewhere (Black et al., 2002). Our design provides abundant opportunities for user error correction, in an effort to enable cooperative users to communicate well enough to accomplish significant tasks that they could not accomplish without the system (or a bilingual human interpreter), despite the error-prone nature of current speech recognition, broad-coverage rapiddevelopment machine translation, and speech synthesis. Determining whether we have met such a goal requires task-based evaluation; while error rates of components are useful information, the real system-level issue is whether communication is achieved, and at what level of effort. Figure 1: Tongues User Interface.
- Ying Zhang, Ralf D. Brown, R. Frederking, A. Lavie. 2001. Pre-processing of bilingual corpora for Mandarin-English EBMT. Abstract: Pre-processing of bilingual corpora plays an important role in Example-Based Machine Translation (EBMT) and Statistical-Based Machine Translation (SBMT). For our Mandarin-English EBMT system, pre-processing includes segmentation for Mandarin, bracketing for English and building a statistical dictionary from the corpora. We used the Mandarin segmenter from the Linguistic Data Consortium (LDC). It uses dynamic programming with a frequency dictionary to segment the text. Although the frequency dictionary is large, it does not completely cover the corpora. In this paper, we describe the work we have done to improve the segmentation for Mandarin and the bracketing process for English to increase the length of English phrases. A statistical dictionary is built from the aligned bilingual corpus. It is used as feedback to segmentation and bracketing to re-segment / re-bracket the corpus. The process iterates several times to achieve better results. The final results of the corpus pre-processing are a segmented/bracketed aligned bilingual corpus and a statistical dictionary. We achieved positive results by increasing the average length of Chinese terms about 60% and 10% for English. The statistical dictionary gained about a 30% increase in coverage.
- Y. Zhang, Ralf D. Brown, R. Frederking. 2001. Adapting an Example-Based Translation System to Chinese. Abstract: We describe an Example-Based Machine Translation (EBMT) system and the adaptations and enhancements made to create a Chinese-English translation system from the Hong Kong legal code and various other bilingual resources available from the Linguistic Data Consortium (LDC).
- J. Moody, E. Steinbrecher, R. Frederking, A. Black, R. Brown. 2001. Machine translation of conversation on the digitized battlefield. Abstract: The technology of the information age and the digitized battlefield apply not only to machine-to-machine communications but support an unprecedented advancement in communication across human language barriers. Modern military operations, including "peace-keeping, " coordination with joint and multinational forces, and operations other than war, often involve the interaction of troops with indigenous peoples where language issues can have a serious impact on mission success. The Audio Voice Translation Guide System (Tongues) is a mobile, networked, speech-to-speech machine translation system that both benefits from and contributes to the digitized battlefield. The Tongues prototype runs on a commercial off-the-shelf (COTS) mobile computing platform, capable of fitting in a cargo pocket, and running a Microsoft Windows operating system. It supports real time interactive translation of bilingual conversation, within and without the selected target domain, with the highest quality appearing within the target domain. Tongues has a modular architecture, allowing for easy upgrades to the speech and language engines and multiple conversational domains and languages. The prototype system supports English/Croatian conversation for deployed US Army chaplains, a domain that includes religious support, humanitarian aid, and operations other than war. The system supports text-entry as well as speech input and provides feedback for verification of translation accuracy. The Tongues system can automatically capture and store textual and audio conversation transcripts in formats that comply with the Joint Technical Architecture-Army (JTA-A). It supports a wide variety of JTA-A compliant means for transferring that information to other information systems. Tongues can run in a stand-alone mode, or it can make a wireless network connection to remote machine translation servers, allowing for increased performance and flexibility.
- C. Hogan, R. Frederking. 2000. WebDIPLOMAT: A Web-Based Interactive Machine Translation System. Abstract: We have implemented an interactive, Web-based, chat-style machine translation system, supporting speech recognition and synthesis, local- or third-party correction of speech recognition and machine translation output, and online learning. The underlying client-server architecture, implemented in JavaTM, provides remote, distributed computation for the translation and speech subsystems. We further describe our Web-based user interfaces, which can easily produce different useful configurations.
- R. Frederking, C. Hogan, Alexander I. Rudnicky. 1999. A new approach to the translating telephone. Abstract: The Translating Telephone has been a major goal of speech translation for many years. Previous approaches have attempted to work from limited-domain, fully-automatic translation towards broad-coverage, fully-automatic translation. We are approaching the problem from a different direction: starting with a broad-coverage but not fully-automatic system, and working towards full automation. We believe that working in this direction will provide us with better feedback, by observing users and collecting language data under realistic conditions, and thus may allow more rapid progress towards the same ultimate goal. Our initial approach relies on the wide-spread availability of Internet connections and web browsers to provide a user interface. We describe our initial work, which is an extension of the Diplomat wearable speech translator.
- J. Carbonell, Yiming Yang, R. Frederking, Ralf D. Brown, Y. Geng, Danny Lee. 1997. Translingual Information Retrieval: A Comparative Evaluation. Abstract: Translingual information retrieval TIR con sists of providing a query in one language and searching document collections in one or more di erent languages This paper introduces new TIR methods and reports on comparative TIR experiments with these new methods and with previously reported ones in a realistic setting Methods fall into two categories query trans lation based and statistical IR approaches es tablishing translingual associations The re sults show that using bilingual corpora for au tomated extraction of term equivalences in con text outperforms other methods Translin gual versions of the Generalized Vector Space Model GVSM and Latent Semantic Indexing LSI perform relatively well as does translin gual pseudo relevance feedback PRF All showed relatively small performance loss be tween monolingual and translingual versions Query translation based on a general machine readable bilingual dictionary heretofore the most popular method did not match the per formance of other more sophisticated methods Also the previous very high LSI results in the literature were discon rmed by more realistic relevance based evaluations
- R. Frederking, T. Mitamura, Eric Nyberg, J. Carbonell. 1997. Translingual Information Access. Abstract: We present an attempt at a coherent vision of an end-to-end translingual information retrieval system. We begin by presenting a sample of the broad range of possibilities, and the results of some initial work comparing the different approaches. We then present an overall workstation architecture, followed by two possible approaches to the actual translingual IR stage presented in detail. Ranking retrieved documents, query-relevant summarization, assimilation of retrieved information, and system evaluation are all discussed in turn.
- M. Eskénazi, C. Hogan, Jeffrey Allen, R. Frederking. 1997. Issues in database creation: recording new populations, faster and better labelling. Abstract: As speech recognition systems become more accurate, they are used for more diverse applications. These applications often involve populations who never used a recogniser before and for whom the standard data for adult male, adult female, or mixed adult speech is not very representative. This paper will deal with issues concerning the collection and processing of data from those new speaker populations and from speakers of different languages. It deals with data collected for various projects, such as the KIDS database [1] and the Diplomat project [2]. It specifically discusses issues related to obtaining quantitatively and qualitatively sufficient amounts of speech from diverse speaker populations. Since the speech of these individuals is very different from the speech collected in the past, we assume that some hand labelling may be necessary and therefore also address the issue of ameliorating the labelling process. 1. ADAPTATON TO NEW APPLICATIONS As speech recognition systems become more accurate, they are ported to more diverse applications. Changing domains involves changes in many levels of processing. Data obtained in the past has varied from large populations of speakers carefully reading relatively small amounts of text (TIMIT), smaller populations reading larger amounts of text in a defined application domain (DARPA RM), heavily constrained, but not read, speech from a relatively small population (ATIS) to more spontaneous speech in a less restrained domain from a fairly small number of speakers (Broadcast News). When a new application is defined, large amounts of speech data typical of that type of variability are collected for training. The speakers have generally been adult natives. As the data for automatic speech recognizers (ASRs) has changed, each newly-defined hurdle has revealed new datagathering issues. Some of the issues in Broadcast News concerned obtaining the broadcast signal and choosing a subset of all that is broadcast. Once the signal was recorded, other issues surfaced, such as segmenting the signal into usable chunks. With new populations of users, such as children, other issues have come up. The information drawn from our new populations will hopefully aid the reader in preparing to deal with yet other populations in the future, and in anticipating issues that have not yet been encountered. The increase in the amounts of data needed for training requires better processing methodologies. To address part of this issue, we will also discuss a new approach to data labelling. 1.1. Description of the projects and their data The few applications of ASRs that presently have children for users have little or no children’s speech data at their disposal. Instead, like Project LISTEN at Carnegie Mellon University [3], they have had to use adult female speech models. In order to furnish more appropriate data, the KIDS database recorded 76 children. Since Project LISTEN aims at helping children learn to read, the data consists of text read aloud. There were 2 populations of speakers. First, a population of good readers (SUM95) was recorded in order to obtain as much speech data as possible. Then, children from a school where reading scores are especially low were recorded (FP) in order to get data representative of local dialect and reading hesitations. The DIPLOMAT project [4] is designed to test the feasibility of rapid-deployment, wearable speech translation systems. This means developing a machine translation system that performs initial translations at a useful level of quality between a new language and English within a matter of days or weeks, with continual, graceful improvement to a good level of quality over a period of months. A potential use for DIPLOMAT is to allow English-speaking soldiers on peacekeeping missions to interview local residents. So far, Diplomat has worked with Serbo-Croatian, Creole, and Korean. Since rapid deployment is central to the project, read speech is used. It is faster and less labor-intensive to develop than spontaneous speech. At present, there are 13 speakers for Haitian Creole (hereafter, Creole) (10m, 3f) with 99 to 231 sentences each. For Korean there are 8 speakers (5m., 3f) with 118 to 180 sentences each. Recordings are still underway in both languages. 2. NEW SPEAKER POPULATIONS We group our observations of new populations according assumptions researchers made in the past. We examine how they are no longer valid, and note how we dealt with them.
- R. Frederking, Ralf D. Brown. 1996. The Pangloss-Lite machine translation system. Abstract: 1. Pangloss-Lite Overview The Pangloss-Lite (PanLite) machine translation system is a standalone C++ re-implementation of several major components from the Pangloss machine translation system [Nirenburg et al. 95]. It incorporates the Pangloss Example-Based MT (EBMT) [Brown 96a] and Transfer-Based MT engines, and its statistical language modeller [Brown and Frederking 95], as well as a newly-implemented morphological analyzer, within the multi-engine MT architecture [Frederking and Nirenburg 94] developed during the course of the project.
- Ralf D. Brown, R. Frederking. 1995. Applying Statistical English Language Modelling to Symbolic Machine Translation. Abstract: The PANGLOSS Mark III system [Frederking et al. 94] was from the outset designed to be a symbolic, human-aided machine translation (MT) system. The need arose to rapidly adapt it for use as a fully-automated MT system. Our solution to this problem was to add a statistical English language model (ELM) to replace the most significant user activity, selecting between alternate translations produced by the system. The language model used is a trigram model with backoff to bigram and unigram probabilities. The language modeling and search procedure are described in detail, and comparison is made to other trigram-based statistical MT work.
- S. Nirenburg, R. Frederking. 1994. Toward Multi-Engine Machine Translation. Abstract: Current MT systems, whatever translation method they at present employ, do not reach an optimum output on free text. Our hypothesis for the experiment reported in this paper is that if an MT environment can use the best results from a variety of MT systems working simultaneously on the same text, the overall quality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart-like data structure and select the overall best translation using a set of simple heuristics. This paper describes the simple mechanism we use for combining the findings of the various translation engines.
- R. Frederking, S. Nirenburg, D. Farwell, Stephen Helmreich, E. Hovy, Kevin Knight, S. Beale, Constantine Domashnev, Donalee H. Attardo, D. Grannes, Robert G. Brown. 1994. Integrating Translations from Multiple Sources within the PANGLOSS Mark III Machine Translation System. Abstract: Since MT systems, whatever translation method they employ, do not reach an optimum output on free text; each method handles some problems better than others. The PANGLOSS Mark III system is an MT environment that uses the best results from a variety of independent MT systems or engines working simultaneously within a single framework on the same text. This paper describes the method used to combine the outputs of the engines into a single text
- R. Frederking, S. Nirenburg. 1994. Three Heads are Better than One. Abstract: Machine translation (MT) systems do not currently achieve optimal quality translation on free text, whatever translation method they employ. Our hypothesis is that the quality of MT will improve if an MT environment uses output from a variety of MT systems working on the same text. In the latest version of the Pangloss MT project, we collect the results of three translation engines---typically, subsentential chunks---in a chart data structure. Since the individual MT systems operate completely independently, their results may be incomplete, conflicting, or redundant. We use simple scoring heuristics to estimate the quality of each chunk, and find the highest-score sequence of chunks (the "best cover"). This paper describes in detail the combining method, presenting the algorithm and illustrations of its progress on one of many actual translations it has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual MT engines will be reported separately and are not, therefore, described in detail here.
- S. Nirenburg, R. Frederking, D. Farwell, Y. Wilks. 1994. Two Types of Adaptive MT Environments. Abstract: A nurnher of propos'ds have come up hi reten! years for hybrhlizitlhm of MT. Currcnl MT inojects both "pure" and hyhrhl, both prcdo,ninantly leclul,.llogy-orienlcd and scienlilic (including those currently funded hy NSF) ;ire single-engine projecls, CalJahle of one parlicular type of source text analysis, one particular rncthod of lh/ding target lan~;lia~,e corre.spondences for source language elcmenls and one prescribed metho(I cd gcncralhlg tile Iitrgel language text. While such pr(~jccls can be quite useful, we believe thnt i l is lime tc~ make tile next step in lhe desiy, n cd machine Ii.{inshtt ion systems and to move lOw;id i~dii])live, mulliplc-crlgine syslems. We describe Ihc architecture of an adaptive multi-engine MT system which uses each of the engines under tile circumstances which are most favorable for ;Is success.
- J. Carbonell, D. Farwell, R. Frederking, Stephen Helmreich, E. Hovy, Kevin Knight, Lori S. Levin, S. Nirenburg. 1994. PANGLOSS. Abstract: The PANGLOSS project is a three-way equal Machine Translation partnership funded since 1991 by the US Advanced Research Projects Agency (ARPA). The three participating partners are the Center for Machine Translation (CMT) at Carnegie Mellon University in Pittsburgh, the Computing Research Laboratory (CRL) at New Mexico State University in Las Cruces, and the Information Sciences Institute (ISI) of the University of Southern California in Marina del Rey.
- Sergei Nirenburg, R. Frederking, David Farwell, Yorick Wilks. 1994. Two Types of Adaptive MT Environments. Abstract: A number of proposal have come up in recent years for hybridization of MT. Current MT projects --- both "pure" and hybrid, both predominantly technology-oriented and scientific (including those currently funded by NSF) are single-engine projects, capable of one particular type of source text analysis, one particular method of finding target language correspondences for source language elements and one prescribed method of generating the target language text. While such projects can be quite useful, we believe that it is time to make the next step in the design of machine translation systems and to move toward adaptive, multiple-engine systems. We describe the architecture of an adaptive multi-engine MT system which uses each of the engines under the circumstances which are most favorable for its success.
- R. Frederking, A. Cohen, D. Grannes, Peter Cousseau, S. Nirenburg. 1993. The PANGLOSS MARK I MAT system. Abstract: The goal of the PANGLOSS projecd is to develop a system which will, from the very beginning, produce highquality translations of unconstrained text. This can only be attained currently by keeping the human in the translation loop, in our case via a software module called the A U O ~ R . The main measure of progress in the development of the Pangloss system will therefore be the gradual decrease in need for user assistance, as the level of automation increases. The analyzer used in the first version of PANGLOSS, PANGLOSS MARK I, is a version of the ULTRA Spanish analyzer from NMSU [Farwell 1990], while generation is carried out by the PENMAN generator from ISI [Mann 1983]. The Translator's Workstation (TWS) provides the user interface and the integration platform [Nirenburg 1992]. This paper focuses on this use of TWS as a substrate for PANGLOSS. PANOLOSS operates in the following mode: a) a fullyautomated translation of each full sentence is attempted; if it fails, then b) a fully-automated translation of smaller chunks of text is attempted (in the first PANGLOSS configuration, PANGLOSS MARK I, these were noun phrases); c) the material that does not get covered by noun phrases is treated in "word-for-word" mode, whereby translation suggestions for each word (or phrase) are sought in the system's MT lexicons, a machine-readable dictionary, and a set of user glossaries; d) The resulting list of translated noun phrases and translation suggestions for words and phrases is displayed in a special editor window of TWS, where the human user finalizes the translation. At stages a) and b) there is an option of the user being presented by the system with disambiguation questions via the AUGMENTOR. We provide an intelligent environment, the CMAT (Constituent Machine-Aided Translation) editor, for postediting. It allows the user to select, move, and delete words and phrases (constituents) quickly and easily, using dynamically-changing menus. As can be seen in Figure 1, each constituent in the target window is surrounded by " >" characters. If the user clicks with the mouse anywhere within a constituent (between the " >" symbols), a CMAT menu for that constituent appears. It contains the word or phrase in the source text if available, the functions Move and Delete, and alternative translations of the word or phrase from the source text if any. Using these popup menus, the user moves, replaces, or deletes a constituent with a single mouse action, rapidly turning the list of translated words
- R. Frederking, D. Grannes, Peter Cousseau, S. Nirenburg. 1993. An MAT Tool and Its Effectiveness. Abstract: Although automatic machine translation (MT) of unconstrained text is beyond the state of the art today, the need for increased translator productivity is urgent. The PANGLOSS system addresses this dilemma by integrating MT with machine-aided translation (MAT). The main measure of progress in the development of the PANGLOSS system is a gradual increase in the level of automation. The current PANGLOSS MT system typically generates sub-sentence-length units of the target text. Any remaining gaps are treated by lexicon lookup. A mixture of these two kinds of components is presented to the user using the CMAT (Component Machine-Aided Translation) editor, which was designed to facilitate the transformation of this output into a high-quality text. An experiment evaluating the utility of the CMAT editor demonstrated its usefulness in this task, and provides useful guidance for further development.
- R. Frederking, N. Muscettola. 1992. Temporal planning for transportation planning and scheduling. Abstract: The authors report preliminary work toward the creation of an integrated solution to a transportation planning and scheduling domain within the HSTS temporal planning framework. The reference problem is a simplified domain that displays some of the main characteristics of the complete transportation problem. The transportation problem is outlined. The fundamental characteristics of HSTS are described. The focus is on the representation of multiple capacity resources. The areas considered are how HSTS has been extended to represent aggregate resource capacity, how the simplified domain can be modeled. and a constraint-directed planner that solves transportation problems in the simplified domain.<<ETX>>
- R. Frederking. 1991. Interactive experiment planning to control knowledge-based simulation. Abstract: The author presents an initial attempt to facilitate the design and execution of simulation experiments through the use of an interactive, opportunistic, hierarchical planner. The planner provides an orderly way to control the planning and execution of simulation experiments without overly restricting the user to a preestablished routine. In addition to planning the simulation and keeping track of its current state, the planner, through its graphical interface, can provide the simulator with a natural, coherent framework for its user interface. Examples are presented of the planner's graphical interface.<<ETX>>
- R. Frederking. 1986. Natural language dialogue in an integrated computational model. Abstract: Natural language dialogue is a continuous, unified phenomenon. Speakers use their conversational context to simplify individual utterances through a number of linguistic devices, including ellipsis and definite references. Yet most computational systems for using natural language treat individual utterances as separate entities, and have distinctly separate processes for handling ellipsis, definite references, and other dialogue phenomena. 
The computational system presented here, Psli3, uses the uniform framework of a production system architecture to carry out natural language understanding and generation in a well-integrated way. This is demonstrated primarily using intersentential ellipsis resolution, in addition to examples of definite reference resolution and interactive error correction. The system's conversational context arises naturally as the result of the persistence of the internal representations of previous utterances in working memory. Natural language input is interpreted within this framework using a modification of the syntactic technique of chart parsing, extended to include semantics, and adapted to the production system architecture. It provides a graceful way of handling ambiguity within this architecture, and allows separate knowledge sources to interact smoothly across different utterances in a highly integrated fashion. 
The design of this system demonstrates how flexible and natural user interactions can be carried out using a system with a naturally flexible control structure. A processing-based taxonomy for ellipsis resolution that we developed is used to analyze our coverage of intersentential ellipsis. The semantic chart parser is further extended to allow several closely related sentences to be treated in a single chart. This allows the relationship between the sentences to be used in a simple way to select between competing alternative interpretations, and provides a natural means of resolving complex elliptical utterances. 
We describe this system in detail, and include a number of extensive examples of the system's processing during user interactions.
- R. Frederking. 1985. An approach to natural language understanding in rule-based systems. Abstract: ,
- R. Frederking. 1985. Syntax and semantics in natural language parsers. Abstract: ,
- W. Boggs, J. Carbonell, R. Frederking, P. Hayes, G. Mouradian, D. Kosy, M. Mauldin, Hiromichi Fujisawa. 1982. Robust man-machine interfaces and dialog modelling: Carnegie-Mellon University. Abstract: A number of projects at the Carnegie-Mellon University Computer Science Department address issues in Natural Language Processing. Since several of these projects share rescarchers and a similar view of the world, we have listed them in a single summary. Our general interests are in robust man-machine interfaces and the modelling of human dialogs.
- R. Frederking. 1981. A Rule-based Conversation Participant. Abstract: The problem of modeling human understanding and generation of a coherent dialog is investigated by simulating a conversation participant. The rule-based system currently under development attempts to capture the intuitive concept of "topic" using data structures consisting of declarative representations of the subjects under discussion linked to the utterances and rules that generated them. Scripts, goal trees, and a semantic network are brought to bear by general, domain-independent conversational rules to understand and generate coherent topic transitions and specific output utterances.
